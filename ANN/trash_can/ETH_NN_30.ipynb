{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5b918ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from datetime import datetime\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n",
    "from keras import losses\n",
    "from keras.constraints import maxnorm\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_validate\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from collections import Counter\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import randint\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, accuracy_score, f1_score, classification_report\n",
    "# import pickle\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, roc_auc_score\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5407e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from commons import mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535a3fac",
   "metadata": {},
   "source": [
    "### Bitcoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1467391e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26820/2421463472.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  btc['Date'][i]  =  datetime.strptime(btc['Date'][i], '%Y/%m/%d')\n"
     ]
    }
   ],
   "source": [
    "regs = pd.read_csv(\"../Data/train_eth_selected_features.csv\")\n",
    "eth = pd.read_csv(\"../Data/train_eth_selected_features.csv\")\n",
    "for i in range(len(btc['Date'])):\n",
    "    eth['Date'][i]  =  datetime.strptime(eth['Date'][i], '%Y/%m/%d')\n",
    "\n",
    "eth = eth.set_index(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d665a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ethData = btc[regs.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8227b01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26820/751970969.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  btcData['returns'] = btcData['priceUSD'].pct_change()\n"
     ]
    }
   ],
   "source": [
    "ethData['returns'] = ethData['priceUSD'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0204bfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = ethData.drop(columns=['priceUSD'])\n",
    "Data = Data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a926d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>difficulty14mom</th>\n",
       "      <th>difficulty30mom</th>\n",
       "      <th>difficulty3rsi</th>\n",
       "      <th>difficulty90mom</th>\n",
       "      <th>difficulty90sma</th>\n",
       "      <th>hashrate90std</th>\n",
       "      <th>hashrate90var</th>\n",
       "      <th>median_transaction_fee90rocUSD</th>\n",
       "      <th>median_transaction_feeUSD</th>\n",
       "      <th>mining_profitability</th>\n",
       "      <th>...</th>\n",
       "      <th>price90wmaUSD</th>\n",
       "      <th>sentinusd90emaUSD</th>\n",
       "      <th>sentinusdUSD</th>\n",
       "      <th>top100cap</th>\n",
       "      <th>top100cap14trx</th>\n",
       "      <th>top100cap30rsi</th>\n",
       "      <th>top100cap30trx</th>\n",
       "      <th>top100cap90rsi</th>\n",
       "      <th>top100cap90trx</th>\n",
       "      <th>returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-07-18</th>\n",
       "      <td>1.621380e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.737230e+02</td>\n",
       "      <td>2.055500e+01</td>\n",
       "      <td>8.384389e+08</td>\n",
       "      <td>1.757449e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>401834.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.612000e+03</td>\n",
       "      <td>25.782</td>\n",
       "      <td>0.139</td>\n",
       "      <td>71.191</td>\n",
       "      <td>0.308</td>\n",
       "      <td>79.756</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-19</th>\n",
       "      <td>1.621380e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.737230e+02</td>\n",
       "      <td>2.248600e+01</td>\n",
       "      <td>8.819952e+08</td>\n",
       "      <td>1.944789e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>481473.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.047000e+03</td>\n",
       "      <td>25.685</td>\n",
       "      <td>0.123</td>\n",
       "      <td>68.863</td>\n",
       "      <td>0.300</td>\n",
       "      <td>78.999</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.183196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-20</th>\n",
       "      <td>1.584060e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.735570e+02</td>\n",
       "      <td>2.441400e+01</td>\n",
       "      <td>9.280542e+08</td>\n",
       "      <td>2.153212e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>431831.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.341000e+03</td>\n",
       "      <td>25.602</td>\n",
       "      <td>0.107</td>\n",
       "      <td>66.923</td>\n",
       "      <td>0.292</td>\n",
       "      <td>78.355</td>\n",
       "      <td>0.544</td>\n",
       "      <td>-0.088475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-21</th>\n",
       "      <td>1.580420e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.700790e+02</td>\n",
       "      <td>2.630400e+01</td>\n",
       "      <td>9.761949e+08</td>\n",
       "      <td>2.382391e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>460783.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.122000e+03</td>\n",
       "      <td>25.638</td>\n",
       "      <td>0.092</td>\n",
       "      <td>67.334</td>\n",
       "      <td>0.284</td>\n",
       "      <td>78.431</td>\n",
       "      <td>0.541</td>\n",
       "      <td>-0.020434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-22</th>\n",
       "      <td>1.580420e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.700790e+02</td>\n",
       "      <td>2.819400e+01</td>\n",
       "      <td>1.019474e+09</td>\n",
       "      <td>2.598318e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>334641.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.570000e+03</td>\n",
       "      <td>25.604</td>\n",
       "      <td>0.078</td>\n",
       "      <td>66.526</td>\n",
       "      <td>0.275</td>\n",
       "      <td>78.166</td>\n",
       "      <td>0.537</td>\n",
       "      <td>-0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-24</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.384949e+13</td>\n",
       "      <td>4.612156e+19</td>\n",
       "      <td>5.317997e+38</td>\n",
       "      <td>165.380</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.0577</td>\n",
       "      <td>...</td>\n",
       "      <td>18820.0</td>\n",
       "      <td>28544682257</td>\n",
       "      <td>6.114577e+09</td>\n",
       "      <td>15.236</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>40.118</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>45.140</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.008075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-25</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.394503e+13</td>\n",
       "      <td>4.542600e+19</td>\n",
       "      <td>5.158804e+38</td>\n",
       "      <td>214.464</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.0541</td>\n",
       "      <td>...</td>\n",
       "      <td>18759.0</td>\n",
       "      <td>28076113671</td>\n",
       "      <td>7.224812e+09</td>\n",
       "      <td>15.231</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>39.880</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>45.047</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.006565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-26</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.404058e+13</td>\n",
       "      <td>4.624135e+19</td>\n",
       "      <td>5.345655e+38</td>\n",
       "      <td>255.004</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.0571</td>\n",
       "      <td>...</td>\n",
       "      <td>18701.0</td>\n",
       "      <td>27566714445</td>\n",
       "      <td>4.898449e+09</td>\n",
       "      <td>15.233</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>40.069</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>45.106</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.004001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-27</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.413612e+13</td>\n",
       "      <td>4.576227e+19</td>\n",
       "      <td>5.235464e+38</td>\n",
       "      <td>113.430</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.0618</td>\n",
       "      <td>...</td>\n",
       "      <td>18642.0</td>\n",
       "      <td>27053515674</td>\n",
       "      <td>4.216170e+09</td>\n",
       "      <td>15.259</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>41.833</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>45.656</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.423166e+13</td>\n",
       "      <td>4.552126e+19</td>\n",
       "      <td>5.180463e+38</td>\n",
       "      <td>104.294</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.0592</td>\n",
       "      <td>...</td>\n",
       "      <td>18578.0</td>\n",
       "      <td>26631320968</td>\n",
       "      <td>7.843657e+09</td>\n",
       "      <td>15.219</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>39.965</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>44.957</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.019828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4517 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            difficulty14mom  difficulty30mom  difficulty3rsi  difficulty90mom  \\\n",
       "Date                                                                            \n",
       "2010-07-18     1.621380e+02     1.641620e+02          100.00     1.737230e+02   \n",
       "2010-07-19     1.621380e+02     1.641620e+02          100.00     1.737230e+02   \n",
       "2010-07-20     1.584060e+02     1.641620e+02          100.00     1.735570e+02   \n",
       "2010-07-21     1.580420e+02     1.641620e+02          100.00     1.700790e+02   \n",
       "2010-07-22     1.580420e+02     1.641620e+02          100.00     1.700790e+02   \n",
       "...                     ...              ...             ...              ...   \n",
       "2022-11-24     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-25     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-26     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-27     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-28     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "\n",
       "            difficulty90sma  hashrate90std  hashrate90var  \\\n",
       "Date                                                        \n",
       "2010-07-18     2.055500e+01   8.384389e+08   1.757449e+17   \n",
       "2010-07-19     2.248600e+01   8.819952e+08   1.944789e+17   \n",
       "2010-07-20     2.441400e+01   9.280542e+08   2.153212e+17   \n",
       "2010-07-21     2.630400e+01   9.761949e+08   2.382391e+17   \n",
       "2010-07-22     2.819400e+01   1.019474e+09   2.598318e+17   \n",
       "...                     ...            ...            ...   \n",
       "2022-11-24     3.384949e+13   4.612156e+19   5.317997e+38   \n",
       "2022-11-25     3.394503e+13   4.542600e+19   5.158804e+38   \n",
       "2022-11-26     3.404058e+13   4.624135e+19   5.345655e+38   \n",
       "2022-11-27     3.413612e+13   4.576227e+19   5.235464e+38   \n",
       "2022-11-28     3.423166e+13   4.552126e+19   5.180463e+38   \n",
       "\n",
       "            median_transaction_fee90rocUSD  median_transaction_feeUSD  \\\n",
       "Date                                                                    \n",
       "2010-07-18                           0.000                      0.000   \n",
       "2010-07-19                           0.000                      0.000   \n",
       "2010-07-20                           0.000                      0.000   \n",
       "2010-07-21                           0.000                      0.000   \n",
       "2010-07-22                           0.000                      0.000   \n",
       "...                                    ...                        ...   \n",
       "2022-11-24                         165.380                      0.818   \n",
       "2022-11-25                         214.464                      0.812   \n",
       "2022-11-26                         255.004                      0.809   \n",
       "2022-11-27                         113.430                      0.619   \n",
       "2022-11-28                         104.294                      0.622   \n",
       "\n",
       "            mining_profitability  ...  price90wmaUSD  sentinusd90emaUSD  \\\n",
       "Date                              ...                                     \n",
       "2010-07-18           401834.0000  ...            0.0                  0   \n",
       "2010-07-19           481473.0000  ...            0.0                  0   \n",
       "2010-07-20           431831.0000  ...            0.0                  0   \n",
       "2010-07-21           460783.0000  ...            0.0                  0   \n",
       "2010-07-22           334641.0000  ...            0.0                  0   \n",
       "...                          ...  ...            ...                ...   \n",
       "2022-11-24                0.0577  ...        18820.0        28544682257   \n",
       "2022-11-25                0.0541  ...        18759.0        28076113671   \n",
       "2022-11-26                0.0571  ...        18701.0        27566714445   \n",
       "2022-11-27                0.0618  ...        18642.0        27053515674   \n",
       "2022-11-28                0.0592  ...        18578.0        26631320968   \n",
       "\n",
       "            sentinusdUSD  top100cap  top100cap14trx  top100cap30rsi  \\\n",
       "Date                                                                  \n",
       "2010-07-18  2.612000e+03     25.782           0.139          71.191   \n",
       "2010-07-19  4.047000e+03     25.685           0.123          68.863   \n",
       "2010-07-20  2.341000e+03     25.602           0.107          66.923   \n",
       "2010-07-21  2.122000e+03     25.638           0.092          67.334   \n",
       "2010-07-22  2.570000e+03     25.604           0.078          66.526   \n",
       "...                  ...        ...             ...             ...   \n",
       "2022-11-24  6.114577e+09     15.236          -0.080          40.118   \n",
       "2022-11-25  7.224812e+09     15.231          -0.075          39.880   \n",
       "2022-11-26  4.898449e+09     15.233          -0.070          40.069   \n",
       "2022-11-27  4.216170e+09     15.259          -0.065          41.833   \n",
       "2022-11-28  7.843657e+09     15.219          -0.061          39.965   \n",
       "\n",
       "            top100cap30trx  top100cap90rsi  top100cap90trx   returns  \n",
       "Date                                                                  \n",
       "2010-07-18           0.308          79.756           0.550  0.466667  \n",
       "2010-07-19           0.300          78.999           0.547  0.183196  \n",
       "2010-07-20           0.292          78.355           0.544 -0.088475  \n",
       "2010-07-21           0.284          78.431           0.541 -0.020434  \n",
       "2010-07-22           0.275          78.166           0.537 -0.153846  \n",
       "...                    ...             ...             ...       ...  \n",
       "2022-11-24          -0.042          45.140           0.001  0.008075  \n",
       "2022-11-25          -0.043          45.047           0.001 -0.006565  \n",
       "2022-11-26          -0.043          45.106           0.001  0.004001  \n",
       "2022-11-27          -0.044          45.656           0.000 -0.001147  \n",
       "2022-11-28          -0.044          44.957           0.000 -0.019828  \n",
       "\n",
       "[4517 rows x 32 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5d4f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide X and Y\n",
    "X = Data.iloc[:,0:]\n",
    "#Y = Data['returns']   # 用returns的话就用这一行，然后把下一行comment掉\n",
    "Y = ethData['priceUSD'].shift(-1)[1:] # 反之亦然"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb3275b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>difficulty14mom</th>\n",
       "      <th>difficulty30mom</th>\n",
       "      <th>difficulty3rsi</th>\n",
       "      <th>difficulty90mom</th>\n",
       "      <th>difficulty90sma</th>\n",
       "      <th>hashrate90std</th>\n",
       "      <th>hashrate90var</th>\n",
       "      <th>median_transaction_fee90rocUSD</th>\n",
       "      <th>median_transaction_feeUSD</th>\n",
       "      <th>mining_profitability</th>\n",
       "      <th>...</th>\n",
       "      <th>price90wmaUSD</th>\n",
       "      <th>sentinusd90emaUSD</th>\n",
       "      <th>sentinusdUSD</th>\n",
       "      <th>top100cap</th>\n",
       "      <th>top100cap14trx</th>\n",
       "      <th>top100cap30rsi</th>\n",
       "      <th>top100cap30trx</th>\n",
       "      <th>top100cap90rsi</th>\n",
       "      <th>top100cap90trx</th>\n",
       "      <th>returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-07-18</th>\n",
       "      <td>162.138</td>\n",
       "      <td>164.162</td>\n",
       "      <td>100.0</td>\n",
       "      <td>173.723</td>\n",
       "      <td>20.555</td>\n",
       "      <td>838438881.0</td>\n",
       "      <td>1.757449e+17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>401834.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2612.0</td>\n",
       "      <td>25.782</td>\n",
       "      <td>0.139</td>\n",
       "      <td>71.191</td>\n",
       "      <td>0.308</td>\n",
       "      <td>79.756</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-19</th>\n",
       "      <td>162.138</td>\n",
       "      <td>164.162</td>\n",
       "      <td>100.0</td>\n",
       "      <td>173.723</td>\n",
       "      <td>22.486</td>\n",
       "      <td>881995244.0</td>\n",
       "      <td>1.944789e+17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>481473.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4047.0</td>\n",
       "      <td>25.685</td>\n",
       "      <td>0.123</td>\n",
       "      <td>68.863</td>\n",
       "      <td>0.300</td>\n",
       "      <td>78.999</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.183196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-20</th>\n",
       "      <td>158.406</td>\n",
       "      <td>164.162</td>\n",
       "      <td>100.0</td>\n",
       "      <td>173.557</td>\n",
       "      <td>24.414</td>\n",
       "      <td>928054231.0</td>\n",
       "      <td>2.153212e+17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>431831.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2341.0</td>\n",
       "      <td>25.602</td>\n",
       "      <td>0.107</td>\n",
       "      <td>66.923</td>\n",
       "      <td>0.292</td>\n",
       "      <td>78.355</td>\n",
       "      <td>0.544</td>\n",
       "      <td>-0.088475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            difficulty14mom  difficulty30mom  difficulty3rsi  difficulty90mom  \\\n",
       "Date                                                                            \n",
       "2010-07-18          162.138          164.162           100.0          173.723   \n",
       "2010-07-19          162.138          164.162           100.0          173.723   \n",
       "2010-07-20          158.406          164.162           100.0          173.557   \n",
       "\n",
       "            difficulty90sma  hashrate90std  hashrate90var  \\\n",
       "Date                                                        \n",
       "2010-07-18           20.555    838438881.0   1.757449e+17   \n",
       "2010-07-19           22.486    881995244.0   1.944789e+17   \n",
       "2010-07-20           24.414    928054231.0   2.153212e+17   \n",
       "\n",
       "            median_transaction_fee90rocUSD  median_transaction_feeUSD  \\\n",
       "Date                                                                    \n",
       "2010-07-18                             0.0                        0.0   \n",
       "2010-07-19                             0.0                        0.0   \n",
       "2010-07-20                             0.0                        0.0   \n",
       "\n",
       "            mining_profitability  ...  price90wmaUSD  sentinusd90emaUSD  \\\n",
       "Date                              ...                                     \n",
       "2010-07-18              401834.0  ...            0.0                  0   \n",
       "2010-07-19              481473.0  ...            0.0                  0   \n",
       "2010-07-20              431831.0  ...            0.0                  0   \n",
       "\n",
       "            sentinusdUSD  top100cap  top100cap14trx  top100cap30rsi  \\\n",
       "Date                                                                  \n",
       "2010-07-18        2612.0     25.782           0.139          71.191   \n",
       "2010-07-19        4047.0     25.685           0.123          68.863   \n",
       "2010-07-20        2341.0     25.602           0.107          66.923   \n",
       "\n",
       "            top100cap30trx  top100cap90rsi  top100cap90trx   returns  \n",
       "Date                                                                  \n",
       "2010-07-18           0.308          79.756           0.550  0.466667  \n",
       "2010-07-19           0.300          78.999           0.547  0.183196  \n",
       "2010-07-20           0.292          78.355           0.544 -0.088475  \n",
       "\n",
       "[3 rows x 32 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd589ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into three data sets\n",
    "X_train = X['2016-01-01':'2019-12-31']\n",
    "X_val = X['2020-01-01':'2021-05-31']\n",
    "X_test = X['2021-06-01':'2023-01-01']\n",
    "\n",
    "Y_train = Y['2016-01-01':'2019-12-31']\n",
    "Y_val = Y['2020-01-01':'2021-05-31']\n",
    "Y_test = Y['2021-06-01':'2023-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e5e9fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............ (step 1 of 2) Processing mixmax, total=   0.0s\n",
      "[Pipeline] ............ (step 2 of 2) Processing robust, total=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;mixmax&#x27;, MinMaxScaler()), [&#x27;robust&#x27;, RobustScaler()]],\n",
       "         verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;mixmax&#x27;, MinMaxScaler()), [&#x27;robust&#x27;, RobustScaler()]],\n",
       "         verbose=True)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RobustScaler</label><div class=\"sk-toggleable__content\"><pre>RobustScaler()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('mixmax', MinMaxScaler()), ['robust', RobustScaler()]],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators=[]\n",
    "estimators.append(['mixmax',MinMaxScaler()])\n",
    "estimators.append(['robust',RobustScaler()])\n",
    "scale=Pipeline(estimators,verbose=True)\n",
    "scale.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c6fc0859",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=scale.transform(X_train)\n",
    "X_test=scale.transform(X_test)\n",
    "X_val = scale.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8a84d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f6916ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "# define neural network model\n",
    "shape=X.shape[1]\n",
    "def sequential_model(initializer='normal', activation='relu', neurons=300, NUM_FEATURES=shape):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_shape=(NUM_FEATURES,), kernel_initializer=initializer, activation=activation))\n",
    "    model.add(Dense(neurons, activation=activation))\n",
    "    model.add(Dense(neurons, activation=activation))\n",
    "    model.add(Dense(1, activation=activation))\n",
    "    # Compile model\n",
    "    adam=keras.optimizers.Adam(lr=lr_schedule(0), amsgrad=True)\n",
    "    #sgd=keras.optimizers.SGD(learning_rate=0.08, momentum=0.9, nesterov=False)\n",
    "    model.compile(loss='logcosh', optimizer=adam, metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72db04c9",
   "metadata": {},
   "source": [
    "val_loss is the value of cost function for your cross-validation data and loss is the value of cost function for your training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85653fe",
   "metadata": {},
   "source": [
    "stop training when val_loss is not increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b48b24b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=100,verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85174a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26820/1361768653.py:1: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  regressor=KerasRegressor(build_fn=sequential_model,epochs=5000,verbose=1, use_multiprocessing=True)\n"
     ]
    }
   ],
   "source": [
    "regressor=KerasRegressor(build_fn=sequential_model,epochs=5000,verbose=1, use_multiprocessing=True, callback=[earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a17646de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Epoch 1/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 4474.6631 - mae: 4475.3574 - val_loss: 18080.3887 - val_mae: 18081.0820\n",
      "Epoch 2/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 2496.9170 - mae: 2497.6099 - val_loss: 4281.4653 - val_mae: 4282.1592\n",
      "Epoch 3/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1014.1127 - mae: 1014.8058 - val_loss: 3231.7222 - val_mae: 3232.4158\n",
      "Epoch 4/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 794.0106 - mae: 794.7033 - val_loss: 2615.4077 - val_mae: 2616.1008\n",
      "Epoch 5/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 636.9713 - mae: 637.6638 - val_loss: 2470.5056 - val_mae: 2471.1985\n",
      "Epoch 6/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 581.8420 - mae: 582.5347 - val_loss: 1886.7018 - val_mae: 1887.3937\n",
      "Epoch 7/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 485.6491 - mae: 486.3399 - val_loss: 1884.6323 - val_mae: 1885.3257\n",
      "Epoch 8/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 429.8745 - mae: 430.5666 - val_loss: 1921.1067 - val_mae: 1921.7992\n",
      "Epoch 9/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 392.0625 - mae: 392.7542 - val_loss: 1612.1432 - val_mae: 1612.8357\n",
      "Epoch 10/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 414.3275 - mae: 415.0199 - val_loss: 2249.3057 - val_mae: 2249.9988\n",
      "Epoch 11/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 355.0909 - mae: 355.7811 - val_loss: 1640.8800 - val_mae: 1641.5729\n",
      "Epoch 12/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 306.8041 - mae: 307.4959 - val_loss: 1517.6263 - val_mae: 1518.3192\n",
      "Epoch 13/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 307.3196 - mae: 308.0106 - val_loss: 1757.7250 - val_mae: 1758.4183\n",
      "Epoch 14/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 291.6733 - mae: 292.3630 - val_loss: 1700.6044 - val_mae: 1701.2961\n",
      "Epoch 15/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 266.5408 - mae: 267.2311 - val_loss: 1699.9752 - val_mae: 1700.6680\n",
      "Epoch 16/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 254.5177 - mae: 255.2060 - val_loss: 1691.2667 - val_mae: 1691.9597\n",
      "Epoch 17/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 248.2263 - mae: 248.9177 - val_loss: 1554.8767 - val_mae: 1555.5698\n",
      "Epoch 18/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 229.9757 - mae: 230.6668 - val_loss: 1609.7352 - val_mae: 1610.4285\n",
      "Epoch 19/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 239.0539 - mae: 239.7454 - val_loss: 1533.6030 - val_mae: 1534.2961\n",
      "Epoch 20/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 237.7101 - mae: 238.4004 - val_loss: 1643.5778 - val_mae: 1644.2710\n",
      "Epoch 21/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 254.1142 - mae: 254.8046 - val_loss: 1800.3529 - val_mae: 1801.0459\n",
      "Epoch 22/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 232.3498 - mae: 233.0398 - val_loss: 1373.5959 - val_mae: 1374.2878\n",
      "Epoch 23/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 217.0180 - mae: 217.7082 - val_loss: 1600.6044 - val_mae: 1601.2965\n",
      "Epoch 24/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 212.7198 - mae: 213.4111 - val_loss: 1751.7545 - val_mae: 1752.4475\n",
      "Epoch 25/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 210.9419 - mae: 211.6317 - val_loss: 1763.0851 - val_mae: 1763.7780\n",
      "Epoch 26/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 195.1153 - mae: 195.8058 - val_loss: 1792.0172 - val_mae: 1792.7096\n",
      "Epoch 27/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 204.2661 - mae: 204.9545 - val_loss: 1444.7877 - val_mae: 1445.4811\n",
      "Epoch 28/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 206.3544 - mae: 207.0423 - val_loss: 1880.3087 - val_mae: 1881.0018\n",
      "Epoch 29/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 198.0150 - mae: 198.7043 - val_loss: 1581.0072 - val_mae: 1581.7007\n",
      "Epoch 30/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 202.3769 - mae: 203.0675 - val_loss: 1487.9399 - val_mae: 1488.6322\n",
      "Epoch 31/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 194.0714 - mae: 194.7619 - val_loss: 1396.8795 - val_mae: 1397.5717\n",
      "Epoch 32/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 190.4966 - mae: 191.1869 - val_loss: 1893.2911 - val_mae: 1893.9843\n",
      "Epoch 33/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 195.0160 - mae: 195.7055 - val_loss: 1481.1660 - val_mae: 1481.8584\n",
      "Epoch 34/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 192.2305 - mae: 192.9205 - val_loss: 1525.7953 - val_mae: 1526.4871\n",
      "Epoch 35/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 196.1225 - mae: 196.8125 - val_loss: 1763.7659 - val_mae: 1764.4573\n",
      "Epoch 36/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 186.4428 - mae: 187.1338 - val_loss: 1569.7448 - val_mae: 1570.4382\n",
      "Epoch 37/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 191.4064 - mae: 192.0965 - val_loss: 2044.5803 - val_mae: 2045.2734\n",
      "Epoch 38/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 180.1868 - mae: 180.8755 - val_loss: 1381.9236 - val_mae: 1382.6169\n",
      "Epoch 39/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 200.4445 - mae: 201.1349 - val_loss: 1528.6984 - val_mae: 1529.3915\n",
      "Epoch 40/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 172.7581 - mae: 173.4467 - val_loss: 1486.9501 - val_mae: 1487.6432\n",
      "Epoch 41/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 180.4548 - mae: 181.1436 - val_loss: 1625.0597 - val_mae: 1625.7528\n",
      "Epoch 42/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 177.3417 - mae: 178.0292 - val_loss: 1710.0251 - val_mae: 1710.7178\n",
      "Epoch 43/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 176.1465 - mae: 176.8362 - val_loss: 1481.4630 - val_mae: 1482.1562\n",
      "Epoch 44/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 194.2397 - mae: 194.9275 - val_loss: 1507.1473 - val_mae: 1507.8405\n",
      "Epoch 45/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 181.1511 - mae: 181.8380 - val_loss: 1598.4796 - val_mae: 1599.1729\n",
      "Epoch 46/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 182.7478 - mae: 183.4387 - val_loss: 1878.5468 - val_mae: 1879.2394\n",
      "Epoch 47/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 181.2398 - mae: 181.9283 - val_loss: 1551.3668 - val_mae: 1552.0599\n",
      "Epoch 48/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 179.1099 - mae: 179.8006 - val_loss: 1823.4001 - val_mae: 1824.0934\n",
      "Epoch 49/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 170.6625 - mae: 171.3524 - val_loss: 1366.3185 - val_mae: 1367.0112\n",
      "Epoch 50/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 176.0386 - mae: 176.7282 - val_loss: 1453.6522 - val_mae: 1454.3452\n",
      "Epoch 51/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 174.7568 - mae: 175.4463 - val_loss: 1271.1359 - val_mae: 1271.8278\n",
      "Epoch 52/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 171.6992 - mae: 172.3899 - val_loss: 1406.0277 - val_mae: 1406.7198\n",
      "Epoch 53/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 188.6807 - mae: 189.3702 - val_loss: 1626.1740 - val_mae: 1626.8673\n",
      "Epoch 54/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 195.6156 - mae: 196.3039 - val_loss: 1272.3728 - val_mae: 1273.0637\n",
      "Epoch 55/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 182.6709 - mae: 183.3601 - val_loss: 1554.0377 - val_mae: 1554.7296\n",
      "Epoch 56/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 160.8171 - mae: 161.5038 - val_loss: 1328.4769 - val_mae: 1329.1699\n",
      "Epoch 57/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 165.2563 - mae: 165.9465 - val_loss: 1455.8888 - val_mae: 1456.5812\n",
      "Epoch 58/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 161.7845 - mae: 162.4739 - val_loss: 1324.7563 - val_mae: 1325.4496\n",
      "Epoch 59/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 178.0260 - mae: 178.7158 - val_loss: 1695.9438 - val_mae: 1696.6372\n",
      "Epoch 60/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 186.6242 - mae: 187.3146 - val_loss: 1655.0178 - val_mae: 1655.7111\n",
      "Epoch 61/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 177.4121 - mae: 178.0996 - val_loss: 1485.8973 - val_mae: 1486.5887\n",
      "Epoch 62/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 163.1655 - mae: 163.8546 - val_loss: 1241.0983 - val_mae: 1241.7878\n",
      "Epoch 63/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 174.1361 - mae: 174.8252 - val_loss: 1331.5356 - val_mae: 1332.2289\n",
      "Epoch 64/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 163.1344 - mae: 163.8226 - val_loss: 1495.1715 - val_mae: 1495.8646\n",
      "Epoch 65/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 160.6866 - mae: 161.3751 - val_loss: 1329.9232 - val_mae: 1330.6163\n",
      "Epoch 66/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 165.3713 - mae: 166.0602 - val_loss: 1628.2535 - val_mae: 1628.9465\n",
      "Epoch 67/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 181.0636 - mae: 181.7529 - val_loss: 1254.2489 - val_mae: 1254.9404\n",
      "Epoch 68/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 161.0436 - mae: 161.7322 - val_loss: 1650.1329 - val_mae: 1650.8260\n",
      "Epoch 69/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 161.6086 - mae: 162.2981 - val_loss: 1156.4146 - val_mae: 1157.1075\n",
      "Epoch 70/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 177.3998 - mae: 178.0888 - val_loss: 1417.1810 - val_mae: 1417.8723\n",
      "Epoch 71/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 187.9973 - mae: 188.6868 - val_loss: 1364.4595 - val_mae: 1365.1530\n",
      "Epoch 72/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 156.1731 - mae: 156.8629 - val_loss: 1669.5265 - val_mae: 1670.2195\n",
      "Epoch 73/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 167.8588 - mae: 168.5483 - val_loss: 1604.2632 - val_mae: 1604.9564\n",
      "Epoch 74/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 159.5132 - mae: 160.2014 - val_loss: 1286.4291 - val_mae: 1287.1222\n",
      "Epoch 75/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 156.4402 - mae: 157.1291 - val_loss: 1522.8477 - val_mae: 1523.5408\n",
      "Epoch 76/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 174.5553 - mae: 175.2440 - val_loss: 1448.2466 - val_mae: 1448.9385\n",
      "Epoch 77/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 156.7752 - mae: 157.4634 - val_loss: 1535.2247 - val_mae: 1535.9172\n",
      "Epoch 78/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 153.8974 - mae: 154.5859 - val_loss: 1687.1420 - val_mae: 1687.8340\n",
      "Epoch 79/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 151.6487 - mae: 152.3376 - val_loss: 1464.1226 - val_mae: 1464.8158\n",
      "Epoch 80/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 158.8495 - mae: 159.5386 - val_loss: 1492.4851 - val_mae: 1493.1780\n",
      "Epoch 81/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 146.6740 - mae: 147.3612 - val_loss: 1396.8809 - val_mae: 1397.5740\n",
      "Epoch 82/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 160.5488 - mae: 161.2379 - val_loss: 1610.3541 - val_mae: 1611.0471\n",
      "Epoch 83/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 156.2022 - mae: 156.8902 - val_loss: 1487.1643 - val_mae: 1487.8575\n",
      "Epoch 84/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 145.7875 - mae: 146.4756 - val_loss: 1391.2855 - val_mae: 1391.9774\n",
      "Epoch 85/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 166.3429 - mae: 167.0275 - val_loss: 1592.9421 - val_mae: 1593.6353\n",
      "Epoch 86/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 157.4948 - mae: 158.1826 - val_loss: 1452.2791 - val_mae: 1452.9722\n",
      "Epoch 87/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 177.1546 - mae: 177.8429 - val_loss: 1282.0642 - val_mae: 1282.7570\n",
      "Epoch 88/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 193.6548 - mae: 194.3442 - val_loss: 1215.6648 - val_mae: 1216.3569\n",
      "Epoch 89/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 159.4261 - mae: 160.1160 - val_loss: 1465.8143 - val_mae: 1466.5072\n",
      "Epoch 90/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 161.6315 - mae: 162.3203 - val_loss: 1394.8757 - val_mae: 1395.5687\n",
      "Epoch 91/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 151.1414 - mae: 151.8271 - val_loss: 1692.2362 - val_mae: 1692.9290\n",
      "Epoch 92/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 147.2861 - mae: 147.9740 - val_loss: 1211.9192 - val_mae: 1212.6124\n",
      "Epoch 93/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 153.1352 - mae: 153.8240 - val_loss: 1573.2131 - val_mae: 1573.9062\n",
      "Epoch 94/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 184.9826 - mae: 185.6702 - val_loss: 1291.4310 - val_mae: 1292.1237\n",
      "Epoch 95/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 157.4201 - mae: 158.1083 - val_loss: 1603.6730 - val_mae: 1604.3661\n",
      "Epoch 96/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 154.6054 - mae: 155.2908 - val_loss: 1473.4091 - val_mae: 1474.1022\n",
      "Epoch 97/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 156.3701 - mae: 157.0594 - val_loss: 1300.6464 - val_mae: 1301.3390\n",
      "Epoch 98/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 147.0755 - mae: 147.7644 - val_loss: 1452.1458 - val_mae: 1452.8389\n",
      "Epoch 99/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 157.9336 - mae: 158.6198 - val_loss: 1332.0657 - val_mae: 1332.7584\n",
      "Epoch 100/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 150.7802 - mae: 151.4669 - val_loss: 1288.0620 - val_mae: 1288.7545\n",
      "Epoch 101/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 150.4101 - mae: 151.0979 - val_loss: 1309.7108 - val_mae: 1310.4025\n",
      "Epoch 102/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 166.3709 - mae: 167.0588 - val_loss: 1436.5240 - val_mae: 1437.2166\n",
      "Epoch 103/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 148.0657 - mae: 148.7537 - val_loss: 1357.2217 - val_mae: 1357.9148\n",
      "Epoch 104/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 154.7885 - mae: 155.4764 - val_loss: 1256.7209 - val_mae: 1257.4141\n",
      "Epoch 105/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 149.0522 - mae: 149.7387 - val_loss: 1207.9834 - val_mae: 1208.6766\n",
      "Epoch 106/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 146.5856 - mae: 147.2749 - val_loss: 1254.9519 - val_mae: 1255.6440\n",
      "Epoch 107/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 153.9825 - mae: 154.6689 - val_loss: 1605.1052 - val_mae: 1605.7983\n",
      "Epoch 108/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 155.6576 - mae: 156.3447 - val_loss: 1887.9656 - val_mae: 1888.6588\n",
      "Epoch 109/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 155.6083 - mae: 156.2958 - val_loss: 1208.5731 - val_mae: 1209.2660\n",
      "Epoch 110/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 160.5493 - mae: 161.2389 - val_loss: 1817.2998 - val_mae: 1817.9928\n",
      "Epoch 111/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 167.4991 - mae: 168.1873 - val_loss: 1562.1188 - val_mae: 1562.8123\n",
      "Epoch 112/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 158.7572 - mae: 159.4464 - val_loss: 1442.4994 - val_mae: 1443.1918\n",
      "Epoch 113/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 161.0294 - mae: 161.7159 - val_loss: 1132.5369 - val_mae: 1133.2299\n",
      "Epoch 114/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 167.9258 - mae: 168.6141 - val_loss: 1543.3252 - val_mae: 1544.0177\n",
      "Epoch 115/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 162.9032 - mae: 163.5913 - val_loss: 1420.1382 - val_mae: 1420.8314\n",
      "Epoch 116/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 146.3709 - mae: 147.0596 - val_loss: 1368.5800 - val_mae: 1369.2723\n",
      "Epoch 117/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 161.7549 - mae: 162.4443 - val_loss: 1108.5199 - val_mae: 1109.2119\n",
      "Epoch 118/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 169.6165 - mae: 170.3062 - val_loss: 1491.4397 - val_mae: 1492.1327\n",
      "Epoch 119/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 140.6664 - mae: 141.3492 - val_loss: 1328.9655 - val_mae: 1329.6575\n",
      "Epoch 120/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 137.7252 - mae: 138.4128 - val_loss: 1217.5118 - val_mae: 1218.2045\n",
      "Epoch 121/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 147.7719 - mae: 148.4591 - val_loss: 1334.4763 - val_mae: 1335.1689\n",
      "Epoch 122/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 147.4709 - mae: 148.1577 - val_loss: 1253.8889 - val_mae: 1254.5815\n",
      "Epoch 123/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 146.3886 - mae: 147.0764 - val_loss: 1410.5688 - val_mae: 1411.2618\n",
      "Epoch 124/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 144.6600 - mae: 145.3475 - val_loss: 1237.6742 - val_mae: 1238.3665\n",
      "Epoch 125/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 142.9858 - mae: 143.6742 - val_loss: 1495.8811 - val_mae: 1496.5741\n",
      "Epoch 126/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 144.2621 - mae: 144.9481 - val_loss: 1317.9192 - val_mae: 1318.6124\n",
      "Epoch 127/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 146.2132 - mae: 146.8982 - val_loss: 1338.3402 - val_mae: 1339.0322\n",
      "Epoch 128/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 143.1790 - mae: 143.8680 - val_loss: 1212.8783 - val_mae: 1213.5701\n",
      "Epoch 129/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 145.4846 - mae: 146.1720 - val_loss: 1355.9388 - val_mae: 1356.6304\n",
      "Epoch 130/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 167.2844 - mae: 167.9722 - val_loss: 1332.1838 - val_mae: 1332.8765\n",
      "Epoch 131/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 163.4882 - mae: 164.1783 - val_loss: 1317.9062 - val_mae: 1318.5996\n",
      "Epoch 132/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 148.1253 - mae: 148.8129 - val_loss: 1314.6383 - val_mae: 1315.3311\n",
      "Epoch 133/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 150.9948 - mae: 151.6814 - val_loss: 1474.9641 - val_mae: 1475.6562\n",
      "Epoch 134/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 150.3475 - mae: 151.0365 - val_loss: 1259.8256 - val_mae: 1260.5167\n",
      "Epoch 135/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 151.1207 - mae: 151.8079 - val_loss: 1308.6005 - val_mae: 1309.2933\n",
      "Epoch 136/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 142.4086 - mae: 143.0963 - val_loss: 1600.1414 - val_mae: 1600.8345\n",
      "Epoch 137/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 137.4488 - mae: 138.1384 - val_loss: 1323.7950 - val_mae: 1324.4882\n",
      "Epoch 138/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 144.1726 - mae: 144.8602 - val_loss: 1293.5079 - val_mae: 1294.2007\n",
      "Epoch 139/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 142.9090 - mae: 143.5957 - val_loss: 1308.4691 - val_mae: 1309.1616\n",
      "Epoch 140/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 149.1045 - mae: 149.7926 - val_loss: 1202.4266 - val_mae: 1203.1196\n",
      "Epoch 141/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 140.6232 - mae: 141.3112 - val_loss: 1283.1299 - val_mae: 1283.8229\n",
      "Epoch 142/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 137.3622 - mae: 138.0497 - val_loss: 1463.7584 - val_mae: 1464.4517\n",
      "Epoch 143/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 141.4117 - mae: 142.1001 - val_loss: 1404.4456 - val_mae: 1405.1387\n",
      "Epoch 144/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 158.7787 - mae: 159.4671 - val_loss: 1863.4949 - val_mae: 1864.1882\n",
      "Epoch 145/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 162.6789 - mae: 163.3659 - val_loss: 1280.2450 - val_mae: 1280.9375\n",
      "Epoch 146/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 151.9797 - mae: 152.6682 - val_loss: 1271.3551 - val_mae: 1272.0482\n",
      "Epoch 147/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 148.1799 - mae: 148.8677 - val_loss: 1415.7765 - val_mae: 1416.4696\n",
      "Epoch 148/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 134.3102 - mae: 134.9981 - val_loss: 1232.6152 - val_mae: 1233.3082\n",
      "Epoch 149/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 134.1774 - mae: 134.8650 - val_loss: 1295.6570 - val_mae: 1296.3501\n",
      "Epoch 150/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 139.4712 - mae: 140.1568 - val_loss: 1460.6149 - val_mae: 1461.3080\n",
      "Epoch 151/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 146.0463 - mae: 146.7331 - val_loss: 1392.7592 - val_mae: 1393.4524\n",
      "Epoch 152/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 140.2397 - mae: 140.9248 - val_loss: 1425.1719 - val_mae: 1425.8649\n",
      "Epoch 153/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 128.3225 - mae: 129.0083 - val_loss: 1358.4384 - val_mae: 1359.1312\n",
      "Epoch 154/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 140.3046 - mae: 140.9907 - val_loss: 1218.0576 - val_mae: 1218.7496\n",
      "Epoch 155/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 148.1319 - mae: 148.8214 - val_loss: 1297.4951 - val_mae: 1298.1866\n",
      "Epoch 156/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 146.7537 - mae: 147.4394 - val_loss: 1126.5330 - val_mae: 1127.2262\n",
      "Epoch 157/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 141.9905 - mae: 142.6777 - val_loss: 1195.8677 - val_mae: 1196.5608\n",
      "Epoch 158/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 134.9285 - mae: 135.6164 - val_loss: 1232.2451 - val_mae: 1232.9376\n",
      "Epoch 159/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 144.2876 - mae: 144.9744 - val_loss: 1416.9646 - val_mae: 1417.6577\n",
      "Epoch 160/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 171.8890 - mae: 172.5791 - val_loss: 1229.3440 - val_mae: 1230.0367\n",
      "Epoch 161/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 140.7520 - mae: 141.4417 - val_loss: 1409.6335 - val_mae: 1410.3268\n",
      "Epoch 162/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 138.8926 - mae: 139.5789 - val_loss: 1716.0308 - val_mae: 1716.7240\n",
      "Epoch 163/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 149.7888 - mae: 150.4751 - val_loss: 1483.4271 - val_mae: 1484.1201\n",
      "Epoch 164/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 156.8185 - mae: 157.5044 - val_loss: 1135.0167 - val_mae: 1135.7086\n",
      "Epoch 165/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 144.0402 - mae: 144.7273 - val_loss: 1188.2816 - val_mae: 1188.9749\n",
      "Epoch 166/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 138.0886 - mae: 138.7754 - val_loss: 1351.6675 - val_mae: 1352.3606\n",
      "Epoch 167/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 134.4345 - mae: 135.1206 - val_loss: 1445.0244 - val_mae: 1445.7177\n",
      "Epoch 168/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 130.7186 - mae: 131.4061 - val_loss: 1613.0978 - val_mae: 1613.7911\n",
      "Epoch 169/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 140.7383 - mae: 141.4266 - val_loss: 1504.4620 - val_mae: 1505.1550\n",
      "Epoch 170/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 133.4151 - mae: 134.1018 - val_loss: 1240.9287 - val_mae: 1241.6204\n",
      "Epoch 171/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 137.3838 - mae: 138.0717 - val_loss: 1548.8555 - val_mae: 1549.5483\n",
      "Epoch 172/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 155.9293 - mae: 156.6180 - val_loss: 1714.9532 - val_mae: 1715.6462\n",
      "Epoch 173/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 158.3712 - mae: 159.0603 - val_loss: 1328.7047 - val_mae: 1329.3972\n",
      "Epoch 174/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 149.1253 - mae: 149.8119 - val_loss: 1390.3834 - val_mae: 1391.0765\n",
      "Epoch 175/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 135.8139 - mae: 136.5004 - val_loss: 1393.7058 - val_mae: 1394.3989\n",
      "Epoch 176/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 143.7619 - mae: 144.4490 - val_loss: 1337.2996 - val_mae: 1337.9928\n",
      "Epoch 177/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 142.4609 - mae: 143.1463 - val_loss: 1589.6888 - val_mae: 1590.3820\n",
      "Epoch 178/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 179.1606 - mae: 179.8484 - val_loss: 1594.8855 - val_mae: 1595.5787\n",
      "Epoch 179/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 132.2961 - mae: 132.9808 - val_loss: 1330.0936 - val_mae: 1330.7859\n",
      "Epoch 180/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 136.5333 - mae: 137.2222 - val_loss: 1376.7842 - val_mae: 1377.4773\n",
      "Epoch 181/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 163.9853 - mae: 164.6737 - val_loss: 1499.0051 - val_mae: 1499.6982\n",
      "Epoch 182/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 142.5458 - mae: 143.2330 - val_loss: 1318.2925 - val_mae: 1318.9850\n",
      "Epoch 183/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 134.7074 - mae: 135.3933 - val_loss: 1586.2715 - val_mae: 1586.9647\n",
      "Epoch 184/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 140.1277 - mae: 140.8156 - val_loss: 1449.5587 - val_mae: 1450.2521\n",
      "Epoch 185/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 147.7886 - mae: 148.4772 - val_loss: 1248.9949 - val_mae: 1249.6880\n",
      "Epoch 186/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 143.3934 - mae: 144.0823 - val_loss: 1652.0148 - val_mae: 1652.7080\n",
      "Epoch 187/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 147.6163 - mae: 148.3054 - val_loss: 1740.9503 - val_mae: 1741.6434\n",
      "Epoch 188/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 141.3191 - mae: 142.0076 - val_loss: 1826.9869 - val_mae: 1827.6805\n",
      "Epoch 189/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 147.3972 - mae: 148.0856 - val_loss: 1250.0267 - val_mae: 1250.7186\n",
      "Epoch 190/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 137.0021 - mae: 137.6906 - val_loss: 1253.2307 - val_mae: 1253.9238\n",
      "Epoch 191/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 143.2776 - mae: 143.9688 - val_loss: 1570.0145 - val_mae: 1570.7076\n",
      "Epoch 192/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 147.2559 - mae: 147.9441 - val_loss: 1526.3131 - val_mae: 1527.0052\n",
      "Epoch 193/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 157.6997 - mae: 158.3865 - val_loss: 1388.4320 - val_mae: 1389.1248\n",
      "Epoch 194/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 124.6188 - mae: 125.3042 - val_loss: 1404.6741 - val_mae: 1405.3674\n",
      "Epoch 195/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 122.5107 - mae: 123.1956 - val_loss: 1698.1273 - val_mae: 1698.8193\n",
      "Epoch 196/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 149.9033 - mae: 150.5907 - val_loss: 1441.3328 - val_mae: 1442.0260\n",
      "Epoch 197/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 149.0717 - mae: 149.7602 - val_loss: 1269.5109 - val_mae: 1270.2031\n",
      "Epoch 198/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 141.8997 - mae: 142.5864 - val_loss: 1599.0525 - val_mae: 1599.7455\n",
      "Epoch 199/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 148.1916 - mae: 148.8798 - val_loss: 1358.6224 - val_mae: 1359.3157\n",
      "Epoch 200/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 137.8035 - mae: 138.4873 - val_loss: 1490.2139 - val_mae: 1490.9070\n",
      "Epoch 201/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 155.7422 - mae: 156.4297 - val_loss: 1228.7162 - val_mae: 1229.4092\n",
      "Epoch 202/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 163.7003 - mae: 164.3869 - val_loss: 1133.7509 - val_mae: 1134.4440\n",
      "Epoch 203/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 140.5380 - mae: 141.2252 - val_loss: 1288.2815 - val_mae: 1288.9742\n",
      "Epoch 204/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 125.9424 - mae: 126.6250 - val_loss: 1397.0358 - val_mae: 1397.7289\n",
      "Epoch 205/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 130.9678 - mae: 131.6542 - val_loss: 1220.8596 - val_mae: 1221.5525\n",
      "Epoch 206/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 133.8969 - mae: 134.5816 - val_loss: 1243.1071 - val_mae: 1243.8002\n",
      "Epoch 207/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 128.4890 - mae: 129.1770 - val_loss: 1422.1575 - val_mae: 1422.8491\n",
      "Epoch 208/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 130.6463 - mae: 131.3311 - val_loss: 1408.0361 - val_mae: 1408.7291\n",
      "Epoch 209/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 141.5875 - mae: 142.2730 - val_loss: 1364.6927 - val_mae: 1365.3851\n",
      "Epoch 210/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 145.1729 - mae: 145.8602 - val_loss: 1399.3575 - val_mae: 1400.0509\n",
      "Epoch 211/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 131.9792 - mae: 132.6639 - val_loss: 1164.3772 - val_mae: 1165.0696\n",
      "Epoch 212/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 129.3414 - mae: 130.0269 - val_loss: 1262.5308 - val_mae: 1263.2239\n",
      "Epoch 213/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 137.0064 - mae: 137.6918 - val_loss: 1400.6920 - val_mae: 1401.3843\n",
      "Epoch 214/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 131.2706 - mae: 131.9571 - val_loss: 1351.3252 - val_mae: 1352.0183\n",
      "Epoch 215/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 148.9662 - mae: 149.6527 - val_loss: 1174.5143 - val_mae: 1175.2073\n",
      "Epoch 216/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 140.6472 - mae: 141.3356 - val_loss: 1279.2688 - val_mae: 1279.9617\n",
      "Epoch 217/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 128.2882 - mae: 128.9753 - val_loss: 1321.8855 - val_mae: 1322.5790\n",
      "Epoch 218/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 132.2878 - mae: 132.9765 - val_loss: 1116.2178 - val_mae: 1116.9103\n",
      "Epoch 219/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 135.6114 - mae: 136.2985 - val_loss: 1587.3879 - val_mae: 1588.0811\n",
      "Epoch 220/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 131.2798 - mae: 131.9658 - val_loss: 1362.5731 - val_mae: 1363.2665\n",
      "Epoch 221/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 138.5259 - mae: 139.2122 - val_loss: 1710.5524 - val_mae: 1711.2456\n",
      "Epoch 222/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 139.5161 - mae: 140.2026 - val_loss: 1153.6664 - val_mae: 1154.3588\n",
      "Epoch 223/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 142.8580 - mae: 143.5450 - val_loss: 1047.9852 - val_mae: 1048.6783\n",
      "Epoch 224/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 153.6492 - mae: 154.3382 - val_loss: 1472.4573 - val_mae: 1473.1505\n",
      "Epoch 225/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 143.8311 - mae: 144.5174 - val_loss: 1183.4069 - val_mae: 1184.0999\n",
      "Epoch 226/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 130.9818 - mae: 131.6680 - val_loss: 1271.6349 - val_mae: 1272.3281\n",
      "Epoch 227/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 141.1620 - mae: 141.8491 - val_loss: 1094.2764 - val_mae: 1094.9690\n",
      "Epoch 228/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 142.2709 - mae: 142.9562 - val_loss: 1234.6454 - val_mae: 1235.3385\n",
      "Epoch 229/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 132.6384 - mae: 133.3230 - val_loss: 1294.0668 - val_mae: 1294.7600\n",
      "Epoch 230/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 132.5921 - mae: 133.2805 - val_loss: 1422.0361 - val_mae: 1422.7289\n",
      "Epoch 231/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 129.5863 - mae: 130.2703 - val_loss: 1573.2688 - val_mae: 1573.9619\n",
      "Epoch 232/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 140.8242 - mae: 141.5135 - val_loss: 1327.8153 - val_mae: 1328.5085\n",
      "Epoch 233/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 152.3676 - mae: 153.0558 - val_loss: 1615.3718 - val_mae: 1616.0642\n",
      "Epoch 234/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 141.0349 - mae: 141.7229 - val_loss: 1477.0842 - val_mae: 1477.7775\n",
      "Epoch 235/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 146.3057 - mae: 146.9936 - val_loss: 1265.6101 - val_mae: 1266.3033\n",
      "Epoch 236/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 141.1173 - mae: 141.8035 - val_loss: 1318.5580 - val_mae: 1319.2507\n",
      "Epoch 237/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 134.2144 - mae: 134.9022 - val_loss: 1467.9930 - val_mae: 1468.6862\n",
      "Epoch 238/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 135.0378 - mae: 135.7242 - val_loss: 1307.1714 - val_mae: 1307.8635\n",
      "Epoch 239/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 151.4584 - mae: 152.1452 - val_loss: 1468.0677 - val_mae: 1468.7609\n",
      "Epoch 240/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 140.2960 - mae: 140.9825 - val_loss: 1332.8719 - val_mae: 1333.5647\n",
      "Epoch 241/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 137.1873 - mae: 137.8729 - val_loss: 1341.0565 - val_mae: 1341.7496\n",
      "Epoch 242/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 128.9555 - mae: 129.6405 - val_loss: 1347.0052 - val_mae: 1347.6985\n",
      "Epoch 243/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 130.6502 - mae: 131.3363 - val_loss: 1364.7291 - val_mae: 1365.4221\n",
      "Epoch 244/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 125.7898 - mae: 126.4781 - val_loss: 1185.2080 - val_mae: 1185.9012\n",
      "Epoch 245/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 128.3866 - mae: 129.0713 - val_loss: 1114.8333 - val_mae: 1115.5262\n",
      "Epoch 246/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 134.0673 - mae: 134.7566 - val_loss: 1317.7123 - val_mae: 1318.4047\n",
      "Epoch 247/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 140.7896 - mae: 141.4763 - val_loss: 1325.5377 - val_mae: 1326.2311\n",
      "Epoch 248/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 137.9217 - mae: 138.6084 - val_loss: 1375.6602 - val_mae: 1376.3535\n",
      "Epoch 249/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 121.7499 - mae: 122.4363 - val_loss: 1396.8195 - val_mae: 1397.5121\n",
      "Epoch 250/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 127.2871 - mae: 127.9728 - val_loss: 1271.2861 - val_mae: 1271.9788\n",
      "Epoch 251/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 137.8878 - mae: 138.5749 - val_loss: 1279.6293 - val_mae: 1280.3224\n",
      "Epoch 252/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 139.6700 - mae: 140.3592 - val_loss: 1290.4993 - val_mae: 1291.1926\n",
      "Epoch 253/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 136.7175 - mae: 137.4026 - val_loss: 1368.3367 - val_mae: 1369.0299\n",
      "Epoch 254/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 157.9559 - mae: 158.6460 - val_loss: 1429.2529 - val_mae: 1429.9448\n",
      "Epoch 255/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 143.4717 - mae: 144.1595 - val_loss: 1210.7839 - val_mae: 1211.4771\n",
      "Epoch 256/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 142.7477 - mae: 143.4316 - val_loss: 1588.3268 - val_mae: 1589.0199\n",
      "Epoch 257/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 136.7484 - mae: 137.4344 - val_loss: 1454.0524 - val_mae: 1454.7454\n",
      "Epoch 258/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 125.1793 - mae: 125.8638 - val_loss: 1188.3806 - val_mae: 1189.0736\n",
      "Epoch 259/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 137.0931 - mae: 137.7781 - val_loss: 1379.2017 - val_mae: 1379.8947\n",
      "Epoch 260/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 136.2325 - mae: 136.9186 - val_loss: 1210.8695 - val_mae: 1211.5624\n",
      "Epoch 261/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 136.1048 - mae: 136.7924 - val_loss: 1392.0197 - val_mae: 1392.7128\n",
      "Epoch 262/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 127.7160 - mae: 128.4011 - val_loss: 1468.2239 - val_mae: 1468.9170\n",
      "Epoch 263/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 125.8662 - mae: 126.5536 - val_loss: 1356.1261 - val_mae: 1356.8191\n",
      "Epoch 264/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 125.8483 - mae: 126.5353 - val_loss: 1243.1261 - val_mae: 1243.8193\n",
      "Epoch 265/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 122.5124 - mae: 123.1997 - val_loss: 1390.0081 - val_mae: 1390.7013\n",
      "Epoch 266/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 130.0799 - mae: 130.7657 - val_loss: 1263.0824 - val_mae: 1263.7744\n",
      "Epoch 267/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 135.9752 - mae: 136.6596 - val_loss: 1235.4283 - val_mae: 1236.1215\n",
      "Epoch 268/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 126.4565 - mae: 127.1440 - val_loss: 1214.5817 - val_mae: 1215.2749\n",
      "Epoch 269/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 126.5286 - mae: 127.2149 - val_loss: 1288.8680 - val_mae: 1289.5605\n",
      "Epoch 270/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 129.0290 - mae: 129.7159 - val_loss: 1178.3899 - val_mae: 1179.0831\n",
      "Epoch 271/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 125.2482 - mae: 125.9322 - val_loss: 1309.1812 - val_mae: 1309.8744\n",
      "Epoch 272/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 137.7641 - mae: 138.4521 - val_loss: 1186.7913 - val_mae: 1187.4839\n",
      "Epoch 273/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 126.9443 - mae: 127.6309 - val_loss: 1166.3525 - val_mae: 1167.0457\n",
      "Epoch 274/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 121.6068 - mae: 122.2925 - val_loss: 1271.2705 - val_mae: 1271.9636\n",
      "Epoch 275/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 123.7563 - mae: 124.4407 - val_loss: 1200.3397 - val_mae: 1201.0328\n",
      "Epoch 276/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 128.7477 - mae: 129.4348 - val_loss: 1312.0829 - val_mae: 1312.7758\n",
      "Epoch 277/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 136.9959 - mae: 137.6817 - val_loss: 1214.8319 - val_mae: 1215.5250\n",
      "Epoch 278/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 132.9512 - mae: 133.6339 - val_loss: 1200.7562 - val_mae: 1201.4493\n",
      "Epoch 279/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 128.1653 - mae: 128.8495 - val_loss: 1212.8431 - val_mae: 1213.5364\n",
      "Epoch 280/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 138.5734 - mae: 139.2598 - val_loss: 1566.0500 - val_mae: 1566.7434\n",
      "Epoch 281/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 143.1231 - mae: 143.8095 - val_loss: 1287.3599 - val_mae: 1288.0531\n",
      "Epoch 282/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 121.5280 - mae: 122.2144 - val_loss: 1250.4486 - val_mae: 1251.1418\n",
      "Epoch 283/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 130.3613 - mae: 131.0473 - val_loss: 1488.7271 - val_mae: 1489.4202\n",
      "Epoch 284/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 122.8010 - mae: 123.4850 - val_loss: 1329.7927 - val_mae: 1330.4856\n",
      "Epoch 285/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 137.0740 - mae: 137.7614 - val_loss: 1471.1730 - val_mae: 1471.8649\n",
      "Epoch 286/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 127.3654 - mae: 128.0518 - val_loss: 1500.7290 - val_mae: 1501.4222\n",
      "Epoch 287/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 124.5443 - mae: 125.2307 - val_loss: 1336.9174 - val_mae: 1337.6104\n",
      "Epoch 288/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 128.1764 - mae: 128.8634 - val_loss: 1510.5336 - val_mae: 1511.2268\n",
      "Epoch 289/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 130.0981 - mae: 130.7830 - val_loss: 1173.4189 - val_mae: 1174.1122\n",
      "Epoch 290/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 124.6280 - mae: 125.3135 - val_loss: 1537.5006 - val_mae: 1538.1938\n",
      "Epoch 291/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 123.4091 - mae: 124.0948 - val_loss: 1468.8793 - val_mae: 1469.5719\n",
      "Epoch 292/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 120.5772 - mae: 121.2611 - val_loss: 1302.6410 - val_mae: 1303.3339\n",
      "Epoch 293/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 136.9572 - mae: 137.6446 - val_loss: 1219.5731 - val_mae: 1220.2650\n",
      "Epoch 294/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 123.3646 - mae: 124.0512 - val_loss: 1179.1979 - val_mae: 1179.8911\n",
      "Epoch 295/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 127.9261 - mae: 128.6110 - val_loss: 1341.1909 - val_mae: 1341.8829\n",
      "Epoch 296/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 136.2691 - mae: 136.9558 - val_loss: 1158.5325 - val_mae: 1159.2256\n",
      "Epoch 297/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 129.3756 - mae: 130.0642 - val_loss: 1185.2589 - val_mae: 1185.9521\n",
      "Epoch 298/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 132.2425 - mae: 132.9288 - val_loss: 1456.5356 - val_mae: 1457.2286\n",
      "Epoch 299/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 126.4936 - mae: 127.1800 - val_loss: 1273.6388 - val_mae: 1274.3319\n",
      "Epoch 300/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 133.3764 - mae: 134.0609 - val_loss: 1246.2716 - val_mae: 1246.9640\n",
      "Epoch 301/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 142.1467 - mae: 142.8335 - val_loss: 1157.7532 - val_mae: 1158.4457\n",
      "Epoch 302/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 129.4682 - mae: 130.1556 - val_loss: 1521.9705 - val_mae: 1522.6622\n",
      "Epoch 303/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 126.3478 - mae: 127.0338 - val_loss: 1229.7792 - val_mae: 1230.4723\n",
      "Epoch 304/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 128.9014 - mae: 129.5892 - val_loss: 1032.5806 - val_mae: 1033.2732\n",
      "Epoch 305/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 151.7769 - mae: 152.4628 - val_loss: 1280.5857 - val_mae: 1281.2788\n",
      "Epoch 306/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 141.1468 - mae: 141.8329 - val_loss: 1462.7350 - val_mae: 1463.4277\n",
      "Epoch 307/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 123.3894 - mae: 124.0762 - val_loss: 1437.3892 - val_mae: 1438.0813\n",
      "Epoch 308/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 130.9697 - mae: 131.6531 - val_loss: 1254.3666 - val_mae: 1255.0597\n",
      "Epoch 309/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 132.9845 - mae: 133.6739 - val_loss: 1688.3104 - val_mae: 1689.0035\n",
      "Epoch 310/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 127.4881 - mae: 128.1743 - val_loss: 1280.5271 - val_mae: 1281.2203\n",
      "Epoch 311/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 123.4665 - mae: 124.1512 - val_loss: 1394.8182 - val_mae: 1395.5114\n",
      "Epoch 312/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 120.6239 - mae: 121.3089 - val_loss: 1235.7711 - val_mae: 1236.4645\n",
      "Epoch 313/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 132.4422 - mae: 133.1276 - val_loss: 1468.1864 - val_mae: 1468.8798\n",
      "Epoch 314/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 130.9532 - mae: 131.6371 - val_loss: 1405.7651 - val_mae: 1406.4583\n",
      "Epoch 315/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 124.0658 - mae: 124.7533 - val_loss: 1475.6234 - val_mae: 1476.3169\n",
      "Epoch 316/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 129.2472 - mae: 129.9364 - val_loss: 1546.6047 - val_mae: 1547.2979\n",
      "Epoch 317/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 141.5809 - mae: 142.2679 - val_loss: 1369.7383 - val_mae: 1370.4315\n",
      "Epoch 318/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 139.3091 - mae: 139.9969 - val_loss: 1179.7233 - val_mae: 1180.4159\n",
      "Epoch 319/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 139.5817 - mae: 140.2667 - val_loss: 1428.1169 - val_mae: 1428.8101\n",
      "Epoch 320/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 131.2131 - mae: 131.8980 - val_loss: 1385.9827 - val_mae: 1386.6760\n",
      "Epoch 321/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 132.5250 - mae: 133.2145 - val_loss: 1391.3197 - val_mae: 1392.0120\n",
      "Epoch 322/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 128.2359 - mae: 128.9246 - val_loss: 1499.2354 - val_mae: 1499.9275\n",
      "Epoch 323/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 129.8671 - mae: 130.5535 - val_loss: 1473.5005 - val_mae: 1474.1927\n",
      "Epoch 324/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 149.1966 - mae: 149.8831 - val_loss: 1433.5923 - val_mae: 1434.2855\n",
      "Epoch 325/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 126.3574 - mae: 127.0432 - val_loss: 1293.0186 - val_mae: 1293.7114\n",
      "Epoch 326/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 124.6086 - mae: 125.2951 - val_loss: 1335.1271 - val_mae: 1335.8187\n",
      "Epoch 327/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 142.0351 - mae: 142.7228 - val_loss: 1560.7047 - val_mae: 1561.3979\n",
      "Epoch 328/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 131.9314 - mae: 132.6168 - val_loss: 1668.9849 - val_mae: 1669.6780\n",
      "Epoch 329/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 126.2324 - mae: 126.9183 - val_loss: 1306.0907 - val_mae: 1306.7838\n",
      "Epoch 330/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 118.0438 - mae: 118.7297 - val_loss: 1523.0494 - val_mae: 1523.7428\n",
      "Epoch 331/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 123.1803 - mae: 123.8648 - val_loss: 1306.5179 - val_mae: 1307.2111\n",
      "Epoch 332/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 129.3822 - mae: 130.0649 - val_loss: 1314.2817 - val_mae: 1314.9746\n",
      "Epoch 333/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 124.1478 - mae: 124.8344 - val_loss: 1441.8921 - val_mae: 1442.5842\n",
      "Epoch 334/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 146.7023 - mae: 147.3895 - val_loss: 1273.8124 - val_mae: 1274.5059\n",
      "Epoch 335/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 120.2067 - mae: 120.8947 - val_loss: 1494.5093 - val_mae: 1495.2026\n",
      "Epoch 336/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 149.2394 - mae: 149.9273 - val_loss: 1388.1079 - val_mae: 1388.8011\n",
      "Epoch 337/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 132.3709 - mae: 133.0579 - val_loss: 1406.4958 - val_mae: 1407.1888\n",
      "Epoch 338/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 125.4845 - mae: 126.1702 - val_loss: 1326.0283 - val_mae: 1326.7217\n",
      "Epoch 339/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 131.0367 - mae: 131.7222 - val_loss: 1259.4800 - val_mae: 1260.1726\n",
      "Epoch 340/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 123.9012 - mae: 124.5892 - val_loss: 1431.3217 - val_mae: 1432.0149\n",
      "Epoch 341/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 117.5321 - mae: 118.2205 - val_loss: 1182.2855 - val_mae: 1182.9788\n",
      "Epoch 342/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 122.2382 - mae: 122.9238 - val_loss: 1436.2706 - val_mae: 1436.9636\n",
      "Epoch 343/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 121.4002 - mae: 122.0864 - val_loss: 1290.0773 - val_mae: 1290.7694\n",
      "Epoch 344/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 129.4601 - mae: 130.1457 - val_loss: 1508.0762 - val_mae: 1508.7693\n",
      "Epoch 345/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 122.3352 - mae: 123.0190 - val_loss: 1245.7567 - val_mae: 1246.4495\n",
      "Epoch 346/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 122.5963 - mae: 123.2814 - val_loss: 1471.4978 - val_mae: 1472.1910\n",
      "Epoch 347/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 115.4033 - mae: 116.0880 - val_loss: 1380.5103 - val_mae: 1381.2032\n",
      "Epoch 348/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 129.2554 - mae: 129.9415 - val_loss: 1296.7754 - val_mae: 1297.4686\n",
      "Epoch 349/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 128.9359 - mae: 129.6213 - val_loss: 1202.4254 - val_mae: 1203.1187\n",
      "Epoch 350/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 126.3455 - mae: 127.0333 - val_loss: 1291.6262 - val_mae: 1292.3190\n",
      "Epoch 351/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 131.7855 - mae: 132.4729 - val_loss: 1381.5062 - val_mae: 1382.1995\n",
      "Epoch 352/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 118.5109 - mae: 119.1973 - val_loss: 1431.7485 - val_mae: 1432.4418\n",
      "Epoch 353/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 131.2396 - mae: 131.9253 - val_loss: 1335.7943 - val_mae: 1336.4869\n",
      "Epoch 354/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 131.4466 - mae: 132.1301 - val_loss: 1219.2028 - val_mae: 1219.8950\n",
      "Epoch 355/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 138.8697 - mae: 139.5577 - val_loss: 1254.5726 - val_mae: 1255.2659\n",
      "Epoch 356/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 123.3256 - mae: 124.0122 - val_loss: 1422.5312 - val_mae: 1423.2246\n",
      "Epoch 357/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 123.4149 - mae: 124.1009 - val_loss: 1319.4331 - val_mae: 1320.1262\n",
      "Epoch 358/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 121.5502 - mae: 122.2360 - val_loss: 1229.8735 - val_mae: 1230.5668\n",
      "Epoch 359/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 117.6824 - mae: 118.3689 - val_loss: 1452.9871 - val_mae: 1453.6803\n",
      "Epoch 360/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 122.2106 - mae: 122.8949 - val_loss: 1175.6172 - val_mae: 1176.3093\n",
      "Epoch 361/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 126.9869 - mae: 127.6743 - val_loss: 1302.1317 - val_mae: 1302.8251\n",
      "Epoch 362/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 126.0227 - mae: 126.7089 - val_loss: 1350.5886 - val_mae: 1351.2812\n",
      "Epoch 363/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 119.1814 - mae: 119.8677 - val_loss: 1268.7938 - val_mae: 1269.4863\n",
      "Epoch 364/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 116.9705 - mae: 117.6561 - val_loss: 1383.2716 - val_mae: 1383.9639\n",
      "Epoch 365/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 124.0075 - mae: 124.6924 - val_loss: 1367.7842 - val_mae: 1368.4775\n",
      "Epoch 366/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 124.9236 - mae: 125.6108 - val_loss: 1404.2511 - val_mae: 1404.9443\n",
      "Epoch 367/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 122.2235 - mae: 122.9105 - val_loss: 1417.8525 - val_mae: 1418.5453\n",
      "Epoch 368/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 126.1356 - mae: 126.8215 - val_loss: 1221.2375 - val_mae: 1221.9305\n",
      "Epoch 369/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 122.9031 - mae: 123.5887 - val_loss: 1247.6292 - val_mae: 1248.3224\n",
      "Epoch 370/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 127.1442 - mae: 127.8308 - val_loss: 1337.1643 - val_mae: 1337.8566\n",
      "Epoch 371/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 136.3520 - mae: 137.0366 - val_loss: 1280.7720 - val_mae: 1281.4652\n",
      "Epoch 372/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 123.7369 - mae: 124.4252 - val_loss: 1258.7939 - val_mae: 1259.4862\n",
      "Epoch 373/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 121.4084 - mae: 122.0949 - val_loss: 1652.9391 - val_mae: 1653.6312\n",
      "Epoch 374/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 118.4127 - mae: 119.0964 - val_loss: 1633.9797 - val_mae: 1634.6725\n",
      "Epoch 375/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 132.9192 - mae: 133.6055 - val_loss: 1540.0714 - val_mae: 1540.7648\n",
      "Epoch 376/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 121.6048 - mae: 122.2897 - val_loss: 1259.0526 - val_mae: 1259.7457\n",
      "Epoch 377/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 123.8016 - mae: 124.4891 - val_loss: 1519.5806 - val_mae: 1520.2737\n",
      "Epoch 378/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 137.0812 - mae: 137.7683 - val_loss: 1179.7225 - val_mae: 1180.4156\n",
      "Epoch 379/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 129.4649 - mae: 130.1530 - val_loss: 1484.2603 - val_mae: 1484.9535\n",
      "Epoch 380/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 148.2182 - mae: 148.9067 - val_loss: 1271.4115 - val_mae: 1272.1035\n",
      "Epoch 381/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 123.7984 - mae: 124.4863 - val_loss: 1416.4434 - val_mae: 1417.1362\n",
      "Epoch 382/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 120.1757 - mae: 120.8589 - val_loss: 1450.1394 - val_mae: 1450.8325\n",
      "Epoch 383/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 119.1153 - mae: 119.8032 - val_loss: 1235.6530 - val_mae: 1236.3456\n",
      "Epoch 384/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 117.2370 - mae: 117.9213 - val_loss: 1264.3127 - val_mae: 1265.0048\n",
      "Epoch 385/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 118.4226 - mae: 119.1060 - val_loss: 1532.9030 - val_mae: 1533.5961\n",
      "Epoch 386/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 139.9422 - mae: 140.6316 - val_loss: 1149.7432 - val_mae: 1150.4351\n",
      "Epoch 387/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 117.2522 - mae: 117.9348 - val_loss: 1284.8080 - val_mae: 1285.5011\n",
      "Epoch 388/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 122.5039 - mae: 123.1906 - val_loss: 1562.1753 - val_mae: 1562.8685\n",
      "Epoch 389/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 132.1565 - mae: 132.8393 - val_loss: 1210.4316 - val_mae: 1211.1248\n",
      "Epoch 390/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 139.3175 - mae: 140.0043 - val_loss: 1411.6948 - val_mae: 1412.3882\n",
      "Epoch 391/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 127.5968 - mae: 128.2848 - val_loss: 1445.7001 - val_mae: 1446.3928\n",
      "Epoch 392/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 121.1989 - mae: 121.8827 - val_loss: 1405.8284 - val_mae: 1406.5215\n",
      "Epoch 393/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 127.5626 - mae: 128.2453 - val_loss: 1424.5944 - val_mae: 1425.2872\n",
      "Epoch 394/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 116.4961 - mae: 117.1805 - val_loss: 1310.3658 - val_mae: 1311.0588\n",
      "Epoch 395/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 125.5875 - mae: 126.2723 - val_loss: 1557.9874 - val_mae: 1558.6808\n",
      "Epoch 396/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 124.1305 - mae: 124.8169 - val_loss: 1380.1003 - val_mae: 1380.7936\n",
      "Epoch 397/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 116.0939 - mae: 116.7790 - val_loss: 1426.9344 - val_mae: 1427.6278\n",
      "Epoch 398/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 114.9869 - mae: 115.6714 - val_loss: 1264.1969 - val_mae: 1264.8895\n",
      "Epoch 399/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 116.2889 - mae: 116.9723 - val_loss: 1311.4358 - val_mae: 1312.1290\n",
      "Epoch 400/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 118.6364 - mae: 119.3217 - val_loss: 1730.3131 - val_mae: 1731.0063\n",
      "Epoch 401/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 132.5990 - mae: 133.2883 - val_loss: 1319.8905 - val_mae: 1320.5833\n",
      "Epoch 402/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 113.7983 - mae: 114.4845 - val_loss: 1396.5607 - val_mae: 1397.2538\n",
      "Epoch 403/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 118.5268 - mae: 119.2145 - val_loss: 1239.3511 - val_mae: 1240.0436\n",
      "Epoch 404/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 123.1842 - mae: 123.8693 - val_loss: 1148.0847 - val_mae: 1148.7780\n",
      "Epoch 405/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 123.5728 - mae: 124.2598 - val_loss: 1379.4977 - val_mae: 1380.1909\n",
      "Epoch 406/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 116.2884 - mae: 116.9727 - val_loss: 1334.9817 - val_mae: 1335.6748\n",
      "Epoch 407/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 134.8520 - mae: 135.5381 - val_loss: 1283.5193 - val_mae: 1284.2124\n",
      "Epoch 408/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 119.9755 - mae: 120.6623 - val_loss: 1638.9827 - val_mae: 1639.6759\n",
      "Epoch 409/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 123.3888 - mae: 124.0773 - val_loss: 1764.7546 - val_mae: 1765.4467\n",
      "Epoch 410/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 132.8646 - mae: 133.5551 - val_loss: 1331.7506 - val_mae: 1332.4434\n",
      "Epoch 411/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 121.0035 - mae: 121.6871 - val_loss: 1368.1193 - val_mae: 1368.8123\n",
      "Epoch 412/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 123.9705 - mae: 124.6542 - val_loss: 1264.8763 - val_mae: 1265.5677\n",
      "Epoch 413/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 121.5741 - mae: 122.2592 - val_loss: 1288.7064 - val_mae: 1289.3993\n",
      "Epoch 414/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 125.9609 - mae: 126.6481 - val_loss: 1504.3641 - val_mae: 1505.0571\n",
      "Epoch 415/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 112.5965 - mae: 113.2823 - val_loss: 1347.7242 - val_mae: 1348.4172\n",
      "Epoch 416/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 120.6305 - mae: 121.3159 - val_loss: 1460.2086 - val_mae: 1460.9006\n",
      "Epoch 417/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 120.6572 - mae: 121.3442 - val_loss: 1420.1587 - val_mae: 1420.8519\n",
      "Epoch 418/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 121.4193 - mae: 122.1052 - val_loss: 1428.2057 - val_mae: 1428.8988\n",
      "Epoch 419/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 137.7169 - mae: 138.4010 - val_loss: 1169.3219 - val_mae: 1170.0139\n",
      "Epoch 420/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 128.1249 - mae: 128.8102 - val_loss: 1145.7444 - val_mae: 1146.4375\n",
      "Epoch 421/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 137.4142 - mae: 138.1022 - val_loss: 1343.7896 - val_mae: 1344.4825\n",
      "Epoch 422/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 115.8129 - mae: 116.4970 - val_loss: 1455.7142 - val_mae: 1456.4071\n",
      "Epoch 423/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 127.8168 - mae: 128.5027 - val_loss: 1336.0806 - val_mae: 1336.7736\n",
      "Epoch 424/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 121.1977 - mae: 121.8841 - val_loss: 1261.5609 - val_mae: 1262.2542\n",
      "Epoch 425/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 120.6664 - mae: 121.3512 - val_loss: 1312.6786 - val_mae: 1313.3710\n",
      "Epoch 426/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 120.4797 - mae: 121.1659 - val_loss: 1468.7034 - val_mae: 1469.3962\n",
      "Epoch 427/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 126.6996 - mae: 127.3884 - val_loss: 1549.3700 - val_mae: 1550.0625\n",
      "Epoch 428/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 129.4284 - mae: 130.1135 - val_loss: 1098.7648 - val_mae: 1099.4574\n",
      "Epoch 429/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 117.9463 - mae: 118.6347 - val_loss: 1617.9083 - val_mae: 1618.6014\n",
      "Epoch 430/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 118.1013 - mae: 118.7853 - val_loss: 1579.7448 - val_mae: 1580.4379\n",
      "Epoch 431/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 118.4989 - mae: 119.1868 - val_loss: 1070.8137 - val_mae: 1071.5065\n",
      "Epoch 432/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 121.0672 - mae: 121.7544 - val_loss: 1247.4476 - val_mae: 1248.1407\n",
      "Epoch 433/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 137.6115 - mae: 138.2976 - val_loss: 1437.6304 - val_mae: 1438.3234\n",
      "Epoch 434/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 121.2259 - mae: 121.9114 - val_loss: 1267.5598 - val_mae: 1268.2531\n",
      "Epoch 435/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 123.3150 - mae: 124.0001 - val_loss: 1347.6766 - val_mae: 1348.3695\n",
      "Epoch 436/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 125.7932 - mae: 126.4797 - val_loss: 1752.5792 - val_mae: 1753.2722\n",
      "Epoch 437/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 136.8822 - mae: 137.5674 - val_loss: 1295.4773 - val_mae: 1296.1704\n",
      "Epoch 438/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 121.8594 - mae: 122.5456 - val_loss: 1325.0552 - val_mae: 1325.7463\n",
      "Epoch 439/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 115.8901 - mae: 116.5775 - val_loss: 1137.4703 - val_mae: 1138.1614\n",
      "Epoch 440/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 120.9645 - mae: 121.6499 - val_loss: 1509.0367 - val_mae: 1509.7301\n",
      "Epoch 441/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 118.1385 - mae: 118.8220 - val_loss: 1362.3237 - val_mae: 1363.0170\n",
      "Epoch 442/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 118.8531 - mae: 119.5377 - val_loss: 1223.0938 - val_mae: 1223.7869\n",
      "Epoch 443/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 113.8656 - mae: 114.5476 - val_loss: 1254.1443 - val_mae: 1254.8375\n",
      "Epoch 444/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 117.0217 - mae: 117.7085 - val_loss: 1125.4547 - val_mae: 1126.1476\n",
      "Epoch 445/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 131.2846 - mae: 131.9697 - val_loss: 1129.7216 - val_mae: 1130.4147\n",
      "Epoch 446/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 119.0539 - mae: 119.7400 - val_loss: 1197.8501 - val_mae: 1198.5435\n",
      "Epoch 447/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 122.3040 - mae: 122.9877 - val_loss: 1315.9261 - val_mae: 1316.6191\n",
      "Epoch 448/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 118.0864 - mae: 118.7710 - val_loss: 1237.8400 - val_mae: 1238.5321\n",
      "Epoch 449/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 113.8361 - mae: 114.5212 - val_loss: 1311.4648 - val_mae: 1312.1576\n",
      "Epoch 450/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 118.4651 - mae: 119.1495 - val_loss: 1399.7161 - val_mae: 1400.4091\n",
      "Epoch 451/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 121.2226 - mae: 121.9083 - val_loss: 1326.1779 - val_mae: 1326.8708\n",
      "Epoch 452/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 114.9811 - mae: 115.6668 - val_loss: 1292.0798 - val_mae: 1292.7728\n",
      "Epoch 453/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 114.2793 - mae: 114.9626 - val_loss: 1171.0420 - val_mae: 1171.7347\n",
      "Epoch 454/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 120.0724 - mae: 120.7597 - val_loss: 1174.8527 - val_mae: 1175.5458\n",
      "Epoch 455/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 117.0036 - mae: 117.6892 - val_loss: 1214.7590 - val_mae: 1215.4521\n",
      "Epoch 456/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 110.7610 - mae: 111.4458 - val_loss: 1275.5757 - val_mae: 1276.2688\n",
      "Epoch 457/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 111.2928 - mae: 111.9801 - val_loss: 1201.3335 - val_mae: 1202.0269\n",
      "Epoch 458/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 133.5959 - mae: 134.2828 - val_loss: 1606.6005 - val_mae: 1607.2937\n",
      "Epoch 459/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 119.7611 - mae: 120.4446 - val_loss: 1220.1522 - val_mae: 1220.8445\n",
      "Epoch 460/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 115.6671 - mae: 116.3518 - val_loss: 1211.9740 - val_mae: 1212.6671\n",
      "Epoch 461/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 112.4015 - mae: 113.0839 - val_loss: 1448.7795 - val_mae: 1449.4724\n",
      "Epoch 462/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 113.0955 - mae: 113.7817 - val_loss: 1356.3977 - val_mae: 1357.0898\n",
      "Epoch 463/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 123.1642 - mae: 123.8480 - val_loss: 1148.7203 - val_mae: 1149.4136\n",
      "Epoch 464/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 115.3907 - mae: 116.0752 - val_loss: 1560.2153 - val_mae: 1560.9083\n",
      "Epoch 465/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 121.6254 - mae: 122.3102 - val_loss: 1089.7384 - val_mae: 1090.4313\n",
      "Epoch 466/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 117.2939 - mae: 117.9789 - val_loss: 1195.6573 - val_mae: 1196.3500\n",
      "Epoch 467/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 117.1777 - mae: 117.8617 - val_loss: 1436.0566 - val_mae: 1436.7495\n",
      "Epoch 468/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 114.7100 - mae: 115.3929 - val_loss: 1271.9324 - val_mae: 1272.6251\n",
      "Epoch 469/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 111.3094 - mae: 111.9947 - val_loss: 1191.3152 - val_mae: 1192.0082\n",
      "Epoch 470/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 119.0409 - mae: 119.7288 - val_loss: 1603.0182 - val_mae: 1603.7107\n",
      "Epoch 471/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 116.4629 - mae: 117.1487 - val_loss: 1383.9175 - val_mae: 1384.6102\n",
      "Epoch 472/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 111.1789 - mae: 111.8601 - val_loss: 1117.0840 - val_mae: 1117.7772\n",
      "Epoch 473/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 113.5012 - mae: 114.1876 - val_loss: 1419.2012 - val_mae: 1419.8944\n",
      "Epoch 474/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 114.9799 - mae: 115.6644 - val_loss: 1210.9507 - val_mae: 1211.6438\n",
      "Epoch 475/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 111.9894 - mae: 112.6734 - val_loss: 1181.3051 - val_mae: 1181.9982\n",
      "Epoch 476/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 117.1789 - mae: 117.8640 - val_loss: 1652.1016 - val_mae: 1652.7943\n",
      "Epoch 477/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 125.9011 - mae: 126.5843 - val_loss: 1495.2422 - val_mae: 1495.9353\n",
      "Epoch 478/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 112.0163 - mae: 112.7019 - val_loss: 1370.9346 - val_mae: 1371.6252\n",
      "Epoch 479/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 115.3386 - mae: 116.0196 - val_loss: 1577.2589 - val_mae: 1577.9523\n",
      "Epoch 480/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 138.4962 - mae: 139.1821 - val_loss: 1256.6299 - val_mae: 1257.3229\n",
      "Epoch 481/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 120.5573 - mae: 121.2419 - val_loss: 1123.2808 - val_mae: 1123.9735\n",
      "Epoch 482/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 133.6436 - mae: 134.3300 - val_loss: 1419.7736 - val_mae: 1420.4664\n",
      "Epoch 483/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 117.1535 - mae: 117.8390 - val_loss: 1504.5784 - val_mae: 1505.2714\n",
      "Epoch 484/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 119.7077 - mae: 120.3911 - val_loss: 1231.7826 - val_mae: 1232.4758\n",
      "Epoch 485/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 120.7464 - mae: 121.4296 - val_loss: 1355.8215 - val_mae: 1356.5139\n",
      "Epoch 486/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 130.1585 - mae: 130.8446 - val_loss: 1433.5668 - val_mae: 1434.2596\n",
      "Epoch 487/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 122.8737 - mae: 123.5618 - val_loss: 1060.2531 - val_mae: 1060.9453\n",
      "Epoch 488/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 121.7298 - mae: 122.4155 - val_loss: 1376.5367 - val_mae: 1377.2286\n",
      "Epoch 489/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 113.6650 - mae: 114.3518 - val_loss: 1293.9438 - val_mae: 1294.6371\n",
      "Epoch 490/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 108.6349 - mae: 109.3184 - val_loss: 1372.8577 - val_mae: 1373.5510\n",
      "Epoch 491/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 127.7501 - mae: 128.4384 - val_loss: 1438.7627 - val_mae: 1439.4558\n",
      "Epoch 492/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 135.8862 - mae: 136.5693 - val_loss: 1269.5332 - val_mae: 1270.2264\n",
      "Epoch 493/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 123.5600 - mae: 124.2476 - val_loss: 1268.1655 - val_mae: 1268.8586\n",
      "Epoch 494/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 114.6207 - mae: 115.3073 - val_loss: 1254.5754 - val_mae: 1255.2673\n",
      "Epoch 495/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 110.8361 - mae: 111.5225 - val_loss: 1434.7896 - val_mae: 1435.4827\n",
      "Epoch 496/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 111.9993 - mae: 112.6851 - val_loss: 1234.2939 - val_mae: 1234.9862\n",
      "Epoch 497/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 118.9443 - mae: 119.6303 - val_loss: 1334.2855 - val_mae: 1334.9775\n",
      "Epoch 498/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 116.5369 - mae: 117.2209 - val_loss: 1194.8815 - val_mae: 1195.5747\n",
      "Epoch 499/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 116.2164 - mae: 116.9010 - val_loss: 1487.5819 - val_mae: 1488.2740\n",
      "Epoch 500/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 118.7117 - mae: 119.3953 - val_loss: 1235.0427 - val_mae: 1235.7358\n",
      "Epoch 501/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 121.3140 - mae: 122.0009 - val_loss: 1229.4695 - val_mae: 1230.1626\n",
      "Epoch 502/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 118.1193 - mae: 118.8064 - val_loss: 1519.1029 - val_mae: 1519.7958\n",
      "Epoch 503/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 117.2903 - mae: 117.9768 - val_loss: 1353.1150 - val_mae: 1353.8080\n",
      "Epoch 504/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 115.9367 - mae: 116.6231 - val_loss: 1500.1377 - val_mae: 1500.8308\n",
      "Epoch 505/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 109.9309 - mae: 110.6148 - val_loss: 1400.7688 - val_mae: 1401.4606\n",
      "Epoch 506/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 109.9147 - mae: 110.5985 - val_loss: 1333.2769 - val_mae: 1333.9698\n",
      "Epoch 507/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 120.7643 - mae: 121.4504 - val_loss: 1436.7213 - val_mae: 1437.4139\n",
      "Epoch 508/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 118.2601 - mae: 118.9437 - val_loss: 1480.6080 - val_mae: 1481.3014\n",
      "Epoch 509/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 119.0856 - mae: 119.7703 - val_loss: 1252.8776 - val_mae: 1253.5707\n",
      "Epoch 510/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 121.9418 - mae: 122.6251 - val_loss: 1282.5598 - val_mae: 1283.2529\n",
      "Epoch 511/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 123.3367 - mae: 124.0223 - val_loss: 1438.1028 - val_mae: 1438.7958\n",
      "Epoch 512/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 110.2066 - mae: 110.8931 - val_loss: 1281.1758 - val_mae: 1281.8684\n",
      "Epoch 513/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 109.1440 - mae: 109.8284 - val_loss: 1191.1140 - val_mae: 1191.8065\n",
      "Epoch 514/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 143.2890 - mae: 143.9777 - val_loss: 1423.0735 - val_mae: 1423.7667\n",
      "Epoch 515/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 112.7737 - mae: 113.4607 - val_loss: 1231.8411 - val_mae: 1232.5341\n",
      "Epoch 516/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 113.8752 - mae: 114.5592 - val_loss: 1196.8201 - val_mae: 1197.5127\n",
      "Epoch 517/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 119.6140 - mae: 120.3003 - val_loss: 1419.0962 - val_mae: 1419.7893\n",
      "Epoch 518/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 127.8813 - mae: 128.5695 - val_loss: 1534.6475 - val_mae: 1535.3407\n",
      "Epoch 519/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 123.1815 - mae: 123.8667 - val_loss: 1368.3923 - val_mae: 1369.0854\n",
      "Epoch 520/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 117.5940 - mae: 118.2804 - val_loss: 1314.6754 - val_mae: 1315.3684\n",
      "Epoch 521/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 126.1598 - mae: 126.8451 - val_loss: 1231.1162 - val_mae: 1231.8082\n",
      "Epoch 522/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 109.6483 - mae: 110.3348 - val_loss: 1424.9666 - val_mae: 1425.6595\n",
      "Epoch 523/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 103.9705 - mae: 104.6507 - val_loss: 1197.5378 - val_mae: 1198.2300\n",
      "Epoch 524/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 112.6058 - mae: 113.2899 - val_loss: 1172.8735 - val_mae: 1173.5668\n",
      "Epoch 525/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 124.0035 - mae: 124.6870 - val_loss: 1354.3180 - val_mae: 1355.0112\n",
      "Epoch 526/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 118.3220 - mae: 119.0055 - val_loss: 1204.7046 - val_mae: 1205.3967\n",
      "Epoch 527/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 111.8116 - mae: 112.4971 - val_loss: 1373.2583 - val_mae: 1373.9518\n",
      "Epoch 528/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 106.4236 - mae: 107.1073 - val_loss: 1335.3383 - val_mae: 1336.0314\n",
      "Epoch 529/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 107.2627 - mae: 107.9460 - val_loss: 1403.2993 - val_mae: 1403.9919\n",
      "Epoch 530/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 112.4799 - mae: 113.1642 - val_loss: 1201.1526 - val_mae: 1201.8453\n",
      "Epoch 531/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 111.6281 - mae: 112.3120 - val_loss: 1468.3123 - val_mae: 1469.0049\n",
      "Epoch 532/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 108.5078 - mae: 109.1932 - val_loss: 1403.4707 - val_mae: 1404.1636\n",
      "Epoch 533/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 124.4656 - mae: 125.1517 - val_loss: 1430.5930 - val_mae: 1431.2861\n",
      "Epoch 534/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 113.4394 - mae: 114.1260 - val_loss: 1307.2672 - val_mae: 1307.9602\n",
      "Epoch 535/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 115.3989 - mae: 116.0843 - val_loss: 1404.2407 - val_mae: 1404.9338\n",
      "Epoch 536/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 114.1108 - mae: 114.7933 - val_loss: 1384.2426 - val_mae: 1384.9355\n",
      "Epoch 537/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 111.4373 - mae: 112.1237 - val_loss: 1302.1234 - val_mae: 1302.8167\n",
      "Epoch 538/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 119.7713 - mae: 120.4586 - val_loss: 1544.3030 - val_mae: 1544.9958\n",
      "Epoch 539/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 110.1825 - mae: 110.8699 - val_loss: 1241.8964 - val_mae: 1242.5895\n",
      "Epoch 540/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 109.6959 - mae: 110.3813 - val_loss: 1264.2014 - val_mae: 1264.8936\n",
      "Epoch 541/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 109.7152 - mae: 110.3999 - val_loss: 1115.3254 - val_mae: 1116.0181\n",
      "Epoch 542/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 112.2852 - mae: 112.9708 - val_loss: 1555.8165 - val_mae: 1556.5096\n",
      "Epoch 543/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 120.7815 - mae: 121.4675 - val_loss: 1429.7460 - val_mae: 1430.4391\n",
      "Epoch 544/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 118.6391 - mae: 119.3237 - val_loss: 1204.5978 - val_mae: 1205.2911\n",
      "Epoch 545/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 113.5305 - mae: 114.2169 - val_loss: 1698.8302 - val_mae: 1699.5233\n",
      "Epoch 546/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 121.1671 - mae: 121.8537 - val_loss: 1284.6617 - val_mae: 1285.3539\n",
      "Epoch 547/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 120.5008 - mae: 121.1863 - val_loss: 1515.7179 - val_mae: 1516.4099\n",
      "Epoch 548/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 114.2112 - mae: 114.8981 - val_loss: 1261.4468 - val_mae: 1262.1400\n",
      "Epoch 549/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 114.2402 - mae: 114.9242 - val_loss: 1354.1831 - val_mae: 1354.8756\n",
      "Epoch 550/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 110.2123 - mae: 110.8977 - val_loss: 1332.2660 - val_mae: 1332.9590\n",
      "Epoch 551/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 110.0763 - mae: 110.7643 - val_loss: 1260.0485 - val_mae: 1260.7408\n",
      "Epoch 552/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 113.4833 - mae: 114.1697 - val_loss: 1250.3953 - val_mae: 1251.0884\n",
      "Epoch 553/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 115.6280 - mae: 116.3143 - val_loss: 1188.5133 - val_mae: 1189.2063\n",
      "Epoch 554/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 117.8039 - mae: 118.4863 - val_loss: 1360.4288 - val_mae: 1361.1219\n",
      "Epoch 555/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 121.9992 - mae: 122.6853 - val_loss: 1265.5770 - val_mae: 1266.2694\n",
      "Epoch 556/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 110.4957 - mae: 111.1797 - val_loss: 1325.2891 - val_mae: 1325.9818\n",
      "Epoch 557/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 110.5956 - mae: 111.2802 - val_loss: 1528.9291 - val_mae: 1529.6223\n",
      "Epoch 558/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 112.1491 - mae: 112.8335 - val_loss: 1211.1847 - val_mae: 1211.8781\n",
      "Epoch 559/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 117.3727 - mae: 118.0578 - val_loss: 1322.7645 - val_mae: 1323.4567\n",
      "Epoch 560/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 110.1468 - mae: 110.8321 - val_loss: 1129.4283 - val_mae: 1130.1198\n",
      "Epoch 561/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 106.2728 - mae: 106.9571 - val_loss: 1298.1821 - val_mae: 1298.8735\n",
      "Epoch 562/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 112.3124 - mae: 112.9961 - val_loss: 1458.2975 - val_mae: 1458.9907\n",
      "Epoch 563/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 109.1298 - mae: 109.8160 - val_loss: 1509.2637 - val_mae: 1509.9568\n",
      "Epoch 564/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 118.8404 - mae: 119.5266 - val_loss: 1113.4546 - val_mae: 1114.1477\n",
      "Epoch 565/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 114.2082 - mae: 114.8906 - val_loss: 1469.3698 - val_mae: 1470.0630\n",
      "Epoch 566/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 123.3311 - mae: 124.0189 - val_loss: 1376.9609 - val_mae: 1377.6539\n",
      "Epoch 567/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 111.8800 - mae: 112.5656 - val_loss: 1319.2078 - val_mae: 1319.8999\n",
      "Epoch 568/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 113.2969 - mae: 113.9841 - val_loss: 1106.4423 - val_mae: 1107.1349\n",
      "Epoch 569/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 111.2972 - mae: 111.9801 - val_loss: 1293.0538 - val_mae: 1293.7469\n",
      "Epoch 570/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 107.3488 - mae: 108.0310 - val_loss: 1097.3771 - val_mae: 1098.0701\n",
      "Epoch 571/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 112.0850 - mae: 112.7714 - val_loss: 1262.2308 - val_mae: 1262.9241\n",
      "Epoch 572/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 125.7118 - mae: 126.3947 - val_loss: 1081.9578 - val_mae: 1082.6506\n",
      "Epoch 573/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 116.5837 - mae: 117.2701 - val_loss: 1174.0400 - val_mae: 1174.7327\n",
      "Epoch 574/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 115.7249 - mae: 116.4070 - val_loss: 1368.6885 - val_mae: 1369.3815\n",
      "Epoch 575/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 112.1500 - mae: 112.8364 - val_loss: 1353.0413 - val_mae: 1353.7343\n",
      "Epoch 576/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 108.2495 - mae: 108.9316 - val_loss: 1562.9423 - val_mae: 1563.6354\n",
      "Epoch 577/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 110.4409 - mae: 111.1257 - val_loss: 1235.0981 - val_mae: 1235.7915\n",
      "Epoch 578/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 110.4827 - mae: 111.1659 - val_loss: 1207.9236 - val_mae: 1208.6158\n",
      "Epoch 579/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 110.6705 - mae: 111.3523 - val_loss: 1275.4435 - val_mae: 1276.1367\n",
      "Epoch 580/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 119.7978 - mae: 120.4807 - val_loss: 1125.9982 - val_mae: 1126.6910\n",
      "Epoch 581/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 118.0006 - mae: 118.6863 - val_loss: 1521.9021 - val_mae: 1522.5948\n",
      "Epoch 582/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 138.5495 - mae: 139.2357 - val_loss: 1352.4568 - val_mae: 1353.1493\n",
      "Epoch 583/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 111.5265 - mae: 112.2114 - val_loss: 1518.8121 - val_mae: 1519.5052\n",
      "Epoch 584/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 119.4952 - mae: 120.1803 - val_loss: 1107.9596 - val_mae: 1108.6525\n",
      "Epoch 585/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 121.9443 - mae: 122.6333 - val_loss: 1142.1812 - val_mae: 1142.8729\n",
      "Epoch 586/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 113.4293 - mae: 114.1143 - val_loss: 1221.2314 - val_mae: 1221.9243\n",
      "Epoch 587/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 114.4862 - mae: 115.1741 - val_loss: 1428.6333 - val_mae: 1429.3267\n",
      "Epoch 588/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 112.4028 - mae: 113.0870 - val_loss: 1161.5215 - val_mae: 1162.2147\n",
      "Epoch 589/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 113.9357 - mae: 114.6187 - val_loss: 1222.1744 - val_mae: 1222.8678\n",
      "Epoch 590/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 123.2795 - mae: 123.9650 - val_loss: 1316.5997 - val_mae: 1317.2916\n",
      "Epoch 591/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 114.2220 - mae: 114.9076 - val_loss: 1241.9697 - val_mae: 1242.6615\n",
      "Epoch 592/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 105.5151 - mae: 106.1965 - val_loss: 1151.3616 - val_mae: 1152.0547\n",
      "Epoch 593/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 112.8313 - mae: 113.5148 - val_loss: 1377.6855 - val_mae: 1378.3787\n",
      "Epoch 594/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 120.9080 - mae: 121.5964 - val_loss: 1485.2877 - val_mae: 1485.9803\n",
      "Epoch 595/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 121.7647 - mae: 122.4484 - val_loss: 1263.0042 - val_mae: 1263.6973\n",
      "Epoch 596/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 108.1861 - mae: 108.8704 - val_loss: 1399.5571 - val_mae: 1400.2502\n",
      "Epoch 597/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 107.2564 - mae: 107.9389 - val_loss: 1298.9760 - val_mae: 1299.6691\n",
      "Epoch 598/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 108.7756 - mae: 109.4612 - val_loss: 1351.9983 - val_mae: 1352.6904\n",
      "Epoch 599/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 111.3456 - mae: 112.0315 - val_loss: 1260.6428 - val_mae: 1261.3352\n",
      "Epoch 600/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 110.5871 - mae: 111.2710 - val_loss: 1299.4656 - val_mae: 1300.1587\n",
      "Epoch 601/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 105.1330 - mae: 105.8173 - val_loss: 1467.8591 - val_mae: 1468.5522\n",
      "Epoch 602/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 108.0866 - mae: 108.7722 - val_loss: 1188.4259 - val_mae: 1189.1184\n",
      "Epoch 603/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 113.1992 - mae: 113.8837 - val_loss: 1605.7010 - val_mae: 1606.3942\n",
      "Epoch 604/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 119.6617 - mae: 120.3457 - val_loss: 1415.4756 - val_mae: 1416.1687\n",
      "Epoch 605/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 122.6427 - mae: 123.3288 - val_loss: 1373.4584 - val_mae: 1374.1519\n",
      "Epoch 606/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 109.6383 - mae: 110.3242 - val_loss: 1503.0546 - val_mae: 1503.7476\n",
      "Epoch 607/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 118.2701 - mae: 118.9568 - val_loss: 1151.7511 - val_mae: 1152.4441\n",
      "Epoch 608/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 124.0936 - mae: 124.7794 - val_loss: 1491.8666 - val_mae: 1492.5591\n",
      "Epoch 609/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 117.7269 - mae: 118.4141 - val_loss: 1518.5376 - val_mae: 1519.2308\n",
      "Epoch 610/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 114.5239 - mae: 115.2091 - val_loss: 1577.6022 - val_mae: 1578.2950\n",
      "Epoch 611/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 119.7005 - mae: 120.3854 - val_loss: 1219.5093 - val_mae: 1220.2024\n",
      "Epoch 612/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 114.6784 - mae: 115.3628 - val_loss: 1057.6129 - val_mae: 1058.3062\n",
      "Epoch 613/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 128.5801 - mae: 129.2665 - val_loss: 1277.4513 - val_mae: 1278.1443\n",
      "Epoch 614/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 114.1072 - mae: 114.7906 - val_loss: 1466.7643 - val_mae: 1467.4575\n",
      "Epoch 615/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 108.4513 - mae: 109.1349 - val_loss: 1481.0092 - val_mae: 1481.7024\n",
      "Epoch 616/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 111.1840 - mae: 111.8695 - val_loss: 1297.6222 - val_mae: 1298.3153\n",
      "Epoch 617/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 107.2874 - mae: 107.9731 - val_loss: 1456.0687 - val_mae: 1456.7618\n",
      "Epoch 618/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 104.3196 - mae: 105.0059 - val_loss: 1249.7819 - val_mae: 1250.4740\n",
      "Epoch 619/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 102.6806 - mae: 103.3626 - val_loss: 1676.4969 - val_mae: 1677.1902\n",
      "Epoch 620/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 121.1828 - mae: 121.8672 - val_loss: 1378.0087 - val_mae: 1378.7019\n",
      "Epoch 621/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 113.3084 - mae: 113.9928 - val_loss: 1200.3544 - val_mae: 1201.0466\n",
      "Epoch 622/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 108.3423 - mae: 109.0255 - val_loss: 1505.6132 - val_mae: 1506.3064\n",
      "Epoch 623/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 105.9999 - mae: 106.6851 - val_loss: 1461.3392 - val_mae: 1462.0322\n",
      "Epoch 624/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 115.4022 - mae: 116.0900 - val_loss: 1123.8638 - val_mae: 1124.5569\n",
      "Epoch 625/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 113.1414 - mae: 113.8254 - val_loss: 1404.4551 - val_mae: 1405.1482\n",
      "Epoch 626/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 106.7916 - mae: 107.4760 - val_loss: 1141.5233 - val_mae: 1142.2161\n",
      "Epoch 627/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 113.2740 - mae: 113.9586 - val_loss: 1456.1090 - val_mae: 1456.8025\n",
      "Epoch 628/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 130.0898 - mae: 130.7737 - val_loss: 1508.6136 - val_mae: 1509.3068\n",
      "Epoch 629/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 118.5318 - mae: 119.2167 - val_loss: 1313.6022 - val_mae: 1314.2952\n",
      "Epoch 630/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 122.7365 - mae: 123.4219 - val_loss: 1156.7922 - val_mae: 1157.4840\n",
      "Epoch 631/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 109.0494 - mae: 109.7343 - val_loss: 1339.4698 - val_mae: 1340.1627\n",
      "Epoch 632/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 121.2087 - mae: 121.8936 - val_loss: 1483.7954 - val_mae: 1484.4889\n",
      "Epoch 633/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 106.8967 - mae: 107.5844 - val_loss: 1473.7482 - val_mae: 1474.4415\n",
      "Epoch 634/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 109.3110 - mae: 109.9977 - val_loss: 1411.2397 - val_mae: 1411.9327\n",
      "Epoch 635/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 103.1697 - mae: 103.8542 - val_loss: 1421.3378 - val_mae: 1422.0304\n",
      "Epoch 636/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 108.8667 - mae: 109.5506 - val_loss: 1310.4412 - val_mae: 1311.1332\n",
      "Epoch 637/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 119.8580 - mae: 120.5431 - val_loss: 1303.0531 - val_mae: 1303.7456\n",
      "Epoch 638/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 112.7915 - mae: 113.4727 - val_loss: 1498.5355 - val_mae: 1499.2288\n",
      "Epoch 639/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 108.2600 - mae: 108.9456 - val_loss: 1475.8419 - val_mae: 1476.5350\n",
      "Epoch 640/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 115.8621 - mae: 116.5477 - val_loss: 1314.3776 - val_mae: 1315.0707\n",
      "Epoch 641/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 104.8976 - mae: 105.5821 - val_loss: 1262.2200 - val_mae: 1262.9121\n",
      "Epoch 642/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 106.6051 - mae: 107.2883 - val_loss: 1170.3093 - val_mae: 1171.0024\n",
      "Epoch 643/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 109.4474 - mae: 110.1341 - val_loss: 1586.8273 - val_mae: 1587.5199\n",
      "Epoch 644/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 112.0395 - mae: 112.7225 - val_loss: 1232.7871 - val_mae: 1233.4803\n",
      "Epoch 645/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 108.4567 - mae: 109.1416 - val_loss: 1456.3384 - val_mae: 1457.0306\n",
      "Epoch 646/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 103.8677 - mae: 104.5527 - val_loss: 1341.5121 - val_mae: 1342.2052\n",
      "Epoch 647/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 108.1530 - mae: 108.8360 - val_loss: 1602.0566 - val_mae: 1602.7499\n",
      "Epoch 648/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 111.2650 - mae: 111.9501 - val_loss: 1390.2430 - val_mae: 1390.9354\n",
      "Epoch 649/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 108.3213 - mae: 109.0043 - val_loss: 1404.3372 - val_mae: 1405.0286\n",
      "Epoch 650/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 114.3835 - mae: 115.0698 - val_loss: 1240.6713 - val_mae: 1241.3634\n",
      "Epoch 651/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 113.6853 - mae: 114.3710 - val_loss: 1429.8813 - val_mae: 1430.5745\n",
      "Epoch 652/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 114.2405 - mae: 114.9211 - val_loss: 1523.3057 - val_mae: 1523.9987\n",
      "Epoch 653/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 112.2459 - mae: 112.9261 - val_loss: 1216.0554 - val_mae: 1216.7482\n",
      "Epoch 654/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 106.9682 - mae: 107.6524 - val_loss: 1517.2281 - val_mae: 1517.9213\n",
      "Epoch 655/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 112.8158 - mae: 113.4988 - val_loss: 1625.9285 - val_mae: 1626.6216\n",
      "Epoch 656/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 111.8949 - mae: 112.5793 - val_loss: 1226.1276 - val_mae: 1226.8208\n",
      "Epoch 657/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 109.7730 - mae: 110.4564 - val_loss: 1371.9337 - val_mae: 1372.6271\n",
      "Epoch 658/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 112.6222 - mae: 113.3069 - val_loss: 1341.6057 - val_mae: 1342.2980\n",
      "Epoch 659/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 127.3672 - mae: 128.0526 - val_loss: 1423.6133 - val_mae: 1424.3063\n",
      "Epoch 660/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 110.1574 - mae: 110.8383 - val_loss: 1411.7303 - val_mae: 1412.4233\n",
      "Epoch 661/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 108.0092 - mae: 108.6918 - val_loss: 1334.3673 - val_mae: 1335.0604\n",
      "Epoch 662/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 110.2202 - mae: 110.9050 - val_loss: 1197.3798 - val_mae: 1198.0723\n",
      "Epoch 663/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 123.4561 - mae: 124.1388 - val_loss: 1486.4453 - val_mae: 1487.1378\n",
      "Epoch 664/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 116.6584 - mae: 117.3422 - val_loss: 1179.6122 - val_mae: 1180.3054\n",
      "Epoch 665/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 107.5100 - mae: 108.1964 - val_loss: 1305.5831 - val_mae: 1306.2764\n",
      "Epoch 666/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 101.5557 - mae: 102.2382 - val_loss: 1306.8650 - val_mae: 1307.5581\n",
      "Epoch 667/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 111.5359 - mae: 112.2199 - val_loss: 1446.9586 - val_mae: 1447.6519\n",
      "Epoch 668/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 114.7065 - mae: 115.3936 - val_loss: 1466.6133 - val_mae: 1467.3063\n",
      "Epoch 669/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 125.0570 - mae: 125.7443 - val_loss: 1330.1005 - val_mae: 1330.7928\n",
      "Epoch 670/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 112.0383 - mae: 112.7240 - val_loss: 1444.0203 - val_mae: 1444.7134\n",
      "Epoch 671/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 128.2713 - mae: 128.9571 - val_loss: 1197.9454 - val_mae: 1198.6381\n",
      "Epoch 672/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 127.4742 - mae: 128.1629 - val_loss: 1486.1564 - val_mae: 1486.8497\n",
      "Epoch 673/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 120.0519 - mae: 120.7362 - val_loss: 1498.2235 - val_mae: 1498.9155\n",
      "Epoch 674/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 112.0518 - mae: 112.7366 - val_loss: 1202.6614 - val_mae: 1203.3546\n",
      "Epoch 675/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 115.4448 - mae: 116.1319 - val_loss: 1169.3624 - val_mae: 1170.0552\n",
      "Epoch 676/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 116.5002 - mae: 117.1876 - val_loss: 1456.7562 - val_mae: 1457.4486\n",
      "Epoch 677/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 127.5080 - mae: 128.1951 - val_loss: 1450.7415 - val_mae: 1451.4344\n",
      "Epoch 678/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 108.0592 - mae: 108.7421 - val_loss: 1157.8016 - val_mae: 1158.4948\n",
      "Epoch 679/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 109.2990 - mae: 109.9852 - val_loss: 1395.4563 - val_mae: 1396.1493\n",
      "Epoch 680/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 112.7358 - mae: 113.4219 - val_loss: 1225.1075 - val_mae: 1225.8008\n",
      "Epoch 681/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 113.0925 - mae: 113.7789 - val_loss: 1226.2627 - val_mae: 1226.9559\n",
      "Epoch 682/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 110.7591 - mae: 111.4455 - val_loss: 1582.9083 - val_mae: 1583.6016\n",
      "Epoch 683/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 114.4214 - mae: 115.1069 - val_loss: 1296.6954 - val_mae: 1297.3888\n",
      "Epoch 684/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 119.2540 - mae: 119.9369 - val_loss: 1759.1237 - val_mae: 1759.8168\n",
      "Epoch 685/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 110.8714 - mae: 111.5556 - val_loss: 1369.8712 - val_mae: 1370.5643\n",
      "Epoch 686/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 102.9324 - mae: 103.6173 - val_loss: 1273.1904 - val_mae: 1273.8834\n",
      "Epoch 687/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 104.8942 - mae: 105.5776 - val_loss: 1422.3917 - val_mae: 1423.0841\n",
      "Epoch 688/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 109.0340 - mae: 109.7183 - val_loss: 1340.6575 - val_mae: 1341.3508\n",
      "Epoch 689/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 107.4572 - mae: 108.1428 - val_loss: 1354.8931 - val_mae: 1355.5857\n",
      "Epoch 690/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 113.2704 - mae: 113.9548 - val_loss: 1341.8142 - val_mae: 1342.5072\n",
      "Epoch 691/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 101.3612 - mae: 102.0454 - val_loss: 1370.9508 - val_mae: 1371.6439\n",
      "Epoch 692/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 113.0415 - mae: 113.7297 - val_loss: 1536.0303 - val_mae: 1536.7234\n",
      "Epoch 693/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 101.9289 - mae: 102.6142 - val_loss: 1264.9406 - val_mae: 1265.6333\n",
      "Epoch 694/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 111.6053 - mae: 112.2899 - val_loss: 1124.9597 - val_mae: 1125.6530\n",
      "Epoch 695/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 115.6910 - mae: 116.3759 - val_loss: 1696.3779 - val_mae: 1697.0713\n",
      "Epoch 696/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 107.4320 - mae: 108.1186 - val_loss: 1330.6887 - val_mae: 1331.3818\n",
      "Epoch 697/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 118.1614 - mae: 118.8470 - val_loss: 1372.9609 - val_mae: 1373.6528\n",
      "Epoch 698/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 108.1188 - mae: 108.8032 - val_loss: 1115.4579 - val_mae: 1116.1510\n",
      "Epoch 699/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 109.1443 - mae: 109.8274 - val_loss: 1446.4161 - val_mae: 1447.1093\n",
      "Epoch 700/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 102.4818 - mae: 103.1670 - val_loss: 1246.5013 - val_mae: 1247.1937\n",
      "Epoch 701/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 103.7216 - mae: 104.4068 - val_loss: 1213.9564 - val_mae: 1214.6487\n",
      "Epoch 702/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 109.5680 - mae: 110.2505 - val_loss: 1352.3566 - val_mae: 1353.0498\n",
      "Epoch 703/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 103.6830 - mae: 104.3676 - val_loss: 1303.1082 - val_mae: 1303.8002\n",
      "Epoch 704/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 110.4647 - mae: 111.1430 - val_loss: 1340.5745 - val_mae: 1341.2651\n",
      "Epoch 705/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 107.2876 - mae: 107.9700 - val_loss: 1405.6079 - val_mae: 1406.3010\n",
      "Epoch 706/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 110.8354 - mae: 111.5196 - val_loss: 1481.2312 - val_mae: 1481.9242\n",
      "Epoch 707/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 108.7676 - mae: 109.4518 - val_loss: 1212.3204 - val_mae: 1213.0137\n",
      "Epoch 708/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 105.4109 - mae: 106.0968 - val_loss: 1338.2651 - val_mae: 1338.9578\n",
      "Epoch 709/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 102.6399 - mae: 103.3272 - val_loss: 1425.6794 - val_mae: 1426.3726\n",
      "Epoch 710/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 111.3422 - mae: 112.0250 - val_loss: 1353.3649 - val_mae: 1354.0577\n",
      "Epoch 711/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 111.5684 - mae: 112.2551 - val_loss: 1401.1312 - val_mae: 1401.8231\n",
      "Epoch 712/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 108.9463 - mae: 109.6303 - val_loss: 1426.5074 - val_mae: 1427.2008\n",
      "Epoch 713/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 105.6980 - mae: 106.3793 - val_loss: 1478.2540 - val_mae: 1478.9459\n",
      "Epoch 714/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 104.7395 - mae: 105.4266 - val_loss: 1304.8777 - val_mae: 1305.5693\n",
      "Epoch 715/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 115.0724 - mae: 115.7584 - val_loss: 1409.1971 - val_mae: 1409.8904\n",
      "Epoch 716/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 107.2511 - mae: 107.9352 - val_loss: 1584.2899 - val_mae: 1584.9830\n",
      "Epoch 717/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 109.6246 - mae: 110.3090 - val_loss: 1618.4015 - val_mae: 1619.0933\n",
      "Epoch 718/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 115.6595 - mae: 116.3444 - val_loss: 1195.8969 - val_mae: 1196.5900\n",
      "Epoch 719/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 111.6647 - mae: 112.3494 - val_loss: 1230.6981 - val_mae: 1231.3909\n",
      "Epoch 720/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 112.9087 - mae: 113.5935 - val_loss: 1126.0729 - val_mae: 1126.7659\n",
      "Epoch 721/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 108.5719 - mae: 109.2528 - val_loss: 1447.8230 - val_mae: 1448.5155\n",
      "Epoch 722/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 112.3406 - mae: 113.0257 - val_loss: 1855.9290 - val_mae: 1856.6219\n",
      "Epoch 723/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 123.1618 - mae: 123.8455 - val_loss: 1230.8563 - val_mae: 1231.5485\n",
      "Epoch 724/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 104.7653 - mae: 105.4493 - val_loss: 1359.9188 - val_mae: 1360.6112\n",
      "Epoch 725/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 101.8290 - mae: 102.5130 - val_loss: 1330.6121 - val_mae: 1331.3054\n",
      "Epoch 726/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 101.4734 - mae: 102.1596 - val_loss: 1308.2562 - val_mae: 1308.9486\n",
      "Epoch 727/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 113.9675 - mae: 114.6528 - val_loss: 1256.7408 - val_mae: 1257.4340\n",
      "Epoch 728/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 104.1859 - mae: 104.8705 - val_loss: 1434.5162 - val_mae: 1435.2094\n",
      "Epoch 729/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 108.1530 - mae: 108.8361 - val_loss: 1259.0795 - val_mae: 1259.7726\n",
      "Epoch 730/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 104.5125 - mae: 105.1968 - val_loss: 1427.2689 - val_mae: 1427.9620\n",
      "Epoch 731/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 122.9448 - mae: 123.6318 - val_loss: 1354.4662 - val_mae: 1355.1583\n",
      "Epoch 732/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 103.1347 - mae: 103.8181 - val_loss: 1220.2218 - val_mae: 1220.9152\n",
      "Epoch 733/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 114.2888 - mae: 114.9747 - val_loss: 1249.6328 - val_mae: 1250.3262\n",
      "Epoch 734/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 105.6848 - mae: 106.3675 - val_loss: 1318.2699 - val_mae: 1318.9630\n",
      "Epoch 735/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 103.8164 - mae: 104.5010 - val_loss: 1522.0752 - val_mae: 1522.7682\n",
      "Epoch 736/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 109.3924 - mae: 110.0764 - val_loss: 1343.5352 - val_mae: 1344.2272\n",
      "Epoch 737/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 110.7543 - mae: 111.4374 - val_loss: 1395.0519 - val_mae: 1395.7449\n",
      "Epoch 738/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 118.0554 - mae: 118.7405 - val_loss: 1232.3973 - val_mae: 1233.0903\n",
      "Epoch 739/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 111.8314 - mae: 112.5149 - val_loss: 1225.3242 - val_mae: 1226.0160\n",
      "Epoch 740/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 104.2189 - mae: 104.8987 - val_loss: 1572.4176 - val_mae: 1573.1102\n",
      "Epoch 741/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 111.4219 - mae: 112.1070 - val_loss: 1442.8943 - val_mae: 1443.5875\n",
      "Epoch 742/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 103.7095 - mae: 104.3943 - val_loss: 1445.3151 - val_mae: 1446.0083\n",
      "Epoch 743/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 100.3677 - mae: 101.0514 - val_loss: 1211.3591 - val_mae: 1212.0520\n",
      "Epoch 744/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 98.8067 - mae: 99.4892 - val_loss: 1433.5668 - val_mae: 1434.2590\n",
      "Epoch 745/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 114.7540 - mae: 115.4401 - val_loss: 1480.6936 - val_mae: 1481.3859\n",
      "Epoch 746/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 106.2993 - mae: 106.9848 - val_loss: 1267.0809 - val_mae: 1267.7742\n",
      "Epoch 747/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 100.8384 - mae: 101.5250 - val_loss: 1311.1914 - val_mae: 1311.8838\n",
      "Epoch 748/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 98.6621 - mae: 99.3421 - val_loss: 1285.4849 - val_mae: 1286.1775\n",
      "Epoch 749/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 109.6320 - mae: 110.3166 - val_loss: 1151.9418 - val_mae: 1152.6346\n",
      "Epoch 750/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 106.2032 - mae: 106.8864 - val_loss: 1414.7471 - val_mae: 1415.4379\n",
      "Epoch 751/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 100.5449 - mae: 101.2270 - val_loss: 1386.0126 - val_mae: 1386.7032\n",
      "Epoch 752/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 100.0933 - mae: 100.7776 - val_loss: 1453.3313 - val_mae: 1454.0244\n",
      "Epoch 753/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 124.2176 - mae: 124.9021 - val_loss: 1167.7097 - val_mae: 1168.4030\n",
      "Epoch 754/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 138.9581 - mae: 139.6467 - val_loss: 1756.3862 - val_mae: 1757.0789\n",
      "Epoch 755/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 127.8413 - mae: 128.5271 - val_loss: 1215.5972 - val_mae: 1216.2904\n",
      "Epoch 756/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 120.2116 - mae: 120.8979 - val_loss: 1406.5596 - val_mae: 1407.2524\n",
      "Epoch 757/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 108.7046 - mae: 109.3920 - val_loss: 1553.4482 - val_mae: 1554.1415\n",
      "Epoch 758/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 106.5467 - mae: 107.2317 - val_loss: 1436.4150 - val_mae: 1437.1083\n",
      "Epoch 759/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 114.5867 - mae: 115.2733 - val_loss: 1196.2156 - val_mae: 1196.9086\n",
      "Epoch 760/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 108.6878 - mae: 109.3729 - val_loss: 1162.7107 - val_mae: 1163.4038\n",
      "Epoch 761/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 106.5089 - mae: 107.1935 - val_loss: 1283.2971 - val_mae: 1283.9891\n",
      "Epoch 762/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 99.0226 - mae: 99.7063 - val_loss: 1488.2484 - val_mae: 1488.9417\n",
      "Epoch 763/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 110.5737 - mae: 111.2565 - val_loss: 1430.7687 - val_mae: 1431.4606\n",
      "Epoch 764/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 117.6791 - mae: 118.3626 - val_loss: 1544.4419 - val_mae: 1545.1340\n",
      "Epoch 765/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 111.7128 - mae: 112.3971 - val_loss: 1418.8308 - val_mae: 1419.5242\n",
      "Epoch 766/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 104.4040 - mae: 105.0891 - val_loss: 1456.5559 - val_mae: 1457.2484\n",
      "Epoch 767/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 101.3060 - mae: 101.9901 - val_loss: 1134.0898 - val_mae: 1134.7831\n",
      "Epoch 768/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 102.3537 - mae: 103.0371 - val_loss: 1496.4196 - val_mae: 1497.1129\n",
      "Epoch 769/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 103.6161 - mae: 104.3014 - val_loss: 1463.9148 - val_mae: 1464.6077\n",
      "Epoch 770/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 102.3481 - mae: 103.0322 - val_loss: 1250.5676 - val_mae: 1251.2594\n",
      "Epoch 771/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 106.5152 - mae: 107.1971 - val_loss: 1394.4792 - val_mae: 1395.1718\n",
      "Epoch 772/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 106.1014 - mae: 106.7831 - val_loss: 1155.0339 - val_mae: 1155.7272\n",
      "Epoch 773/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 102.0220 - mae: 102.7067 - val_loss: 1390.2805 - val_mae: 1390.9706\n",
      "Epoch 774/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 111.1676 - mae: 111.8516 - val_loss: 1204.5574 - val_mae: 1205.2505\n",
      "Epoch 775/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 104.8562 - mae: 105.5394 - val_loss: 1383.1948 - val_mae: 1383.8878\n",
      "Epoch 776/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 116.6016 - mae: 117.2870 - val_loss: 1580.9940 - val_mae: 1581.6871\n",
      "Epoch 777/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 102.7059 - mae: 103.3921 - val_loss: 1276.9615 - val_mae: 1277.6549\n",
      "Epoch 778/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 106.1651 - mae: 106.8514 - val_loss: 1458.6270 - val_mae: 1459.3187\n",
      "Epoch 779/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 105.7988 - mae: 106.4844 - val_loss: 1426.9729 - val_mae: 1427.6646\n",
      "Epoch 780/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 102.6877 - mae: 103.3732 - val_loss: 1312.3892 - val_mae: 1313.0823\n",
      "Epoch 781/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 109.7959 - mae: 110.4788 - val_loss: 1278.0164 - val_mae: 1278.7092\n",
      "Epoch 782/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 106.3629 - mae: 107.0476 - val_loss: 1501.0436 - val_mae: 1501.7369\n",
      "Epoch 783/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 108.0507 - mae: 108.7359 - val_loss: 1487.9314 - val_mae: 1488.6238\n",
      "Epoch 784/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 105.9725 - mae: 106.6541 - val_loss: 1331.8376 - val_mae: 1332.5308\n",
      "Epoch 785/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 112.7696 - mae: 113.4564 - val_loss: 1299.3896 - val_mae: 1300.0828\n",
      "Epoch 786/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 111.0250 - mae: 111.7121 - val_loss: 1254.0599 - val_mae: 1254.7532\n",
      "Epoch 787/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 103.0463 - mae: 103.7295 - val_loss: 1388.0189 - val_mae: 1388.7117\n",
      "Epoch 788/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 123.2368 - mae: 123.9180 - val_loss: 1539.6057 - val_mae: 1540.2987\n",
      "Epoch 789/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 99.3232 - mae: 100.0066 - val_loss: 1312.3870 - val_mae: 1313.0802\n",
      "Epoch 790/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 99.7861 - mae: 100.4698 - val_loss: 1417.0042 - val_mae: 1417.6969\n",
      "Epoch 791/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 104.4970 - mae: 105.1808 - val_loss: 1574.6000 - val_mae: 1575.2922\n",
      "Epoch 792/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 100.3371 - mae: 101.0204 - val_loss: 1462.6062 - val_mae: 1463.2993\n",
      "Epoch 793/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 101.4235 - mae: 102.1088 - val_loss: 1433.1420 - val_mae: 1433.8352\n",
      "Epoch 794/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 111.2444 - mae: 111.9295 - val_loss: 1287.3125 - val_mae: 1288.0057\n",
      "Epoch 795/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 100.1686 - mae: 100.8508 - val_loss: 1318.9918 - val_mae: 1319.6842\n",
      "Epoch 796/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 102.5167 - mae: 103.2024 - val_loss: 1296.2511 - val_mae: 1296.9443\n",
      "Epoch 797/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 101.2542 - mae: 101.9367 - val_loss: 1263.8209 - val_mae: 1264.5142\n",
      "Epoch 798/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 112.6891 - mae: 113.3730 - val_loss: 1492.1644 - val_mae: 1492.8571\n",
      "Epoch 799/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 116.1562 - mae: 116.8408 - val_loss: 1574.0375 - val_mae: 1574.7302\n",
      "Epoch 800/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 101.8574 - mae: 102.5389 - val_loss: 1370.8264 - val_mae: 1371.5182\n",
      "Epoch 801/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 105.1045 - mae: 105.7895 - val_loss: 1262.5710 - val_mae: 1263.2637\n",
      "Epoch 802/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 106.5637 - mae: 107.2475 - val_loss: 1238.1346 - val_mae: 1238.8278\n",
      "Epoch 803/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 103.5523 - mae: 104.2354 - val_loss: 1276.3082 - val_mae: 1277.0016\n",
      "Epoch 804/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 99.5497 - mae: 100.2353 - val_loss: 1582.6896 - val_mae: 1583.3816\n",
      "Epoch 805/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 110.2542 - mae: 110.9354 - val_loss: 1541.2029 - val_mae: 1541.8960\n",
      "Epoch 806/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 102.6152 - mae: 103.2978 - val_loss: 1376.1488 - val_mae: 1376.8418\n",
      "Epoch 807/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 100.9699 - mae: 101.6548 - val_loss: 1389.8644 - val_mae: 1390.5570\n",
      "Epoch 808/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 100.9536 - mae: 101.6383 - val_loss: 1282.8086 - val_mae: 1283.5017\n",
      "Epoch 809/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 103.0773 - mae: 103.7604 - val_loss: 1211.1189 - val_mae: 1211.8118\n",
      "Epoch 810/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 110.6136 - mae: 111.3018 - val_loss: 1312.9546 - val_mae: 1313.6477\n",
      "Epoch 811/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 100.6957 - mae: 101.3759 - val_loss: 1083.1794 - val_mae: 1083.8724\n",
      "Epoch 812/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 102.8613 - mae: 103.5455 - val_loss: 1262.4470 - val_mae: 1263.1398\n",
      "Epoch 813/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 102.4881 - mae: 103.1683 - val_loss: 1399.7511 - val_mae: 1400.4443\n",
      "Epoch 814/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 108.7599 - mae: 109.4425 - val_loss: 1519.5868 - val_mae: 1520.2800\n",
      "Epoch 815/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 104.0145 - mae: 104.6982 - val_loss: 1156.3441 - val_mae: 1157.0372\n",
      "Epoch 816/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 103.8282 - mae: 104.5098 - val_loss: 1519.9785 - val_mae: 1520.6715\n",
      "Epoch 817/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 101.8446 - mae: 102.5279 - val_loss: 1122.7583 - val_mae: 1123.4513\n",
      "Epoch 818/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 101.9460 - mae: 102.6333 - val_loss: 1188.8031 - val_mae: 1189.4961\n",
      "Epoch 819/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 95.5651 - mae: 96.2481 - val_loss: 1425.6641 - val_mae: 1426.3562\n",
      "Epoch 820/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 99.7420 - mae: 100.4251 - val_loss: 1218.1876 - val_mae: 1218.8807\n",
      "Epoch 821/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 105.4559 - mae: 106.1416 - val_loss: 1422.8075 - val_mae: 1423.4999\n",
      "Epoch 822/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 104.2335 - mae: 104.9205 - val_loss: 1245.7477 - val_mae: 1246.4408\n",
      "Epoch 823/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 104.2702 - mae: 104.9539 - val_loss: 1421.2764 - val_mae: 1421.9696\n",
      "Epoch 824/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 96.1825 - mae: 96.8644 - val_loss: 1424.0398 - val_mae: 1424.7316\n",
      "Epoch 825/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 101.2270 - mae: 101.9100 - val_loss: 1564.5065 - val_mae: 1565.1997\n",
      "Epoch 826/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 109.1057 - mae: 109.7896 - val_loss: 1152.3456 - val_mae: 1153.0375\n",
      "Epoch 827/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 108.1493 - mae: 108.8334 - val_loss: 1396.9366 - val_mae: 1397.6289\n",
      "Epoch 828/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 114.7735 - mae: 115.4580 - val_loss: 1221.6582 - val_mae: 1222.3514\n",
      "Epoch 829/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 107.8946 - mae: 108.5772 - val_loss: 1377.9979 - val_mae: 1378.6912\n",
      "Epoch 830/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 101.0782 - mae: 101.7626 - val_loss: 1419.2091 - val_mae: 1419.9012\n",
      "Epoch 831/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 101.4871 - mae: 102.1680 - val_loss: 1631.5510 - val_mae: 1632.2440\n",
      "Epoch 832/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 112.2453 - mae: 112.9315 - val_loss: 1222.9247 - val_mae: 1223.6172\n",
      "Epoch 833/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 103.8807 - mae: 104.5652 - val_loss: 1360.3523 - val_mae: 1361.0454\n",
      "Epoch 834/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 103.6755 - mae: 104.3624 - val_loss: 1382.9000 - val_mae: 1383.5934\n",
      "Epoch 835/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 104.0314 - mae: 104.7120 - val_loss: 1657.3257 - val_mae: 1658.0188\n",
      "Epoch 836/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 135.4709 - mae: 136.1601 - val_loss: 1302.6241 - val_mae: 1303.3168\n",
      "Epoch 837/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 120.8038 - mae: 121.4923 - val_loss: 1629.9412 - val_mae: 1630.6344\n",
      "Epoch 838/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 101.2468 - mae: 101.9282 - val_loss: 1268.8070 - val_mae: 1269.5002\n",
      "Epoch 839/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 100.5478 - mae: 101.2322 - val_loss: 1276.1741 - val_mae: 1276.8672\n",
      "Epoch 840/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 94.7447 - mae: 95.4308 - val_loss: 1570.8884 - val_mae: 1571.5815\n",
      "Epoch 841/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 106.9269 - mae: 107.6092 - val_loss: 1240.8103 - val_mae: 1241.5031\n",
      "Epoch 842/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 115.3330 - mae: 116.0204 - val_loss: 1255.0208 - val_mae: 1255.7139\n",
      "Epoch 843/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 101.5287 - mae: 102.2147 - val_loss: 1249.1918 - val_mae: 1249.8840\n",
      "Epoch 844/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 106.6499 - mae: 107.3362 - val_loss: 1337.9415 - val_mae: 1338.6346\n",
      "Epoch 845/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 105.7790 - mae: 106.4608 - val_loss: 1491.6489 - val_mae: 1492.3423\n",
      "Epoch 846/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 101.3570 - mae: 102.0427 - val_loss: 1320.1541 - val_mae: 1320.8472\n",
      "Epoch 847/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 107.3935 - mae: 108.0771 - val_loss: 1190.8230 - val_mae: 1191.5150\n",
      "Epoch 848/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 103.9200 - mae: 104.6004 - val_loss: 1161.1688 - val_mae: 1161.8619\n",
      "Epoch 849/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 103.4761 - mae: 104.1605 - val_loss: 1323.8147 - val_mae: 1324.5077\n",
      "Epoch 850/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 98.1447 - mae: 98.8265 - val_loss: 1563.5315 - val_mae: 1564.2241\n",
      "Epoch 851/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 101.2355 - mae: 101.9179 - val_loss: 1189.7883 - val_mae: 1190.4816\n",
      "Epoch 852/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 95.9460 - mae: 96.6301 - val_loss: 1429.8705 - val_mae: 1430.5636\n",
      "Epoch 853/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 109.0307 - mae: 109.7155 - val_loss: 1424.0679 - val_mae: 1424.7610\n",
      "Epoch 854/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 99.0493 - mae: 99.7309 - val_loss: 1180.9775 - val_mae: 1181.6700\n",
      "Epoch 855/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 99.9795 - mae: 100.6655 - val_loss: 1220.4211 - val_mae: 1221.1140\n",
      "Epoch 856/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 106.4528 - mae: 107.1358 - val_loss: 1331.8550 - val_mae: 1332.5468\n",
      "Epoch 857/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 100.4483 - mae: 101.1326 - val_loss: 1429.8206 - val_mae: 1430.5134\n",
      "Epoch 858/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 105.4780 - mae: 106.1650 - val_loss: 1365.5859 - val_mae: 1366.2789\n",
      "Epoch 859/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 99.3337 - mae: 100.0170 - val_loss: 1336.3920 - val_mae: 1337.0850\n",
      "Epoch 860/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 100.2210 - mae: 100.9011 - val_loss: 1366.9507 - val_mae: 1367.6427\n",
      "Epoch 861/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 97.6928 - mae: 98.3774 - val_loss: 1356.1047 - val_mae: 1356.7976\n",
      "Epoch 862/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 99.4404 - mae: 100.1243 - val_loss: 1328.5505 - val_mae: 1329.2438\n",
      "Epoch 863/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 100.3485 - mae: 101.0328 - val_loss: 1321.0823 - val_mae: 1321.7755\n",
      "Epoch 864/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 103.5872 - mae: 104.2723 - val_loss: 1399.9894 - val_mae: 1400.6820\n",
      "Epoch 865/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 102.7616 - mae: 103.4462 - val_loss: 1408.0032 - val_mae: 1408.6964\n",
      "Epoch 866/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 97.3208 - mae: 98.0038 - val_loss: 1297.3218 - val_mae: 1298.0142\n",
      "Epoch 867/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 98.1852 - mae: 98.8674 - val_loss: 1190.2322 - val_mae: 1190.9254\n",
      "Epoch 868/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 101.0825 - mae: 101.7687 - val_loss: 1325.6033 - val_mae: 1326.2965\n",
      "Epoch 869/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 102.2136 - mae: 102.8985 - val_loss: 1433.9868 - val_mae: 1434.6779\n",
      "Epoch 870/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 102.3342 - mae: 103.0217 - val_loss: 1302.9143 - val_mae: 1303.6049\n",
      "Epoch 871/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 99.4228 - mae: 100.1089 - val_loss: 1318.2930 - val_mae: 1318.9857\n",
      "Epoch 872/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 117.4109 - mae: 118.0971 - val_loss: 1235.4884 - val_mae: 1236.1815\n",
      "Epoch 873/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 107.3767 - mae: 108.0595 - val_loss: 1348.9568 - val_mae: 1349.6501\n",
      "Epoch 874/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 107.0491 - mae: 107.7335 - val_loss: 1494.9451 - val_mae: 1495.6382\n",
      "Epoch 875/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 125.6688 - mae: 126.3538 - val_loss: 1313.4691 - val_mae: 1314.1609\n",
      "Epoch 876/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 100.5628 - mae: 101.2468 - val_loss: 1385.0757 - val_mae: 1385.7688\n",
      "Epoch 877/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 104.4233 - mae: 105.1085 - val_loss: 1203.7375 - val_mae: 1204.4307\n",
      "Epoch 878/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 106.6236 - mae: 107.3089 - val_loss: 1296.4519 - val_mae: 1297.1439\n",
      "Epoch 879/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 104.8376 - mae: 105.5221 - val_loss: 1177.0020 - val_mae: 1177.6943\n",
      "Epoch 880/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 118.8116 - mae: 119.4976 - val_loss: 1304.5503 - val_mae: 1305.2432\n",
      "Epoch 881/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 97.0079 - mae: 97.6916 - val_loss: 1251.3655 - val_mae: 1252.0585\n",
      "Epoch 882/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 98.9981 - mae: 99.6831 - val_loss: 1471.9443 - val_mae: 1472.6375\n",
      "Epoch 883/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 102.4245 - mae: 103.1062 - val_loss: 1354.2174 - val_mae: 1354.9106\n",
      "Epoch 884/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 95.6766 - mae: 96.3634 - val_loss: 1375.9724 - val_mae: 1376.6654\n",
      "Epoch 885/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 99.6291 - mae: 100.3122 - val_loss: 1242.6768 - val_mae: 1243.3700\n",
      "Epoch 886/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 95.8939 - mae: 96.5760 - val_loss: 1486.2070 - val_mae: 1486.9000\n",
      "Epoch 887/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 107.2035 - mae: 107.8884 - val_loss: 1404.5479 - val_mae: 1405.2410\n",
      "Epoch 888/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 100.8718 - mae: 101.5548 - val_loss: 1422.6106 - val_mae: 1423.3022\n",
      "Epoch 889/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 100.7009 - mae: 101.3867 - val_loss: 1230.0872 - val_mae: 1230.7804\n",
      "Epoch 890/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 96.8659 - mae: 97.5490 - val_loss: 1276.7252 - val_mae: 1277.4186\n",
      "Epoch 891/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 105.9717 - mae: 106.6560 - val_loss: 1222.7266 - val_mae: 1223.4197\n",
      "Epoch 892/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 107.8148 - mae: 108.4979 - val_loss: 1196.4879 - val_mae: 1197.1804\n",
      "Epoch 893/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 102.9032 - mae: 103.5890 - val_loss: 1486.0345 - val_mae: 1486.7278\n",
      "Epoch 894/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 97.6458 - mae: 98.3291 - val_loss: 1262.2192 - val_mae: 1262.9120\n",
      "Epoch 895/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 98.3882 - mae: 99.0706 - val_loss: 1545.3812 - val_mae: 1546.0745\n",
      "Epoch 896/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 102.7275 - mae: 103.4094 - val_loss: 1359.9774 - val_mae: 1360.6705\n",
      "Epoch 897/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 103.4221 - mae: 104.1078 - val_loss: 1569.5240 - val_mae: 1570.2172\n",
      "Epoch 898/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 106.2158 - mae: 106.8991 - val_loss: 1155.3947 - val_mae: 1156.0878\n",
      "Epoch 899/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 100.0194 - mae: 100.7042 - val_loss: 1274.5817 - val_mae: 1275.2748\n",
      "Epoch 900/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 121.7428 - mae: 122.4296 - val_loss: 1256.1948 - val_mae: 1256.8881\n",
      "Epoch 901/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 116.7532 - mae: 117.4393 - val_loss: 1304.4081 - val_mae: 1305.1008\n",
      "Epoch 902/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 96.9942 - mae: 97.6786 - val_loss: 1298.1288 - val_mae: 1298.8210\n",
      "Epoch 903/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 102.5230 - mae: 103.2077 - val_loss: 1112.5211 - val_mae: 1113.2135\n",
      "Epoch 904/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 106.7888 - mae: 107.4735 - val_loss: 1171.2052 - val_mae: 1171.8965\n",
      "Epoch 905/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 101.2736 - mae: 101.9574 - val_loss: 1432.4965 - val_mae: 1433.1879\n",
      "Epoch 906/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 101.1756 - mae: 101.8637 - val_loss: 1412.8546 - val_mae: 1413.5477\n",
      "Epoch 907/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 101.7265 - mae: 102.4106 - val_loss: 1265.3087 - val_mae: 1266.0020\n",
      "Epoch 908/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 109.6095 - mae: 110.2956 - val_loss: 1305.8933 - val_mae: 1306.5865\n",
      "Epoch 909/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 108.9945 - mae: 109.6777 - val_loss: 1161.7781 - val_mae: 1162.4711\n",
      "Epoch 910/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 110.3834 - mae: 111.0712 - val_loss: 1412.5187 - val_mae: 1413.2106\n",
      "Epoch 911/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 103.1978 - mae: 103.8824 - val_loss: 1495.2169 - val_mae: 1495.9100\n",
      "Epoch 912/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 103.3861 - mae: 104.0695 - val_loss: 1461.1022 - val_mae: 1461.7954\n",
      "Epoch 913/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 105.4910 - mae: 106.1758 - val_loss: 1606.0839 - val_mae: 1606.7770\n",
      "Epoch 914/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 112.3847 - mae: 113.0685 - val_loss: 1349.4729 - val_mae: 1350.1661\n",
      "Epoch 915/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 105.2404 - mae: 105.9229 - val_loss: 1220.1598 - val_mae: 1220.8530\n",
      "Epoch 916/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 104.8792 - mae: 105.5657 - val_loss: 1279.5399 - val_mae: 1280.2330\n",
      "Epoch 917/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 97.7761 - mae: 98.4599 - val_loss: 1673.4291 - val_mae: 1674.1219\n",
      "Epoch 918/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 100.3700 - mae: 101.0528 - val_loss: 1475.5045 - val_mae: 1476.1970\n",
      "Epoch 919/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 103.0434 - mae: 103.7257 - val_loss: 1333.5808 - val_mae: 1334.2731\n",
      "Epoch 920/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 97.5171 - mae: 98.2043 - val_loss: 1253.6743 - val_mae: 1254.3668\n",
      "Epoch 921/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 105.1487 - mae: 105.8335 - val_loss: 1341.3536 - val_mae: 1342.0450\n",
      "Epoch 922/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 103.4909 - mae: 104.1740 - val_loss: 1400.9774 - val_mae: 1401.6697\n",
      "Epoch 923/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 96.2376 - mae: 96.9211 - val_loss: 1374.7021 - val_mae: 1375.3953\n",
      "Epoch 924/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 104.6510 - mae: 105.3342 - val_loss: 1264.1821 - val_mae: 1264.8752\n",
      "Epoch 925/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 102.5351 - mae: 103.2157 - val_loss: 1400.4879 - val_mae: 1401.1808\n",
      "Epoch 926/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 95.9169 - mae: 96.5998 - val_loss: 1231.2521 - val_mae: 1231.9445\n",
      "Epoch 927/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 97.8951 - mae: 98.5795 - val_loss: 1350.5336 - val_mae: 1351.2263\n",
      "Epoch 928/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 98.5202 - mae: 99.2034 - val_loss: 1206.8297 - val_mae: 1207.5229\n",
      "Epoch 929/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 104.9649 - mae: 105.6483 - val_loss: 1517.2169 - val_mae: 1517.9097\n",
      "Epoch 930/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 106.8512 - mae: 107.5368 - val_loss: 1345.8740 - val_mae: 1346.5673\n",
      "Epoch 931/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 107.5942 - mae: 108.2765 - val_loss: 1381.7612 - val_mae: 1382.4545\n",
      "Epoch 932/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 95.2462 - mae: 95.9295 - val_loss: 1282.3931 - val_mae: 1283.0854\n",
      "Epoch 933/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 94.3894 - mae: 95.0713 - val_loss: 1435.4624 - val_mae: 1436.1558\n",
      "Epoch 934/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 99.5184 - mae: 100.2039 - val_loss: 1586.8282 - val_mae: 1587.5214\n",
      "Epoch 935/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 102.7119 - mae: 103.3952 - val_loss: 1469.2620 - val_mae: 1469.9553\n",
      "Epoch 936/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 102.2713 - mae: 102.9539 - val_loss: 1466.7471 - val_mae: 1467.4388\n",
      "Epoch 937/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 106.7229 - mae: 107.4072 - val_loss: 1530.1191 - val_mae: 1530.8125\n",
      "Epoch 938/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 107.9643 - mae: 108.6484 - val_loss: 1343.2960 - val_mae: 1343.9893\n",
      "Epoch 939/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 99.8643 - mae: 100.5455 - val_loss: 1095.9425 - val_mae: 1096.6349\n",
      "Epoch 940/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 97.6027 - mae: 98.2875 - val_loss: 1435.1273 - val_mae: 1435.8197\n",
      "Epoch 941/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 97.5155 - mae: 98.2016 - val_loss: 1232.5677 - val_mae: 1233.2609\n",
      "Epoch 942/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 96.4364 - mae: 97.1215 - val_loss: 1233.3193 - val_mae: 1234.0116\n",
      "Epoch 943/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 99.1722 - mae: 99.8566 - val_loss: 1227.0493 - val_mae: 1227.7422\n",
      "Epoch 944/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 94.8775 - mae: 95.5617 - val_loss: 1327.5582 - val_mae: 1328.2511\n",
      "Epoch 945/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 94.9871 - mae: 95.6716 - val_loss: 1536.3589 - val_mae: 1537.0520\n",
      "Epoch 946/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 104.6379 - mae: 105.3224 - val_loss: 1724.7512 - val_mae: 1725.4440\n",
      "Epoch 947/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 104.9705 - mae: 105.6582 - val_loss: 1251.3242 - val_mae: 1252.0176\n",
      "Epoch 948/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 96.7343 - mae: 97.4166 - val_loss: 1416.3881 - val_mae: 1417.0811\n",
      "Epoch 949/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 102.3947 - mae: 103.0813 - val_loss: 1543.0153 - val_mae: 1543.7084\n",
      "Epoch 950/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 95.6868 - mae: 96.3704 - val_loss: 1226.5723 - val_mae: 1227.2654\n",
      "Epoch 951/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 94.6334 - mae: 95.3169 - val_loss: 1425.0408 - val_mae: 1425.7324\n",
      "Epoch 952/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 89.0361 - mae: 89.7169 - val_loss: 1246.9260 - val_mae: 1247.6185\n",
      "Epoch 953/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 96.9183 - mae: 97.6035 - val_loss: 1263.1364 - val_mae: 1263.8291\n",
      "Epoch 954/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 102.4033 - mae: 103.0882 - val_loss: 1315.7449 - val_mae: 1316.4381\n",
      "Epoch 955/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 106.5484 - mae: 107.2329 - val_loss: 1289.0168 - val_mae: 1289.7081\n",
      "Epoch 956/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 102.7665 - mae: 103.4521 - val_loss: 1491.3459 - val_mae: 1492.0392\n",
      "Epoch 957/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 95.9221 - mae: 96.6067 - val_loss: 1383.6554 - val_mae: 1384.3485\n",
      "Epoch 958/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 92.5653 - mae: 93.2450 - val_loss: 1437.5999 - val_mae: 1438.2930\n",
      "Epoch 959/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 93.9925 - mae: 94.6757 - val_loss: 1332.0239 - val_mae: 1332.7169\n",
      "Epoch 960/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 96.7804 - mae: 97.4629 - val_loss: 1474.0100 - val_mae: 1474.7034\n",
      "Epoch 961/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 97.2688 - mae: 97.9503 - val_loss: 1296.8442 - val_mae: 1297.5372\n",
      "Epoch 962/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 97.1071 - mae: 97.7907 - val_loss: 1310.1011 - val_mae: 1310.7931\n",
      "Epoch 963/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 96.8583 - mae: 97.5406 - val_loss: 1461.9489 - val_mae: 1462.6420\n",
      "Epoch 964/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 98.0437 - mae: 98.7287 - val_loss: 1491.6398 - val_mae: 1492.3330\n",
      "Epoch 965/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 100.1558 - mae: 100.8426 - val_loss: 1286.4546 - val_mae: 1287.1464\n",
      "Epoch 966/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 95.2593 - mae: 95.9429 - val_loss: 1338.0073 - val_mae: 1338.7007\n",
      "Epoch 967/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 101.5002 - mae: 102.1852 - val_loss: 1086.5015 - val_mae: 1087.1945\n",
      "Epoch 968/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 103.5585 - mae: 104.2410 - val_loss: 1084.1162 - val_mae: 1084.8082\n",
      "Epoch 969/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 101.0386 - mae: 101.7220 - val_loss: 1424.5081 - val_mae: 1425.2002\n",
      "Epoch 970/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 96.8202 - mae: 97.5030 - val_loss: 1402.1021 - val_mae: 1402.7928\n",
      "Epoch 971/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 96.1263 - mae: 96.8102 - val_loss: 1215.4762 - val_mae: 1216.1691\n",
      "Epoch 972/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 96.2109 - mae: 96.8968 - val_loss: 1577.6190 - val_mae: 1578.3125\n",
      "Epoch 973/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 103.5015 - mae: 104.1883 - val_loss: 1375.0129 - val_mae: 1375.7063\n",
      "Epoch 974/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 95.9594 - mae: 96.6423 - val_loss: 1352.3901 - val_mae: 1353.0817\n",
      "Epoch 975/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 94.2675 - mae: 94.9506 - val_loss: 1374.9012 - val_mae: 1375.5942\n",
      "Epoch 976/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 94.8590 - mae: 95.5403 - val_loss: 1302.2321 - val_mae: 1302.9244\n",
      "Epoch 977/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 95.0915 - mae: 95.7743 - val_loss: 1334.8966 - val_mae: 1335.5900\n",
      "Epoch 978/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 104.1878 - mae: 104.8733 - val_loss: 1349.4086 - val_mae: 1350.1010\n",
      "Epoch 979/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 110.6293 - mae: 111.3129 - val_loss: 1722.5038 - val_mae: 1723.1970\n",
      "Epoch 980/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 109.8370 - mae: 110.5219 - val_loss: 1289.6167 - val_mae: 1290.3086\n",
      "Epoch 981/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 99.6095 - mae: 100.2945 - val_loss: 1560.1105 - val_mae: 1560.8036\n",
      "Epoch 982/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 102.7592 - mae: 103.4439 - val_loss: 1409.9812 - val_mae: 1410.6744\n",
      "Epoch 983/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 100.1641 - mae: 100.8485 - val_loss: 1245.3915 - val_mae: 1246.0845\n",
      "Epoch 984/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 93.3110 - mae: 93.9952 - val_loss: 1499.8190 - val_mae: 1500.5118\n",
      "Epoch 985/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 104.7331 - mae: 105.4166 - val_loss: 1248.8455 - val_mae: 1249.5385\n",
      "Epoch 986/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 93.8691 - mae: 94.5503 - val_loss: 1377.2242 - val_mae: 1377.9169\n",
      "Epoch 987/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 113.3788 - mae: 114.0669 - val_loss: 1382.2111 - val_mae: 1382.9027\n",
      "Epoch 988/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 105.0727 - mae: 105.7585 - val_loss: 1298.5377 - val_mae: 1299.2308\n",
      "Epoch 989/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 99.8833 - mae: 100.5675 - val_loss: 1484.1877 - val_mae: 1484.8810\n",
      "Epoch 990/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 97.4541 - mae: 98.1371 - val_loss: 1335.6863 - val_mae: 1336.3794\n",
      "Epoch 991/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 89.7454 - mae: 90.4299 - val_loss: 1640.1539 - val_mae: 1640.8455\n",
      "Epoch 992/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 117.2578 - mae: 117.9463 - val_loss: 1254.1732 - val_mae: 1254.8665\n",
      "Epoch 993/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 96.5051 - mae: 97.1920 - val_loss: 1225.2384 - val_mae: 1225.9307\n",
      "Epoch 994/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 94.6989 - mae: 95.3822 - val_loss: 1296.3658 - val_mae: 1297.0591\n",
      "Epoch 995/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 99.2805 - mae: 99.9638 - val_loss: 1322.5488 - val_mae: 1323.2410\n",
      "Epoch 996/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 100.2615 - mae: 100.9451 - val_loss: 1429.2861 - val_mae: 1429.9794\n",
      "Epoch 997/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 95.1788 - mae: 95.8642 - val_loss: 1411.1022 - val_mae: 1411.7954\n",
      "Epoch 998/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 94.5558 - mae: 95.2399 - val_loss: 1186.0974 - val_mae: 1186.7908\n",
      "Epoch 999/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 94.8197 - mae: 95.5052 - val_loss: 1364.8259 - val_mae: 1365.5188\n",
      "Epoch 1000/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 102.3593 - mae: 103.0457 - val_loss: 1335.0216 - val_mae: 1335.7150\n",
      "Epoch 1001/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 98.8066 - mae: 99.4906 - val_loss: 1364.0935 - val_mae: 1364.7863\n",
      "Epoch 1002/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 93.9648 - mae: 94.6513 - val_loss: 1277.2469 - val_mae: 1277.9402\n",
      "Epoch 1003/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 101.2369 - mae: 101.9236 - val_loss: 1147.5706 - val_mae: 1148.2637\n",
      "Epoch 1004/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 111.4546 - mae: 112.1366 - val_loss: 1384.4894 - val_mae: 1385.1823\n",
      "Epoch 1005/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 103.5527 - mae: 104.2359 - val_loss: 1482.6737 - val_mae: 1483.3668\n",
      "Epoch 1006/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 98.5665 - mae: 99.2486 - val_loss: 1326.5321 - val_mae: 1327.2253\n",
      "Epoch 1007/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 89.8430 - mae: 90.5275 - val_loss: 1432.9481 - val_mae: 1433.6415\n",
      "Epoch 1008/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 100.5388 - mae: 101.2245 - val_loss: 1233.9458 - val_mae: 1234.6378\n",
      "Epoch 1009/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 107.8864 - mae: 108.5730 - val_loss: 1287.7764 - val_mae: 1288.4690\n",
      "Epoch 1010/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 96.6359 - mae: 97.3207 - val_loss: 1405.7332 - val_mae: 1406.4259\n",
      "Epoch 1011/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 105.1853 - mae: 105.8715 - val_loss: 1269.5022 - val_mae: 1270.1953\n",
      "Epoch 1012/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 92.5212 - mae: 93.2046 - val_loss: 1316.8386 - val_mae: 1317.5319\n",
      "Epoch 1013/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 96.9581 - mae: 97.6442 - val_loss: 1235.6652 - val_mae: 1236.3580\n",
      "Epoch 1014/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 97.4958 - mae: 98.1796 - val_loss: 1459.3754 - val_mae: 1460.0685\n",
      "Epoch 1015/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 99.4331 - mae: 100.1159 - val_loss: 1135.8171 - val_mae: 1136.5100\n",
      "Epoch 1016/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 117.6182 - mae: 118.3012 - val_loss: 1429.6876 - val_mae: 1430.3802\n",
      "Epoch 1017/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 99.7190 - mae: 100.4039 - val_loss: 1474.2974 - val_mae: 1474.9907\n",
      "Epoch 1018/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 96.6657 - mae: 97.3537 - val_loss: 1368.9371 - val_mae: 1369.6295\n",
      "Epoch 1019/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 104.1058 - mae: 104.7906 - val_loss: 1210.0671 - val_mae: 1210.7603\n",
      "Epoch 1020/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 102.4818 - mae: 103.1686 - val_loss: 1303.9015 - val_mae: 1304.5947\n",
      "Epoch 1021/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 99.7695 - mae: 100.4538 - val_loss: 1307.7343 - val_mae: 1308.4259\n",
      "Epoch 1022/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 97.6998 - mae: 98.3842 - val_loss: 1370.6366 - val_mae: 1371.3287\n",
      "Epoch 1023/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 95.1922 - mae: 95.8785 - val_loss: 1323.1882 - val_mae: 1323.8813\n",
      "Epoch 1024/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 100.6037 - mae: 101.2848 - val_loss: 1173.4464 - val_mae: 1174.1396\n",
      "Epoch 1025/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 98.4671 - mae: 99.1517 - val_loss: 1139.1471 - val_mae: 1139.8405\n",
      "Epoch 1026/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 104.7670 - mae: 105.4511 - val_loss: 1117.1104 - val_mae: 1117.8033\n",
      "Epoch 1027/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 104.8454 - mae: 105.5319 - val_loss: 1411.4686 - val_mae: 1412.1619\n",
      "Epoch 1028/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 97.6540 - mae: 98.3382 - val_loss: 1378.2903 - val_mae: 1378.9835\n",
      "Epoch 1029/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 93.7642 - mae: 94.4474 - val_loss: 1236.8671 - val_mae: 1237.5591\n",
      "Epoch 1030/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 99.3463 - mae: 100.0293 - val_loss: 1125.8750 - val_mae: 1126.5670\n",
      "Epoch 1031/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 98.0466 - mae: 98.7325 - val_loss: 1327.9220 - val_mae: 1328.6146\n",
      "Epoch 1032/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 101.0013 - mae: 101.6844 - val_loss: 1055.0276 - val_mae: 1055.7190\n",
      "Epoch 1033/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 96.7900 - mae: 97.4718 - val_loss: 1343.0145 - val_mae: 1343.7072\n",
      "Epoch 1034/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 92.7006 - mae: 93.3815 - val_loss: 1228.1458 - val_mae: 1228.8392\n",
      "Epoch 1035/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 98.9552 - mae: 99.6400 - val_loss: 1366.8999 - val_mae: 1367.5920\n",
      "Epoch 1036/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 88.4436 - mae: 89.1265 - val_loss: 1371.9170 - val_mae: 1372.6097\n",
      "Epoch 1037/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 93.7382 - mae: 94.4226 - val_loss: 1483.2380 - val_mae: 1483.9313\n",
      "Epoch 1038/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 92.6750 - mae: 93.3577 - val_loss: 1268.0646 - val_mae: 1268.7572\n",
      "Epoch 1039/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 102.3374 - mae: 103.0221 - val_loss: 1236.4446 - val_mae: 1237.1377\n",
      "Epoch 1040/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 102.7070 - mae: 103.3915 - val_loss: 1090.5135 - val_mae: 1091.2062\n",
      "Epoch 1041/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 88.7218 - mae: 89.4029 - val_loss: 1407.8245 - val_mae: 1408.5178\n",
      "Epoch 1042/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 93.2348 - mae: 93.9150 - val_loss: 1290.1785 - val_mae: 1290.8710\n",
      "Epoch 1043/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 98.0510 - mae: 98.7343 - val_loss: 1237.3699 - val_mae: 1238.0631\n",
      "Epoch 1044/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 100.4582 - mae: 101.1431 - val_loss: 1290.6335 - val_mae: 1291.3267\n",
      "Epoch 1045/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 98.1673 - mae: 98.8512 - val_loss: 1326.0413 - val_mae: 1326.7344\n",
      "Epoch 1046/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 97.0283 - mae: 97.7108 - val_loss: 1178.8086 - val_mae: 1179.5017\n",
      "Epoch 1047/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 94.4935 - mae: 95.1761 - val_loss: 1245.4730 - val_mae: 1246.1664\n",
      "Epoch 1048/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 100.3129 - mae: 100.9989 - val_loss: 1167.1785 - val_mae: 1167.8717\n",
      "Epoch 1049/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 96.4308 - mae: 97.1158 - val_loss: 1339.1306 - val_mae: 1339.8236\n",
      "Epoch 1050/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 91.0042 - mae: 91.6898 - val_loss: 1373.6365 - val_mae: 1374.3296\n",
      "Epoch 1051/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 95.2053 - mae: 95.8899 - val_loss: 1342.8940 - val_mae: 1343.5875\n",
      "Epoch 1052/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 92.5091 - mae: 93.1921 - val_loss: 1405.1554 - val_mae: 1405.8483\n",
      "Epoch 1053/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 104.6983 - mae: 105.3822 - val_loss: 1113.4303 - val_mae: 1114.1226\n",
      "Epoch 1054/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 93.2706 - mae: 93.9557 - val_loss: 1223.4012 - val_mae: 1224.0933\n",
      "Epoch 1055/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 92.5254 - mae: 93.2089 - val_loss: 1234.2047 - val_mae: 1234.8977\n",
      "Epoch 1056/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 87.2093 - mae: 87.8907 - val_loss: 1370.4366 - val_mae: 1371.1300\n",
      "Epoch 1057/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 96.7485 - mae: 97.4297 - val_loss: 1346.0822 - val_mae: 1346.7753\n",
      "Epoch 1058/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 97.6903 - mae: 98.3780 - val_loss: 1436.2454 - val_mae: 1436.9380\n",
      "Epoch 1059/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 93.8166 - mae: 94.4987 - val_loss: 1349.5759 - val_mae: 1350.2690\n",
      "Epoch 1060/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 91.8715 - mae: 92.5538 - val_loss: 1381.0559 - val_mae: 1381.7479\n",
      "Epoch 1061/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 104.3768 - mae: 105.0618 - val_loss: 1192.2950 - val_mae: 1192.9882\n",
      "Epoch 1062/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 108.0773 - mae: 108.7628 - val_loss: 1246.1851 - val_mae: 1246.8778\n",
      "Epoch 1063/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 95.5338 - mae: 96.2168 - val_loss: 1287.3099 - val_mae: 1288.0032\n",
      "Epoch 1064/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 89.3067 - mae: 89.9893 - val_loss: 1134.2635 - val_mae: 1134.9558\n",
      "Epoch 1065/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 96.6387 - mae: 97.3238 - val_loss: 1561.9883 - val_mae: 1562.6814\n",
      "Epoch 1066/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 93.1905 - mae: 93.8719 - val_loss: 1418.3904 - val_mae: 1419.0836\n",
      "Epoch 1067/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 93.0924 - mae: 93.7734 - val_loss: 1256.0004 - val_mae: 1256.6935\n",
      "Epoch 1068/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 96.7342 - mae: 97.4167 - val_loss: 1231.7385 - val_mae: 1232.4318\n",
      "Epoch 1069/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 94.7363 - mae: 95.4202 - val_loss: 1437.7190 - val_mae: 1438.4122\n",
      "Epoch 1070/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 102.3192 - mae: 103.0062 - val_loss: 1061.8080 - val_mae: 1062.5000\n",
      "Epoch 1071/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 111.0080 - mae: 111.6936 - val_loss: 1352.8575 - val_mae: 1353.5505\n",
      "Epoch 1072/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 109.4816 - mae: 110.1644 - val_loss: 1036.6740 - val_mae: 1037.3669\n",
      "Epoch 1073/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 103.8234 - mae: 104.5109 - val_loss: 1262.0284 - val_mae: 1262.7214\n",
      "Epoch 1074/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 97.3735 - mae: 98.0590 - val_loss: 1306.6726 - val_mae: 1307.3658\n",
      "Epoch 1075/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 107.5088 - mae: 108.1946 - val_loss: 1666.2046 - val_mae: 1666.8971\n",
      "Epoch 1076/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 108.2810 - mae: 108.9694 - val_loss: 1437.5029 - val_mae: 1438.1958\n",
      "Epoch 1077/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 124.6632 - mae: 125.3497 - val_loss: 1366.5225 - val_mae: 1367.2158\n",
      "Epoch 1078/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 102.0418 - mae: 102.7253 - val_loss: 1479.8428 - val_mae: 1480.5358\n",
      "Epoch 1079/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 97.4182 - mae: 98.1031 - val_loss: 1452.0371 - val_mae: 1452.7292\n",
      "Epoch 1080/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 113.0239 - mae: 113.7054 - val_loss: 1281.1854 - val_mae: 1281.8787\n",
      "Epoch 1081/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 127.6295 - mae: 128.3178 - val_loss: 1272.4182 - val_mae: 1273.1115\n",
      "Epoch 1082/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 90.9202 - mae: 91.6041 - val_loss: 1277.4738 - val_mae: 1278.1669\n",
      "Epoch 1083/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 98.4366 - mae: 99.1199 - val_loss: 1539.5629 - val_mae: 1540.2562\n",
      "Epoch 1084/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 102.2337 - mae: 102.9166 - val_loss: 1170.5376 - val_mae: 1171.2306\n",
      "Epoch 1085/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 100.7889 - mae: 101.4724 - val_loss: 1444.8711 - val_mae: 1445.5640\n",
      "Epoch 1086/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 91.8790 - mae: 92.5618 - val_loss: 1159.1344 - val_mae: 1159.8258\n",
      "Epoch 1087/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 97.1126 - mae: 97.7964 - val_loss: 1244.1769 - val_mae: 1244.8699\n",
      "Epoch 1088/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 101.7232 - mae: 102.4083 - val_loss: 1223.7534 - val_mae: 1224.4454\n",
      "Epoch 1089/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 97.4974 - mae: 98.1780 - val_loss: 1418.5048 - val_mae: 1419.1978\n",
      "Epoch 1090/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 89.4901 - mae: 90.1730 - val_loss: 1312.7974 - val_mae: 1313.4902\n",
      "Epoch 1091/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 103.3177 - mae: 104.0037 - val_loss: 1239.2500 - val_mae: 1239.9432\n",
      "Epoch 1092/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 91.2445 - mae: 91.9296 - val_loss: 1181.7578 - val_mae: 1182.4507\n",
      "Epoch 1093/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 90.3190 - mae: 90.9996 - val_loss: 1256.2649 - val_mae: 1256.9580\n",
      "Epoch 1094/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 94.1630 - mae: 94.8480 - val_loss: 1454.8682 - val_mae: 1455.5614\n",
      "Epoch 1095/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 105.5041 - mae: 106.1900 - val_loss: 1486.8452 - val_mae: 1487.5385\n",
      "Epoch 1096/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 92.8215 - mae: 93.5047 - val_loss: 1586.8259 - val_mae: 1587.5189\n",
      "Epoch 1097/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 92.2970 - mae: 92.9809 - val_loss: 1383.4257 - val_mae: 1384.1183\n",
      "Epoch 1098/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 89.2191 - mae: 89.9030 - val_loss: 1387.3260 - val_mae: 1388.0193\n",
      "Epoch 1099/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 107.6971 - mae: 108.3773 - val_loss: 1295.5861 - val_mae: 1296.2792\n",
      "Epoch 1100/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 98.7922 - mae: 99.4742 - val_loss: 1203.5818 - val_mae: 1204.2749\n",
      "Epoch 1101/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 93.3052 - mae: 93.9892 - val_loss: 1285.3195 - val_mae: 1286.0117\n",
      "Epoch 1102/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 91.7440 - mae: 92.4254 - val_loss: 1323.4386 - val_mae: 1324.1313\n",
      "Epoch 1103/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 93.6764 - mae: 94.3616 - val_loss: 1318.1614 - val_mae: 1318.8542\n",
      "Epoch 1104/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 93.3259 - mae: 94.0079 - val_loss: 1319.1759 - val_mae: 1319.8689\n",
      "Epoch 1105/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 93.4906 - mae: 94.1731 - val_loss: 1280.6597 - val_mae: 1281.3527\n",
      "Epoch 1106/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 92.0405 - mae: 92.7219 - val_loss: 1280.4222 - val_mae: 1281.1156\n",
      "Epoch 1107/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 93.9775 - mae: 94.6620 - val_loss: 1464.2510 - val_mae: 1464.9434\n",
      "Epoch 1108/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 91.1967 - mae: 91.8765 - val_loss: 1396.9069 - val_mae: 1397.6000\n",
      "Epoch 1109/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 102.4650 - mae: 103.1489 - val_loss: 1167.6765 - val_mae: 1168.3695\n",
      "Epoch 1110/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 95.5609 - mae: 96.2435 - val_loss: 1312.0300 - val_mae: 1312.7231\n",
      "Epoch 1111/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 93.2156 - mae: 93.8989 - val_loss: 1474.1499 - val_mae: 1474.8425\n",
      "Epoch 1112/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 91.6544 - mae: 92.3369 - val_loss: 1249.6696 - val_mae: 1250.3627\n",
      "Epoch 1113/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 91.8316 - mae: 92.5145 - val_loss: 1341.7841 - val_mae: 1342.4768\n",
      "Epoch 1114/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 96.2317 - mae: 96.9173 - val_loss: 1404.8184 - val_mae: 1405.5105\n",
      "Epoch 1115/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 90.7271 - mae: 91.4081 - val_loss: 1211.7571 - val_mae: 1212.4502\n",
      "Epoch 1116/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 94.4664 - mae: 95.1475 - val_loss: 1266.0216 - val_mae: 1266.7148\n",
      "Epoch 1117/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 94.2788 - mae: 94.9621 - val_loss: 1176.6147 - val_mae: 1177.3074\n",
      "Epoch 1118/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 103.6415 - mae: 104.3251 - val_loss: 1167.3802 - val_mae: 1168.0732\n",
      "Epoch 1119/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 98.2977 - mae: 98.9846 - val_loss: 1559.0186 - val_mae: 1559.7119\n",
      "Epoch 1120/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 108.8003 - mae: 109.4854 - val_loss: 1127.8710 - val_mae: 1128.5642\n",
      "Epoch 1121/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 93.8130 - mae: 94.4937 - val_loss: 1369.8191 - val_mae: 1370.5123\n",
      "Epoch 1122/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 100.6449 - mae: 101.3291 - val_loss: 1547.5353 - val_mae: 1548.2285\n",
      "Epoch 1123/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 88.9080 - mae: 89.5888 - val_loss: 1369.8905 - val_mae: 1370.5835\n",
      "Epoch 1124/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 89.5864 - mae: 90.2717 - val_loss: 1272.2928 - val_mae: 1272.9860\n",
      "Epoch 1125/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 92.7047 - mae: 93.3894 - val_loss: 1563.1431 - val_mae: 1563.8341\n",
      "Epoch 1126/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 103.7530 - mae: 104.4406 - val_loss: 1396.4969 - val_mae: 1397.1902\n",
      "Epoch 1127/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 93.2221 - mae: 93.9045 - val_loss: 1423.0874 - val_mae: 1423.7805\n",
      "Epoch 1128/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 96.4247 - mae: 97.1097 - val_loss: 1257.1354 - val_mae: 1257.8285\n",
      "Epoch 1129/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 107.2987 - mae: 107.9821 - val_loss: 1402.8123 - val_mae: 1403.5055\n",
      "Epoch 1130/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 95.4254 - mae: 96.1108 - val_loss: 1649.0854 - val_mae: 1649.7788\n",
      "Epoch 1131/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 101.4469 - mae: 102.1317 - val_loss: 1403.0067 - val_mae: 1403.6996\n",
      "Epoch 1132/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 92.3731 - mae: 93.0582 - val_loss: 1137.9498 - val_mae: 1138.6432\n",
      "Epoch 1133/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 90.6414 - mae: 91.3249 - val_loss: 1351.3374 - val_mae: 1352.0306\n",
      "Epoch 1134/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 99.7881 - mae: 100.4727 - val_loss: 1398.9219 - val_mae: 1399.6144\n",
      "Epoch 1135/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 95.4036 - mae: 96.0859 - val_loss: 1183.0807 - val_mae: 1183.7727\n",
      "Epoch 1136/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 88.5702 - mae: 89.2528 - val_loss: 1251.2416 - val_mae: 1251.9337\n",
      "Epoch 1137/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 95.4200 - mae: 96.1031 - val_loss: 1266.1235 - val_mae: 1266.8167\n",
      "Epoch 1138/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 93.0966 - mae: 93.7811 - val_loss: 1238.8616 - val_mae: 1239.5547\n",
      "Epoch 1139/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 102.8454 - mae: 103.5305 - val_loss: 1277.3201 - val_mae: 1278.0134\n",
      "Epoch 1140/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 98.9142 - mae: 99.5955 - val_loss: 1390.5211 - val_mae: 1391.2125\n",
      "Epoch 1141/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 91.4576 - mae: 92.1388 - val_loss: 1217.3947 - val_mae: 1218.0864\n",
      "Epoch 1142/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 92.7067 - mae: 93.3893 - val_loss: 1295.0128 - val_mae: 1295.7061\n",
      "Epoch 1143/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 96.7598 - mae: 97.4432 - val_loss: 1374.8115 - val_mae: 1375.5046\n",
      "Epoch 1144/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 91.7251 - mae: 92.4083 - val_loss: 1266.2997 - val_mae: 1266.9921\n",
      "Epoch 1145/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 88.2434 - mae: 88.9236 - val_loss: 1377.7458 - val_mae: 1378.4379\n",
      "Epoch 1146/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 86.6496 - mae: 87.3318 - val_loss: 1449.6799 - val_mae: 1450.3730\n",
      "Epoch 1147/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 89.7794 - mae: 90.4592 - val_loss: 1389.7642 - val_mae: 1390.4564\n",
      "Epoch 1148/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 90.7945 - mae: 91.4798 - val_loss: 1258.1075 - val_mae: 1258.8010\n",
      "Epoch 1149/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 88.1589 - mae: 88.8398 - val_loss: 1364.4713 - val_mae: 1365.1646\n",
      "Epoch 1150/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 94.5181 - mae: 95.2022 - val_loss: 1715.7388 - val_mae: 1716.4316\n",
      "Epoch 1151/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 99.4281 - mae: 100.1112 - val_loss: 1299.0366 - val_mae: 1299.7286\n",
      "Epoch 1152/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 87.0684 - mae: 87.7536 - val_loss: 1362.6707 - val_mae: 1363.3638\n",
      "Epoch 1153/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 93.2043 - mae: 93.8902 - val_loss: 1228.1368 - val_mae: 1228.8296\n",
      "Epoch 1154/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 92.1650 - mae: 92.8478 - val_loss: 1211.6438 - val_mae: 1212.3369\n",
      "Epoch 1155/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 88.8964 - mae: 89.5806 - val_loss: 1404.8370 - val_mae: 1405.5298\n",
      "Epoch 1156/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 95.1111 - mae: 95.7933 - val_loss: 1492.2654 - val_mae: 1492.9572\n",
      "Epoch 1157/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 90.3409 - mae: 91.0253 - val_loss: 1266.0225 - val_mae: 1266.7158\n",
      "Epoch 1158/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 100.1353 - mae: 100.8231 - val_loss: 1599.2838 - val_mae: 1599.9771\n",
      "Epoch 1159/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 93.0303 - mae: 93.7150 - val_loss: 1424.6193 - val_mae: 1425.3113\n",
      "Epoch 1160/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 89.2896 - mae: 89.9722 - val_loss: 1325.1168 - val_mae: 1325.8091\n",
      "Epoch 1161/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 87.3750 - mae: 88.0591 - val_loss: 1176.8213 - val_mae: 1177.5145\n",
      "Epoch 1162/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 95.8385 - mae: 96.5220 - val_loss: 1470.8110 - val_mae: 1471.5043\n",
      "Epoch 1163/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 98.6043 - mae: 99.2876 - val_loss: 1351.6697 - val_mae: 1352.3630\n",
      "Epoch 1164/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 90.6968 - mae: 91.3784 - val_loss: 1135.3977 - val_mae: 1136.0909\n",
      "Epoch 1165/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 91.6955 - mae: 92.3815 - val_loss: 1219.7643 - val_mae: 1220.4573\n",
      "Epoch 1166/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 97.4931 - mae: 98.1741 - val_loss: 1790.2166 - val_mae: 1790.9089\n",
      "Epoch 1167/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 94.9570 - mae: 95.6372 - val_loss: 1274.3075 - val_mae: 1275.0007\n",
      "Epoch 1168/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 92.5502 - mae: 93.2333 - val_loss: 1251.6703 - val_mae: 1252.3634\n",
      "Epoch 1169/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 95.3442 - mae: 96.0250 - val_loss: 1552.7390 - val_mae: 1553.4321\n",
      "Epoch 1170/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 104.8292 - mae: 105.5139 - val_loss: 1329.6707 - val_mae: 1330.3639\n",
      "Epoch 1171/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 94.5356 - mae: 95.2188 - val_loss: 1349.7808 - val_mae: 1350.4740\n",
      "Epoch 1172/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 94.1723 - mae: 94.8523 - val_loss: 1119.3795 - val_mae: 1120.0697\n",
      "Epoch 1173/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 91.0313 - mae: 91.7134 - val_loss: 1322.3256 - val_mae: 1323.0182\n",
      "Epoch 1174/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 96.7967 - mae: 97.4805 - val_loss: 1350.3499 - val_mae: 1351.0421\n",
      "Epoch 1175/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 100.6497 - mae: 101.3366 - val_loss: 1580.9764 - val_mae: 1581.6696\n",
      "Epoch 1176/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 89.3814 - mae: 90.0648 - val_loss: 1388.5793 - val_mae: 1389.2723\n",
      "Epoch 1177/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 94.1722 - mae: 94.8545 - val_loss: 1242.0802 - val_mae: 1242.7734\n",
      "Epoch 1178/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 89.8185 - mae: 90.5004 - val_loss: 1218.8662 - val_mae: 1219.5593\n",
      "Epoch 1179/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 91.7123 - mae: 92.3994 - val_loss: 1301.9083 - val_mae: 1302.6012\n",
      "Epoch 1180/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 86.2700 - mae: 86.9497 - val_loss: 1123.1984 - val_mae: 1123.8914\n",
      "Epoch 1181/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 92.0854 - mae: 92.7684 - val_loss: 1303.8173 - val_mae: 1304.5103\n",
      "Epoch 1182/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 87.1381 - mae: 87.8189 - val_loss: 1343.7933 - val_mae: 1344.4863\n",
      "Epoch 1183/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 97.2975 - mae: 97.9850 - val_loss: 1378.0763 - val_mae: 1378.7688\n",
      "Epoch 1184/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 93.4996 - mae: 94.1802 - val_loss: 1282.0665 - val_mae: 1282.7598\n",
      "Epoch 1185/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 97.9076 - mae: 98.5886 - val_loss: 1332.0840 - val_mae: 1332.7770\n",
      "Epoch 1186/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 93.7421 - mae: 94.4278 - val_loss: 1422.4117 - val_mae: 1423.1049\n",
      "Epoch 1187/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 100.9024 - mae: 101.5881 - val_loss: 1373.3362 - val_mae: 1374.0293\n",
      "Epoch 1188/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 114.0492 - mae: 114.7343 - val_loss: 1240.3750 - val_mae: 1241.0682\n",
      "Epoch 1189/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 100.8786 - mae: 101.5602 - val_loss: 1215.5919 - val_mae: 1216.2852\n",
      "Epoch 1190/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 94.4727 - mae: 95.1536 - val_loss: 1171.4922 - val_mae: 1172.1849\n",
      "Epoch 1191/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 104.2834 - mae: 104.9674 - val_loss: 1329.2201 - val_mae: 1329.9135\n",
      "Epoch 1192/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 94.4557 - mae: 95.1389 - val_loss: 1224.4524 - val_mae: 1225.1443\n",
      "Epoch 1193/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 89.4948 - mae: 90.1767 - val_loss: 1210.5658 - val_mae: 1211.2589\n",
      "Epoch 1194/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 91.2627 - mae: 91.9426 - val_loss: 1064.2578 - val_mae: 1064.9504\n",
      "Epoch 1195/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 95.7055 - mae: 96.3885 - val_loss: 1262.9377 - val_mae: 1263.6309\n",
      "Epoch 1196/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 96.1226 - mae: 96.8067 - val_loss: 1098.6243 - val_mae: 1099.3174\n",
      "Epoch 1197/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 93.0074 - mae: 93.6870 - val_loss: 1284.9362 - val_mae: 1285.6294\n",
      "Epoch 1198/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 93.6138 - mae: 94.3009 - val_loss: 1124.0815 - val_mae: 1124.7747\n",
      "Epoch 1199/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 92.4126 - mae: 93.0945 - val_loss: 1381.6227 - val_mae: 1382.3159\n",
      "Epoch 1200/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 88.0088 - mae: 88.6876 - val_loss: 1393.0880 - val_mae: 1393.7803\n",
      "Epoch 1201/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 89.8460 - mae: 90.5286 - val_loss: 1250.4684 - val_mae: 1251.1615\n",
      "Epoch 1202/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 91.2820 - mae: 91.9646 - val_loss: 1206.9856 - val_mae: 1207.6787\n",
      "Epoch 1203/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 86.8374 - mae: 87.5182 - val_loss: 1322.6073 - val_mae: 1323.3005\n",
      "Epoch 1204/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 94.1827 - mae: 94.8668 - val_loss: 1087.8978 - val_mae: 1088.5911\n",
      "Epoch 1205/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 92.6982 - mae: 93.3788 - val_loss: 1201.7313 - val_mae: 1202.4232\n",
      "Epoch 1206/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 89.5907 - mae: 90.2770 - val_loss: 1253.6166 - val_mae: 1254.3093\n",
      "Epoch 1207/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 95.4457 - mae: 96.1346 - val_loss: 1370.1320 - val_mae: 1370.8253\n",
      "Epoch 1208/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 89.7608 - mae: 90.4418 - val_loss: 1144.5734 - val_mae: 1145.2666\n",
      "Epoch 1209/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 96.3527 - mae: 97.0390 - val_loss: 1219.7255 - val_mae: 1220.4186\n",
      "Epoch 1210/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 87.2384 - mae: 87.9181 - val_loss: 1305.0100 - val_mae: 1305.7028\n",
      "Epoch 1211/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 96.4100 - mae: 97.0920 - val_loss: 1217.0096 - val_mae: 1217.7030\n",
      "Epoch 1212/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 93.5712 - mae: 94.2527 - val_loss: 1151.7404 - val_mae: 1152.4321\n",
      "Epoch 1213/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 89.1927 - mae: 89.8734 - val_loss: 1269.1667 - val_mae: 1269.8597\n",
      "Epoch 1214/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 90.4695 - mae: 91.1510 - val_loss: 1291.6050 - val_mae: 1292.2976\n",
      "Epoch 1215/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 91.9987 - mae: 92.6830 - val_loss: 1217.7784 - val_mae: 1218.4716\n",
      "Epoch 1216/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 92.4637 - mae: 93.1483 - val_loss: 1492.7294 - val_mae: 1493.4226\n",
      "Epoch 1217/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 91.1770 - mae: 91.8625 - val_loss: 1269.8777 - val_mae: 1270.5696\n",
      "Epoch 1218/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 93.5847 - mae: 94.2683 - val_loss: 1337.3569 - val_mae: 1338.0500\n",
      "Epoch 1219/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 86.2383 - mae: 86.9215 - val_loss: 1390.6349 - val_mae: 1391.3282\n",
      "Epoch 1220/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 91.5170 - mae: 92.2015 - val_loss: 1286.4979 - val_mae: 1287.1912\n",
      "Epoch 1221/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 86.6544 - mae: 87.3348 - val_loss: 1343.8052 - val_mae: 1344.4985\n",
      "Epoch 1222/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 85.3932 - mae: 86.0727 - val_loss: 1273.1309 - val_mae: 1273.8240\n",
      "Epoch 1223/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 85.0890 - mae: 85.7728 - val_loss: 1295.9219 - val_mae: 1296.6151\n",
      "Epoch 1224/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 98.6949 - mae: 99.3776 - val_loss: 1265.4786 - val_mae: 1266.1718\n",
      "Epoch 1225/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 104.6823 - mae: 105.3668 - val_loss: 1346.2284 - val_mae: 1346.9204\n",
      "Epoch 1226/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 93.4868 - mae: 94.1688 - val_loss: 1347.3943 - val_mae: 1348.0874\n",
      "Epoch 1227/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 97.8675 - mae: 98.5522 - val_loss: 1446.4167 - val_mae: 1447.1097\n",
      "Epoch 1228/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 95.8423 - mae: 96.5265 - val_loss: 1397.6304 - val_mae: 1398.3236\n",
      "Epoch 1229/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 90.6990 - mae: 91.3835 - val_loss: 1291.8887 - val_mae: 1292.5818\n",
      "Epoch 1230/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 96.2161 - mae: 96.9002 - val_loss: 1514.1990 - val_mae: 1514.8901\n",
      "Epoch 1231/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 118.5101 - mae: 119.1976 - val_loss: 1275.1897 - val_mae: 1275.8828\n",
      "Epoch 1232/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 98.3204 - mae: 99.0039 - val_loss: 1225.6152 - val_mae: 1226.3083\n",
      "Epoch 1233/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 90.2187 - mae: 90.8997 - val_loss: 1300.5909 - val_mae: 1301.2833\n",
      "Epoch 1234/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 92.5804 - mae: 93.2642 - val_loss: 1384.6234 - val_mae: 1385.3168\n",
      "Epoch 1235/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 89.7858 - mae: 90.4689 - val_loss: 1356.0946 - val_mae: 1356.7878\n",
      "Epoch 1236/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 93.0488 - mae: 93.7341 - val_loss: 1650.5317 - val_mae: 1651.2250\n",
      "Epoch 1237/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 95.9629 - mae: 96.6483 - val_loss: 1489.1075 - val_mae: 1489.8008\n",
      "Epoch 1238/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 106.8440 - mae: 107.5306 - val_loss: 1393.9482 - val_mae: 1394.6404\n",
      "Epoch 1239/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 88.6157 - mae: 89.2988 - val_loss: 1252.1996 - val_mae: 1252.8929\n",
      "Epoch 1240/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 94.1376 - mae: 94.8213 - val_loss: 1204.7397 - val_mae: 1205.4327\n",
      "Epoch 1241/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 96.3017 - mae: 96.9874 - val_loss: 1498.8558 - val_mae: 1499.5490\n",
      "Epoch 1242/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 87.8696 - mae: 88.5535 - val_loss: 1185.4526 - val_mae: 1186.1460\n",
      "Epoch 1243/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 100.7831 - mae: 101.4674 - val_loss: 1281.4712 - val_mae: 1282.1644\n",
      "Epoch 1244/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 110.6601 - mae: 111.3446 - val_loss: 1108.5819 - val_mae: 1109.2732\n",
      "Epoch 1245/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 92.1099 - mae: 92.7940 - val_loss: 1354.8455 - val_mae: 1355.5378\n",
      "Epoch 1246/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 95.6238 - mae: 96.3073 - val_loss: 1569.4042 - val_mae: 1570.0963\n",
      "Epoch 1247/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 90.6592 - mae: 91.3438 - val_loss: 1338.1241 - val_mae: 1338.8174\n",
      "Epoch 1248/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 87.3814 - mae: 88.0638 - val_loss: 1268.9935 - val_mae: 1269.6864\n",
      "Epoch 1249/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 88.9355 - mae: 89.6159 - val_loss: 1230.3036 - val_mae: 1230.9960\n",
      "Epoch 1250/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 94.0146 - mae: 94.6977 - val_loss: 1164.6075 - val_mae: 1165.3008\n",
      "Epoch 1251/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 92.4585 - mae: 93.1424 - val_loss: 1339.1307 - val_mae: 1339.8239\n",
      "Epoch 1252/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 96.8108 - mae: 97.4953 - val_loss: 1231.2847 - val_mae: 1231.9775\n",
      "Epoch 1253/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 88.5017 - mae: 89.1863 - val_loss: 1241.6224 - val_mae: 1242.3156\n",
      "Epoch 1254/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 94.6551 - mae: 95.3385 - val_loss: 1332.3358 - val_mae: 1333.0289\n",
      "Epoch 1255/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 87.5993 - mae: 88.2843 - val_loss: 1102.5728 - val_mae: 1103.2660\n",
      "Epoch 1256/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 95.4792 - mae: 96.1647 - val_loss: 1475.8132 - val_mae: 1476.5060\n",
      "Epoch 1257/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 107.5610 - mae: 108.2441 - val_loss: 1415.5953 - val_mae: 1416.2883\n",
      "Epoch 1258/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 92.3076 - mae: 92.9898 - val_loss: 1405.3907 - val_mae: 1406.0831\n",
      "Epoch 1259/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 98.7266 - mae: 99.4094 - val_loss: 1352.3824 - val_mae: 1353.0754\n",
      "Epoch 1260/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 88.5227 - mae: 89.2053 - val_loss: 1277.6378 - val_mae: 1278.3304\n",
      "Epoch 1261/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 87.9633 - mae: 88.6439 - val_loss: 1371.6952 - val_mae: 1372.3884\n",
      "Epoch 1262/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 91.2179 - mae: 91.9000 - val_loss: 1262.2975 - val_mae: 1262.9908\n",
      "Epoch 1263/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 88.7488 - mae: 89.4321 - val_loss: 1318.9044 - val_mae: 1319.5977\n",
      "Epoch 1264/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 90.7987 - mae: 91.4853 - val_loss: 1214.1355 - val_mae: 1214.8285\n",
      "Epoch 1265/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 89.2242 - mae: 89.9038 - val_loss: 1288.9196 - val_mae: 1289.6128\n",
      "Epoch 1266/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 92.7160 - mae: 93.4009 - val_loss: 1347.0214 - val_mae: 1347.7147\n",
      "Epoch 1267/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 102.6989 - mae: 103.3837 - val_loss: 1458.4971 - val_mae: 1459.1902\n",
      "Epoch 1268/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 90.3001 - mae: 90.9850 - val_loss: 1322.7218 - val_mae: 1323.4152\n",
      "Epoch 1269/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 86.3635 - mae: 87.0391 - val_loss: 1117.0691 - val_mae: 1117.7618\n",
      "Epoch 1270/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 92.7412 - mae: 93.4250 - val_loss: 1232.2128 - val_mae: 1232.9060\n",
      "Epoch 1271/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 92.1595 - mae: 92.8437 - val_loss: 1308.1647 - val_mae: 1308.8577\n",
      "Epoch 1272/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 85.7026 - mae: 86.3859 - val_loss: 1161.1229 - val_mae: 1161.8160\n",
      "Epoch 1273/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 96.9846 - mae: 97.6690 - val_loss: 1155.3727 - val_mae: 1156.0652\n",
      "Epoch 1274/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 96.3619 - mae: 97.0460 - val_loss: 1177.7302 - val_mae: 1178.4221\n",
      "Epoch 1275/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 85.7671 - mae: 86.4464 - val_loss: 1327.9792 - val_mae: 1328.6726\n",
      "Epoch 1276/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 90.1189 - mae: 90.8032 - val_loss: 1242.6100 - val_mae: 1243.3032\n",
      "Epoch 1277/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 89.1616 - mae: 89.8429 - val_loss: 1630.3274 - val_mae: 1631.0203\n",
      "Epoch 1278/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 88.1244 - mae: 88.8056 - val_loss: 1156.3887 - val_mae: 1157.0808\n",
      "Epoch 1279/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 86.3713 - mae: 87.0545 - val_loss: 1338.7324 - val_mae: 1339.4252\n",
      "Epoch 1280/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 85.9160 - mae: 86.5954 - val_loss: 1225.1451 - val_mae: 1225.8381\n",
      "Epoch 1281/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 105.9510 - mae: 106.6359 - val_loss: 1272.4240 - val_mae: 1273.1171\n",
      "Epoch 1282/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 89.0276 - mae: 89.7116 - val_loss: 1225.2473 - val_mae: 1225.9403\n",
      "Epoch 1283/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 98.2045 - mae: 98.8844 - val_loss: 1304.1555 - val_mae: 1304.8484\n",
      "Epoch 1284/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 89.5058 - mae: 90.1896 - val_loss: 1320.8600 - val_mae: 1321.5532\n",
      "Epoch 1285/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 101.6128 - mae: 102.2955 - val_loss: 1432.3430 - val_mae: 1433.0359\n",
      "Epoch 1286/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 92.4014 - mae: 93.0836 - val_loss: 1405.5204 - val_mae: 1406.2134\n",
      "Epoch 1287/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 84.5714 - mae: 85.2529 - val_loss: 1331.2327 - val_mae: 1331.9258\n",
      "Epoch 1288/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 87.9372 - mae: 88.6196 - val_loss: 1280.9822 - val_mae: 1281.6747\n",
      "Epoch 1289/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 85.8812 - mae: 86.5627 - val_loss: 1410.9592 - val_mae: 1411.6526\n",
      "Epoch 1290/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 88.2327 - mae: 88.9144 - val_loss: 1493.5162 - val_mae: 1494.2094\n",
      "Epoch 1291/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 95.4244 - mae: 96.1045 - val_loss: 1349.0906 - val_mae: 1349.7836\n",
      "Epoch 1292/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 88.4417 - mae: 89.1240 - val_loss: 1538.6660 - val_mae: 1539.3591\n",
      "Epoch 1293/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 93.2803 - mae: 93.9590 - val_loss: 1525.7649 - val_mae: 1526.4580\n",
      "Epoch 1294/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 91.8188 - mae: 92.5006 - val_loss: 1254.8696 - val_mae: 1255.5630\n",
      "Epoch 1295/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 86.8386 - mae: 87.5217 - val_loss: 1191.4843 - val_mae: 1192.1775\n",
      "Epoch 1296/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 87.3919 - mae: 88.0775 - val_loss: 1270.0504 - val_mae: 1270.7437\n",
      "Epoch 1297/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 90.5303 - mae: 91.2128 - val_loss: 1171.1885 - val_mae: 1171.8816\n",
      "Epoch 1298/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 86.1982 - mae: 86.8794 - val_loss: 1441.3854 - val_mae: 1442.0782\n",
      "Epoch 1299/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 85.5274 - mae: 86.2077 - val_loss: 1483.4670 - val_mae: 1484.1600\n",
      "Epoch 1300/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 91.7285 - mae: 92.4134 - val_loss: 1428.4432 - val_mae: 1429.1364\n",
      "Epoch 1301/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 87.5872 - mae: 88.2683 - val_loss: 1376.5665 - val_mae: 1377.2596\n",
      "Epoch 1302/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 110.8317 - mae: 111.5179 - val_loss: 1304.1569 - val_mae: 1304.8491\n",
      "Epoch 1303/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 98.2079 - mae: 98.8888 - val_loss: 1283.7010 - val_mae: 1284.3942\n",
      "Epoch 1304/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 87.2449 - mae: 87.9291 - val_loss: 1260.4302 - val_mae: 1261.1233\n",
      "Epoch 1305/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 87.1644 - mae: 87.8460 - val_loss: 1347.1545 - val_mae: 1347.8464\n",
      "Epoch 1306/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 93.2570 - mae: 93.9385 - val_loss: 1289.7170 - val_mae: 1290.4100\n",
      "Epoch 1307/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 93.1035 - mae: 93.7892 - val_loss: 1331.5044 - val_mae: 1332.1974\n",
      "Epoch 1308/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 89.2102 - mae: 89.8898 - val_loss: 1195.2931 - val_mae: 1195.9863\n",
      "Epoch 1309/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 87.2019 - mae: 87.8805 - val_loss: 1289.6362 - val_mae: 1290.3289\n",
      "Epoch 1310/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 83.8549 - mae: 84.5386 - val_loss: 1267.8573 - val_mae: 1268.5504\n",
      "Epoch 1311/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 94.9830 - mae: 95.6695 - val_loss: 1240.1150 - val_mae: 1240.8081\n",
      "Epoch 1312/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 91.1709 - mae: 91.8578 - val_loss: 1358.2319 - val_mae: 1358.9253\n",
      "Epoch 1313/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 87.0268 - mae: 87.7106 - val_loss: 1358.1169 - val_mae: 1358.8101\n",
      "Epoch 1314/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 84.4569 - mae: 85.1398 - val_loss: 1456.3309 - val_mae: 1457.0240\n",
      "Epoch 1315/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 87.0979 - mae: 87.7781 - val_loss: 1309.3051 - val_mae: 1309.9978\n",
      "Epoch 1316/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 89.0734 - mae: 89.7532 - val_loss: 1419.0974 - val_mae: 1419.7909\n",
      "Epoch 1317/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 89.9541 - mae: 90.6350 - val_loss: 1216.9132 - val_mae: 1217.6060\n",
      "Epoch 1318/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 91.6601 - mae: 92.3445 - val_loss: 1425.9448 - val_mae: 1426.6381\n",
      "Epoch 1319/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 93.2856 - mae: 93.9681 - val_loss: 1329.6976 - val_mae: 1330.3907\n",
      "Epoch 1320/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 92.7094 - mae: 93.3916 - val_loss: 1123.3416 - val_mae: 1124.0348\n",
      "Epoch 1321/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 112.1565 - mae: 112.8409 - val_loss: 1266.5626 - val_mae: 1267.2557\n",
      "Epoch 1322/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 85.8906 - mae: 86.5732 - val_loss: 1159.9077 - val_mae: 1160.6011\n",
      "Epoch 1323/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 87.5759 - mae: 88.2627 - val_loss: 1338.0026 - val_mae: 1338.6945\n",
      "Epoch 1324/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 84.8164 - mae: 85.4952 - val_loss: 1111.4939 - val_mae: 1112.1873\n",
      "Epoch 1325/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 88.3681 - mae: 89.0489 - val_loss: 1260.7626 - val_mae: 1261.4557\n",
      "Epoch 1326/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 92.0985 - mae: 92.7811 - val_loss: 1219.4092 - val_mae: 1220.1023\n",
      "Epoch 1327/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 91.0296 - mae: 91.7153 - val_loss: 1106.6368 - val_mae: 1107.3296\n",
      "Epoch 1328/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 96.6543 - mae: 97.3392 - val_loss: 1232.6322 - val_mae: 1233.3256\n",
      "Epoch 1329/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 87.3834 - mae: 88.0628 - val_loss: 1186.7108 - val_mae: 1187.4041\n",
      "Epoch 1330/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 86.4622 - mae: 87.1434 - val_loss: 1324.2655 - val_mae: 1324.9586\n",
      "Epoch 1331/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 101.3169 - mae: 102.0022 - val_loss: 1181.2599 - val_mae: 1181.9531\n",
      "Epoch 1332/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 93.3285 - mae: 94.0131 - val_loss: 1382.9331 - val_mae: 1383.6257\n",
      "Epoch 1333/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 86.8733 - mae: 87.5530 - val_loss: 1282.6248 - val_mae: 1283.3180\n",
      "Epoch 1334/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 102.4304 - mae: 103.1160 - val_loss: 1318.4364 - val_mae: 1319.1296\n",
      "Epoch 1335/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 95.0603 - mae: 95.7453 - val_loss: 1283.9132 - val_mae: 1284.6053\n",
      "Epoch 1336/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 90.9483 - mae: 91.6334 - val_loss: 1347.2998 - val_mae: 1347.9923\n",
      "Epoch 1337/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 90.2506 - mae: 90.9314 - val_loss: 1173.6575 - val_mae: 1174.3507\n",
      "Epoch 1338/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 86.2682 - mae: 86.9508 - val_loss: 1150.6744 - val_mae: 1151.3676\n",
      "Epoch 1339/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 95.3766 - mae: 96.0598 - val_loss: 1097.8422 - val_mae: 1098.5353\n",
      "Epoch 1340/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 93.0077 - mae: 93.6924 - val_loss: 1285.8871 - val_mae: 1286.5803\n",
      "Epoch 1341/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 93.2795 - mae: 93.9625 - val_loss: 1279.4089 - val_mae: 1280.1021\n",
      "Epoch 1342/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 90.3343 - mae: 91.0185 - val_loss: 1329.6970 - val_mae: 1330.3898\n",
      "Epoch 1343/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 87.2237 - mae: 87.9085 - val_loss: 1303.5151 - val_mae: 1304.2074\n",
      "Epoch 1344/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 93.9827 - mae: 94.6654 - val_loss: 1304.2544 - val_mae: 1304.9475\n",
      "Epoch 1345/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 89.6634 - mae: 90.3473 - val_loss: 1252.0310 - val_mae: 1252.7235\n",
      "Epoch 1346/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 87.9992 - mae: 88.6798 - val_loss: 1295.6300 - val_mae: 1296.3229\n",
      "Epoch 1347/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 87.3486 - mae: 88.0274 - val_loss: 1513.9739 - val_mae: 1514.6670\n",
      "Epoch 1348/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 87.2364 - mae: 87.9180 - val_loss: 1229.4214 - val_mae: 1230.1144\n",
      "Epoch 1349/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 83.7657 - mae: 84.4498 - val_loss: 1254.4884 - val_mae: 1255.1816\n",
      "Epoch 1350/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 94.5500 - mae: 95.2368 - val_loss: 1336.4144 - val_mae: 1337.1077\n",
      "Epoch 1351/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 90.6901 - mae: 91.3726 - val_loss: 1313.5443 - val_mae: 1314.2374\n",
      "Epoch 1352/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 86.6572 - mae: 87.3331 - val_loss: 1384.9946 - val_mae: 1385.6880\n",
      "Epoch 1353/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 100.3611 - mae: 101.0425 - val_loss: 1060.4218 - val_mae: 1061.1146\n",
      "Epoch 1354/5000\n",
      "46/46 [==============================] - 2s 32ms/step - loss: 96.6406 - mae: 97.3231 - val_loss: 1296.4684 - val_mae: 1297.1614\n",
      "Epoch 1355/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 80.6348 - mae: 81.3133 - val_loss: 1220.0150 - val_mae: 1220.7080\n",
      "Epoch 1356/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 93.6411 - mae: 94.3241 - val_loss: 1180.5173 - val_mae: 1181.2104\n",
      "Epoch 1357/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 85.1104 - mae: 85.7904 - val_loss: 1274.0890 - val_mae: 1274.7821\n",
      "Epoch 1358/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 86.8752 - mae: 87.5592 - val_loss: 1364.1947 - val_mae: 1364.8876\n",
      "Epoch 1359/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 91.2650 - mae: 91.9469 - val_loss: 1117.7203 - val_mae: 1118.4135\n",
      "Epoch 1360/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 96.8870 - mae: 97.5690 - val_loss: 1387.9335 - val_mae: 1388.6251\n",
      "Epoch 1361/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 112.1855 - mae: 112.8711 - val_loss: 1243.5042 - val_mae: 1244.1968\n",
      "Epoch 1362/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 92.9194 - mae: 93.6041 - val_loss: 1093.1556 - val_mae: 1093.8486\n",
      "Epoch 1363/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 92.2271 - mae: 92.9104 - val_loss: 1212.8862 - val_mae: 1213.5786\n",
      "Epoch 1364/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 82.1752 - mae: 82.8567 - val_loss: 1190.3647 - val_mae: 1191.0579\n",
      "Epoch 1365/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 87.9136 - mae: 88.5913 - val_loss: 1435.2444 - val_mae: 1435.9376\n",
      "Epoch 1366/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 92.5366 - mae: 93.2220 - val_loss: 1260.8019 - val_mae: 1261.4950\n",
      "Epoch 1367/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 85.1151 - mae: 85.7954 - val_loss: 1460.5568 - val_mae: 1461.2501\n",
      "Epoch 1368/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 90.7849 - mae: 91.4676 - val_loss: 1315.3744 - val_mae: 1316.0676\n",
      "Epoch 1369/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 83.7346 - mae: 84.4159 - val_loss: 1401.2817 - val_mae: 1401.9752\n",
      "Epoch 1370/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 89.0954 - mae: 89.7771 - val_loss: 1287.9409 - val_mae: 1288.6339\n",
      "Epoch 1371/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 88.0038 - mae: 88.6854 - val_loss: 1298.2607 - val_mae: 1298.9536\n",
      "Epoch 1372/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 99.1160 - mae: 99.8014 - val_loss: 1529.1765 - val_mae: 1529.8696\n",
      "Epoch 1373/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 89.9437 - mae: 90.6238 - val_loss: 1380.1216 - val_mae: 1380.8151\n",
      "Epoch 1374/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 89.7993 - mae: 90.4804 - val_loss: 1263.4873 - val_mae: 1264.1796\n",
      "Epoch 1375/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 87.8001 - mae: 88.4837 - val_loss: 1256.7485 - val_mae: 1257.4418\n",
      "Epoch 1376/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 90.6810 - mae: 91.3639 - val_loss: 1137.1807 - val_mae: 1137.8728\n",
      "Epoch 1377/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 85.2924 - mae: 85.9763 - val_loss: 1467.8534 - val_mae: 1468.5465\n",
      "Epoch 1378/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 94.8826 - mae: 95.5702 - val_loss: 1487.4779 - val_mae: 1488.1710\n",
      "Epoch 1379/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 103.1914 - mae: 103.8757 - val_loss: 1288.2521 - val_mae: 1288.9451\n",
      "Epoch 1380/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 84.7284 - mae: 85.4094 - val_loss: 1192.6776 - val_mae: 1193.3706\n",
      "Epoch 1381/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 94.7889 - mae: 95.4707 - val_loss: 1247.5840 - val_mae: 1248.2771\n",
      "Epoch 1382/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 88.4724 - mae: 89.1571 - val_loss: 1187.5714 - val_mae: 1188.2646\n",
      "Epoch 1383/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 87.6744 - mae: 88.3588 - val_loss: 1271.1420 - val_mae: 1271.8340\n",
      "Epoch 1384/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 96.7967 - mae: 97.4811 - val_loss: 1408.3132 - val_mae: 1409.0065\n",
      "Epoch 1385/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 93.0132 - mae: 93.6926 - val_loss: 1510.8932 - val_mae: 1511.5865\n",
      "Epoch 1386/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 93.7274 - mae: 94.4098 - val_loss: 1161.7046 - val_mae: 1162.3978\n",
      "Epoch 1387/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 87.3596 - mae: 88.0436 - val_loss: 1354.4802 - val_mae: 1355.1731\n",
      "Epoch 1388/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 83.7757 - mae: 84.4568 - val_loss: 1426.0667 - val_mae: 1426.7598\n",
      "Epoch 1389/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 83.5839 - mae: 84.2629 - val_loss: 1384.8639 - val_mae: 1385.5563\n",
      "Epoch 1390/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 89.9293 - mae: 90.6088 - val_loss: 1321.5466 - val_mae: 1322.2399\n",
      "Epoch 1391/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 81.9744 - mae: 82.6556 - val_loss: 1531.8036 - val_mae: 1532.4954\n",
      "Epoch 1392/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 86.1932 - mae: 86.8738 - val_loss: 1437.9528 - val_mae: 1438.6460\n",
      "Epoch 1393/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 80.2149 - mae: 80.8929 - val_loss: 1215.1039 - val_mae: 1215.7970\n",
      "Epoch 1394/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 80.7529 - mae: 81.4324 - val_loss: 1421.2495 - val_mae: 1421.9427\n",
      "Epoch 1395/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 87.3006 - mae: 87.9807 - val_loss: 1336.2225 - val_mae: 1336.9159\n",
      "Epoch 1396/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 81.8761 - mae: 82.5590 - val_loss: 1296.0839 - val_mae: 1296.7771\n",
      "Epoch 1397/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 87.3803 - mae: 88.0645 - val_loss: 1283.7399 - val_mae: 1284.4329\n",
      "Epoch 1398/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 82.0881 - mae: 82.7714 - val_loss: 1434.7703 - val_mae: 1435.4635\n",
      "Epoch 1399/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 93.4268 - mae: 94.1088 - val_loss: 1455.3496 - val_mae: 1456.0426\n",
      "Epoch 1400/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 84.4404 - mae: 85.1204 - val_loss: 1332.0813 - val_mae: 1332.7744\n",
      "Epoch 1401/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 83.2320 - mae: 83.9159 - val_loss: 1235.5437 - val_mae: 1236.2367\n",
      "Epoch 1402/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 91.3376 - mae: 92.0168 - val_loss: 1123.6863 - val_mae: 1124.3793\n",
      "Epoch 1403/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 87.4581 - mae: 88.1406 - val_loss: 1308.5503 - val_mae: 1309.2422\n",
      "Epoch 1404/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 85.1242 - mae: 85.8055 - val_loss: 1332.6919 - val_mae: 1333.3849\n",
      "Epoch 1405/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 83.9222 - mae: 84.6042 - val_loss: 1355.1693 - val_mae: 1355.8623\n",
      "Epoch 1406/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 86.6341 - mae: 87.3197 - val_loss: 1366.4532 - val_mae: 1367.1464\n",
      "Epoch 1407/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 85.2564 - mae: 85.9379 - val_loss: 1225.5813 - val_mae: 1226.2744\n",
      "Epoch 1408/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 82.2915 - mae: 82.9734 - val_loss: 1363.6339 - val_mae: 1364.3271\n",
      "Epoch 1409/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 88.9609 - mae: 89.6371 - val_loss: 1343.5004 - val_mae: 1344.1929\n",
      "Epoch 1410/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 86.2484 - mae: 86.9315 - val_loss: 1242.3221 - val_mae: 1243.0148\n",
      "Epoch 1411/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 88.2209 - mae: 88.9000 - val_loss: 1293.9346 - val_mae: 1294.6277\n",
      "Epoch 1412/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 84.5070 - mae: 85.1903 - val_loss: 1180.2788 - val_mae: 1180.9720\n",
      "Epoch 1413/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 91.9063 - mae: 92.5905 - val_loss: 1249.7612 - val_mae: 1250.4545\n",
      "Epoch 1414/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 86.9589 - mae: 87.6420 - val_loss: 1272.7933 - val_mae: 1273.4865\n",
      "Epoch 1415/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 89.8869 - mae: 90.5686 - val_loss: 1181.7485 - val_mae: 1182.4418\n",
      "Epoch 1416/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 96.4726 - mae: 97.1523 - val_loss: 1347.4066 - val_mae: 1348.1000\n",
      "Epoch 1417/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 87.0679 - mae: 87.7497 - val_loss: 1411.4677 - val_mae: 1412.1606\n",
      "Epoch 1418/5000\n",
      "46/46 [==============================] - 2s 43ms/step - loss: 93.5417 - mae: 94.2255 - val_loss: 1275.7107 - val_mae: 1276.4039\n",
      "Epoch 1419/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 90.4403 - mae: 91.1266 - val_loss: 1283.6970 - val_mae: 1284.3890\n",
      "Epoch 1420/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 82.9453 - mae: 83.6277 - val_loss: 1321.0780 - val_mae: 1321.7704\n",
      "Epoch 1421/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 96.7899 - mae: 97.4713 - val_loss: 1178.5808 - val_mae: 1179.2739\n",
      "Epoch 1422/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 90.5015 - mae: 91.1855 - val_loss: 1243.4618 - val_mae: 1244.1538\n",
      "Epoch 1423/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 87.0819 - mae: 87.7641 - val_loss: 1381.2981 - val_mae: 1381.9908\n",
      "Epoch 1424/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 82.4565 - mae: 83.1379 - val_loss: 1481.5173 - val_mae: 1482.2103\n",
      "Epoch 1425/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 90.2755 - mae: 90.9599 - val_loss: 1471.2677 - val_mae: 1471.9594\n",
      "Epoch 1426/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 87.7812 - mae: 88.4640 - val_loss: 1329.7894 - val_mae: 1330.4821\n",
      "Epoch 1427/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 96.7285 - mae: 97.4103 - val_loss: 1230.2046 - val_mae: 1230.8977\n",
      "Epoch 1428/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 87.7404 - mae: 88.4223 - val_loss: 1319.7701 - val_mae: 1320.4625\n",
      "Epoch 1429/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 83.5016 - mae: 84.1841 - val_loss: 1352.0718 - val_mae: 1352.7648\n",
      "Epoch 1430/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 87.4755 - mae: 88.1587 - val_loss: 1183.1088 - val_mae: 1183.8016\n",
      "Epoch 1431/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 92.4047 - mae: 93.0870 - val_loss: 1274.2721 - val_mae: 1274.9653\n",
      "Epoch 1432/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 88.3998 - mae: 89.0846 - val_loss: 1190.1193 - val_mae: 1190.8118\n",
      "Epoch 1433/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 99.1169 - mae: 99.8012 - val_loss: 1599.5081 - val_mae: 1600.2013\n",
      "Epoch 1434/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 93.4155 - mae: 94.0988 - val_loss: 1331.7247 - val_mae: 1332.4178\n",
      "Epoch 1435/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 94.6199 - mae: 95.3020 - val_loss: 1324.9563 - val_mae: 1325.6493\n",
      "Epoch 1436/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 90.0530 - mae: 90.7345 - val_loss: 1258.1140 - val_mae: 1258.8073\n",
      "Epoch 1437/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 87.4376 - mae: 88.1207 - val_loss: 1457.1483 - val_mae: 1457.8414\n",
      "Epoch 1438/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 89.3715 - mae: 90.0557 - val_loss: 1219.5452 - val_mae: 1220.2383\n",
      "Epoch 1439/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 82.2474 - mae: 82.9291 - val_loss: 1173.8337 - val_mae: 1174.5256\n",
      "Epoch 1440/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 100.5677 - mae: 101.2527 - val_loss: 1269.2540 - val_mae: 1269.9471\n",
      "Epoch 1441/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 88.1024 - mae: 88.7849 - val_loss: 1275.2642 - val_mae: 1275.9574\n",
      "Epoch 1442/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 84.1210 - mae: 84.8042 - val_loss: 1343.4437 - val_mae: 1344.1368\n",
      "Epoch 1443/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 83.3132 - mae: 83.9940 - val_loss: 1260.6969 - val_mae: 1261.3896\n",
      "Epoch 1444/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 83.6470 - mae: 84.3302 - val_loss: 1249.4591 - val_mae: 1250.1525\n",
      "Epoch 1445/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 85.4988 - mae: 86.1819 - val_loss: 1249.4370 - val_mae: 1250.1298\n",
      "Epoch 1446/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 87.2678 - mae: 87.9508 - val_loss: 1145.2520 - val_mae: 1145.9441\n",
      "Epoch 1447/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 83.2818 - mae: 83.9619 - val_loss: 1477.8312 - val_mae: 1478.5242\n",
      "Epoch 1448/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 82.4460 - mae: 83.1229 - val_loss: 1386.3228 - val_mae: 1387.0153\n",
      "Epoch 1449/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 87.1666 - mae: 87.8497 - val_loss: 1408.9360 - val_mae: 1409.6282\n",
      "Epoch 1450/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 90.7767 - mae: 91.4572 - val_loss: 1260.3510 - val_mae: 1261.0439\n",
      "Epoch 1451/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 93.3266 - mae: 94.0107 - val_loss: 1207.9106 - val_mae: 1208.6039\n",
      "Epoch 1452/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 84.8669 - mae: 85.5503 - val_loss: 1347.8490 - val_mae: 1348.5421\n",
      "Epoch 1453/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 84.3660 - mae: 85.0500 - val_loss: 1370.1678 - val_mae: 1370.8610\n",
      "Epoch 1454/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 85.5388 - mae: 86.2209 - val_loss: 1368.0643 - val_mae: 1368.7574\n",
      "Epoch 1455/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 84.7382 - mae: 85.4196 - val_loss: 1090.0360 - val_mae: 1090.7291\n",
      "Epoch 1456/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 105.0651 - mae: 105.7461 - val_loss: 1184.5854 - val_mae: 1185.2786\n",
      "Epoch 1457/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 93.8566 - mae: 94.5377 - val_loss: 1277.7719 - val_mae: 1278.4650\n",
      "Epoch 1458/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 94.9316 - mae: 95.6174 - val_loss: 1219.3976 - val_mae: 1220.0911\n",
      "Epoch 1459/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 89.4858 - mae: 90.1697 - val_loss: 1070.9614 - val_mae: 1071.6547\n",
      "Epoch 1460/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 88.2729 - mae: 88.9556 - val_loss: 1243.3442 - val_mae: 1244.0376\n",
      "Epoch 1461/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 85.7606 - mae: 86.4436 - val_loss: 1115.3729 - val_mae: 1116.0637\n",
      "Epoch 1462/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 90.0225 - mae: 90.7061 - val_loss: 1080.9082 - val_mae: 1081.6000\n",
      "Epoch 1463/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 90.3785 - mae: 91.0607 - val_loss: 1255.7322 - val_mae: 1256.4242\n",
      "Epoch 1464/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 83.3625 - mae: 84.0450 - val_loss: 1158.2645 - val_mae: 1158.9576\n",
      "Epoch 1465/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 93.8435 - mae: 94.5310 - val_loss: 1119.6624 - val_mae: 1120.3552\n",
      "Epoch 1466/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 94.6892 - mae: 95.3694 - val_loss: 1472.5034 - val_mae: 1473.1964\n",
      "Epoch 1467/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 85.7201 - mae: 86.4017 - val_loss: 1183.2078 - val_mae: 1183.9005\n",
      "Epoch 1468/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 88.2056 - mae: 88.8865 - val_loss: 1203.8219 - val_mae: 1204.5145\n",
      "Epoch 1469/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 84.4143 - mae: 85.0967 - val_loss: 1428.3330 - val_mae: 1429.0261\n",
      "Epoch 1470/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 86.0326 - mae: 86.7174 - val_loss: 1280.3230 - val_mae: 1281.0155\n",
      "Epoch 1471/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 87.6622 - mae: 88.3425 - val_loss: 1323.5596 - val_mae: 1324.2527\n",
      "Epoch 1472/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 94.9094 - mae: 95.5917 - val_loss: 1134.4235 - val_mae: 1135.1166\n",
      "Epoch 1473/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 88.5237 - mae: 89.2066 - val_loss: 1236.9762 - val_mae: 1237.6689\n",
      "Epoch 1474/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 90.8684 - mae: 91.5491 - val_loss: 1175.8558 - val_mae: 1176.5483\n",
      "Epoch 1475/5000\n",
      "46/46 [==============================] - 2s 42ms/step - loss: 84.0045 - mae: 84.6856 - val_loss: 1331.2899 - val_mae: 1331.9829\n",
      "Epoch 1476/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 85.4772 - mae: 86.1625 - val_loss: 1360.1205 - val_mae: 1360.8136\n",
      "Epoch 1477/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 87.6901 - mae: 88.3740 - val_loss: 1445.5898 - val_mae: 1446.2828\n",
      "Epoch 1478/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 80.1221 - mae: 80.8071 - val_loss: 1351.7810 - val_mae: 1352.4738\n",
      "Epoch 1479/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 81.1625 - mae: 81.8478 - val_loss: 1293.4241 - val_mae: 1294.1166\n",
      "Epoch 1480/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 85.5612 - mae: 86.2448 - val_loss: 1390.7356 - val_mae: 1391.4288\n",
      "Epoch 1481/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 83.9414 - mae: 84.6223 - val_loss: 1316.3861 - val_mae: 1317.0785\n",
      "Epoch 1482/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 85.1520 - mae: 85.8340 - val_loss: 1231.3719 - val_mae: 1232.0649\n",
      "Epoch 1483/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 84.9429 - mae: 85.6226 - val_loss: 1268.2095 - val_mae: 1268.9026\n",
      "Epoch 1484/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 82.7970 - mae: 83.4816 - val_loss: 1232.3745 - val_mae: 1233.0675\n",
      "Epoch 1485/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 81.3513 - mae: 82.0329 - val_loss: 1565.9805 - val_mae: 1566.6732\n",
      "Epoch 1486/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 84.4526 - mae: 85.1349 - val_loss: 1440.6152 - val_mae: 1441.3083\n",
      "Epoch 1487/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 89.8156 - mae: 90.4998 - val_loss: 1252.1250 - val_mae: 1252.8179\n",
      "Epoch 1488/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 82.9674 - mae: 83.6481 - val_loss: 1151.9454 - val_mae: 1152.6377\n",
      "Epoch 1489/5000\n",
      "46/46 [==============================] - 2s 40ms/step - loss: 84.4676 - mae: 85.1495 - val_loss: 1310.6780 - val_mae: 1311.3710\n",
      "Epoch 1490/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 78.1448 - mae: 78.8237 - val_loss: 1185.2244 - val_mae: 1185.9165\n",
      "Epoch 1491/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 79.2978 - mae: 79.9785 - val_loss: 1326.6322 - val_mae: 1327.3256\n",
      "Epoch 1492/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 84.9160 - mae: 85.5987 - val_loss: 1083.1156 - val_mae: 1083.8083\n",
      "Epoch 1493/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 94.6178 - mae: 95.3000 - val_loss: 1504.5665 - val_mae: 1505.2594\n",
      "Epoch 1494/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 83.1841 - mae: 83.8639 - val_loss: 1294.2260 - val_mae: 1294.9183\n",
      "Epoch 1495/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 82.4987 - mae: 83.1791 - val_loss: 1298.5818 - val_mae: 1299.2750\n",
      "Epoch 1496/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 85.6625 - mae: 86.3406 - val_loss: 1234.9130 - val_mae: 1235.6057\n",
      "Epoch 1497/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 82.6913 - mae: 83.3740 - val_loss: 1320.5822 - val_mae: 1321.2754\n",
      "Epoch 1498/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 82.3183 - mae: 83.0003 - val_loss: 1358.2351 - val_mae: 1358.9282\n",
      "Epoch 1499/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 82.7892 - mae: 83.4746 - val_loss: 1218.5801 - val_mae: 1219.2729\n",
      "Epoch 1500/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 80.1690 - mae: 80.8536 - val_loss: 1286.2548 - val_mae: 1286.9476\n",
      "Epoch 1501/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 79.4241 - mae: 80.1027 - val_loss: 1305.3024 - val_mae: 1305.9952\n",
      "Epoch 1502/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 90.1495 - mae: 90.8309 - val_loss: 1399.3962 - val_mae: 1400.0894\n",
      "Epoch 1503/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 94.0502 - mae: 94.7329 - val_loss: 1489.7980 - val_mae: 1490.4906\n",
      "Epoch 1504/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 91.7999 - mae: 92.4855 - val_loss: 1393.1396 - val_mae: 1393.8315\n",
      "Epoch 1505/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 81.2198 - mae: 81.9000 - val_loss: 1195.1415 - val_mae: 1195.8340\n",
      "Epoch 1506/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 98.5155 - mae: 99.1982 - val_loss: 1520.1498 - val_mae: 1520.8429\n",
      "Epoch 1507/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 81.9699 - mae: 82.6502 - val_loss: 1257.1423 - val_mae: 1257.8352\n",
      "Epoch 1508/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 87.8772 - mae: 88.5587 - val_loss: 1276.1931 - val_mae: 1276.8859\n",
      "Epoch 1509/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 100.4883 - mae: 101.1729 - val_loss: 1200.4418 - val_mae: 1201.1346\n",
      "Epoch 1510/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 86.2783 - mae: 86.9611 - val_loss: 1290.5808 - val_mae: 1291.2739\n",
      "Epoch 1511/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 85.8025 - mae: 86.4853 - val_loss: 1200.8364 - val_mae: 1201.5288\n",
      "Epoch 1512/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 89.5619 - mae: 90.2450 - val_loss: 1340.8802 - val_mae: 1341.5731\n",
      "Epoch 1513/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 90.9998 - mae: 91.6831 - val_loss: 1224.0769 - val_mae: 1224.7699\n",
      "Epoch 1514/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 87.4468 - mae: 88.1328 - val_loss: 1436.1566 - val_mae: 1436.8499\n",
      "Epoch 1515/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 82.3998 - mae: 83.0814 - val_loss: 1351.6234 - val_mae: 1352.3168\n",
      "Epoch 1516/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 79.0603 - mae: 79.7409 - val_loss: 1523.9901 - val_mae: 1524.6827\n",
      "Epoch 1517/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 82.3904 - mae: 83.0689 - val_loss: 1345.1033 - val_mae: 1345.7964\n",
      "Epoch 1518/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 76.5850 - mae: 77.2633 - val_loss: 1276.4308 - val_mae: 1277.1240\n",
      "Epoch 1519/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 85.1964 - mae: 85.8775 - val_loss: 1222.1443 - val_mae: 1222.8374\n",
      "Epoch 1520/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 100.7323 - mae: 101.4139 - val_loss: 1253.4244 - val_mae: 1254.1171\n",
      "Epoch 1521/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 109.1221 - mae: 109.8075 - val_loss: 1541.7014 - val_mae: 1542.3942\n",
      "Epoch 1522/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 91.4555 - mae: 92.1383 - val_loss: 1320.8203 - val_mae: 1321.5133\n",
      "Epoch 1523/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 91.1977 - mae: 91.8792 - val_loss: 1209.5411 - val_mae: 1210.2339\n",
      "Epoch 1524/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 86.6297 - mae: 87.3132 - val_loss: 1245.2728 - val_mae: 1245.9652\n",
      "Epoch 1525/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 93.3129 - mae: 93.9974 - val_loss: 1530.8688 - val_mae: 1531.5619\n",
      "Epoch 1526/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 83.1006 - mae: 83.7867 - val_loss: 1400.6814 - val_mae: 1401.3746\n",
      "Epoch 1527/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 84.2257 - mae: 84.9051 - val_loss: 1412.3241 - val_mae: 1413.0173\n",
      "Epoch 1528/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 89.3804 - mae: 90.0609 - val_loss: 1251.6727 - val_mae: 1252.3660\n",
      "Epoch 1529/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 76.6280 - mae: 77.3075 - val_loss: 1382.4758 - val_mae: 1383.1688\n",
      "Epoch 1530/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 82.7588 - mae: 83.4417 - val_loss: 1269.3264 - val_mae: 1270.0195\n",
      "Epoch 1531/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 84.5205 - mae: 85.2023 - val_loss: 1202.1917 - val_mae: 1202.8843\n",
      "Epoch 1532/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 85.3580 - mae: 86.0414 - val_loss: 1483.4779 - val_mae: 1484.1710\n",
      "Epoch 1533/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 87.2566 - mae: 87.9386 - val_loss: 1292.6116 - val_mae: 1293.3048\n",
      "Epoch 1534/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 90.8348 - mae: 91.5182 - val_loss: 1362.9851 - val_mae: 1363.6785\n",
      "Epoch 1535/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 86.9757 - mae: 87.6551 - val_loss: 1339.0234 - val_mae: 1339.7166\n",
      "Epoch 1536/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 89.5087 - mae: 90.1886 - val_loss: 1353.1011 - val_mae: 1353.7943\n",
      "Epoch 1537/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 86.2803 - mae: 86.9609 - val_loss: 1353.5636 - val_mae: 1354.2566\n",
      "Epoch 1538/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 81.8274 - mae: 82.5098 - val_loss: 1349.6217 - val_mae: 1350.3149\n",
      "Epoch 1539/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 85.4234 - mae: 86.1088 - val_loss: 1062.0463 - val_mae: 1062.7386\n",
      "Epoch 1540/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 90.1961 - mae: 90.8815 - val_loss: 1389.2849 - val_mae: 1389.9781\n",
      "Epoch 1541/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 87.5350 - mae: 88.2187 - val_loss: 1149.8970 - val_mae: 1150.5896\n",
      "Epoch 1542/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 93.8967 - mae: 94.5784 - val_loss: 1288.1173 - val_mae: 1288.8105\n",
      "Epoch 1543/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 80.3029 - mae: 80.9806 - val_loss: 1257.0415 - val_mae: 1257.7346\n",
      "Epoch 1544/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 80.0342 - mae: 80.7148 - val_loss: 1414.5728 - val_mae: 1415.2660\n",
      "Epoch 1545/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 89.3285 - mae: 90.0091 - val_loss: 1432.2555 - val_mae: 1432.9485\n",
      "Epoch 1546/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 79.3614 - mae: 80.0429 - val_loss: 1328.6788 - val_mae: 1329.3717\n",
      "Epoch 1547/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 89.4945 - mae: 90.1794 - val_loss: 1453.6318 - val_mae: 1454.3251\n",
      "Epoch 1548/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 86.4295 - mae: 87.1133 - val_loss: 1313.3483 - val_mae: 1314.0414\n",
      "Epoch 1549/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 85.0501 - mae: 85.7314 - val_loss: 1463.2191 - val_mae: 1463.9114\n",
      "Epoch 1550/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 93.9275 - mae: 94.6113 - val_loss: 1288.5264 - val_mae: 1289.2195\n",
      "Epoch 1551/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 83.4461 - mae: 84.1300 - val_loss: 1283.5999 - val_mae: 1284.2928\n",
      "Epoch 1552/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 83.8801 - mae: 84.5593 - val_loss: 1299.9215 - val_mae: 1300.6139\n",
      "Epoch 1553/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 78.8061 - mae: 79.4862 - val_loss: 1343.5571 - val_mae: 1344.2504\n",
      "Epoch 1554/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 78.7034 - mae: 79.3855 - val_loss: 1371.3254 - val_mae: 1372.0170\n",
      "Epoch 1555/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 85.9313 - mae: 86.6120 - val_loss: 1363.3696 - val_mae: 1364.0620\n",
      "Epoch 1556/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 89.9144 - mae: 90.5988 - val_loss: 1179.0034 - val_mae: 1179.6964\n",
      "Epoch 1557/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 87.1888 - mae: 87.8728 - val_loss: 1039.3540 - val_mae: 1040.0466\n",
      "Epoch 1558/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 87.0017 - mae: 87.6853 - val_loss: 1285.4902 - val_mae: 1286.1832\n",
      "Epoch 1559/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 80.3046 - mae: 80.9868 - val_loss: 1185.3987 - val_mae: 1186.0919\n",
      "Epoch 1560/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 88.8481 - mae: 89.5297 - val_loss: 1182.1278 - val_mae: 1182.8209\n",
      "Epoch 1561/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 91.2504 - mae: 91.9319 - val_loss: 1305.7474 - val_mae: 1306.4393\n",
      "Epoch 1562/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 82.0639 - mae: 82.7435 - val_loss: 1431.9685 - val_mae: 1432.6616\n",
      "Epoch 1563/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 93.3506 - mae: 94.0313 - val_loss: 1305.9803 - val_mae: 1306.6735\n",
      "Epoch 1564/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 84.5274 - mae: 85.2112 - val_loss: 1293.0103 - val_mae: 1293.7032\n",
      "Epoch 1565/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 82.3842 - mae: 83.0679 - val_loss: 1267.5190 - val_mae: 1268.2112\n",
      "Epoch 1566/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 79.5535 - mae: 80.2328 - val_loss: 1190.3148 - val_mae: 1191.0079\n",
      "Epoch 1567/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 79.7403 - mae: 80.4240 - val_loss: 1261.0527 - val_mae: 1261.7457\n",
      "Epoch 1568/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 84.5594 - mae: 85.2434 - val_loss: 1254.1187 - val_mae: 1254.8118\n",
      "Epoch 1569/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 81.6649 - mae: 82.3487 - val_loss: 1278.2974 - val_mae: 1278.9905\n",
      "Epoch 1570/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 83.1178 - mae: 83.7985 - val_loss: 1209.3690 - val_mae: 1210.0621\n",
      "Epoch 1571/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 88.9329 - mae: 89.6191 - val_loss: 1386.5782 - val_mae: 1387.2714\n",
      "Epoch 1572/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 87.0807 - mae: 87.7666 - val_loss: 1243.2532 - val_mae: 1243.9459\n",
      "Epoch 1573/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 78.0721 - mae: 78.7561 - val_loss: 1119.8204 - val_mae: 1120.5121\n",
      "Epoch 1574/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 96.7710 - mae: 97.4544 - val_loss: 1059.4537 - val_mae: 1060.1451\n",
      "Epoch 1575/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 92.9051 - mae: 93.5887 - val_loss: 1339.6512 - val_mae: 1340.3442\n",
      "Epoch 1576/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 81.1999 - mae: 81.8782 - val_loss: 1564.8624 - val_mae: 1565.5533\n",
      "Epoch 1577/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 83.8947 - mae: 84.5779 - val_loss: 1229.6902 - val_mae: 1230.3832\n",
      "Epoch 1578/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 78.0332 - mae: 78.7122 - val_loss: 1334.3218 - val_mae: 1335.0145\n",
      "Epoch 1579/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 90.4274 - mae: 91.1079 - val_loss: 1267.6179 - val_mae: 1268.3109\n",
      "Epoch 1580/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 89.1995 - mae: 89.8856 - val_loss: 1256.7745 - val_mae: 1257.4678\n",
      "Epoch 1581/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 89.7352 - mae: 90.4183 - val_loss: 1284.1848 - val_mae: 1284.8782\n",
      "Epoch 1582/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 92.5607 - mae: 93.2404 - val_loss: 1491.6324 - val_mae: 1492.3247\n",
      "Epoch 1583/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 83.2611 - mae: 83.9473 - val_loss: 1272.6910 - val_mae: 1273.3838\n",
      "Epoch 1584/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 77.6482 - mae: 78.3311 - val_loss: 1208.5160 - val_mae: 1209.2090\n",
      "Epoch 1585/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 84.7216 - mae: 85.4014 - val_loss: 1251.8594 - val_mae: 1252.5515\n",
      "Epoch 1586/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 99.0898 - mae: 99.7746 - val_loss: 1355.6633 - val_mae: 1356.3563\n",
      "Epoch 1587/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 96.0784 - mae: 96.7643 - val_loss: 1329.3788 - val_mae: 1330.0719\n",
      "Epoch 1588/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 86.2756 - mae: 86.9588 - val_loss: 1489.3542 - val_mae: 1490.0476\n",
      "Epoch 1589/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 93.9134 - mae: 94.5950 - val_loss: 1101.9155 - val_mae: 1102.6086\n",
      "Epoch 1590/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 88.7596 - mae: 89.4422 - val_loss: 1503.7074 - val_mae: 1504.4009\n",
      "Epoch 1591/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 89.0608 - mae: 89.7436 - val_loss: 1249.9009 - val_mae: 1250.5939\n",
      "Epoch 1592/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 87.8534 - mae: 88.5344 - val_loss: 1239.6954 - val_mae: 1240.3882\n",
      "Epoch 1593/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 81.4452 - mae: 82.1299 - val_loss: 1269.0112 - val_mae: 1269.7042\n",
      "Epoch 1594/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 78.5183 - mae: 79.1985 - val_loss: 1239.9575 - val_mae: 1240.6506\n",
      "Epoch 1595/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 86.1114 - mae: 86.7954 - val_loss: 1243.9279 - val_mae: 1244.6210\n",
      "Epoch 1596/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 80.7816 - mae: 81.4588 - val_loss: 1167.0214 - val_mae: 1167.7147\n",
      "Epoch 1597/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 83.6087 - mae: 84.2907 - val_loss: 1288.6089 - val_mae: 1289.3003\n",
      "Epoch 1598/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 84.9404 - mae: 85.6250 - val_loss: 1160.3539 - val_mae: 1161.0469\n",
      "Epoch 1599/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 78.0997 - mae: 78.7824 - val_loss: 1323.9948 - val_mae: 1324.6874\n",
      "Epoch 1600/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 84.9344 - mae: 85.6149 - val_loss: 1276.4521 - val_mae: 1277.1449\n",
      "Epoch 1601/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 83.4276 - mae: 84.1120 - val_loss: 1496.1760 - val_mae: 1496.8691\n",
      "Epoch 1602/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 86.0689 - mae: 86.7526 - val_loss: 1494.4293 - val_mae: 1495.1223\n",
      "Epoch 1603/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 93.4071 - mae: 94.0911 - val_loss: 1290.9283 - val_mae: 1291.6216\n",
      "Epoch 1604/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 83.5778 - mae: 84.2596 - val_loss: 1134.7771 - val_mae: 1135.4703\n",
      "Epoch 1605/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 89.1003 - mae: 89.7820 - val_loss: 1369.5059 - val_mae: 1370.1990\n",
      "Epoch 1606/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 85.9657 - mae: 86.6491 - val_loss: 1382.6464 - val_mae: 1383.3396\n",
      "Epoch 1607/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 81.1688 - mae: 81.8491 - val_loss: 1358.3451 - val_mae: 1359.0382\n",
      "Epoch 1608/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 81.8422 - mae: 82.5269 - val_loss: 1471.9761 - val_mae: 1472.6691\n",
      "Epoch 1609/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 85.7046 - mae: 86.3863 - val_loss: 1278.5892 - val_mae: 1279.2814\n",
      "Epoch 1610/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 82.8922 - mae: 83.5760 - val_loss: 1226.4697 - val_mae: 1227.1627\n",
      "Epoch 1611/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 77.8265 - mae: 78.5065 - val_loss: 1421.0802 - val_mae: 1421.7728\n",
      "Epoch 1612/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 79.9948 - mae: 80.6770 - val_loss: 1376.4089 - val_mae: 1377.1019\n",
      "Epoch 1613/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 90.3429 - mae: 91.0268 - val_loss: 1522.7435 - val_mae: 1523.4364\n",
      "Epoch 1614/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 84.5107 - mae: 85.1931 - val_loss: 1223.7190 - val_mae: 1224.4110\n",
      "Epoch 1615/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 78.2202 - mae: 78.9020 - val_loss: 1140.3693 - val_mae: 1141.0625\n",
      "Epoch 1616/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 88.2466 - mae: 88.9294 - val_loss: 1214.0388 - val_mae: 1214.7308\n",
      "Epoch 1617/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 84.9526 - mae: 85.6365 - val_loss: 1450.2727 - val_mae: 1450.9641\n",
      "Epoch 1618/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 84.2544 - mae: 84.9362 - val_loss: 1264.3005 - val_mae: 1264.9938\n",
      "Epoch 1619/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 84.3601 - mae: 85.0405 - val_loss: 1195.8109 - val_mae: 1196.5042\n",
      "Epoch 1620/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 90.8216 - mae: 91.5067 - val_loss: 1389.4940 - val_mae: 1390.1871\n",
      "Epoch 1621/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 93.9626 - mae: 94.6475 - val_loss: 1430.4000 - val_mae: 1431.0928\n",
      "Epoch 1622/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 84.0863 - mae: 84.7675 - val_loss: 1502.2576 - val_mae: 1502.9510\n",
      "Epoch 1623/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 90.6740 - mae: 91.3546 - val_loss: 1461.3271 - val_mae: 1462.0201\n",
      "Epoch 1624/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 83.0456 - mae: 83.7291 - val_loss: 1270.2249 - val_mae: 1270.9161\n",
      "Epoch 1625/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 86.8186 - mae: 87.5019 - val_loss: 1126.2532 - val_mae: 1126.9462\n",
      "Epoch 1626/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 78.5600 - mae: 79.2438 - val_loss: 1247.5190 - val_mae: 1248.2120\n",
      "Epoch 1627/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 88.4558 - mae: 89.1426 - val_loss: 1237.6066 - val_mae: 1238.2993\n",
      "Epoch 1628/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 87.5908 - mae: 88.2709 - val_loss: 1147.3854 - val_mae: 1148.0779\n",
      "Epoch 1629/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 85.3519 - mae: 86.0381 - val_loss: 1403.9227 - val_mae: 1404.6160\n",
      "Epoch 1630/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 95.1358 - mae: 95.8217 - val_loss: 1212.4757 - val_mae: 1213.1680\n",
      "Epoch 1631/5000\n",
      "46/46 [==============================] - 2s 40ms/step - loss: 87.0489 - mae: 87.7327 - val_loss: 1339.8951 - val_mae: 1340.5884\n",
      "Epoch 1632/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 81.9925 - mae: 82.6716 - val_loss: 1267.2583 - val_mae: 1267.9514\n",
      "Epoch 1633/5000\n",
      "46/46 [==============================] - 2s 42ms/step - loss: 79.7866 - mae: 80.4659 - val_loss: 1327.0568 - val_mae: 1327.7496\n",
      "Epoch 1634/5000\n",
      "46/46 [==============================] - 2s 47ms/step - loss: 84.3674 - mae: 85.0484 - val_loss: 1296.1288 - val_mae: 1296.8210\n",
      "Epoch 1635/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 79.6034 - mae: 80.2855 - val_loss: 1328.9751 - val_mae: 1329.6682\n",
      "Epoch 1636/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 78.4836 - mae: 79.1648 - val_loss: 1212.5916 - val_mae: 1213.2841\n",
      "Epoch 1637/5000\n",
      "46/46 [==============================] - 2s 44ms/step - loss: 80.8039 - mae: 81.4835 - val_loss: 1257.1340 - val_mae: 1257.8270\n",
      "Epoch 1638/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 83.9444 - mae: 84.6268 - val_loss: 1221.3901 - val_mae: 1222.0830\n",
      "Epoch 1639/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 78.2288 - mae: 78.9085 - val_loss: 1305.6732 - val_mae: 1306.3662\n",
      "Epoch 1640/5000\n",
      "46/46 [==============================] - 2s 41ms/step - loss: 78.2474 - mae: 78.9292 - val_loss: 1208.9896 - val_mae: 1209.6826\n",
      "Epoch 1641/5000\n",
      "46/46 [==============================] - 2s 43ms/step - loss: 83.8726 - mae: 84.5486 - val_loss: 1400.6735 - val_mae: 1401.3668\n",
      "Epoch 1642/5000\n",
      "46/46 [==============================] - 2s 46ms/step - loss: 86.3766 - mae: 87.0600 - val_loss: 1296.9700 - val_mae: 1297.6613\n",
      "Epoch 1643/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 83.4829 - mae: 84.1672 - val_loss: 1165.3656 - val_mae: 1166.0587\n",
      "Epoch 1644/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 76.9064 - mae: 77.5890 - val_loss: 1342.0145 - val_mae: 1342.7072\n",
      "Epoch 1645/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 97.2142 - mae: 97.9000 - val_loss: 1092.3196 - val_mae: 1093.0128\n",
      "Epoch 1646/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 85.3933 - mae: 86.0780 - val_loss: 1197.1895 - val_mae: 1197.8826\n",
      "Epoch 1647/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 90.0188 - mae: 90.7036 - val_loss: 1163.4480 - val_mae: 1164.1412\n",
      "Epoch 1648/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 78.7294 - mae: 79.4126 - val_loss: 1373.0653 - val_mae: 1373.7582\n",
      "Epoch 1649/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 80.1454 - mae: 80.8265 - val_loss: 1200.9283 - val_mae: 1201.6201\n",
      "Epoch 1650/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 90.1252 - mae: 90.8096 - val_loss: 1187.7740 - val_mae: 1188.4673\n",
      "Epoch 1651/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 79.4106 - mae: 80.0948 - val_loss: 1145.6283 - val_mae: 1146.3208\n",
      "Epoch 1652/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 84.3458 - mae: 85.0293 - val_loss: 1172.1163 - val_mae: 1172.8088\n",
      "Epoch 1653/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 81.0724 - mae: 81.7550 - val_loss: 1157.5288 - val_mae: 1158.2218\n",
      "Epoch 1654/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 83.9281 - mae: 84.6147 - val_loss: 1088.5400 - val_mae: 1089.2330\n",
      "Epoch 1655/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 87.8447 - mae: 88.5259 - val_loss: 1350.0225 - val_mae: 1350.7159\n",
      "Epoch 1656/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 85.6051 - mae: 86.2884 - val_loss: 1282.7327 - val_mae: 1283.4258\n",
      "Epoch 1657/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 78.9393 - mae: 79.6228 - val_loss: 1205.2502 - val_mae: 1205.9435\n",
      "Epoch 1658/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 80.9626 - mae: 81.6418 - val_loss: 1364.5408 - val_mae: 1365.2341\n",
      "Epoch 1659/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 100.1227 - mae: 100.8074 - val_loss: 1243.0057 - val_mae: 1243.6980\n",
      "Epoch 1660/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 88.9672 - mae: 89.6462 - val_loss: 1068.9777 - val_mae: 1069.6707\n",
      "Epoch 1661/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 87.0651 - mae: 87.7475 - val_loss: 1368.3268 - val_mae: 1369.0188\n",
      "Epoch 1662/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 78.1539 - mae: 78.8344 - val_loss: 1124.1989 - val_mae: 1124.8921\n",
      "Epoch 1663/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 79.5323 - mae: 80.2127 - val_loss: 1263.6747 - val_mae: 1264.3668\n",
      "Epoch 1664/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 87.8653 - mae: 88.5482 - val_loss: 1260.2269 - val_mae: 1260.9187\n",
      "Epoch 1665/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 81.8539 - mae: 82.5340 - val_loss: 1265.8314 - val_mae: 1266.5245\n",
      "Epoch 1666/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 78.2006 - mae: 78.8848 - val_loss: 1227.8871 - val_mae: 1228.5802\n",
      "Epoch 1667/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 81.9534 - mae: 82.6395 - val_loss: 1128.8276 - val_mae: 1129.5208\n",
      "Epoch 1668/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 78.4749 - mae: 79.1545 - val_loss: 1213.6648 - val_mae: 1214.3553\n",
      "Epoch 1669/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 86.5882 - mae: 87.2686 - val_loss: 1301.0399 - val_mae: 1301.7330\n",
      "Epoch 1670/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 87.3856 - mae: 88.0675 - val_loss: 1266.5773 - val_mae: 1267.2705\n",
      "Epoch 1671/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 86.0320 - mae: 86.7177 - val_loss: 1315.8357 - val_mae: 1316.5289\n",
      "Epoch 1672/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 85.1445 - mae: 85.8221 - val_loss: 1218.5701 - val_mae: 1219.2628\n",
      "Epoch 1673/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 81.0340 - mae: 81.7192 - val_loss: 1309.9756 - val_mae: 1310.6678\n",
      "Epoch 1674/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 76.2677 - mae: 76.9470 - val_loss: 1103.3260 - val_mae: 1104.0190\n",
      "Epoch 1675/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 79.5671 - mae: 80.2493 - val_loss: 1328.8885 - val_mae: 1329.5818\n",
      "Epoch 1676/5000\n",
      "46/46 [==============================] - 2s 42ms/step - loss: 75.9714 - mae: 76.6527 - val_loss: 1274.2747 - val_mae: 1274.9675\n",
      "Epoch 1677/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 77.5460 - mae: 78.2281 - val_loss: 1146.9550 - val_mae: 1147.6481\n",
      "Epoch 1678/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 82.9967 - mae: 83.6797 - val_loss: 1264.1732 - val_mae: 1264.8662\n",
      "Epoch 1679/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 76.2408 - mae: 76.9208 - val_loss: 1263.1188 - val_mae: 1263.8118\n",
      "Epoch 1680/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 77.3315 - mae: 78.0131 - val_loss: 1430.3074 - val_mae: 1431.0007\n",
      "Epoch 1681/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 78.4463 - mae: 79.1281 - val_loss: 1215.9154 - val_mae: 1216.6084\n",
      "Epoch 1682/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 76.9309 - mae: 77.6131 - val_loss: 1257.4476 - val_mae: 1258.1409\n",
      "Epoch 1683/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 81.5886 - mae: 82.2695 - val_loss: 1184.7361 - val_mae: 1185.4292\n",
      "Epoch 1684/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 85.7282 - mae: 86.4141 - val_loss: 1073.5409 - val_mae: 1074.2336\n",
      "Epoch 1685/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 91.0624 - mae: 91.7474 - val_loss: 1333.3187 - val_mae: 1334.0117\n",
      "Epoch 1686/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 78.9276 - mae: 79.6111 - val_loss: 1304.9718 - val_mae: 1305.6649\n",
      "Epoch 1687/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 76.5997 - mae: 77.2823 - val_loss: 1271.3423 - val_mae: 1272.0354\n",
      "Epoch 1688/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 77.2600 - mae: 77.9419 - val_loss: 1232.4938 - val_mae: 1233.1860\n",
      "Epoch 1689/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 81.8198 - mae: 82.5003 - val_loss: 1281.4395 - val_mae: 1282.1324\n",
      "Epoch 1690/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 85.3069 - mae: 85.9873 - val_loss: 1362.7229 - val_mae: 1363.4163\n",
      "Epoch 1691/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 79.2210 - mae: 79.9043 - val_loss: 1449.3209 - val_mae: 1450.0133\n",
      "Epoch 1692/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 82.1918 - mae: 82.8723 - val_loss: 1365.7949 - val_mae: 1366.4883\n",
      "Epoch 1693/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 74.3221 - mae: 75.0054 - val_loss: 1273.7472 - val_mae: 1274.4406\n",
      "Epoch 1694/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 84.9974 - mae: 85.6796 - val_loss: 1362.3403 - val_mae: 1363.0333\n",
      "Epoch 1695/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 77.9395 - mae: 78.6191 - val_loss: 1215.6082 - val_mae: 1216.3011\n",
      "Epoch 1696/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 77.2899 - mae: 77.9668 - val_loss: 1225.0596 - val_mae: 1225.7529\n",
      "Epoch 1697/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 82.0966 - mae: 82.7802 - val_loss: 1480.7108 - val_mae: 1481.4042\n",
      "Epoch 1698/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 79.0201 - mae: 79.7034 - val_loss: 1220.6521 - val_mae: 1221.3451\n",
      "Epoch 1699/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 79.1030 - mae: 79.7823 - val_loss: 1344.4670 - val_mae: 1345.1600\n",
      "Epoch 1700/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 84.7509 - mae: 85.4339 - val_loss: 1649.2148 - val_mae: 1649.9078\n",
      "Epoch 1701/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 102.5807 - mae: 103.2664 - val_loss: 1324.1608 - val_mae: 1324.8538\n",
      "Epoch 1702/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 77.2384 - mae: 77.9196 - val_loss: 1279.1208 - val_mae: 1279.8140\n",
      "Epoch 1703/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 76.5603 - mae: 77.2399 - val_loss: 1382.8806 - val_mae: 1383.5736\n",
      "Epoch 1704/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 75.4805 - mae: 76.1585 - val_loss: 1405.9507 - val_mae: 1406.6438\n",
      "Epoch 1705/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 81.1461 - mae: 81.8245 - val_loss: 1198.0616 - val_mae: 1198.7549\n",
      "Epoch 1706/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 76.8535 - mae: 77.5353 - val_loss: 1364.7350 - val_mae: 1365.4283\n",
      "Epoch 1707/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 84.6494 - mae: 85.3318 - val_loss: 1349.9990 - val_mae: 1350.6921\n",
      "Epoch 1708/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 83.2123 - mae: 83.8914 - val_loss: 1365.5850 - val_mae: 1366.2780\n",
      "Epoch 1709/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 80.7819 - mae: 81.4609 - val_loss: 1214.2599 - val_mae: 1214.9532\n",
      "Epoch 1710/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 84.6041 - mae: 85.2854 - val_loss: 1314.5878 - val_mae: 1315.2806\n",
      "Epoch 1711/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 77.6500 - mae: 78.3339 - val_loss: 1134.1912 - val_mae: 1134.8842\n",
      "Epoch 1712/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 81.4628 - mae: 82.1424 - val_loss: 1233.5212 - val_mae: 1234.2136\n",
      "Epoch 1713/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 79.4654 - mae: 80.1513 - val_loss: 1372.9169 - val_mae: 1373.6100\n",
      "Epoch 1714/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 81.4147 - mae: 82.0981 - val_loss: 1249.4834 - val_mae: 1250.1766\n",
      "Epoch 1715/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 76.5948 - mae: 77.2748 - val_loss: 1345.1488 - val_mae: 1345.8408\n",
      "Epoch 1716/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 81.6279 - mae: 82.3123 - val_loss: 1475.7948 - val_mae: 1476.4875\n",
      "Epoch 1717/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 73.4066 - mae: 74.0872 - val_loss: 1463.1106 - val_mae: 1463.8022\n",
      "Epoch 1718/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 80.1290 - mae: 80.8127 - val_loss: 1285.1180 - val_mae: 1285.8113\n",
      "Epoch 1719/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 88.5849 - mae: 89.2694 - val_loss: 1293.7062 - val_mae: 1294.3988\n",
      "Epoch 1720/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 82.5447 - mae: 83.2289 - val_loss: 1315.5930 - val_mae: 1316.2856\n",
      "Epoch 1721/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 81.5917 - mae: 82.2731 - val_loss: 1357.2900 - val_mae: 1357.9825\n",
      "Epoch 1722/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 78.9588 - mae: 79.6394 - val_loss: 1298.8074 - val_mae: 1299.5006\n",
      "Epoch 1723/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 78.3354 - mae: 79.0196 - val_loss: 1371.2195 - val_mae: 1371.9125\n",
      "Epoch 1724/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 75.2744 - mae: 75.9531 - val_loss: 1412.9757 - val_mae: 1413.6688\n",
      "Epoch 1725/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 87.4172 - mae: 88.0992 - val_loss: 1498.6674 - val_mae: 1499.3594\n",
      "Epoch 1726/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 93.2634 - mae: 93.9464 - val_loss: 1469.5859 - val_mae: 1470.2783\n",
      "Epoch 1727/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 87.8735 - mae: 88.5569 - val_loss: 1575.6702 - val_mae: 1576.3633\n",
      "Epoch 1728/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 88.1651 - mae: 88.8485 - val_loss: 1502.2933 - val_mae: 1502.9865\n",
      "Epoch 1729/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 80.1783 - mae: 80.8595 - val_loss: 1264.7023 - val_mae: 1265.3954\n",
      "Epoch 1730/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 85.7394 - mae: 86.4225 - val_loss: 1264.5490 - val_mae: 1265.2418\n",
      "Epoch 1731/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 90.5941 - mae: 91.2759 - val_loss: 1225.3549 - val_mae: 1226.0471\n",
      "Epoch 1732/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 81.5316 - mae: 82.2140 - val_loss: 1296.6545 - val_mae: 1297.3477\n",
      "Epoch 1733/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 81.8425 - mae: 82.5237 - val_loss: 1181.3479 - val_mae: 1182.0406\n",
      "Epoch 1734/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 76.1989 - mae: 76.8789 - val_loss: 1257.7009 - val_mae: 1258.3940\n",
      "Epoch 1735/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 93.7806 - mae: 94.4635 - val_loss: 1384.0312 - val_mae: 1384.7244\n",
      "Epoch 1736/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 82.3217 - mae: 83.0015 - val_loss: 1555.2167 - val_mae: 1555.9095\n",
      "Epoch 1737/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 80.6802 - mae: 81.3603 - val_loss: 1218.0187 - val_mae: 1218.7101\n",
      "Epoch 1738/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 85.9859 - mae: 86.6692 - val_loss: 1371.9688 - val_mae: 1372.6619\n",
      "Epoch 1739/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 98.5053 - mae: 99.1864 - val_loss: 1523.2595 - val_mae: 1523.9528\n",
      "Epoch 1740/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 79.4224 - mae: 80.1062 - val_loss: 1342.0967 - val_mae: 1342.7876\n",
      "Epoch 1741/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 96.0471 - mae: 96.7320 - val_loss: 1425.2664 - val_mae: 1425.9596\n",
      "Epoch 1742/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 84.6705 - mae: 85.3541 - val_loss: 1409.5231 - val_mae: 1410.2156\n",
      "Epoch 1743/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 85.2553 - mae: 85.9399 - val_loss: 1206.4194 - val_mae: 1207.1125\n",
      "Epoch 1744/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 82.4581 - mae: 83.1396 - val_loss: 1279.4298 - val_mae: 1280.1229\n",
      "Epoch 1745/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 80.2636 - mae: 80.9445 - val_loss: 1387.8774 - val_mae: 1388.5703\n",
      "Epoch 1746/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 84.1075 - mae: 84.7890 - val_loss: 1349.4006 - val_mae: 1350.0938\n",
      "Epoch 1747/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 84.3399 - mae: 85.0251 - val_loss: 1502.2596 - val_mae: 1502.9520\n",
      "Epoch 1748/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 77.2388 - mae: 77.9186 - val_loss: 1521.5200 - val_mae: 1522.2133\n",
      "Epoch 1749/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 83.8953 - mae: 84.5780 - val_loss: 1399.7582 - val_mae: 1400.4506\n",
      "Epoch 1750/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 79.2660 - mae: 79.9486 - val_loss: 1196.3553 - val_mae: 1197.0485\n",
      "Epoch 1751/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 86.7332 - mae: 87.4138 - val_loss: 1266.9524 - val_mae: 1267.6455\n",
      "Epoch 1752/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 74.3669 - mae: 75.0503 - val_loss: 1291.0957 - val_mae: 1291.7889\n",
      "Epoch 1753/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 80.0722 - mae: 80.7537 - val_loss: 1124.4600 - val_mae: 1125.1528\n",
      "Epoch 1754/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 78.8680 - mae: 79.5472 - val_loss: 1298.5614 - val_mae: 1299.2546\n",
      "Epoch 1755/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 87.7192 - mae: 88.3997 - val_loss: 1539.9412 - val_mae: 1540.6343\n",
      "Epoch 1756/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 85.5239 - mae: 86.2099 - val_loss: 1170.2079 - val_mae: 1170.9012\n",
      "Epoch 1757/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 91.7967 - mae: 92.4782 - val_loss: 1351.8363 - val_mae: 1352.5294\n",
      "Epoch 1758/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 88.8380 - mae: 89.5249 - val_loss: 1378.1270 - val_mae: 1378.8201\n",
      "Epoch 1759/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 75.3778 - mae: 76.0581 - val_loss: 1431.3563 - val_mae: 1432.0496\n",
      "Epoch 1760/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 77.0190 - mae: 77.6963 - val_loss: 1287.7720 - val_mae: 1288.4651\n",
      "Epoch 1761/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 78.8735 - mae: 79.5544 - val_loss: 1487.5643 - val_mae: 1488.2576\n",
      "Epoch 1762/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 89.2620 - mae: 89.9456 - val_loss: 1261.7405 - val_mae: 1262.4331\n",
      "Epoch 1763/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 82.5875 - mae: 83.2668 - val_loss: 1303.4071 - val_mae: 1304.1000\n",
      "Epoch 1764/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 76.0015 - mae: 76.6830 - val_loss: 1366.2949 - val_mae: 1366.9869\n",
      "Epoch 1765/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 85.0249 - mae: 85.7098 - val_loss: 1126.6078 - val_mae: 1127.3009\n",
      "Epoch 1766/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 84.2860 - mae: 84.9662 - val_loss: 1345.8057 - val_mae: 1346.4988\n",
      "Epoch 1767/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 77.3112 - mae: 77.9914 - val_loss: 1272.6458 - val_mae: 1273.3381\n",
      "Epoch 1768/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 76.5277 - mae: 77.2084 - val_loss: 1165.1389 - val_mae: 1165.8319\n",
      "Epoch 1769/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 79.1413 - mae: 79.8213 - val_loss: 1397.9044 - val_mae: 1398.5962\n",
      "Epoch 1770/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 89.5320 - mae: 90.2120 - val_loss: 1255.0769 - val_mae: 1255.7692\n",
      "Epoch 1771/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 78.0110 - mae: 78.6909 - val_loss: 1276.9846 - val_mae: 1277.6774\n",
      "Epoch 1772/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 75.4718 - mae: 76.1530 - val_loss: 1251.5101 - val_mae: 1252.2034\n",
      "Epoch 1773/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 79.2415 - mae: 79.9214 - val_loss: 1336.2988 - val_mae: 1336.9919\n",
      "Epoch 1774/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 83.4656 - mae: 84.1491 - val_loss: 1395.1141 - val_mae: 1395.8073\n",
      "Epoch 1775/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 83.7582 - mae: 84.4384 - val_loss: 1156.1267 - val_mae: 1156.8199\n",
      "Epoch 1776/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 89.1063 - mae: 89.7904 - val_loss: 1370.2751 - val_mae: 1370.9684\n",
      "Epoch 1777/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 78.1269 - mae: 78.8077 - val_loss: 1375.7876 - val_mae: 1376.4807\n",
      "Epoch 1778/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 84.1858 - mae: 84.8666 - val_loss: 1194.6069 - val_mae: 1195.2999\n",
      "Epoch 1779/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 83.5546 - mae: 84.2391 - val_loss: 1331.5734 - val_mae: 1332.2667\n",
      "Epoch 1780/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 83.5253 - mae: 84.2081 - val_loss: 1457.5621 - val_mae: 1458.2546\n",
      "Epoch 1781/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 92.6933 - mae: 93.3758 - val_loss: 1396.2362 - val_mae: 1396.9296\n",
      "Epoch 1782/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 82.5325 - mae: 83.2143 - val_loss: 1251.2671 - val_mae: 1251.9592\n",
      "Epoch 1783/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 78.0289 - mae: 78.7124 - val_loss: 1276.1797 - val_mae: 1276.8727\n",
      "Epoch 1784/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 80.0156 - mae: 80.7000 - val_loss: 1407.7822 - val_mae: 1408.4756\n",
      "Epoch 1785/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 80.5486 - mae: 81.2300 - val_loss: 1240.0272 - val_mae: 1240.7198\n",
      "Epoch 1786/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 79.7373 - mae: 80.4218 - val_loss: 1436.3210 - val_mae: 1437.0144\n",
      "Epoch 1787/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 85.1757 - mae: 85.8570 - val_loss: 1278.6627 - val_mae: 1279.3556\n",
      "Epoch 1788/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 74.9087 - mae: 75.5909 - val_loss: 1387.9059 - val_mae: 1388.5985\n",
      "Epoch 1789/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 91.6439 - mae: 92.3312 - val_loss: 1224.1871 - val_mae: 1224.8790\n",
      "Epoch 1790/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 78.5669 - mae: 79.2453 - val_loss: 1497.7114 - val_mae: 1498.4047\n",
      "Epoch 1791/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 91.0441 - mae: 91.7291 - val_loss: 1201.8167 - val_mae: 1202.5094\n",
      "Epoch 1792/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 85.3386 - mae: 86.0174 - val_loss: 1429.9016 - val_mae: 1430.5947\n",
      "Epoch 1793/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 79.4659 - mae: 80.1449 - val_loss: 1235.9041 - val_mae: 1236.5964\n",
      "Epoch 1794/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 79.2975 - mae: 79.9797 - val_loss: 1183.8165 - val_mae: 1184.5094\n",
      "Epoch 1795/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 85.2526 - mae: 85.9343 - val_loss: 1361.6068 - val_mae: 1362.3003\n",
      "Epoch 1796/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 77.8992 - mae: 78.5842 - val_loss: 1252.1204 - val_mae: 1252.8115\n",
      "Epoch 1797/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 81.8981 - mae: 82.5829 - val_loss: 1362.8126 - val_mae: 1363.5055\n",
      "Epoch 1798/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 86.4470 - mae: 87.1285 - val_loss: 1480.9038 - val_mae: 1481.5969\n",
      "Epoch 1799/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 80.7505 - mae: 81.4319 - val_loss: 1377.9014 - val_mae: 1378.5944\n",
      "Epoch 1800/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 72.5588 - mae: 73.2397 - val_loss: 1355.1700 - val_mae: 1355.8628\n",
      "Epoch 1801/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 75.4807 - mae: 76.1628 - val_loss: 1345.1006 - val_mae: 1345.7936\n",
      "Epoch 1802/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 74.3541 - mae: 75.0346 - val_loss: 1302.9192 - val_mae: 1303.6123\n",
      "Epoch 1803/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 82.1796 - mae: 82.8574 - val_loss: 1348.6965 - val_mae: 1349.3898\n",
      "Epoch 1804/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 78.8711 - mae: 79.5522 - val_loss: 1250.5103 - val_mae: 1251.2035\n",
      "Epoch 1805/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 84.3586 - mae: 85.0410 - val_loss: 1166.7805 - val_mae: 1167.4734\n",
      "Epoch 1806/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 81.5933 - mae: 82.2704 - val_loss: 1409.2368 - val_mae: 1409.9287\n",
      "Epoch 1807/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 79.5220 - mae: 80.2006 - val_loss: 1231.1144 - val_mae: 1231.8075\n",
      "Epoch 1808/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 75.6251 - mae: 76.3049 - val_loss: 1224.0543 - val_mae: 1224.7461\n",
      "Epoch 1809/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 71.7441 - mae: 72.4201 - val_loss: 1395.2186 - val_mae: 1395.9115\n",
      "Epoch 1810/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 83.6001 - mae: 84.2818 - val_loss: 1335.4773 - val_mae: 1336.1694\n",
      "Epoch 1811/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 72.9664 - mae: 73.6454 - val_loss: 1329.2646 - val_mae: 1329.9567\n",
      "Epoch 1812/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 75.6028 - mae: 76.2821 - val_loss: 1272.6885 - val_mae: 1273.3815\n",
      "Epoch 1813/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 73.3230 - mae: 74.0035 - val_loss: 1312.4646 - val_mae: 1313.1577\n",
      "Epoch 1814/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 77.6479 - mae: 78.3280 - val_loss: 1344.2416 - val_mae: 1344.9348\n",
      "Epoch 1815/5000\n",
      "46/46 [==============================] - 2s 32ms/step - loss: 83.7522 - mae: 84.4369 - val_loss: 1386.4381 - val_mae: 1387.1313\n",
      "Epoch 1816/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 79.7401 - mae: 80.4224 - val_loss: 1208.2488 - val_mae: 1208.9420\n",
      "Epoch 1817/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 84.6777 - mae: 85.3614 - val_loss: 1171.3685 - val_mae: 1172.0616\n",
      "Epoch 1818/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 82.7936 - mae: 83.4759 - val_loss: 1483.2882 - val_mae: 1483.9814\n",
      "Epoch 1819/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 76.6894 - mae: 77.3724 - val_loss: 1313.6187 - val_mae: 1314.3116\n",
      "Epoch 1820/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 75.4447 - mae: 76.1208 - val_loss: 1484.6627 - val_mae: 1485.3560\n",
      "Epoch 1821/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 85.1088 - mae: 85.7893 - val_loss: 1221.7567 - val_mae: 1222.4500\n",
      "Epoch 1822/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 76.3525 - mae: 77.0351 - val_loss: 1350.5293 - val_mae: 1351.2224\n",
      "Epoch 1823/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 78.8197 - mae: 79.5030 - val_loss: 1164.0848 - val_mae: 1164.7780\n",
      "Epoch 1824/5000\n",
      "46/46 [==============================] - 2s 32ms/step - loss: 76.4653 - mae: 77.1468 - val_loss: 1374.8250 - val_mae: 1375.5181\n",
      "Epoch 1825/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 77.2554 - mae: 77.9374 - val_loss: 1560.9595 - val_mae: 1561.6525\n",
      "Epoch 1826/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 80.9114 - mae: 81.5898 - val_loss: 1307.3340 - val_mae: 1308.0261\n",
      "Epoch 1827/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 76.4747 - mae: 77.1547 - val_loss: 1471.6184 - val_mae: 1472.3109\n",
      "Epoch 1828/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 77.1260 - mae: 77.8037 - val_loss: 1248.2108 - val_mae: 1248.9036\n",
      "Epoch 1829/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 78.8893 - mae: 79.5775 - val_loss: 1264.8875 - val_mae: 1265.5802\n",
      "Epoch 1830/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 77.2171 - mae: 77.8983 - val_loss: 1402.9664 - val_mae: 1403.6595\n",
      "Epoch 1831/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 75.9903 - mae: 76.6686 - val_loss: 1197.1151 - val_mae: 1197.8049\n",
      "Epoch 1832/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 78.7647 - mae: 79.4454 - val_loss: 1264.5039 - val_mae: 1265.1970\n",
      "Epoch 1833/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 76.4697 - mae: 77.1509 - val_loss: 1316.4512 - val_mae: 1317.1438\n",
      "Epoch 1834/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 77.1698 - mae: 77.8520 - val_loss: 1234.1296 - val_mae: 1234.8221\n",
      "Epoch 1835/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 82.3209 - mae: 83.0064 - val_loss: 1245.9802 - val_mae: 1246.6731\n",
      "Epoch 1836/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 80.3014 - mae: 80.9846 - val_loss: 1244.5830 - val_mae: 1245.2750\n",
      "Epoch 1837/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 72.4706 - mae: 73.1514 - val_loss: 1368.7545 - val_mae: 1369.4478\n",
      "Epoch 1838/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 82.8733 - mae: 83.5575 - val_loss: 1346.3250 - val_mae: 1347.0181\n",
      "Epoch 1839/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 78.5135 - mae: 79.1928 - val_loss: 1265.9280 - val_mae: 1266.6211\n",
      "Epoch 1840/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 75.1567 - mae: 75.8343 - val_loss: 1276.1346 - val_mae: 1276.8279\n",
      "Epoch 1841/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 74.9488 - mae: 75.6272 - val_loss: 1344.0680 - val_mae: 1344.7596\n",
      "Epoch 1842/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 81.2682 - mae: 81.9467 - val_loss: 1396.1987 - val_mae: 1396.8912\n",
      "Epoch 1843/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 74.6396 - mae: 75.3199 - val_loss: 1372.5885 - val_mae: 1373.2811\n",
      "Epoch 1844/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 81.2717 - mae: 81.9529 - val_loss: 1375.5769 - val_mae: 1376.2700\n",
      "Epoch 1845/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 72.0433 - mae: 72.7238 - val_loss: 1301.6879 - val_mae: 1302.3810\n",
      "Epoch 1846/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 83.2759 - mae: 83.9559 - val_loss: 1245.6593 - val_mae: 1246.3522\n",
      "Epoch 1847/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 86.2442 - mae: 86.9286 - val_loss: 1295.2887 - val_mae: 1295.9807\n",
      "Epoch 1848/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 81.4292 - mae: 82.1125 - val_loss: 1264.5848 - val_mae: 1265.2778\n",
      "Epoch 1849/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 81.8893 - mae: 82.5729 - val_loss: 1234.7966 - val_mae: 1235.4886\n",
      "Epoch 1850/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 76.6935 - mae: 77.3736 - val_loss: 1311.5630 - val_mae: 1312.2561\n",
      "Epoch 1851/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 80.9569 - mae: 81.6369 - val_loss: 1346.9406 - val_mae: 1347.6337\n",
      "Epoch 1852/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 75.8896 - mae: 76.5733 - val_loss: 1256.1902 - val_mae: 1256.8806\n",
      "Epoch 1853/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 76.7747 - mae: 77.4581 - val_loss: 1286.9159 - val_mae: 1287.6085\n",
      "Epoch 1854/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 77.8305 - mae: 78.5131 - val_loss: 1440.3569 - val_mae: 1441.0502\n",
      "Epoch 1855/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 78.8970 - mae: 79.5760 - val_loss: 1335.6296 - val_mae: 1336.3225\n",
      "Epoch 1856/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 76.5069 - mae: 77.1883 - val_loss: 1356.3406 - val_mae: 1357.0327\n",
      "Epoch 1857/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 71.7705 - mae: 72.4488 - val_loss: 1329.3903 - val_mae: 1330.0826\n",
      "Epoch 1858/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 82.7230 - mae: 83.4080 - val_loss: 1329.3593 - val_mae: 1330.0524\n",
      "Epoch 1859/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 71.3050 - mae: 71.9863 - val_loss: 1280.9066 - val_mae: 1281.5999\n",
      "Epoch 1860/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 73.8801 - mae: 74.5616 - val_loss: 1327.0869 - val_mae: 1327.7803\n",
      "Epoch 1861/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 77.3234 - mae: 78.0044 - val_loss: 1302.3872 - val_mae: 1303.0797\n",
      "Epoch 1862/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 82.0749 - mae: 82.7556 - val_loss: 1170.7955 - val_mae: 1171.4884\n",
      "Epoch 1863/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 78.9820 - mae: 79.6631 - val_loss: 1298.0150 - val_mae: 1298.7083\n",
      "Epoch 1864/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 79.4862 - mae: 80.1671 - val_loss: 1433.4062 - val_mae: 1434.0995\n",
      "Epoch 1865/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 84.3042 - mae: 84.9867 - val_loss: 1200.9803 - val_mae: 1201.6732\n",
      "Epoch 1866/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 72.2898 - mae: 72.9710 - val_loss: 1198.8507 - val_mae: 1199.5421\n",
      "Epoch 1867/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 74.6760 - mae: 75.3558 - val_loss: 1243.5603 - val_mae: 1244.2527\n",
      "Epoch 1868/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 75.6195 - mae: 76.3018 - val_loss: 1289.5583 - val_mae: 1290.2516\n",
      "Epoch 1869/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 79.3809 - mae: 80.0611 - val_loss: 1314.2710 - val_mae: 1314.9639\n",
      "Epoch 1870/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 76.1028 - mae: 76.7831 - val_loss: 1364.7645 - val_mae: 1365.4572\n",
      "Epoch 1871/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 82.2496 - mae: 82.9354 - val_loss: 1323.7700 - val_mae: 1324.4633\n",
      "Epoch 1872/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 80.4998 - mae: 81.1812 - val_loss: 1289.3046 - val_mae: 1289.9977\n",
      "Epoch 1873/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 76.2097 - mae: 76.8887 - val_loss: 1390.3619 - val_mae: 1391.0547\n",
      "Epoch 1874/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 80.3649 - mae: 81.0410 - val_loss: 1340.6320 - val_mae: 1341.3250\n",
      "Epoch 1875/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 89.9252 - mae: 90.6076 - val_loss: 1256.2000 - val_mae: 1256.8931\n",
      "Epoch 1876/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 80.6430 - mae: 81.3251 - val_loss: 1280.3018 - val_mae: 1280.9935\n",
      "Epoch 1877/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 83.7880 - mae: 84.4663 - val_loss: 1710.5006 - val_mae: 1711.1937\n",
      "Epoch 1878/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 80.3627 - mae: 81.0443 - val_loss: 1282.0226 - val_mae: 1282.7157\n",
      "Epoch 1879/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 75.1637 - mae: 75.8428 - val_loss: 1289.5177 - val_mae: 1290.2107\n",
      "Epoch 1880/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 72.0741 - mae: 72.7537 - val_loss: 1150.6334 - val_mae: 1151.3256\n",
      "Epoch 1881/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 76.4451 - mae: 77.1283 - val_loss: 1312.9540 - val_mae: 1313.6469\n",
      "Epoch 1882/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 87.3875 - mae: 88.0682 - val_loss: 1338.8610 - val_mae: 1339.5522\n",
      "Epoch 1883/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 80.2519 - mae: 80.9394 - val_loss: 1171.7021 - val_mae: 1172.3943\n",
      "Epoch 1884/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 89.6432 - mae: 90.3259 - val_loss: 1149.9111 - val_mae: 1150.6038\n",
      "Epoch 1885/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 83.9951 - mae: 84.6752 - val_loss: 1254.5299 - val_mae: 1255.2229\n",
      "Epoch 1886/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 73.8354 - mae: 74.5125 - val_loss: 1274.0128 - val_mae: 1274.7059\n",
      "Epoch 1887/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 98.8936 - mae: 99.5761 - val_loss: 1413.6366 - val_mae: 1414.3297\n",
      "Epoch 1888/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 78.1805 - mae: 78.8611 - val_loss: 1358.3207 - val_mae: 1359.0134\n",
      "Epoch 1889/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 78.5774 - mae: 79.2586 - val_loss: 1202.3525 - val_mae: 1203.0457\n",
      "Epoch 1890/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 88.7502 - mae: 89.4363 - val_loss: 1365.2867 - val_mae: 1365.9800\n",
      "Epoch 1891/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 77.6909 - mae: 78.3715 - val_loss: 1316.9249 - val_mae: 1317.6176\n",
      "Epoch 1892/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 80.4707 - mae: 81.1526 - val_loss: 1392.6799 - val_mae: 1393.3729\n",
      "Epoch 1893/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 79.2356 - mae: 79.9180 - val_loss: 1244.5165 - val_mae: 1245.2097\n",
      "Epoch 1894/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 75.3093 - mae: 75.9865 - val_loss: 1281.7372 - val_mae: 1282.4290\n",
      "Epoch 1895/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 82.8268 - mae: 83.5089 - val_loss: 1240.6982 - val_mae: 1241.3909\n",
      "Epoch 1896/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 75.6891 - mae: 76.3695 - val_loss: 1309.7358 - val_mae: 1310.4291\n",
      "Epoch 1897/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 72.1982 - mae: 72.8772 - val_loss: 1368.2180 - val_mae: 1368.9106\n",
      "Epoch 1898/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 87.1602 - mae: 87.8434 - val_loss: 1272.9509 - val_mae: 1273.6440\n",
      "Epoch 1899/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 71.0352 - mae: 71.7129 - val_loss: 1300.0942 - val_mae: 1300.7874\n",
      "Epoch 1900/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 76.1488 - mae: 76.8271 - val_loss: 1303.8672 - val_mae: 1304.5594\n",
      "Epoch 1901/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 76.2565 - mae: 76.9393 - val_loss: 1378.1919 - val_mae: 1378.8851\n",
      "Epoch 1902/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 76.9987 - mae: 77.6808 - val_loss: 1345.6583 - val_mae: 1346.3514\n",
      "Epoch 1903/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 85.5583 - mae: 86.2412 - val_loss: 1246.0345 - val_mae: 1246.7279\n",
      "Epoch 1904/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 78.5696 - mae: 79.2498 - val_loss: 1436.4070 - val_mae: 1437.1001\n",
      "Epoch 1905/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 80.4773 - mae: 81.1616 - val_loss: 1367.1937 - val_mae: 1367.8868\n",
      "Epoch 1906/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 85.1565 - mae: 85.8376 - val_loss: 1342.3254 - val_mae: 1343.0186\n",
      "Epoch 1907/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 82.9773 - mae: 83.6600 - val_loss: 1472.9326 - val_mae: 1473.6259\n",
      "Epoch 1908/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 80.9197 - mae: 81.6011 - val_loss: 1462.3385 - val_mae: 1463.0315\n",
      "Epoch 1909/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 89.9175 - mae: 90.6033 - val_loss: 1268.4943 - val_mae: 1269.1866\n",
      "Epoch 1910/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 79.6173 - mae: 80.2974 - val_loss: 1148.8973 - val_mae: 1149.5907\n",
      "Epoch 1911/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 73.2648 - mae: 73.9466 - val_loss: 1277.6744 - val_mae: 1278.3658\n",
      "Epoch 1912/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 72.3939 - mae: 73.0761 - val_loss: 1332.2804 - val_mae: 1332.9735\n",
      "Epoch 1913/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 75.9466 - mae: 76.6232 - val_loss: 1315.9343 - val_mae: 1316.6276\n",
      "Epoch 1914/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 75.0238 - mae: 75.7068 - val_loss: 1238.7872 - val_mae: 1239.4801\n",
      "Epoch 1915/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 71.8894 - mae: 72.5713 - val_loss: 1229.7615 - val_mae: 1230.4547\n",
      "Epoch 1916/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 74.8980 - mae: 75.5776 - val_loss: 1241.2227 - val_mae: 1241.9158\n",
      "Epoch 1917/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 74.5860 - mae: 75.2673 - val_loss: 1360.1676 - val_mae: 1360.8607\n",
      "Epoch 1918/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 73.8819 - mae: 74.5599 - val_loss: 1393.1046 - val_mae: 1393.7966\n",
      "Epoch 1919/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 78.3227 - mae: 79.0036 - val_loss: 1280.9020 - val_mae: 1281.5951\n",
      "Epoch 1920/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 74.1797 - mae: 74.8600 - val_loss: 1332.5229 - val_mae: 1333.2153\n",
      "Epoch 1921/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 85.2943 - mae: 85.9800 - val_loss: 1275.7859 - val_mae: 1276.4790\n",
      "Epoch 1922/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 81.2092 - mae: 81.8930 - val_loss: 1391.9780 - val_mae: 1392.6705\n",
      "Epoch 1923/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 84.1102 - mae: 84.7916 - val_loss: 1265.4070 - val_mae: 1266.1001\n",
      "Epoch 1924/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 82.4426 - mae: 83.1211 - val_loss: 1177.6400 - val_mae: 1178.3333\n",
      "Epoch 1925/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 81.2498 - mae: 81.9312 - val_loss: 1350.4216 - val_mae: 1351.1139\n",
      "Epoch 1926/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 79.6557 - mae: 80.3330 - val_loss: 1260.7588 - val_mae: 1261.4504\n",
      "Epoch 1927/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 77.0669 - mae: 77.7459 - val_loss: 1237.0021 - val_mae: 1237.6948\n",
      "Epoch 1928/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 79.6571 - mae: 80.3361 - val_loss: 1340.3546 - val_mae: 1341.0477\n",
      "Epoch 1929/5000\n",
      "46/46 [==============================] - 2s 48ms/step - loss: 82.4184 - mae: 83.1010 - val_loss: 1275.3003 - val_mae: 1275.9927\n",
      "Epoch 1930/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 86.4871 - mae: 87.1680 - val_loss: 1385.8802 - val_mae: 1386.5734\n",
      "Epoch 1931/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 73.2718 - mae: 73.9517 - val_loss: 1392.8112 - val_mae: 1393.5038\n",
      "Epoch 1932/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 77.6838 - mae: 78.3678 - val_loss: 1347.5972 - val_mae: 1348.2904\n",
      "Epoch 1933/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 72.2095 - mae: 72.8871 - val_loss: 1198.7151 - val_mae: 1199.4083\n",
      "Epoch 1934/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 76.0941 - mae: 76.7724 - val_loss: 1237.4279 - val_mae: 1238.1201\n",
      "Epoch 1935/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 82.5084 - mae: 83.1880 - val_loss: 1227.5116 - val_mae: 1228.2045\n",
      "Epoch 1936/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 81.3234 - mae: 82.0061 - val_loss: 1179.0374 - val_mae: 1179.7285\n",
      "Epoch 1937/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 74.4996 - mae: 75.1824 - val_loss: 1325.4906 - val_mae: 1326.1837\n",
      "Epoch 1938/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 86.9978 - mae: 87.6828 - val_loss: 1303.7057 - val_mae: 1304.3990\n",
      "Epoch 1939/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 73.0759 - mae: 73.7577 - val_loss: 1372.8934 - val_mae: 1373.5854\n",
      "Epoch 1940/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 87.3504 - mae: 88.0351 - val_loss: 1865.8406 - val_mae: 1866.5333\n",
      "Epoch 1941/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 96.2987 - mae: 96.9833 - val_loss: 1290.3541 - val_mae: 1291.0453\n",
      "Epoch 1942/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 79.0249 - mae: 79.7094 - val_loss: 1377.7596 - val_mae: 1378.4528\n",
      "Epoch 1943/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 83.1808 - mae: 83.8666 - val_loss: 1418.0167 - val_mae: 1418.7098\n",
      "Epoch 1944/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 75.0306 - mae: 75.7119 - val_loss: 1394.9086 - val_mae: 1395.6013\n",
      "Epoch 1945/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 77.6475 - mae: 78.3283 - val_loss: 1360.0454 - val_mae: 1360.7361\n",
      "Epoch 1946/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 73.6353 - mae: 74.3168 - val_loss: 1467.9539 - val_mae: 1468.6469\n",
      "Epoch 1947/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 75.5630 - mae: 76.2487 - val_loss: 1278.2489 - val_mae: 1278.9421\n",
      "Epoch 1948/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 74.8166 - mae: 75.4977 - val_loss: 1343.3002 - val_mae: 1343.9929\n",
      "Epoch 1949/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 75.8350 - mae: 76.5138 - val_loss: 1397.6263 - val_mae: 1398.3195\n",
      "Epoch 1950/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 85.1849 - mae: 85.8705 - val_loss: 1394.4279 - val_mae: 1395.1208\n",
      "Epoch 1951/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 71.9357 - mae: 72.6162 - val_loss: 1247.4260 - val_mae: 1248.1185\n",
      "Epoch 1952/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 84.3892 - mae: 85.0683 - val_loss: 1273.4934 - val_mae: 1274.1860\n",
      "Epoch 1953/5000\n",
      "46/46 [==============================] - 2s 32ms/step - loss: 86.8932 - mae: 87.5755 - val_loss: 1208.3929 - val_mae: 1209.0859\n",
      "Epoch 1954/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 79.8662 - mae: 80.5522 - val_loss: 1179.7166 - val_mae: 1180.4098\n",
      "Epoch 1955/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 71.6908 - mae: 72.3698 - val_loss: 1300.4984 - val_mae: 1301.1915\n",
      "Epoch 1956/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 72.7842 - mae: 73.4619 - val_loss: 1269.3270 - val_mae: 1270.0200\n",
      "Epoch 1957/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 74.8546 - mae: 75.5360 - val_loss: 1251.4562 - val_mae: 1252.1489\n",
      "Epoch 1958/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 76.4175 - mae: 77.0977 - val_loss: 1411.8199 - val_mae: 1412.5132\n",
      "Epoch 1959/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 87.2943 - mae: 87.9778 - val_loss: 1174.1914 - val_mae: 1174.8835\n",
      "Epoch 1960/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 78.9413 - mae: 79.6221 - val_loss: 1563.7006 - val_mae: 1564.3938\n",
      "Epoch 1961/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 98.9364 - mae: 99.6204 - val_loss: 1357.1959 - val_mae: 1357.8892\n",
      "Epoch 1962/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 77.3722 - mae: 78.0552 - val_loss: 1275.4679 - val_mae: 1276.1609\n",
      "Epoch 1963/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 76.2133 - mae: 76.8955 - val_loss: 1198.3833 - val_mae: 1199.0757\n",
      "Epoch 1964/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 76.3546 - mae: 77.0366 - val_loss: 1505.4565 - val_mae: 1506.1498\n",
      "Epoch 1965/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 76.0412 - mae: 76.7229 - val_loss: 1178.2161 - val_mae: 1178.9091\n",
      "Epoch 1966/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 72.3023 - mae: 72.9808 - val_loss: 1312.8870 - val_mae: 1313.5803\n",
      "Epoch 1967/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 77.3262 - mae: 78.0080 - val_loss: 1218.2869 - val_mae: 1218.9795\n",
      "Epoch 1968/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 71.2877 - mae: 71.9715 - val_loss: 1289.8038 - val_mae: 1290.4955\n",
      "Epoch 1969/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 74.3151 - mae: 74.9935 - val_loss: 1379.1593 - val_mae: 1379.8514\n",
      "Epoch 1970/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 75.6136 - mae: 76.2954 - val_loss: 1186.0951 - val_mae: 1186.7872\n",
      "Epoch 1971/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 72.2455 - mae: 72.9268 - val_loss: 1275.1537 - val_mae: 1275.8459\n",
      "Epoch 1972/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 80.4306 - mae: 81.1130 - val_loss: 1396.3038 - val_mae: 1396.9969\n",
      "Epoch 1973/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 91.3201 - mae: 92.0008 - val_loss: 1397.8983 - val_mae: 1398.5913\n",
      "Epoch 1974/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 77.7236 - mae: 78.4050 - val_loss: 1275.4727 - val_mae: 1276.1655\n",
      "Epoch 1975/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 78.2621 - mae: 78.9425 - val_loss: 1260.3936 - val_mae: 1261.0869\n",
      "Epoch 1976/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 80.0257 - mae: 80.7066 - val_loss: 1404.0703 - val_mae: 1404.7635\n",
      "Epoch 1977/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 74.7955 - mae: 75.4751 - val_loss: 1218.8340 - val_mae: 1219.5271\n",
      "Epoch 1978/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 77.1625 - mae: 77.8429 - val_loss: 1336.1505 - val_mae: 1336.8438\n",
      "Epoch 1979/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 75.4023 - mae: 76.0825 - val_loss: 1268.2094 - val_mae: 1268.9019\n",
      "Epoch 1980/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 69.9544 - mae: 70.6361 - val_loss: 1231.0261 - val_mae: 1231.7191\n",
      "Epoch 1981/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 70.9817 - mae: 71.6613 - val_loss: 1337.4564 - val_mae: 1338.1497\n",
      "Epoch 1982/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 75.2532 - mae: 75.9332 - val_loss: 1278.5807 - val_mae: 1279.2729\n",
      "Epoch 1983/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 78.1734 - mae: 78.8552 - val_loss: 1242.7466 - val_mae: 1243.4398\n",
      "Epoch 1984/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 72.0977 - mae: 72.7792 - val_loss: 1364.6613 - val_mae: 1365.3544\n",
      "Epoch 1985/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 73.1106 - mae: 73.7891 - val_loss: 1382.5657 - val_mae: 1383.2583\n",
      "Epoch 1986/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 71.3297 - mae: 72.0077 - val_loss: 1365.8734 - val_mae: 1366.5664\n",
      "Epoch 1987/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 78.0395 - mae: 78.7215 - val_loss: 1305.9841 - val_mae: 1306.6772\n",
      "Epoch 1988/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 76.4591 - mae: 77.1432 - val_loss: 1221.5673 - val_mae: 1222.2600\n",
      "Epoch 1989/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 73.0447 - mae: 73.7251 - val_loss: 1287.7592 - val_mae: 1288.4525\n",
      "Epoch 1990/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 80.7967 - mae: 81.4772 - val_loss: 1198.2655 - val_mae: 1198.9583\n",
      "Epoch 1991/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 78.0802 - mae: 78.7628 - val_loss: 1260.8413 - val_mae: 1261.5342\n",
      "Epoch 1992/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 73.7811 - mae: 74.4602 - val_loss: 1321.0819 - val_mae: 1321.7737\n",
      "Epoch 1993/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 76.1002 - mae: 76.7813 - val_loss: 1178.4163 - val_mae: 1179.1083\n",
      "Epoch 1994/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 80.2371 - mae: 80.9177 - val_loss: 1251.5458 - val_mae: 1252.2390\n",
      "Epoch 1995/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 76.2292 - mae: 76.9097 - val_loss: 1322.1415 - val_mae: 1322.8345\n",
      "Epoch 1996/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 75.8771 - mae: 76.5587 - val_loss: 1333.9976 - val_mae: 1334.6909\n",
      "Epoch 1997/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 72.5896 - mae: 73.2713 - val_loss: 1340.1560 - val_mae: 1340.8492\n",
      "Epoch 1998/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 72.9556 - mae: 73.6374 - val_loss: 1357.8322 - val_mae: 1358.5248\n",
      "Epoch 1999/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 68.6072 - mae: 69.2839 - val_loss: 1229.3450 - val_mae: 1230.0380\n",
      "Epoch 2000/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 74.4505 - mae: 75.1353 - val_loss: 1473.6337 - val_mae: 1474.3265\n",
      "Epoch 2001/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 76.7935 - mae: 77.4777 - val_loss: 1330.9703 - val_mae: 1331.6621\n",
      "Epoch 2002/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 69.2929 - mae: 69.9731 - val_loss: 1327.6687 - val_mae: 1328.3617\n",
      "Epoch 2003/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 77.3782 - mae: 78.0570 - val_loss: 1351.5148 - val_mae: 1352.2079\n",
      "Epoch 2004/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 81.3315 - mae: 82.0110 - val_loss: 1362.7126 - val_mae: 1363.4059\n",
      "Epoch 2005/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 66.4005 - mae: 67.0806 - val_loss: 1281.6715 - val_mae: 1282.3649\n",
      "Epoch 2006/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 81.6319 - mae: 82.3105 - val_loss: 1353.0237 - val_mae: 1353.7167\n",
      "Epoch 2007/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 74.3676 - mae: 75.0457 - val_loss: 1204.0917 - val_mae: 1204.7848\n",
      "Epoch 2008/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 83.2362 - mae: 83.9196 - val_loss: 1191.1104 - val_mae: 1191.8032\n",
      "Epoch 2009/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 94.4420 - mae: 95.1246 - val_loss: 1312.6301 - val_mae: 1313.3232\n",
      "Epoch 2010/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 78.6828 - mae: 79.3591 - val_loss: 1222.7170 - val_mae: 1223.4103\n",
      "Epoch 2011/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 78.1553 - mae: 78.8368 - val_loss: 1364.6090 - val_mae: 1365.3018\n",
      "Epoch 2012/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 88.9134 - mae: 89.5979 - val_loss: 1235.8264 - val_mae: 1236.5194\n",
      "Epoch 2013/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 72.9367 - mae: 73.6145 - val_loss: 1216.7830 - val_mae: 1217.4760\n",
      "Epoch 2014/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 72.4935 - mae: 73.1718 - val_loss: 1321.2955 - val_mae: 1321.9885\n",
      "Epoch 2015/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 80.0139 - mae: 80.6898 - val_loss: 1333.0809 - val_mae: 1333.7729\n",
      "Epoch 2016/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 67.5001 - mae: 68.1819 - val_loss: 1283.9727 - val_mae: 1284.6656\n",
      "Epoch 2017/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 73.6302 - mae: 74.3085 - val_loss: 1227.8322 - val_mae: 1228.5242\n",
      "Epoch 2018/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 70.3596 - mae: 71.0443 - val_loss: 1292.1925 - val_mae: 1292.8856\n",
      "Epoch 2019/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 72.7114 - mae: 73.3918 - val_loss: 1349.4150 - val_mae: 1350.1079\n",
      "Epoch 2020/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 76.3554 - mae: 77.0359 - val_loss: 1258.7760 - val_mae: 1259.4684\n",
      "Epoch 2021/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 83.9037 - mae: 84.5877 - val_loss: 1295.1758 - val_mae: 1295.8685\n",
      "Epoch 2022/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 78.7128 - mae: 79.3903 - val_loss: 1308.3151 - val_mae: 1309.0079\n",
      "Epoch 2023/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 77.2756 - mae: 77.9583 - val_loss: 1330.8002 - val_mae: 1331.4934\n",
      "Epoch 2024/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 77.7278 - mae: 78.4129 - val_loss: 1407.0640 - val_mae: 1407.7572\n",
      "Epoch 2025/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 72.9510 - mae: 73.6327 - val_loss: 1348.3378 - val_mae: 1349.0310\n",
      "Epoch 2026/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 75.7060 - mae: 76.3901 - val_loss: 1340.0377 - val_mae: 1340.7300\n",
      "Epoch 2027/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 74.7383 - mae: 75.4204 - val_loss: 1254.5891 - val_mae: 1255.2822\n",
      "Epoch 2028/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 77.8335 - mae: 78.5139 - val_loss: 1360.2068 - val_mae: 1360.9001\n",
      "Epoch 2029/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 73.2375 - mae: 73.9161 - val_loss: 1330.5504 - val_mae: 1331.2423\n",
      "Epoch 2030/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 73.3931 - mae: 74.0724 - val_loss: 1390.2179 - val_mae: 1390.9109\n",
      "Epoch 2031/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 72.0647 - mae: 72.7482 - val_loss: 1243.8242 - val_mae: 1244.5166\n",
      "Epoch 2032/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 87.5937 - mae: 88.2735 - val_loss: 1585.3828 - val_mae: 1586.0760\n",
      "Epoch 2033/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 71.6482 - mae: 72.3238 - val_loss: 1262.9637 - val_mae: 1263.6562\n",
      "Epoch 2034/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 68.5140 - mae: 69.1904 - val_loss: 1247.7781 - val_mae: 1248.4688\n",
      "Epoch 2035/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 72.9059 - mae: 73.5859 - val_loss: 1203.7904 - val_mae: 1204.4822\n",
      "Epoch 2036/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 78.2551 - mae: 78.9367 - val_loss: 1274.4910 - val_mae: 1275.1831\n",
      "Epoch 2037/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 80.9521 - mae: 81.6341 - val_loss: 1241.6381 - val_mae: 1242.3295\n",
      "Epoch 2038/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 82.6494 - mae: 83.3312 - val_loss: 1504.5337 - val_mae: 1505.2271\n",
      "Epoch 2039/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 76.5542 - mae: 77.2339 - val_loss: 1279.4568 - val_mae: 1280.1498\n",
      "Epoch 2040/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 73.1633 - mae: 73.8473 - val_loss: 1190.7799 - val_mae: 1191.4727\n",
      "Epoch 2041/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 73.4594 - mae: 74.1374 - val_loss: 1210.0214 - val_mae: 1210.7146\n",
      "Epoch 2042/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 73.9044 - mae: 74.5866 - val_loss: 1184.7729 - val_mae: 1185.4663\n",
      "Epoch 2043/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 80.5011 - mae: 81.1845 - val_loss: 1351.4584 - val_mae: 1352.1519\n",
      "Epoch 2044/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 70.0551 - mae: 70.7377 - val_loss: 1260.6642 - val_mae: 1261.3568\n",
      "Epoch 2045/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 70.9528 - mae: 71.6341 - val_loss: 1369.7358 - val_mae: 1370.4292\n",
      "Epoch 2046/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 71.6789 - mae: 72.3602 - val_loss: 1453.0872 - val_mae: 1453.7802\n",
      "Epoch 2047/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 69.6564 - mae: 70.3359 - val_loss: 1325.6796 - val_mae: 1326.3704\n",
      "Epoch 2048/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 74.0618 - mae: 74.7426 - val_loss: 1365.4103 - val_mae: 1366.1031\n",
      "Epoch 2049/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 73.0838 - mae: 73.7631 - val_loss: 1249.9706 - val_mae: 1250.6638\n",
      "Epoch 2050/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 69.5021 - mae: 70.1823 - val_loss: 1255.5315 - val_mae: 1256.2246\n",
      "Epoch 2051/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 70.7955 - mae: 71.4733 - val_loss: 1372.8669 - val_mae: 1373.5597\n",
      "Epoch 2052/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 72.7933 - mae: 73.4693 - val_loss: 1340.3290 - val_mae: 1341.0205\n",
      "Epoch 2053/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 74.0352 - mae: 74.7164 - val_loss: 1220.0530 - val_mae: 1220.7460\n",
      "Epoch 2054/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 80.0436 - mae: 80.7285 - val_loss: 1295.7738 - val_mae: 1296.4669\n",
      "Epoch 2055/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 77.4504 - mae: 78.1314 - val_loss: 1188.0518 - val_mae: 1188.7446\n",
      "Epoch 2056/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 76.7083 - mae: 77.3919 - val_loss: 1281.0798 - val_mae: 1281.7732\n",
      "Epoch 2057/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 68.9156 - mae: 69.5922 - val_loss: 1208.3735 - val_mae: 1209.0659\n",
      "Epoch 2058/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 76.2736 - mae: 76.9537 - val_loss: 1355.8870 - val_mae: 1356.5802\n",
      "Epoch 2059/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 67.2884 - mae: 67.9703 - val_loss: 1318.0200 - val_mae: 1318.7131\n",
      "Epoch 2060/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 65.5155 - mae: 66.1939 - val_loss: 1301.9703 - val_mae: 1302.6636\n",
      "Epoch 2061/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 80.4429 - mae: 81.1220 - val_loss: 1334.7902 - val_mae: 1335.4827\n",
      "Epoch 2062/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 78.7755 - mae: 79.4580 - val_loss: 1213.9177 - val_mae: 1214.6108\n",
      "Epoch 2063/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 78.8342 - mae: 79.5157 - val_loss: 1274.7933 - val_mae: 1275.4854\n",
      "Epoch 2064/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 72.4930 - mae: 73.1766 - val_loss: 1270.3772 - val_mae: 1271.0701\n",
      "Epoch 2065/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 70.6259 - mae: 71.3043 - val_loss: 1375.3457 - val_mae: 1376.0391\n",
      "Epoch 2066/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 79.5036 - mae: 80.1835 - val_loss: 1366.3994 - val_mae: 1367.0916\n",
      "Epoch 2067/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 75.4466 - mae: 76.1269 - val_loss: 1134.1855 - val_mae: 1134.8787\n",
      "Epoch 2068/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 81.8731 - mae: 82.5567 - val_loss: 1559.6552 - val_mae: 1560.3483\n",
      "Epoch 2069/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 84.5855 - mae: 85.2696 - val_loss: 1287.7087 - val_mae: 1288.4021\n",
      "Epoch 2070/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 72.2776 - mae: 72.9557 - val_loss: 1290.1298 - val_mae: 1290.8212\n",
      "Epoch 2071/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 70.8644 - mae: 71.5435 - val_loss: 1433.5780 - val_mae: 1434.2709\n",
      "Epoch 2072/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 71.8244 - mae: 72.5037 - val_loss: 1245.1705 - val_mae: 1245.8630\n",
      "Epoch 2073/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 68.5279 - mae: 69.2057 - val_loss: 1293.4265 - val_mae: 1294.1196\n",
      "Epoch 2074/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 83.7986 - mae: 84.4781 - val_loss: 1536.4911 - val_mae: 1537.1843\n",
      "Epoch 2075/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 101.2795 - mae: 101.9641 - val_loss: 1236.3824 - val_mae: 1237.0758\n",
      "Epoch 2076/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 80.0480 - mae: 80.7287 - val_loss: 1326.2671 - val_mae: 1326.9601\n",
      "Epoch 2077/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 71.7898 - mae: 72.4707 - val_loss: 1307.8812 - val_mae: 1308.5741\n",
      "Epoch 2078/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 75.4139 - mae: 76.0961 - val_loss: 1261.7123 - val_mae: 1262.4047\n",
      "Epoch 2079/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 76.8152 - mae: 77.4990 - val_loss: 1457.7278 - val_mae: 1458.4209\n",
      "Epoch 2080/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 81.7701 - mae: 82.4552 - val_loss: 1241.1534 - val_mae: 1241.8459\n",
      "Epoch 2081/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 69.0564 - mae: 69.7420 - val_loss: 1305.9294 - val_mae: 1306.6224\n",
      "Epoch 2082/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 70.1682 - mae: 70.8533 - val_loss: 1298.4418 - val_mae: 1299.1350\n",
      "Epoch 2083/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 74.8687 - mae: 75.5531 - val_loss: 1325.4670 - val_mae: 1326.1600\n",
      "Epoch 2084/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 71.5596 - mae: 72.2398 - val_loss: 1238.9814 - val_mae: 1239.6743\n",
      "Epoch 2085/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 82.7302 - mae: 83.4114 - val_loss: 1241.9822 - val_mae: 1242.6755\n",
      "Epoch 2086/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 72.4776 - mae: 73.1577 - val_loss: 1293.7511 - val_mae: 1294.4435\n",
      "Epoch 2087/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 79.6791 - mae: 80.3620 - val_loss: 1287.5281 - val_mae: 1288.2203\n",
      "Epoch 2088/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 80.9547 - mae: 81.6384 - val_loss: 1432.6906 - val_mae: 1433.3835\n",
      "Epoch 2089/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 76.7274 - mae: 77.4110 - val_loss: 1497.9366 - val_mae: 1498.6299\n",
      "Epoch 2090/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 78.0767 - mae: 78.7564 - val_loss: 1388.7657 - val_mae: 1389.4589\n",
      "Epoch 2091/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 75.8471 - mae: 76.5252 - val_loss: 1205.4406 - val_mae: 1206.1337\n",
      "Epoch 2092/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 77.7449 - mae: 78.4281 - val_loss: 1252.3235 - val_mae: 1253.0167\n",
      "Epoch 2093/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 69.4890 - mae: 70.1701 - val_loss: 1299.3364 - val_mae: 1300.0297\n",
      "Epoch 2094/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 70.3238 - mae: 71.0047 - val_loss: 1325.6278 - val_mae: 1326.3210\n",
      "Epoch 2095/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 73.6794 - mae: 74.3620 - val_loss: 1365.3948 - val_mae: 1366.0880\n",
      "Epoch 2096/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 71.1413 - mae: 71.8196 - val_loss: 1387.8784 - val_mae: 1388.5714\n",
      "Epoch 2097/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 77.3946 - mae: 78.0752 - val_loss: 1233.5658 - val_mae: 1234.2585\n",
      "Epoch 2098/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 77.2194 - mae: 77.8930 - val_loss: 1240.5424 - val_mae: 1241.2349\n",
      "Epoch 2099/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 74.4652 - mae: 75.1440 - val_loss: 1246.1797 - val_mae: 1246.8719\n",
      "Epoch 2100/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 74.6829 - mae: 75.3623 - val_loss: 1249.7306 - val_mae: 1250.4237\n",
      "Epoch 2101/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 72.3814 - mae: 73.0641 - val_loss: 1235.5533 - val_mae: 1236.2458\n",
      "Epoch 2102/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 75.8760 - mae: 76.5567 - val_loss: 1309.5348 - val_mae: 1310.2280\n",
      "Epoch 2103/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 80.2072 - mae: 80.8897 - val_loss: 1279.9218 - val_mae: 1280.6147\n",
      "Epoch 2104/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 76.8393 - mae: 77.5220 - val_loss: 1234.6971 - val_mae: 1235.3901\n",
      "Epoch 2105/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 69.5247 - mae: 70.2054 - val_loss: 1176.5846 - val_mae: 1177.2766\n",
      "Epoch 2106/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 75.2981 - mae: 75.9800 - val_loss: 1368.0466 - val_mae: 1368.7396\n",
      "Epoch 2107/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 69.5135 - mae: 70.1947 - val_loss: 1330.5795 - val_mae: 1331.2726\n",
      "Epoch 2108/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 80.2552 - mae: 80.9386 - val_loss: 1284.7179 - val_mae: 1285.4110\n",
      "Epoch 2109/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 67.4992 - mae: 68.1817 - val_loss: 1403.5530 - val_mae: 1404.2458\n",
      "Epoch 2110/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 74.4096 - mae: 75.0931 - val_loss: 1355.9583 - val_mae: 1356.6516\n",
      "Epoch 2111/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 67.8195 - mae: 68.4992 - val_loss: 1329.4414 - val_mae: 1330.1345\n",
      "Epoch 2112/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 65.9868 - mae: 66.6664 - val_loss: 1353.4471 - val_mae: 1354.1403\n",
      "Epoch 2113/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 76.0980 - mae: 76.7787 - val_loss: 1235.5135 - val_mae: 1236.2064\n",
      "Epoch 2114/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 84.8948 - mae: 85.5746 - val_loss: 1386.4282 - val_mae: 1387.1213\n",
      "Epoch 2115/5000\n",
      "46/46 [==============================] - 2s 43ms/step - loss: 69.9052 - mae: 70.5866 - val_loss: 1464.7275 - val_mae: 1465.4205\n",
      "Epoch 2116/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 76.7829 - mae: 77.4634 - val_loss: 1324.2520 - val_mae: 1324.9437\n",
      "Epoch 2117/5000\n",
      "46/46 [==============================] - 2s 32ms/step - loss: 93.7767 - mae: 94.4610 - val_loss: 1195.5282 - val_mae: 1196.2212\n",
      "Epoch 2118/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 76.6404 - mae: 77.3205 - val_loss: 1297.8268 - val_mae: 1298.5198\n",
      "Epoch 2119/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 72.4338 - mae: 73.1148 - val_loss: 1215.3295 - val_mae: 1216.0225\n",
      "Epoch 2120/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 73.2772 - mae: 73.9569 - val_loss: 1294.4302 - val_mae: 1295.1233\n",
      "Epoch 2121/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 75.2658 - mae: 75.9488 - val_loss: 1278.9194 - val_mae: 1279.6127\n",
      "Epoch 2122/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 80.9196 - mae: 81.6046 - val_loss: 1148.9388 - val_mae: 1149.6312\n",
      "Epoch 2123/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 73.9250 - mae: 74.6074 - val_loss: 1268.2219 - val_mae: 1268.9150\n",
      "Epoch 2124/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 74.3274 - mae: 75.0100 - val_loss: 1163.8221 - val_mae: 1164.5135\n",
      "Epoch 2125/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 78.7978 - mae: 79.4798 - val_loss: 1255.9933 - val_mae: 1256.6855\n",
      "Epoch 2126/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 70.2662 - mae: 70.9502 - val_loss: 1372.7281 - val_mae: 1373.4203\n",
      "Epoch 2127/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 74.5072 - mae: 75.1861 - val_loss: 1223.8275 - val_mae: 1224.5199\n",
      "Epoch 2128/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 77.1590 - mae: 77.8420 - val_loss: 1209.6331 - val_mae: 1210.3262\n",
      "Epoch 2129/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 75.4431 - mae: 76.1206 - val_loss: 1292.4417 - val_mae: 1293.1349\n",
      "Epoch 2130/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 71.0218 - mae: 71.7052 - val_loss: 1404.6777 - val_mae: 1405.3689\n",
      "Epoch 2131/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 73.3525 - mae: 74.0339 - val_loss: 1308.3127 - val_mae: 1309.0055\n",
      "Epoch 2132/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 66.8791 - mae: 67.5598 - val_loss: 1285.7944 - val_mae: 1286.4875\n",
      "Epoch 2133/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 68.8906 - mae: 69.5701 - val_loss: 1277.5398 - val_mae: 1278.2322\n",
      "Epoch 2134/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 73.5783 - mae: 74.2567 - val_loss: 1234.2694 - val_mae: 1234.9617\n",
      "Epoch 2135/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 76.1985 - mae: 76.8859 - val_loss: 1347.8427 - val_mae: 1348.5358\n",
      "Epoch 2136/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 69.4458 - mae: 70.1271 - val_loss: 1329.9279 - val_mae: 1330.6210\n",
      "Epoch 2137/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 67.4230 - mae: 68.1028 - val_loss: 1271.9661 - val_mae: 1272.6592\n",
      "Epoch 2138/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 74.5914 - mae: 75.2712 - val_loss: 1324.4290 - val_mae: 1325.1221\n",
      "Epoch 2139/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 72.8485 - mae: 73.5254 - val_loss: 1248.8376 - val_mae: 1249.5297\n",
      "Epoch 2140/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 73.3326 - mae: 74.0136 - val_loss: 1282.3617 - val_mae: 1283.0537\n",
      "Epoch 2141/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 83.2623 - mae: 83.9444 - val_loss: 1350.7975 - val_mae: 1351.4901\n",
      "Epoch 2142/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 73.9602 - mae: 74.6392 - val_loss: 1321.8342 - val_mae: 1322.5271\n",
      "Epoch 2143/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 66.5470 - mae: 67.2307 - val_loss: 1290.4083 - val_mae: 1291.1014\n",
      "Epoch 2144/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 75.3708 - mae: 76.0523 - val_loss: 1310.4381 - val_mae: 1311.1312\n",
      "Epoch 2145/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 73.8448 - mae: 74.5258 - val_loss: 1419.4847 - val_mae: 1420.1776\n",
      "Epoch 2146/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 71.9941 - mae: 72.6695 - val_loss: 1449.1187 - val_mae: 1449.8113\n",
      "Epoch 2147/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 77.7904 - mae: 78.4732 - val_loss: 1329.1543 - val_mae: 1329.8473\n",
      "Epoch 2148/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 68.9942 - mae: 69.6765 - val_loss: 1285.6768 - val_mae: 1286.3689\n",
      "Epoch 2149/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 71.5916 - mae: 72.2728 - val_loss: 1251.9728 - val_mae: 1252.6659\n",
      "Epoch 2150/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 70.0974 - mae: 70.7784 - val_loss: 1439.8765 - val_mae: 1440.5696\n",
      "Epoch 2151/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 93.0540 - mae: 93.7404 - val_loss: 1416.2524 - val_mae: 1416.9453\n",
      "Epoch 2152/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 65.4287 - mae: 66.1031 - val_loss: 1348.8424 - val_mae: 1349.5354\n",
      "Epoch 2153/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 70.8161 - mae: 71.4937 - val_loss: 1339.2175 - val_mae: 1339.9105\n",
      "Epoch 2154/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 76.0941 - mae: 76.7803 - val_loss: 1310.7997 - val_mae: 1311.4919\n",
      "Epoch 2155/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 75.8486 - mae: 76.5294 - val_loss: 1409.3577 - val_mae: 1410.0505\n",
      "Epoch 2156/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 76.5194 - mae: 77.2033 - val_loss: 1338.6387 - val_mae: 1339.3317\n",
      "Epoch 2157/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 72.4521 - mae: 73.1308 - val_loss: 1440.4414 - val_mae: 1441.1342\n",
      "Epoch 2158/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 76.7862 - mae: 77.4708 - val_loss: 1263.6923 - val_mae: 1264.3854\n",
      "Epoch 2159/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 67.6845 - mae: 68.3649 - val_loss: 1270.9648 - val_mae: 1271.6578\n",
      "Epoch 2160/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 68.8063 - mae: 69.4871 - val_loss: 1360.5559 - val_mae: 1361.2472\n",
      "Epoch 2161/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 76.5816 - mae: 77.2622 - val_loss: 1397.9688 - val_mae: 1398.6621\n",
      "Epoch 2162/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 90.4731 - mae: 91.1526 - val_loss: 1515.4773 - val_mae: 1516.1705\n",
      "Epoch 2163/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 76.0278 - mae: 76.7066 - val_loss: 1206.2849 - val_mae: 1206.9779\n",
      "Epoch 2164/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 74.9165 - mae: 75.5968 - val_loss: 1192.6794 - val_mae: 1193.3715\n",
      "Epoch 2165/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 73.9740 - mae: 74.6590 - val_loss: 1312.6523 - val_mae: 1313.3440\n",
      "Epoch 2166/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 80.0560 - mae: 80.7365 - val_loss: 1293.9087 - val_mae: 1294.6018\n",
      "Epoch 2167/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 87.9850 - mae: 88.6733 - val_loss: 1197.8131 - val_mae: 1198.5063\n",
      "Epoch 2168/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 81.6457 - mae: 82.3236 - val_loss: 1130.3583 - val_mae: 1131.0499\n",
      "Epoch 2169/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 82.2833 - mae: 82.9647 - val_loss: 1275.5165 - val_mae: 1276.2094\n",
      "Epoch 2170/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 68.7804 - mae: 69.4607 - val_loss: 1239.0532 - val_mae: 1239.7463\n",
      "Epoch 2171/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 69.0387 - mae: 69.7228 - val_loss: 1284.2949 - val_mae: 1284.9883\n",
      "Epoch 2172/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 76.6102 - mae: 77.2906 - val_loss: 1450.4053 - val_mae: 1451.0981\n",
      "Epoch 2173/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 80.4060 - mae: 81.0936 - val_loss: 1278.3605 - val_mae: 1279.0537\n",
      "Epoch 2174/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 68.8832 - mae: 69.5659 - val_loss: 1273.0620 - val_mae: 1273.7548\n",
      "Epoch 2175/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 77.5866 - mae: 78.2686 - val_loss: 1335.9592 - val_mae: 1336.6523\n",
      "Epoch 2176/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 72.0316 - mae: 72.7104 - val_loss: 1303.9744 - val_mae: 1304.6670\n",
      "Epoch 2177/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 69.7074 - mae: 70.3879 - val_loss: 1251.4562 - val_mae: 1252.1492\n",
      "Epoch 2178/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 72.8526 - mae: 73.5328 - val_loss: 1315.1410 - val_mae: 1315.8335\n",
      "Epoch 2179/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 88.8880 - mae: 89.5725 - val_loss: 1443.2950 - val_mae: 1443.9880\n",
      "Epoch 2180/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 65.9092 - mae: 66.5861 - val_loss: 1302.6847 - val_mae: 1303.3770\n",
      "Epoch 2181/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 81.3898 - mae: 82.0754 - val_loss: 1361.2153 - val_mae: 1361.9083\n",
      "Epoch 2182/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 69.0448 - mae: 69.7228 - val_loss: 1236.4468 - val_mae: 1237.1400\n",
      "Epoch 2183/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 67.9274 - mae: 68.6083 - val_loss: 1385.6053 - val_mae: 1386.2982\n",
      "Epoch 2184/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 69.8561 - mae: 70.5361 - val_loss: 1254.4961 - val_mae: 1255.1890\n",
      "Epoch 2185/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 72.6860 - mae: 73.3669 - val_loss: 1275.8503 - val_mae: 1276.5433\n",
      "Epoch 2186/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 73.0976 - mae: 73.7801 - val_loss: 1394.2139 - val_mae: 1394.9071\n",
      "Epoch 2187/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 69.4097 - mae: 70.0880 - val_loss: 1511.3798 - val_mae: 1512.0728\n",
      "Epoch 2188/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 82.4093 - mae: 83.0921 - val_loss: 1266.1641 - val_mae: 1266.8564\n",
      "Epoch 2189/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 73.9179 - mae: 74.5999 - val_loss: 1271.6199 - val_mae: 1272.3130\n",
      "Epoch 2190/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 67.0313 - mae: 67.7092 - val_loss: 1284.3408 - val_mae: 1285.0323\n",
      "Epoch 2191/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 69.4619 - mae: 70.1411 - val_loss: 1295.3405 - val_mae: 1296.0336\n",
      "Epoch 2192/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 76.8568 - mae: 77.5374 - val_loss: 1214.8556 - val_mae: 1215.5470\n",
      "Epoch 2193/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 74.8702 - mae: 75.5506 - val_loss: 1228.5170 - val_mae: 1229.2101\n",
      "Epoch 2194/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 71.9258 - mae: 72.6081 - val_loss: 1245.1802 - val_mae: 1245.8707\n",
      "Epoch 2195/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 69.9017 - mae: 70.5817 - val_loss: 1262.2263 - val_mae: 1262.9193\n",
      "Epoch 2196/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 68.1900 - mae: 68.8668 - val_loss: 1307.9722 - val_mae: 1308.6647\n",
      "Epoch 2197/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 71.9094 - mae: 72.5911 - val_loss: 1310.7983 - val_mae: 1311.4905\n",
      "Epoch 2198/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 79.4594 - mae: 80.1449 - val_loss: 1117.9500 - val_mae: 1118.6422\n",
      "Epoch 2199/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 77.4353 - mae: 78.1177 - val_loss: 1317.9757 - val_mae: 1318.6685\n",
      "Epoch 2200/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 72.6732 - mae: 73.3540 - val_loss: 1334.9529 - val_mae: 1335.6460\n",
      "Epoch 2201/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 69.6674 - mae: 70.3498 - val_loss: 1203.2396 - val_mae: 1203.9324\n",
      "Epoch 2202/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 78.6236 - mae: 79.3063 - val_loss: 1217.9552 - val_mae: 1218.6483\n",
      "Epoch 2203/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 77.4355 - mae: 78.1190 - val_loss: 1224.2628 - val_mae: 1224.9561\n",
      "Epoch 2204/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 73.7440 - mae: 74.4235 - val_loss: 1322.7714 - val_mae: 1323.4647\n",
      "Epoch 2205/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 71.4125 - mae: 72.0946 - val_loss: 1235.8400 - val_mae: 1236.5328\n",
      "Epoch 2206/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 74.0102 - mae: 74.6903 - val_loss: 1347.9364 - val_mae: 1348.6296\n",
      "Epoch 2207/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 70.4983 - mae: 71.1794 - val_loss: 1317.1869 - val_mae: 1317.8796\n",
      "Epoch 2208/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 71.0973 - mae: 71.7770 - val_loss: 1309.4901 - val_mae: 1310.1831\n",
      "Epoch 2209/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 77.1200 - mae: 77.8003 - val_loss: 1274.4993 - val_mae: 1275.1910\n",
      "Epoch 2210/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 77.8730 - mae: 78.5550 - val_loss: 1379.8253 - val_mae: 1380.5184\n",
      "Epoch 2211/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 71.2039 - mae: 71.8842 - val_loss: 1264.5226 - val_mae: 1265.2155\n",
      "Epoch 2212/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 84.1871 - mae: 84.8662 - val_loss: 1219.9515 - val_mae: 1220.6437\n",
      "Epoch 2213/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 73.8447 - mae: 74.5275 - val_loss: 1199.8271 - val_mae: 1200.5203\n",
      "Epoch 2214/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 72.3873 - mae: 73.0677 - val_loss: 1443.7737 - val_mae: 1444.4669\n",
      "Epoch 2215/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 72.8635 - mae: 73.5406 - val_loss: 1319.5348 - val_mae: 1320.2278\n",
      "Epoch 2216/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 68.5761 - mae: 69.2589 - val_loss: 1254.5964 - val_mae: 1255.2898\n",
      "Epoch 2217/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 66.1604 - mae: 66.8405 - val_loss: 1295.1409 - val_mae: 1295.8329\n",
      "Epoch 2218/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 64.6539 - mae: 65.3317 - val_loss: 1370.1017 - val_mae: 1370.7946\n",
      "Epoch 2219/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 72.9865 - mae: 73.6682 - val_loss: 1349.9225 - val_mae: 1350.6157\n",
      "Epoch 2220/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 72.3760 - mae: 73.0607 - val_loss: 1341.9132 - val_mae: 1342.6064\n",
      "Epoch 2221/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 77.3886 - mae: 78.0725 - val_loss: 1402.7897 - val_mae: 1403.4827\n",
      "Epoch 2222/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 78.0271 - mae: 78.7067 - val_loss: 1391.0911 - val_mae: 1391.7841\n",
      "Epoch 2223/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 72.4084 - mae: 73.0888 - val_loss: 1191.9888 - val_mae: 1192.6807\n",
      "Epoch 2224/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 76.8506 - mae: 77.5365 - val_loss: 1220.7084 - val_mae: 1221.4016\n",
      "Epoch 2225/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 76.3632 - mae: 77.0450 - val_loss: 1243.7140 - val_mae: 1244.4070\n",
      "Epoch 2226/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 71.8657 - mae: 72.5434 - val_loss: 1368.0005 - val_mae: 1368.6932\n",
      "Epoch 2227/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 70.5722 - mae: 71.2532 - val_loss: 1353.7892 - val_mae: 1354.4823\n",
      "Epoch 2228/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 73.9329 - mae: 74.6153 - val_loss: 1374.3700 - val_mae: 1375.0634\n",
      "Epoch 2229/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 75.0952 - mae: 75.7777 - val_loss: 1307.9858 - val_mae: 1308.6790\n",
      "Epoch 2230/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 68.9790 - mae: 69.6571 - val_loss: 1275.0850 - val_mae: 1275.7778\n",
      "Epoch 2231/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 76.3728 - mae: 77.0507 - val_loss: 1202.1508 - val_mae: 1202.8440\n",
      "Epoch 2232/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 78.6509 - mae: 79.3295 - val_loss: 1290.6030 - val_mae: 1291.2961\n",
      "Epoch 2233/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 74.1640 - mae: 74.8463 - val_loss: 1354.9894 - val_mae: 1355.6826\n",
      "Epoch 2234/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 68.0856 - mae: 68.7669 - val_loss: 1345.1093 - val_mae: 1345.8024\n",
      "Epoch 2235/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 78.5146 - mae: 79.1932 - val_loss: 1321.1725 - val_mae: 1321.8657\n",
      "Epoch 2236/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 84.1061 - mae: 84.7895 - val_loss: 1346.7731 - val_mae: 1347.4657\n",
      "Epoch 2237/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 81.7656 - mae: 82.4459 - val_loss: 1279.5258 - val_mae: 1280.2189\n",
      "Epoch 2238/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 70.7282 - mae: 71.4046 - val_loss: 1254.8337 - val_mae: 1255.5267\n",
      "Epoch 2239/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 73.0851 - mae: 73.7627 - val_loss: 1359.0819 - val_mae: 1359.7750\n",
      "Epoch 2240/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 67.0015 - mae: 67.6806 - val_loss: 1258.5044 - val_mae: 1259.1967\n",
      "Epoch 2241/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 67.9233 - mae: 68.6009 - val_loss: 1381.5503 - val_mae: 1382.2435\n",
      "Epoch 2242/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 78.7357 - mae: 79.4114 - val_loss: 1504.0858 - val_mae: 1504.7773\n",
      "Epoch 2243/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 79.5075 - mae: 80.1914 - val_loss: 1336.3180 - val_mae: 1337.0104\n",
      "Epoch 2244/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 67.3053 - mae: 67.9852 - val_loss: 1272.6273 - val_mae: 1273.3203\n",
      "Epoch 2245/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 68.0828 - mae: 68.7623 - val_loss: 1220.6857 - val_mae: 1221.3789\n",
      "Epoch 2246/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 73.5245 - mae: 74.2074 - val_loss: 1320.8334 - val_mae: 1321.5265\n",
      "Epoch 2247/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 66.6103 - mae: 67.2899 - val_loss: 1298.7839 - val_mae: 1299.4773\n",
      "Epoch 2248/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 76.2976 - mae: 76.9783 - val_loss: 1314.5774 - val_mae: 1315.2678\n",
      "Epoch 2249/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 74.5312 - mae: 75.2136 - val_loss: 1341.0939 - val_mae: 1341.7872\n",
      "Epoch 2250/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 75.9843 - mae: 76.6651 - val_loss: 1420.4271 - val_mae: 1421.1173\n",
      "Epoch 2251/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 73.3223 - mae: 74.0034 - val_loss: 1312.9408 - val_mae: 1313.6339\n",
      "Epoch 2252/5000\n",
      "46/46 [==============================] - 2s 32ms/step - loss: 65.5718 - mae: 66.2543 - val_loss: 1246.6559 - val_mae: 1247.3478\n",
      "Epoch 2253/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 72.4715 - mae: 73.1510 - val_loss: 1435.2014 - val_mae: 1435.8938\n",
      "Epoch 2254/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 69.8614 - mae: 70.5411 - val_loss: 1260.8896 - val_mae: 1261.5829\n",
      "Epoch 2255/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 63.3875 - mae: 64.0641 - val_loss: 1269.1449 - val_mae: 1269.8370\n",
      "Epoch 2256/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 74.8936 - mae: 75.5750 - val_loss: 1312.3809 - val_mae: 1313.0736\n",
      "Epoch 2257/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 74.5672 - mae: 75.2508 - val_loss: 1354.8711 - val_mae: 1355.5645\n",
      "Epoch 2258/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 71.4686 - mae: 72.1482 - val_loss: 1276.2925 - val_mae: 1276.9854\n",
      "Epoch 2259/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 68.2199 - mae: 68.9020 - val_loss: 1216.6844 - val_mae: 1217.3774\n",
      "Epoch 2260/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 79.1500 - mae: 79.8357 - val_loss: 1316.6174 - val_mae: 1317.3096\n",
      "Epoch 2261/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 70.8249 - mae: 71.5077 - val_loss: 1314.9934 - val_mae: 1315.6855\n",
      "Epoch 2262/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 70.0496 - mae: 70.7262 - val_loss: 1352.2313 - val_mae: 1352.9244\n",
      "Epoch 2263/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 66.7587 - mae: 67.4374 - val_loss: 1321.5491 - val_mae: 1322.2424\n",
      "Epoch 2264/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 64.6579 - mae: 65.3358 - val_loss: 1311.0635 - val_mae: 1311.7567\n",
      "Epoch 2265/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 66.3339 - mae: 67.0180 - val_loss: 1288.6172 - val_mae: 1289.3096\n",
      "Epoch 2266/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 70.1703 - mae: 70.8508 - val_loss: 1247.7646 - val_mae: 1248.4564\n",
      "Epoch 2267/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 64.7980 - mae: 65.4732 - val_loss: 1262.6691 - val_mae: 1263.3617\n",
      "Epoch 2268/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 67.2367 - mae: 67.9160 - val_loss: 1367.0161 - val_mae: 1367.7092\n",
      "Epoch 2269/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 72.2194 - mae: 72.8988 - val_loss: 1294.5795 - val_mae: 1295.2726\n",
      "Epoch 2270/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 71.9350 - mae: 72.6174 - val_loss: 1404.4382 - val_mae: 1405.1309\n",
      "Epoch 2271/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 75.0191 - mae: 75.6988 - val_loss: 1350.5466 - val_mae: 1351.2399\n",
      "Epoch 2272/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 67.1364 - mae: 67.8159 - val_loss: 1359.7426 - val_mae: 1360.4354\n",
      "Epoch 2273/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 66.5730 - mae: 67.2558 - val_loss: 1276.3949 - val_mae: 1277.0880\n",
      "Epoch 2274/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 71.3508 - mae: 72.0316 - val_loss: 1403.1541 - val_mae: 1403.8472\n",
      "Epoch 2275/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 72.5430 - mae: 73.2224 - val_loss: 1315.7915 - val_mae: 1316.4846\n",
      "Epoch 2276/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 74.0117 - mae: 74.6907 - val_loss: 1306.8323 - val_mae: 1307.5248\n",
      "Epoch 2277/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 66.2130 - mae: 66.8875 - val_loss: 1376.6730 - val_mae: 1377.3649\n",
      "Epoch 2278/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 73.1228 - mae: 73.8034 - val_loss: 1284.4193 - val_mae: 1285.1121\n",
      "Epoch 2279/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 67.1993 - mae: 67.8771 - val_loss: 1189.4824 - val_mae: 1190.1758\n",
      "Epoch 2280/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 68.1030 - mae: 68.7824 - val_loss: 1311.8564 - val_mae: 1312.5488\n",
      "Epoch 2281/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 85.9328 - mae: 86.6121 - val_loss: 1234.1555 - val_mae: 1234.8474\n",
      "Epoch 2282/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 75.5510 - mae: 76.2326 - val_loss: 1418.1743 - val_mae: 1418.8663\n",
      "Epoch 2283/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 69.9442 - mae: 70.6256 - val_loss: 1305.3110 - val_mae: 1306.0031\n",
      "Epoch 2284/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 68.3029 - mae: 68.9828 - val_loss: 1483.6357 - val_mae: 1484.3290\n",
      "Epoch 2285/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 69.8854 - mae: 70.5650 - val_loss: 1316.9476 - val_mae: 1317.6403\n",
      "Epoch 2286/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 69.2629 - mae: 69.9434 - val_loss: 1308.3801 - val_mae: 1309.0732\n",
      "Epoch 2287/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 83.3258 - mae: 84.0089 - val_loss: 1171.3870 - val_mae: 1172.0796\n",
      "Epoch 2288/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 72.7453 - mae: 73.4248 - val_loss: 1277.4384 - val_mae: 1278.1315\n",
      "Epoch 2289/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 70.4455 - mae: 71.1247 - val_loss: 1443.3540 - val_mae: 1444.0458\n",
      "Epoch 2290/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 70.8661 - mae: 71.5449 - val_loss: 1330.3883 - val_mae: 1331.0813\n",
      "Epoch 2291/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 68.1345 - mae: 68.8138 - val_loss: 1298.9745 - val_mae: 1299.6676\n",
      "Epoch 2292/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 76.8559 - mae: 77.5374 - val_loss: 1296.9116 - val_mae: 1297.6047\n",
      "Epoch 2293/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 70.3945 - mae: 71.0733 - val_loss: 1346.1226 - val_mae: 1346.8157\n",
      "Epoch 2294/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 74.2185 - mae: 74.9024 - val_loss: 1380.4121 - val_mae: 1381.1045\n",
      "Epoch 2295/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 74.5405 - mae: 75.2197 - val_loss: 1408.0304 - val_mae: 1408.7236\n",
      "Epoch 2296/5000\n",
      "46/46 [==============================] - 2s 42ms/step - loss: 82.9873 - mae: 83.6709 - val_loss: 1482.7448 - val_mae: 1483.4369\n",
      "Epoch 2297/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 74.5557 - mae: 75.2337 - val_loss: 1269.9958 - val_mae: 1270.6891\n",
      "Epoch 2298/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 73.6591 - mae: 74.3401 - val_loss: 1305.0602 - val_mae: 1305.7531\n",
      "Epoch 2299/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 62.9885 - mae: 63.6656 - val_loss: 1309.2369 - val_mae: 1309.9288\n",
      "Epoch 2300/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 73.8393 - mae: 74.5182 - val_loss: 1230.2445 - val_mae: 1230.9366\n",
      "Epoch 2301/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 67.3363 - mae: 68.0178 - val_loss: 1315.8940 - val_mae: 1316.5847\n",
      "Epoch 2302/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 66.3194 - mae: 66.9968 - val_loss: 1314.8677 - val_mae: 1315.5608\n",
      "Epoch 2303/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 65.3398 - mae: 66.0158 - val_loss: 1267.7338 - val_mae: 1268.4259\n",
      "Epoch 2304/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 64.8033 - mae: 65.4806 - val_loss: 1296.7178 - val_mae: 1297.4110\n",
      "Epoch 2305/5000\n",
      "46/46 [==============================] - 2s 43ms/step - loss: 67.6238 - mae: 68.2975 - val_loss: 1447.0320 - val_mae: 1447.7251\n",
      "Epoch 2306/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 81.4134 - mae: 82.0998 - val_loss: 1196.7063 - val_mae: 1197.3993\n",
      "Epoch 2307/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 68.4761 - mae: 69.1549 - val_loss: 1414.3038 - val_mae: 1414.9971\n",
      "Epoch 2308/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 68.3693 - mae: 69.0449 - val_loss: 1274.1770 - val_mae: 1274.8700\n",
      "Epoch 2309/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 62.4675 - mae: 63.1442 - val_loss: 1382.5397 - val_mae: 1383.2325\n",
      "Epoch 2310/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 75.2526 - mae: 75.9325 - val_loss: 1345.4641 - val_mae: 1346.1571\n",
      "Epoch 2311/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 74.7645 - mae: 75.4452 - val_loss: 1158.4438 - val_mae: 1159.1368\n",
      "Epoch 2312/5000\n",
      "46/46 [==============================] - 2s 40ms/step - loss: 84.4692 - mae: 85.1524 - val_loss: 1283.9132 - val_mae: 1284.6062\n",
      "Epoch 2313/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 85.3482 - mae: 86.0345 - val_loss: 1383.7073 - val_mae: 1384.4006\n",
      "Epoch 2314/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 68.4128 - mae: 69.0920 - val_loss: 1387.9528 - val_mae: 1388.6458\n",
      "Epoch 2315/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 67.7796 - mae: 68.4616 - val_loss: 1302.1980 - val_mae: 1302.8910\n",
      "Epoch 2316/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 70.3595 - mae: 71.0363 - val_loss: 1252.8418 - val_mae: 1253.5347\n",
      "Epoch 2317/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 67.6538 - mae: 68.3299 - val_loss: 1254.3547 - val_mae: 1255.0474\n",
      "Epoch 2318/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 66.8802 - mae: 67.5564 - val_loss: 1312.4580 - val_mae: 1313.1514\n",
      "Epoch 2319/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 68.7670 - mae: 69.4438 - val_loss: 1257.9415 - val_mae: 1258.6343\n",
      "Epoch 2320/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 67.6575 - mae: 68.3372 - val_loss: 1361.4927 - val_mae: 1362.1853\n",
      "Epoch 2321/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 79.7190 - mae: 80.4028 - val_loss: 1300.0790 - val_mae: 1300.7716\n",
      "Epoch 2322/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 76.4605 - mae: 77.1431 - val_loss: 1216.7588 - val_mae: 1217.4520\n",
      "Epoch 2323/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 70.1723 - mae: 70.8529 - val_loss: 1263.3317 - val_mae: 1264.0248\n",
      "Epoch 2324/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 88.0341 - mae: 88.7172 - val_loss: 1253.8203 - val_mae: 1254.5134\n",
      "Epoch 2325/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 78.8687 - mae: 79.5532 - val_loss: 1308.0443 - val_mae: 1308.7362\n",
      "Epoch 2326/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 78.1374 - mae: 78.8186 - val_loss: 1243.7861 - val_mae: 1244.4794\n",
      "Epoch 2327/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 67.7150 - mae: 68.3896 - val_loss: 1238.0010 - val_mae: 1238.6938\n",
      "Epoch 2328/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 70.4131 - mae: 71.0886 - val_loss: 1311.5431 - val_mae: 1312.2338\n",
      "Epoch 2329/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 66.9475 - mae: 67.6266 - val_loss: 1380.5597 - val_mae: 1381.2518\n",
      "Epoch 2330/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 68.2705 - mae: 68.9534 - val_loss: 1337.3717 - val_mae: 1338.0648\n",
      "Epoch 2331/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 65.6827 - mae: 66.3604 - val_loss: 1431.1829 - val_mae: 1431.8761\n",
      "Epoch 2332/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 69.6032 - mae: 70.2813 - val_loss: 1298.5594 - val_mae: 1299.2524\n",
      "Epoch 2333/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 67.8061 - mae: 68.4850 - val_loss: 1325.0265 - val_mae: 1325.7196\n",
      "Epoch 2334/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 79.3446 - mae: 80.0242 - val_loss: 1360.6176 - val_mae: 1361.3105\n",
      "Epoch 2335/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 69.3197 - mae: 70.0001 - val_loss: 1215.7455 - val_mae: 1216.4386\n",
      "Epoch 2336/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 71.3301 - mae: 72.0086 - val_loss: 1261.1504 - val_mae: 1261.8435\n",
      "Epoch 2337/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 66.4740 - mae: 67.1516 - val_loss: 1348.6823 - val_mae: 1349.3756\n",
      "Epoch 2338/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 64.8359 - mae: 65.5152 - val_loss: 1320.8195 - val_mae: 1321.5120\n",
      "Epoch 2339/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 71.7698 - mae: 72.4517 - val_loss: 1372.4958 - val_mae: 1373.1879\n",
      "Epoch 2340/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 65.7739 - mae: 66.4553 - val_loss: 1343.8939 - val_mae: 1344.5873\n",
      "Epoch 2341/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 80.0324 - mae: 80.7116 - val_loss: 1349.8151 - val_mae: 1350.5082\n",
      "Epoch 2342/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 72.7382 - mae: 73.4182 - val_loss: 1277.7725 - val_mae: 1278.4656\n",
      "Epoch 2343/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 72.7950 - mae: 73.4731 - val_loss: 1350.7639 - val_mae: 1351.4551\n",
      "Epoch 2344/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 67.7701 - mae: 68.4481 - val_loss: 1275.0326 - val_mae: 1275.7258\n",
      "Epoch 2345/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 67.8992 - mae: 68.5738 - val_loss: 1269.6373 - val_mae: 1270.3303\n",
      "Epoch 2346/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 66.8560 - mae: 67.5354 - val_loss: 1284.1958 - val_mae: 1284.8887\n",
      "Epoch 2347/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 67.4039 - mae: 68.0833 - val_loss: 1288.0530 - val_mae: 1288.7460\n",
      "Epoch 2348/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 70.0183 - mae: 70.7023 - val_loss: 1262.7595 - val_mae: 1263.4524\n",
      "Epoch 2349/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 73.2948 - mae: 73.9744 - val_loss: 1322.7384 - val_mae: 1323.4314\n",
      "Epoch 2350/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 69.8384 - mae: 70.5148 - val_loss: 1278.6031 - val_mae: 1279.2954\n",
      "Epoch 2351/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 75.9019 - mae: 76.5813 - val_loss: 1236.5640 - val_mae: 1237.2567\n",
      "Epoch 2352/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 79.3503 - mae: 80.0319 - val_loss: 1311.7843 - val_mae: 1312.4777\n",
      "Epoch 2353/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 70.5116 - mae: 71.1913 - val_loss: 1263.5015 - val_mae: 1264.1940\n",
      "Epoch 2354/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 71.3038 - mae: 71.9854 - val_loss: 1352.7642 - val_mae: 1353.4572\n",
      "Epoch 2355/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 71.4789 - mae: 72.1598 - val_loss: 1402.0464 - val_mae: 1402.7394\n",
      "Epoch 2356/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 74.6530 - mae: 75.3341 - val_loss: 1227.4757 - val_mae: 1228.1687\n",
      "Epoch 2357/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 78.8831 - mae: 79.5664 - val_loss: 1278.7842 - val_mae: 1279.4775\n",
      "Epoch 2358/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 66.7167 - mae: 67.3965 - val_loss: 1271.8035 - val_mae: 1272.4965\n",
      "Epoch 2359/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 70.4908 - mae: 71.1704 - val_loss: 1307.3323 - val_mae: 1308.0256\n",
      "Epoch 2360/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 72.4233 - mae: 73.1056 - val_loss: 1255.1550 - val_mae: 1255.8478\n",
      "Epoch 2361/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 68.8957 - mae: 69.5786 - val_loss: 1308.1521 - val_mae: 1308.8451\n",
      "Epoch 2362/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 71.8953 - mae: 72.5766 - val_loss: 1227.9706 - val_mae: 1228.6630\n",
      "Epoch 2363/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 65.5964 - mae: 66.2720 - val_loss: 1333.3208 - val_mae: 1334.0139\n",
      "Epoch 2364/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 72.2516 - mae: 72.9294 - val_loss: 1348.8469 - val_mae: 1349.5402\n",
      "Epoch 2365/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 70.1691 - mae: 70.8514 - val_loss: 1399.6006 - val_mae: 1400.2939\n",
      "Epoch 2366/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 69.9826 - mae: 70.6623 - val_loss: 1332.0851 - val_mae: 1332.7783\n",
      "Epoch 2367/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 75.9102 - mae: 76.5890 - val_loss: 1180.7053 - val_mae: 1181.3982\n",
      "Epoch 2368/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 72.0836 - mae: 72.7636 - val_loss: 1254.7576 - val_mae: 1255.4504\n",
      "Epoch 2369/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 65.1173 - mae: 65.7986 - val_loss: 1313.9348 - val_mae: 1314.6281\n",
      "Epoch 2370/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 67.2967 - mae: 67.9743 - val_loss: 1315.5292 - val_mae: 1316.2224\n",
      "Epoch 2371/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 64.6815 - mae: 65.3583 - val_loss: 1251.5627 - val_mae: 1252.2559\n",
      "Epoch 2372/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 65.9961 - mae: 66.6770 - val_loss: 1320.2510 - val_mae: 1320.9440\n",
      "Epoch 2373/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 71.0330 - mae: 71.7128 - val_loss: 1217.9829 - val_mae: 1218.6764\n",
      "Epoch 2374/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 76.4624 - mae: 77.1440 - val_loss: 1260.8568 - val_mae: 1261.5490\n",
      "Epoch 2375/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 73.7756 - mae: 74.4541 - val_loss: 1321.7151 - val_mae: 1322.4082\n",
      "Epoch 2376/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 66.2553 - mae: 66.9334 - val_loss: 1247.0051 - val_mae: 1247.6973\n",
      "Epoch 2377/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 69.6592 - mae: 70.3375 - val_loss: 1222.5114 - val_mae: 1223.2046\n",
      "Epoch 2378/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 68.3066 - mae: 68.9902 - val_loss: 1327.0687 - val_mae: 1327.7620\n",
      "Epoch 2379/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 70.9120 - mae: 71.5922 - val_loss: 1392.7142 - val_mae: 1393.4071\n",
      "Epoch 2380/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 73.0366 - mae: 73.7154 - val_loss: 1234.1432 - val_mae: 1234.8362\n",
      "Epoch 2381/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 76.5666 - mae: 77.2494 - val_loss: 1321.8667 - val_mae: 1322.5596\n",
      "Epoch 2382/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 62.1110 - mae: 62.7915 - val_loss: 1394.9236 - val_mae: 1395.6157\n",
      "Epoch 2383/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 66.1173 - mae: 66.7969 - val_loss: 1425.9734 - val_mae: 1426.6664\n",
      "Epoch 2384/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 72.4283 - mae: 73.1130 - val_loss: 1339.7030 - val_mae: 1340.3956\n",
      "Epoch 2385/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 76.5275 - mae: 77.2087 - val_loss: 1317.7202 - val_mae: 1318.4115\n",
      "Epoch 2386/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 70.7473 - mae: 71.4300 - val_loss: 1289.6233 - val_mae: 1290.3165\n",
      "Epoch 2387/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 73.8972 - mae: 74.5803 - val_loss: 1482.9152 - val_mae: 1483.6084\n",
      "Epoch 2388/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 66.8433 - mae: 67.5249 - val_loss: 1315.4341 - val_mae: 1316.1276\n",
      "Epoch 2389/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 67.0691 - mae: 67.7501 - val_loss: 1421.9561 - val_mae: 1422.6489\n",
      "Epoch 2390/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 70.2622 - mae: 70.9439 - val_loss: 1397.0488 - val_mae: 1397.7421\n",
      "Epoch 2391/5000\n",
      "46/46 [==============================] - 2s 32ms/step - loss: 63.9921 - mae: 64.6696 - val_loss: 1284.8406 - val_mae: 1285.5331\n",
      "Epoch 2392/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 73.4571 - mae: 74.1346 - val_loss: 1565.5605 - val_mae: 1566.2527\n",
      "Epoch 2393/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 70.8862 - mae: 71.5641 - val_loss: 1359.4221 - val_mae: 1360.1154\n",
      "Epoch 2394/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 64.0362 - mae: 64.7145 - val_loss: 1431.4152 - val_mae: 1432.1083\n",
      "Epoch 2395/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 71.5784 - mae: 72.2579 - val_loss: 1250.3601 - val_mae: 1251.0515\n",
      "Epoch 2396/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 66.1614 - mae: 66.8397 - val_loss: 1298.2714 - val_mae: 1298.9646\n",
      "Epoch 2397/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 66.9022 - mae: 67.5830 - val_loss: 1293.2742 - val_mae: 1293.9670\n",
      "Epoch 2398/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 68.1631 - mae: 68.8402 - val_loss: 1247.5708 - val_mae: 1248.2639\n",
      "Epoch 2399/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 65.2630 - mae: 65.9421 - val_loss: 1343.4926 - val_mae: 1344.1855\n",
      "Epoch 2400/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 67.5956 - mae: 68.2764 - val_loss: 1276.1469 - val_mae: 1276.8400\n",
      "Epoch 2401/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 65.9204 - mae: 66.6009 - val_loss: 1323.0938 - val_mae: 1323.7869\n",
      "Epoch 2402/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 70.6649 - mae: 71.3442 - val_loss: 1302.4214 - val_mae: 1303.1146\n",
      "Epoch 2403/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 70.5746 - mae: 71.2576 - val_loss: 1269.0271 - val_mae: 1269.7202\n",
      "Epoch 2404/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 68.9550 - mae: 69.6326 - val_loss: 1374.2026 - val_mae: 1374.8945\n",
      "Epoch 2405/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 84.7538 - mae: 85.4380 - val_loss: 1319.3687 - val_mae: 1320.0616\n",
      "Epoch 2406/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 69.5597 - mae: 70.2438 - val_loss: 1252.6954 - val_mae: 1253.3881\n",
      "Epoch 2407/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 66.5196 - mae: 67.2037 - val_loss: 1257.8400 - val_mae: 1258.5325\n",
      "Epoch 2408/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 70.3733 - mae: 71.0532 - val_loss: 1169.7887 - val_mae: 1170.4797\n",
      "Epoch 2409/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 77.2356 - mae: 77.9162 - val_loss: 1369.1389 - val_mae: 1369.8308\n",
      "Epoch 2410/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 70.6265 - mae: 71.3068 - val_loss: 1260.0413 - val_mae: 1260.7341\n",
      "Epoch 2411/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 70.0068 - mae: 70.6881 - val_loss: 1289.5247 - val_mae: 1290.2175\n",
      "Epoch 2412/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 68.1877 - mae: 68.8672 - val_loss: 1392.6831 - val_mae: 1393.3759\n",
      "Epoch 2413/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 68.4544 - mae: 69.1375 - val_loss: 1342.4658 - val_mae: 1343.1588\n",
      "Epoch 2414/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 66.0816 - mae: 66.7630 - val_loss: 1476.9680 - val_mae: 1477.6613\n",
      "Epoch 2415/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 68.6947 - mae: 69.3723 - val_loss: 1416.2191 - val_mae: 1416.9103\n",
      "Epoch 2416/5000\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 71.0777 - mae: 71.7575 - val_loss: 1281.1206 - val_mae: 1281.8134\n",
      "Epoch 2417/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 64.9742 - mae: 65.6505 - val_loss: 1352.3262 - val_mae: 1353.0172\n",
      "Epoch 2418/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 70.2828 - mae: 70.9621 - val_loss: 1276.3151 - val_mae: 1277.0076\n",
      "Epoch 2419/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 69.3379 - mae: 70.0172 - val_loss: 1417.1161 - val_mae: 1417.8082\n",
      "Epoch 2420/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 69.6428 - mae: 70.3241 - val_loss: 1316.0507 - val_mae: 1316.7438\n",
      "Epoch 2421/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 68.4658 - mae: 69.1448 - val_loss: 1321.8092 - val_mae: 1322.5023\n",
      "Epoch 2422/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 67.6552 - mae: 68.3327 - val_loss: 1231.1312 - val_mae: 1231.8243\n",
      "Epoch 2423/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 76.5996 - mae: 77.2779 - val_loss: 1247.9128 - val_mae: 1248.6060\n",
      "Epoch 2424/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 68.7717 - mae: 69.4524 - val_loss: 1305.5328 - val_mae: 1306.2262\n",
      "Epoch 2425/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 66.7688 - mae: 67.4501 - val_loss: 1267.9591 - val_mae: 1268.6523\n",
      "Epoch 2426/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 67.9632 - mae: 68.6406 - val_loss: 1259.9916 - val_mae: 1260.6842\n",
      "Epoch 2427/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 72.4081 - mae: 73.0911 - val_loss: 1356.5863 - val_mae: 1357.2794\n",
      "Epoch 2428/5000\n",
      "46/46 [==============================] - 2s 41ms/step - loss: 75.8629 - mae: 76.5441 - val_loss: 1370.4468 - val_mae: 1371.1400\n",
      "Epoch 2429/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 67.2274 - mae: 67.9089 - val_loss: 1245.4521 - val_mae: 1246.1450\n",
      "Epoch 2430/5000\n",
      "46/46 [==============================] - 2s 40ms/step - loss: 69.3835 - mae: 70.0614 - val_loss: 1244.3079 - val_mae: 1245.0006\n",
      "Epoch 2431/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 71.1357 - mae: 71.8196 - val_loss: 1390.2872 - val_mae: 1390.9803\n",
      "Epoch 2432/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 72.7430 - mae: 73.4232 - val_loss: 1358.1562 - val_mae: 1358.8492\n",
      "Epoch 2433/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 72.4312 - mae: 73.1098 - val_loss: 1291.8683 - val_mae: 1292.5614\n",
      "Epoch 2434/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 71.1375 - mae: 71.8199 - val_loss: 1330.9082 - val_mae: 1331.6013\n",
      "Epoch 2435/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 61.4641 - mae: 62.1440 - val_loss: 1247.5468 - val_mae: 1248.2378\n",
      "Epoch 2436/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 60.9843 - mae: 61.6619 - val_loss: 1342.2097 - val_mae: 1342.9028\n",
      "Epoch 2437/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 65.7862 - mae: 66.4650 - val_loss: 1311.0338 - val_mae: 1311.7272\n",
      "Epoch 2438/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 64.1012 - mae: 64.7790 - val_loss: 1362.0259 - val_mae: 1362.7178\n",
      "Epoch 2439/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 71.5026 - mae: 72.1819 - val_loss: 1324.6062 - val_mae: 1325.2986\n",
      "Epoch 2440/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 72.5586 - mae: 73.2386 - val_loss: 1299.3931 - val_mae: 1300.0862\n",
      "Epoch 2441/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 65.7777 - mae: 66.4597 - val_loss: 1360.6046 - val_mae: 1361.2976\n",
      "Epoch 2442/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 68.0729 - mae: 68.7544 - val_loss: 1336.9978 - val_mae: 1337.6910\n",
      "Epoch 2443/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 72.1295 - mae: 72.8041 - val_loss: 1410.8320 - val_mae: 1411.5251\n",
      "Epoch 2444/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 72.6501 - mae: 73.3267 - val_loss: 1296.2250 - val_mae: 1296.9170\n",
      "Epoch 2445/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 64.8571 - mae: 65.5349 - val_loss: 1390.2063 - val_mae: 1390.8994\n",
      "Epoch 2446/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 70.8190 - mae: 71.4997 - val_loss: 1341.1514 - val_mae: 1341.8445\n",
      "Epoch 2447/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 67.2302 - mae: 67.9082 - val_loss: 1332.0376 - val_mae: 1332.7310\n",
      "Epoch 2448/5000\n",
      "46/46 [==============================] - 2s 52ms/step - loss: 68.4948 - mae: 69.1742 - val_loss: 1208.1433 - val_mae: 1208.8351\n",
      "Epoch 2449/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 74.2269 - mae: 74.9105 - val_loss: 1299.3843 - val_mae: 1300.0775\n",
      "Epoch 2450/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 71.0686 - mae: 71.7478 - val_loss: 1269.9491 - val_mae: 1270.6403\n",
      "Epoch 2451/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 68.7164 - mae: 69.3965 - val_loss: 1344.5388 - val_mae: 1345.2318\n",
      "Epoch 2452/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 70.1363 - mae: 70.8151 - val_loss: 1325.1550 - val_mae: 1325.8481\n",
      "Epoch 2453/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 63.5828 - mae: 64.2614 - val_loss: 1255.5736 - val_mae: 1256.2660\n",
      "Epoch 2454/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 63.8400 - mae: 64.5215 - val_loss: 1347.2336 - val_mae: 1347.9268\n",
      "Epoch 2455/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 68.9089 - mae: 69.5910 - val_loss: 1316.1210 - val_mae: 1316.8136\n",
      "Epoch 2456/5000\n",
      "46/46 [==============================] - 2s 32ms/step - loss: 67.2475 - mae: 67.9310 - val_loss: 1348.6062 - val_mae: 1349.2996\n",
      "Epoch 2457/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 62.8131 - mae: 63.4894 - val_loss: 1363.5415 - val_mae: 1364.2325\n",
      "Epoch 2458/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 63.3369 - mae: 64.0178 - val_loss: 1430.7793 - val_mae: 1431.4711\n",
      "Epoch 2459/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 70.8427 - mae: 71.5236 - val_loss: 1352.8677 - val_mae: 1353.5608\n",
      "Epoch 2460/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 72.1224 - mae: 72.8047 - val_loss: 1189.4111 - val_mae: 1190.1042\n",
      "Epoch 2461/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 63.7181 - mae: 64.3952 - val_loss: 1465.3606 - val_mae: 1466.0538\n",
      "Epoch 2462/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 71.1717 - mae: 71.8523 - val_loss: 1278.1465 - val_mae: 1278.8396\n",
      "Epoch 2463/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 70.5329 - mae: 71.2124 - val_loss: 1454.3484 - val_mae: 1455.0416\n",
      "Epoch 2464/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 65.0327 - mae: 65.7091 - val_loss: 1378.8326 - val_mae: 1379.5255\n",
      "Epoch 2465/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 71.8288 - mae: 72.5075 - val_loss: 1237.1467 - val_mae: 1237.8391\n",
      "Epoch 2466/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 69.2010 - mae: 69.8832 - val_loss: 1292.9806 - val_mae: 1293.6731\n",
      "Epoch 2467/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 66.3481 - mae: 67.0273 - val_loss: 1356.4594 - val_mae: 1357.1515\n",
      "Epoch 2468/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 72.9925 - mae: 73.6713 - val_loss: 1227.4854 - val_mae: 1228.1772\n",
      "Epoch 2469/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 74.7445 - mae: 75.4283 - val_loss: 1248.5800 - val_mae: 1249.2717\n",
      "Epoch 2470/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 69.1647 - mae: 69.8444 - val_loss: 1238.7478 - val_mae: 1239.4412\n",
      "Epoch 2471/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 72.8920 - mae: 73.5740 - val_loss: 1223.2340 - val_mae: 1223.9272\n",
      "Epoch 2472/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 70.7613 - mae: 71.4419 - val_loss: 1322.1149 - val_mae: 1322.8080\n",
      "Epoch 2473/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 75.0356 - mae: 75.7154 - val_loss: 1286.5361 - val_mae: 1287.2292\n",
      "Epoch 2474/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 72.9010 - mae: 73.5814 - val_loss: 1495.7822 - val_mae: 1496.4751\n",
      "Epoch 2475/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 72.7310 - mae: 73.4084 - val_loss: 1211.3544 - val_mae: 1212.0464\n",
      "Epoch 2476/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 72.9415 - mae: 73.6240 - val_loss: 1354.4987 - val_mae: 1355.1918\n",
      "Epoch 2477/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 66.5043 - mae: 67.1850 - val_loss: 1325.4098 - val_mae: 1326.1029\n",
      "Epoch 2478/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 71.1442 - mae: 71.8223 - val_loss: 1294.1525 - val_mae: 1294.8442\n",
      "Epoch 2479/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 65.1911 - mae: 65.8673 - val_loss: 1408.7981 - val_mae: 1409.4913\n",
      "Epoch 2480/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 67.4207 - mae: 68.1037 - val_loss: 1488.5337 - val_mae: 1489.2271\n",
      "Epoch 2481/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 79.3600 - mae: 80.0402 - val_loss: 1246.3721 - val_mae: 1247.0653\n",
      "Epoch 2482/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 69.5491 - mae: 70.2321 - val_loss: 1334.6476 - val_mae: 1335.3409\n",
      "Epoch 2483/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 67.2935 - mae: 67.9724 - val_loss: 1352.6298 - val_mae: 1353.3225\n",
      "Epoch 2484/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 68.3563 - mae: 69.0356 - val_loss: 1318.0656 - val_mae: 1318.7585\n",
      "Epoch 2485/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 62.3686 - mae: 63.0481 - val_loss: 1325.6616 - val_mae: 1326.3545\n",
      "Epoch 2486/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 62.3005 - mae: 62.9794 - val_loss: 1259.5995 - val_mae: 1260.2927\n",
      "Epoch 2487/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 66.3542 - mae: 67.0344 - val_loss: 1404.0438 - val_mae: 1404.7369\n",
      "Epoch 2488/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 69.3356 - mae: 70.0156 - val_loss: 1309.0515 - val_mae: 1309.7446\n",
      "Epoch 2489/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 64.1352 - mae: 64.8145 - val_loss: 1498.7050 - val_mae: 1499.3962\n",
      "Epoch 2490/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 69.8820 - mae: 70.5626 - val_loss: 1360.1587 - val_mae: 1360.8519\n",
      "Epoch 2491/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 79.9031 - mae: 80.5835 - val_loss: 1237.4038 - val_mae: 1238.0969\n",
      "Epoch 2492/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 68.9303 - mae: 69.6109 - val_loss: 1332.3064 - val_mae: 1332.9995\n",
      "Epoch 2493/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 67.3174 - mae: 67.9979 - val_loss: 1253.6003 - val_mae: 1254.2936\n",
      "Epoch 2494/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 77.1776 - mae: 77.8612 - val_loss: 1236.6838 - val_mae: 1237.3773\n",
      "Epoch 2495/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 76.6507 - mae: 77.3334 - val_loss: 1297.3696 - val_mae: 1298.0626\n",
      "Epoch 2496/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 65.3714 - mae: 66.0524 - val_loss: 1458.4264 - val_mae: 1459.1194\n",
      "Epoch 2497/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 67.9896 - mae: 68.6638 - val_loss: 1357.3052 - val_mae: 1357.9974\n",
      "Epoch 2498/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 71.0279 - mae: 71.7066 - val_loss: 1255.8331 - val_mae: 1256.5261\n",
      "Epoch 2499/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 70.9482 - mae: 71.6293 - val_loss: 1280.4752 - val_mae: 1281.1677\n",
      "Epoch 2500/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 67.4593 - mae: 68.1382 - val_loss: 1347.6748 - val_mae: 1348.3682\n",
      "Epoch 2501/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 68.0099 - mae: 68.6893 - val_loss: 1277.3873 - val_mae: 1278.0802\n",
      "Epoch 2502/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 70.6817 - mae: 71.3599 - val_loss: 1291.5079 - val_mae: 1292.2010\n",
      "Epoch 2503/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 75.0870 - mae: 75.7692 - val_loss: 1440.4874 - val_mae: 1441.1807\n",
      "Epoch 2504/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 66.4671 - mae: 67.1467 - val_loss: 1251.3895 - val_mae: 1252.0828\n",
      "Epoch 2505/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 84.8512 - mae: 85.5359 - val_loss: 1433.9314 - val_mae: 1434.6246\n",
      "Epoch 2506/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 71.4022 - mae: 72.0833 - val_loss: 1393.9430 - val_mae: 1394.6340\n",
      "Epoch 2507/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 65.3912 - mae: 66.0701 - val_loss: 1335.1427 - val_mae: 1335.8356\n",
      "Epoch 2508/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 63.6192 - mae: 64.2997 - val_loss: 1311.7826 - val_mae: 1312.4758\n",
      "Epoch 2509/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 67.9495 - mae: 68.6294 - val_loss: 1389.9774 - val_mae: 1390.6704\n",
      "Epoch 2510/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 71.2483 - mae: 71.9306 - val_loss: 1396.9561 - val_mae: 1397.6493\n",
      "Epoch 2511/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 68.8247 - mae: 69.5042 - val_loss: 1344.1284 - val_mae: 1344.8215\n",
      "Epoch 2512/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 82.3046 - mae: 82.9866 - val_loss: 1470.2164 - val_mae: 1470.9092\n",
      "Epoch 2513/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 69.0999 - mae: 69.7816 - val_loss: 1270.9573 - val_mae: 1271.6499\n",
      "Epoch 2514/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 79.9390 - mae: 80.6239 - val_loss: 1344.8481 - val_mae: 1345.5413\n",
      "Epoch 2515/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 66.4823 - mae: 67.1626 - val_loss: 1329.1490 - val_mae: 1329.8422\n",
      "Epoch 2516/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 70.6598 - mae: 71.3377 - val_loss: 1381.9357 - val_mae: 1382.6288\n",
      "Epoch 2517/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 65.8635 - mae: 66.5465 - val_loss: 1281.5009 - val_mae: 1282.1940\n",
      "Epoch 2518/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 68.2406 - mae: 68.9217 - val_loss: 1332.3973 - val_mae: 1333.0905\n",
      "Epoch 2519/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 63.8968 - mae: 64.5733 - val_loss: 1271.1794 - val_mae: 1271.8715\n",
      "Epoch 2520/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 62.3792 - mae: 63.0521 - val_loss: 1266.3584 - val_mae: 1267.0502\n",
      "Epoch 2521/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 67.7595 - mae: 68.4451 - val_loss: 1289.9675 - val_mae: 1290.6608\n",
      "Epoch 2522/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 79.9950 - mae: 80.6794 - val_loss: 1274.5974 - val_mae: 1275.2904\n",
      "Epoch 2523/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 69.7223 - mae: 70.4023 - val_loss: 1373.9110 - val_mae: 1374.6041\n",
      "Epoch 2524/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 64.7610 - mae: 65.4435 - val_loss: 1316.9779 - val_mae: 1317.6705\n",
      "Epoch 2525/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 72.7362 - mae: 73.4126 - val_loss: 1438.0411 - val_mae: 1438.7343\n",
      "Epoch 2526/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 70.6584 - mae: 71.3376 - val_loss: 1384.3564 - val_mae: 1385.0491\n",
      "Epoch 2527/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 73.6191 - mae: 74.3041 - val_loss: 1387.9716 - val_mae: 1388.6636\n",
      "Epoch 2528/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 67.0153 - mae: 67.6899 - val_loss: 1374.7466 - val_mae: 1375.4393\n",
      "Epoch 2529/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 70.3893 - mae: 71.0689 - val_loss: 1357.0232 - val_mae: 1357.7151\n",
      "Epoch 2530/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 71.1460 - mae: 71.8251 - val_loss: 1295.6050 - val_mae: 1296.2976\n",
      "Epoch 2531/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 74.4158 - mae: 75.0985 - val_loss: 1251.8890 - val_mae: 1252.5813\n",
      "Epoch 2532/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 74.6003 - mae: 75.2840 - val_loss: 1321.6455 - val_mae: 1322.3386\n",
      "Epoch 2533/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 61.8489 - mae: 62.5272 - val_loss: 1349.9868 - val_mae: 1350.6801\n",
      "Epoch 2534/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 66.1025 - mae: 66.7807 - val_loss: 1413.2449 - val_mae: 1413.9380\n",
      "Epoch 2535/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 66.8553 - mae: 67.5354 - val_loss: 1410.1897 - val_mae: 1410.8821\n",
      "Epoch 2536/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 60.9169 - mae: 61.5989 - val_loss: 1392.1190 - val_mae: 1392.8121\n",
      "Epoch 2537/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 67.4192 - mae: 68.0973 - val_loss: 1324.4335 - val_mae: 1325.1266\n",
      "Epoch 2538/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 63.2515 - mae: 63.9262 - val_loss: 1367.5566 - val_mae: 1368.2499\n",
      "Epoch 2539/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 74.0399 - mae: 74.7211 - val_loss: 1376.8176 - val_mae: 1377.5106\n",
      "Epoch 2540/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 72.5890 - mae: 73.2703 - val_loss: 1329.6240 - val_mae: 1330.3171\n",
      "Epoch 2541/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 62.7003 - mae: 63.3778 - val_loss: 1334.2977 - val_mae: 1334.9907\n",
      "Epoch 2542/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 72.3993 - mae: 73.0809 - val_loss: 1310.5591 - val_mae: 1311.2520\n",
      "Epoch 2543/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 73.5787 - mae: 74.2602 - val_loss: 1380.8466 - val_mae: 1381.5397\n",
      "Epoch 2544/5000\n",
      "46/46 [==============================] - 2s 42ms/step - loss: 68.9892 - mae: 69.6670 - val_loss: 1411.0057 - val_mae: 1411.6989\n",
      "Epoch 2545/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 63.2570 - mae: 63.9361 - val_loss: 1241.7938 - val_mae: 1242.4869\n",
      "Epoch 2546/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 67.2718 - mae: 67.9497 - val_loss: 1306.3514 - val_mae: 1307.0444\n",
      "Epoch 2547/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 69.6102 - mae: 70.2927 - val_loss: 1517.9025 - val_mae: 1518.5955\n",
      "Epoch 2548/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 65.1159 - mae: 65.7950 - val_loss: 1273.4209 - val_mae: 1274.1140\n",
      "Epoch 2549/5000\n",
      "46/46 [==============================] - 2s 44ms/step - loss: 62.8654 - mae: 63.5457 - val_loss: 1220.2812 - val_mae: 1220.9734\n",
      "Epoch 2550/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 65.7644 - mae: 66.4427 - val_loss: 1312.7278 - val_mae: 1313.4207\n",
      "Epoch 2551/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 64.6437 - mae: 65.3224 - val_loss: 1306.9470 - val_mae: 1307.6400\n",
      "Epoch 2552/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 63.5437 - mae: 64.2234 - val_loss: 1300.8207 - val_mae: 1301.5140\n",
      "Epoch 2553/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 64.4633 - mae: 65.1462 - val_loss: 1374.1610 - val_mae: 1374.8541\n",
      "Epoch 2554/5000\n",
      "46/46 [==============================] - 2s 48ms/step - loss: 61.0944 - mae: 61.7759 - val_loss: 1363.2782 - val_mae: 1363.9712\n",
      "Epoch 2555/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 71.5624 - mae: 72.2406 - val_loss: 1413.5471 - val_mae: 1414.2404\n",
      "Epoch 2556/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 79.4611 - mae: 80.1423 - val_loss: 1379.6302 - val_mae: 1380.3228\n",
      "Epoch 2557/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 67.6277 - mae: 68.3051 - val_loss: 1431.3588 - val_mae: 1432.0519\n",
      "Epoch 2558/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 72.3855 - mae: 73.0669 - val_loss: 1375.3540 - val_mae: 1376.0470\n",
      "Epoch 2559/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 68.4890 - mae: 69.1688 - val_loss: 1288.3680 - val_mae: 1289.0610\n",
      "Epoch 2560/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 69.4568 - mae: 70.1355 - val_loss: 1397.4436 - val_mae: 1398.1368\n",
      "Epoch 2561/5000\n",
      "46/46 [==============================] - 2s 31ms/step - loss: 64.3347 - mae: 65.0137 - val_loss: 1329.4440 - val_mae: 1330.1371\n",
      "Epoch 2562/5000\n",
      "46/46 [==============================] - 2s 32ms/step - loss: 68.2920 - mae: 68.9704 - val_loss: 1305.6071 - val_mae: 1306.3002\n",
      "Epoch 2563/5000\n",
      "46/46 [==============================] - 2s 42ms/step - loss: 68.6948 - mae: 69.3779 - val_loss: 1432.9574 - val_mae: 1433.6506\n",
      "Epoch 2564/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 64.9031 - mae: 65.5799 - val_loss: 1276.8605 - val_mae: 1277.5536\n",
      "Epoch 2565/5000\n",
      "46/46 [==============================] - 2s 32ms/step - loss: 68.8185 - mae: 69.5030 - val_loss: 1295.5853 - val_mae: 1296.2786\n",
      "Epoch 2566/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 66.5981 - mae: 67.2782 - val_loss: 1250.1794 - val_mae: 1250.8726\n",
      "Epoch 2567/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 64.2734 - mae: 64.9542 - val_loss: 1211.4432 - val_mae: 1212.1359\n",
      "Epoch 2568/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 69.3474 - mae: 70.0265 - val_loss: 1307.7563 - val_mae: 1308.4490\n",
      "Epoch 2569/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 75.2018 - mae: 75.8829 - val_loss: 1317.7837 - val_mae: 1318.4755\n",
      "Epoch 2570/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 73.1508 - mae: 73.8284 - val_loss: 1354.9869 - val_mae: 1355.6802\n",
      "Epoch 2571/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 63.8962 - mae: 64.5748 - val_loss: 1451.0555 - val_mae: 1451.7485\n",
      "Epoch 2572/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 79.5398 - mae: 80.2233 - val_loss: 1322.1399 - val_mae: 1322.8319\n",
      "Epoch 2573/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 72.7207 - mae: 73.4021 - val_loss: 1353.6265 - val_mae: 1354.3195\n",
      "Epoch 2574/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 61.9870 - mae: 62.6666 - val_loss: 1273.8229 - val_mae: 1274.5160\n",
      "Epoch 2575/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 67.3221 - mae: 68.0003 - val_loss: 1373.2643 - val_mae: 1373.9573\n",
      "Epoch 2576/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 68.9084 - mae: 69.5853 - val_loss: 1242.7336 - val_mae: 1243.4270\n",
      "Epoch 2577/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 68.4544 - mae: 69.1310 - val_loss: 1272.0065 - val_mae: 1272.6984\n",
      "Epoch 2578/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 68.3679 - mae: 69.0452 - val_loss: 1256.5555 - val_mae: 1257.2487\n",
      "Epoch 2579/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 69.7651 - mae: 70.4438 - val_loss: 1352.4106 - val_mae: 1353.1038\n",
      "Epoch 2580/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 65.0435 - mae: 65.7221 - val_loss: 1382.9622 - val_mae: 1383.6553\n",
      "Epoch 2581/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 67.7168 - mae: 68.4000 - val_loss: 1446.6412 - val_mae: 1447.3344\n",
      "Epoch 2582/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 63.5865 - mae: 64.2645 - val_loss: 1342.4548 - val_mae: 1343.1472\n",
      "Epoch 2583/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 63.9432 - mae: 64.6252 - val_loss: 1267.9718 - val_mae: 1268.6648\n",
      "Epoch 2584/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 60.8568 - mae: 61.5366 - val_loss: 1337.3101 - val_mae: 1338.0029\n",
      "Epoch 2585/5000\n",
      "46/46 [==============================] - 2s 41ms/step - loss: 64.0464 - mae: 64.7245 - val_loss: 1303.7566 - val_mae: 1304.4490\n",
      "Epoch 2586/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 71.3131 - mae: 71.9940 - val_loss: 1352.9788 - val_mae: 1353.6718\n",
      "Epoch 2587/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 69.2234 - mae: 69.9042 - val_loss: 1366.0892 - val_mae: 1366.7815\n",
      "Epoch 2588/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 69.4374 - mae: 70.1176 - val_loss: 1280.4989 - val_mae: 1281.1921\n",
      "Epoch 2589/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 65.6598 - mae: 66.3418 - val_loss: 1401.0956 - val_mae: 1401.7885\n",
      "Epoch 2590/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 66.4510 - mae: 67.1293 - val_loss: 1356.2878 - val_mae: 1356.9810\n",
      "Epoch 2591/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 68.1928 - mae: 68.8735 - val_loss: 1251.2490 - val_mae: 1251.9423\n",
      "Epoch 2592/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 72.5831 - mae: 73.2583 - val_loss: 1375.2039 - val_mae: 1375.8959\n",
      "Epoch 2593/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 70.6454 - mae: 71.3234 - val_loss: 1392.8019 - val_mae: 1393.4949\n",
      "Epoch 2594/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 62.5232 - mae: 63.1989 - val_loss: 1351.9802 - val_mae: 1352.6729\n",
      "Epoch 2595/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 59.3903 - mae: 60.0713 - val_loss: 1373.3086 - val_mae: 1374.0009\n",
      "Epoch 2596/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 62.0834 - mae: 62.7585 - val_loss: 1355.2325 - val_mae: 1355.9258\n",
      "Epoch 2597/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 68.0334 - mae: 68.7146 - val_loss: 1352.3015 - val_mae: 1352.9946\n",
      "Epoch 2598/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 58.6742 - mae: 59.3500 - val_loss: 1418.1527 - val_mae: 1418.8459\n",
      "Epoch 2599/5000\n",
      "46/46 [==============================] - 2s 42ms/step - loss: 69.0784 - mae: 69.7598 - val_loss: 1312.7034 - val_mae: 1313.3966\n",
      "Epoch 2600/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 59.3518 - mae: 60.0287 - val_loss: 1342.9899 - val_mae: 1343.6826\n",
      "Epoch 2601/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 60.4199 - mae: 61.0971 - val_loss: 1315.3502 - val_mae: 1316.0436\n",
      "Epoch 2602/5000\n",
      "46/46 [==============================] - 2s 43ms/step - loss: 66.2544 - mae: 66.9336 - val_loss: 1318.7281 - val_mae: 1319.4214\n",
      "Epoch 2603/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 78.3436 - mae: 79.0303 - val_loss: 1287.1692 - val_mae: 1287.8623\n",
      "Epoch 2604/5000\n",
      "46/46 [==============================] - 2s 48ms/step - loss: 69.1791 - mae: 69.8611 - val_loss: 1371.0601 - val_mae: 1371.7532\n",
      "Epoch 2605/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 60.5857 - mae: 61.2635 - val_loss: 1365.1279 - val_mae: 1365.8210\n",
      "Epoch 2606/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 66.6250 - mae: 67.3092 - val_loss: 1277.9910 - val_mae: 1278.6842\n",
      "Epoch 2607/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 67.4754 - mae: 68.1551 - val_loss: 1324.5681 - val_mae: 1325.2609\n",
      "Epoch 2608/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 62.6609 - mae: 63.3401 - val_loss: 1414.9663 - val_mae: 1415.6593\n",
      "Epoch 2609/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 70.4610 - mae: 71.1446 - val_loss: 1252.7513 - val_mae: 1253.4447\n",
      "Epoch 2610/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 64.1755 - mae: 64.8551 - val_loss: 1266.0045 - val_mae: 1266.6976\n",
      "Epoch 2611/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 64.8679 - mae: 65.5503 - val_loss: 1336.6677 - val_mae: 1337.3607\n",
      "Epoch 2612/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 61.6173 - mae: 62.2978 - val_loss: 1341.7578 - val_mae: 1342.4504\n",
      "Epoch 2613/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 64.2522 - mae: 64.9294 - val_loss: 1271.4213 - val_mae: 1272.1135\n",
      "Epoch 2614/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 61.8264 - mae: 62.5057 - val_loss: 1331.6294 - val_mae: 1332.3224\n",
      "Epoch 2615/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 65.2705 - mae: 65.9530 - val_loss: 1466.3940 - val_mae: 1467.0865\n",
      "Epoch 2616/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 69.5716 - mae: 70.2532 - val_loss: 1352.7517 - val_mae: 1353.4448\n",
      "Epoch 2617/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 64.4773 - mae: 65.1605 - val_loss: 1370.4119 - val_mae: 1371.1046\n",
      "Epoch 2618/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 64.0046 - mae: 64.6822 - val_loss: 1319.9988 - val_mae: 1320.6896\n",
      "Epoch 2619/5000\n",
      "46/46 [==============================] - 2s 42ms/step - loss: 74.5851 - mae: 75.2620 - val_loss: 1357.2216 - val_mae: 1357.9148\n",
      "Epoch 2620/5000\n",
      "46/46 [==============================] - 2s 40ms/step - loss: 67.4023 - mae: 68.0826 - val_loss: 1423.4413 - val_mae: 1424.1339\n",
      "Epoch 2621/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 73.6974 - mae: 74.3757 - val_loss: 1325.7054 - val_mae: 1326.3976\n",
      "Epoch 2622/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 65.8660 - mae: 66.5502 - val_loss: 1335.7855 - val_mae: 1336.4786\n",
      "Epoch 2623/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 66.1575 - mae: 66.8337 - val_loss: 1249.2041 - val_mae: 1249.8958\n",
      "Epoch 2624/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 63.7672 - mae: 64.4480 - val_loss: 1355.0210 - val_mae: 1355.7140\n",
      "Epoch 2625/5000\n",
      "46/46 [==============================] - 2s 40ms/step - loss: 67.6182 - mae: 68.3004 - val_loss: 1450.7236 - val_mae: 1451.4167\n",
      "Epoch 2626/5000\n",
      "46/46 [==============================] - 2s 40ms/step - loss: 85.0619 - mae: 85.7463 - val_loss: 1288.3093 - val_mae: 1289.0026\n",
      "Epoch 2627/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 68.8522 - mae: 69.5308 - val_loss: 1347.1572 - val_mae: 1347.8503\n",
      "Epoch 2628/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 77.2951 - mae: 77.9761 - val_loss: 1405.2142 - val_mae: 1405.9075\n",
      "Epoch 2629/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 80.2755 - mae: 80.9572 - val_loss: 1442.2292 - val_mae: 1442.9221\n",
      "Epoch 2630/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 61.6792 - mae: 62.3528 - val_loss: 1311.5299 - val_mae: 1312.2230\n",
      "Epoch 2631/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 74.3324 - mae: 75.0139 - val_loss: 1442.0981 - val_mae: 1442.7910\n",
      "Epoch 2632/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 69.8610 - mae: 70.5397 - val_loss: 1420.5265 - val_mae: 1421.2189\n",
      "Epoch 2633/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 64.2139 - mae: 64.8925 - val_loss: 1316.2408 - val_mae: 1316.9338\n",
      "Epoch 2634/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 69.8114 - mae: 70.4885 - val_loss: 1301.1522 - val_mae: 1301.8453\n",
      "Epoch 2635/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 71.7689 - mae: 72.4490 - val_loss: 1240.0051 - val_mae: 1240.6979\n",
      "Epoch 2636/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 81.9941 - mae: 82.6724 - val_loss: 1455.5302 - val_mae: 1456.2233\n",
      "Epoch 2637/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 70.0221 - mae: 70.7041 - val_loss: 1314.9675 - val_mae: 1315.6608\n",
      "Epoch 2638/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 62.3546 - mae: 63.0354 - val_loss: 1300.1200 - val_mae: 1300.8130\n",
      "Epoch 2639/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 67.2466 - mae: 67.9267 - val_loss: 1381.6309 - val_mae: 1382.3240\n",
      "Epoch 2640/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 71.2038 - mae: 71.8828 - val_loss: 1391.5813 - val_mae: 1392.2747\n",
      "Epoch 2641/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 82.0966 - mae: 82.7769 - val_loss: 1313.3665 - val_mae: 1314.0597\n",
      "Epoch 2642/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 69.2675 - mae: 69.9491 - val_loss: 1299.2083 - val_mae: 1299.9015\n",
      "Epoch 2643/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 68.2993 - mae: 68.9775 - val_loss: 1380.2390 - val_mae: 1380.9315\n",
      "Epoch 2644/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 76.4444 - mae: 77.1263 - val_loss: 1350.8463 - val_mae: 1351.5394\n",
      "Epoch 2645/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 67.1776 - mae: 67.8616 - val_loss: 1272.4172 - val_mae: 1273.1102\n",
      "Epoch 2646/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 64.2053 - mae: 64.8856 - val_loss: 1286.5587 - val_mae: 1287.2522\n",
      "Epoch 2647/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 70.0382 - mae: 70.7173 - val_loss: 1399.0037 - val_mae: 1399.6956\n",
      "Epoch 2648/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 65.2628 - mae: 65.9402 - val_loss: 1379.1207 - val_mae: 1379.8141\n",
      "Epoch 2649/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 62.7857 - mae: 63.4674 - val_loss: 1376.1193 - val_mae: 1376.8124\n",
      "Epoch 2650/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 73.7802 - mae: 74.4602 - val_loss: 1377.6967 - val_mae: 1378.3898\n",
      "Epoch 2651/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 59.8512 - mae: 60.5265 - val_loss: 1322.5077 - val_mae: 1323.2009\n",
      "Epoch 2652/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 68.3529 - mae: 69.0362 - val_loss: 1326.5646 - val_mae: 1327.2570\n",
      "Epoch 2653/5000\n",
      "46/46 [==============================] - 2s 49ms/step - loss: 75.3036 - mae: 75.9848 - val_loss: 1303.9548 - val_mae: 1304.6475\n",
      "Epoch 2654/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 67.9151 - mae: 68.5922 - val_loss: 1305.7385 - val_mae: 1306.4313\n",
      "Epoch 2655/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 63.1444 - mae: 63.8241 - val_loss: 1376.3181 - val_mae: 1377.0111\n",
      "Epoch 2656/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 71.4510 - mae: 72.1284 - val_loss: 1382.9132 - val_mae: 1383.6064\n",
      "Epoch 2657/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 62.8192 - mae: 63.4986 - val_loss: 1425.0554 - val_mae: 1425.7482\n",
      "Epoch 2658/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 61.6795 - mae: 62.3524 - val_loss: 1364.0121 - val_mae: 1364.7046\n",
      "Epoch 2659/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 61.3243 - mae: 62.0034 - val_loss: 1357.6528 - val_mae: 1358.3459\n",
      "Epoch 2660/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 67.5227 - mae: 68.1998 - val_loss: 1299.6022 - val_mae: 1300.2931\n",
      "Epoch 2661/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 67.1735 - mae: 67.8494 - val_loss: 1305.5344 - val_mae: 1306.2279\n",
      "Epoch 2662/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 66.1752 - mae: 66.8512 - val_loss: 1371.8236 - val_mae: 1372.5165\n",
      "Epoch 2663/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 68.1545 - mae: 68.8354 - val_loss: 1320.3510 - val_mae: 1321.0441\n",
      "Epoch 2664/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 62.5686 - mae: 63.2478 - val_loss: 1318.3037 - val_mae: 1318.9960\n",
      "Epoch 2665/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 66.8392 - mae: 67.5205 - val_loss: 1307.1898 - val_mae: 1307.8827\n",
      "Epoch 2666/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 59.6764 - mae: 60.3535 - val_loss: 1329.2489 - val_mae: 1329.9423\n",
      "Epoch 2667/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 64.6529 - mae: 65.3304 - val_loss: 1277.6302 - val_mae: 1278.3232\n",
      "Epoch 2668/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 63.3434 - mae: 64.0204 - val_loss: 1322.9933 - val_mae: 1323.6864\n",
      "Epoch 2669/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 58.1192 - mae: 58.7988 - val_loss: 1356.1758 - val_mae: 1356.8691\n",
      "Epoch 2670/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 65.7286 - mae: 66.4083 - val_loss: 1261.8887 - val_mae: 1262.5819\n",
      "Epoch 2671/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 63.7250 - mae: 64.4035 - val_loss: 1353.2397 - val_mae: 1353.9329\n",
      "Epoch 2672/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 64.9333 - mae: 65.6168 - val_loss: 1340.6002 - val_mae: 1341.2936\n",
      "Epoch 2673/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 72.4166 - mae: 73.0979 - val_loss: 1334.8892 - val_mae: 1335.5822\n",
      "Epoch 2674/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 65.4161 - mae: 66.0939 - val_loss: 1351.7628 - val_mae: 1352.4558\n",
      "Epoch 2675/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 68.5303 - mae: 69.2094 - val_loss: 1428.4115 - val_mae: 1429.1047\n",
      "Epoch 2676/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 62.1446 - mae: 62.8219 - val_loss: 1312.2408 - val_mae: 1312.9338\n",
      "Epoch 2677/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 63.9341 - mae: 64.6159 - val_loss: 1409.0161 - val_mae: 1409.7089\n",
      "Epoch 2678/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 68.4251 - mae: 69.1062 - val_loss: 1416.8005 - val_mae: 1417.4934\n",
      "Epoch 2679/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 72.6817 - mae: 73.3614 - val_loss: 1321.1409 - val_mae: 1321.8339\n",
      "Epoch 2680/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 71.0095 - mae: 71.6909 - val_loss: 1280.9320 - val_mae: 1281.6248\n",
      "Epoch 2681/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 61.7092 - mae: 62.3883 - val_loss: 1357.3970 - val_mae: 1358.0902\n",
      "Epoch 2682/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 61.0037 - mae: 61.6808 - val_loss: 1389.0959 - val_mae: 1389.7891\n",
      "Epoch 2683/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 65.2690 - mae: 65.9478 - val_loss: 1390.5117 - val_mae: 1391.2048\n",
      "Epoch 2684/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 62.4677 - mae: 63.1472 - val_loss: 1263.1733 - val_mae: 1263.8662\n",
      "Epoch 2685/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 69.3468 - mae: 70.0288 - val_loss: 1454.3756 - val_mae: 1455.0687\n",
      "Epoch 2686/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 78.2677 - mae: 78.9450 - val_loss: 1364.7023 - val_mae: 1365.3947\n",
      "Epoch 2687/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 64.8475 - mae: 65.5268 - val_loss: 1301.1014 - val_mae: 1301.7933\n",
      "Epoch 2688/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 62.5731 - mae: 63.2537 - val_loss: 1288.5771 - val_mae: 1289.2690\n",
      "Epoch 2689/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 63.2891 - mae: 63.9699 - val_loss: 1234.9353 - val_mae: 1235.6287\n",
      "Epoch 2690/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 78.6857 - mae: 79.3646 - val_loss: 1319.3037 - val_mae: 1319.9958\n",
      "Epoch 2691/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 64.8763 - mae: 65.5539 - val_loss: 1313.3994 - val_mae: 1314.0922\n",
      "Epoch 2692/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 67.9463 - mae: 68.6268 - val_loss: 1323.3066 - val_mae: 1323.9984\n",
      "Epoch 2693/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 65.3533 - mae: 66.0322 - val_loss: 1355.8286 - val_mae: 1356.5219\n",
      "Epoch 2694/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 67.6288 - mae: 68.3081 - val_loss: 1384.5540 - val_mae: 1385.2471\n",
      "Epoch 2695/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 66.2561 - mae: 66.9347 - val_loss: 1465.6919 - val_mae: 1466.3850\n",
      "Epoch 2696/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 65.0466 - mae: 65.7292 - val_loss: 1400.2064 - val_mae: 1400.8997\n",
      "Epoch 2697/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 60.7062 - mae: 61.3869 - val_loss: 1334.9874 - val_mae: 1335.6807\n",
      "Epoch 2698/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 60.3313 - mae: 61.0121 - val_loss: 1358.8448 - val_mae: 1359.5378\n",
      "Epoch 2699/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 67.2803 - mae: 67.9594 - val_loss: 1450.7350 - val_mae: 1451.4276\n",
      "Epoch 2700/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 67.8984 - mae: 68.5799 - val_loss: 1346.4340 - val_mae: 1347.1270\n",
      "Epoch 2701/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 65.9843 - mae: 66.6650 - val_loss: 1280.4630 - val_mae: 1281.1561\n",
      "Epoch 2702/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 65.4819 - mae: 66.1648 - val_loss: 1342.3165 - val_mae: 1343.0098\n",
      "Epoch 2703/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 65.0338 - mae: 65.7108 - val_loss: 1322.9735 - val_mae: 1323.6667\n",
      "Epoch 2704/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 64.8835 - mae: 65.5598 - val_loss: 1464.7676 - val_mae: 1465.4598\n",
      "Epoch 2705/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 63.1059 - mae: 63.7869 - val_loss: 1311.5280 - val_mae: 1312.2208\n",
      "Epoch 2706/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 67.4377 - mae: 68.1185 - val_loss: 1217.1641 - val_mae: 1217.8560\n",
      "Epoch 2707/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 64.2879 - mae: 64.9649 - val_loss: 1350.0996 - val_mae: 1350.7919\n",
      "Epoch 2708/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 61.6232 - mae: 62.3041 - val_loss: 1323.2638 - val_mae: 1323.9557\n",
      "Epoch 2709/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 65.8425 - mae: 66.5232 - val_loss: 1436.3683 - val_mae: 1437.0614\n",
      "Epoch 2710/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 69.2781 - mae: 69.9566 - val_loss: 1356.0076 - val_mae: 1356.7002\n",
      "Epoch 2711/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 70.0142 - mae: 70.6903 - val_loss: 1283.3892 - val_mae: 1284.0820\n",
      "Epoch 2712/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 75.5992 - mae: 76.2796 - val_loss: 1235.3959 - val_mae: 1236.0890\n",
      "Epoch 2713/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 74.5670 - mae: 75.2495 - val_loss: 1313.2659 - val_mae: 1313.9586\n",
      "Epoch 2714/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 69.7476 - mae: 70.4300 - val_loss: 1377.4606 - val_mae: 1378.1532\n",
      "Epoch 2715/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 63.2341 - mae: 63.9125 - val_loss: 1248.9208 - val_mae: 1249.6139\n",
      "Epoch 2716/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 78.3541 - mae: 79.0342 - val_loss: 1343.1816 - val_mae: 1343.8737\n",
      "Epoch 2717/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 69.0806 - mae: 69.7589 - val_loss: 1415.9685 - val_mae: 1416.6615\n",
      "Epoch 2718/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 81.3976 - mae: 82.0817 - val_loss: 1409.4153 - val_mae: 1410.1084\n",
      "Epoch 2719/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 67.4397 - mae: 68.1160 - val_loss: 1335.7349 - val_mae: 1336.4268\n",
      "Epoch 2720/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 62.9024 - mae: 63.5805 - val_loss: 1325.9675 - val_mae: 1326.6606\n",
      "Epoch 2721/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 66.8455 - mae: 67.5263 - val_loss: 1328.6335 - val_mae: 1329.3267\n",
      "Epoch 2722/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 78.2251 - mae: 78.9080 - val_loss: 1546.1105 - val_mae: 1546.8031\n",
      "Epoch 2723/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 69.3018 - mae: 69.9808 - val_loss: 1263.5398 - val_mae: 1264.2328\n",
      "Epoch 2724/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 70.5095 - mae: 71.1868 - val_loss: 1397.3966 - val_mae: 1398.0892\n",
      "Epoch 2725/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 61.2624 - mae: 61.9404 - val_loss: 1460.3346 - val_mae: 1461.0278\n",
      "Epoch 2726/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 84.7743 - mae: 85.4550 - val_loss: 1444.5913 - val_mae: 1445.2844\n",
      "Epoch 2727/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 75.8523 - mae: 76.5296 - val_loss: 1315.4453 - val_mae: 1316.1379\n",
      "Epoch 2728/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 63.5867 - mae: 64.2637 - val_loss: 1352.2937 - val_mae: 1352.9867\n",
      "Epoch 2729/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 64.4522 - mae: 65.1301 - val_loss: 1425.4912 - val_mae: 1426.1844\n",
      "Epoch 2730/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 63.5026 - mae: 64.1786 - val_loss: 1310.1556 - val_mae: 1310.8480\n",
      "Epoch 2731/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 66.4400 - mae: 67.1178 - val_loss: 1343.5543 - val_mae: 1344.2476\n",
      "Epoch 2732/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 63.9845 - mae: 64.6626 - val_loss: 1246.5724 - val_mae: 1247.2655\n",
      "Epoch 2733/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 62.8207 - mae: 63.4978 - val_loss: 1326.9470 - val_mae: 1327.6401\n",
      "Epoch 2734/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 71.0570 - mae: 71.7365 - val_loss: 1304.0614 - val_mae: 1304.7544\n",
      "Epoch 2735/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 68.7915 - mae: 69.4698 - val_loss: 1314.1768 - val_mae: 1314.8698\n",
      "Epoch 2736/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 64.9041 - mae: 65.5872 - val_loss: 1465.8242 - val_mae: 1466.5175\n",
      "Epoch 2737/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 72.0792 - mae: 72.7582 - val_loss: 1385.2518 - val_mae: 1385.9438\n",
      "Epoch 2738/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 66.0808 - mae: 66.7610 - val_loss: 1328.7389 - val_mae: 1329.4321\n",
      "Epoch 2739/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 65.9839 - mae: 66.6658 - val_loss: 1295.9443 - val_mae: 1296.6353\n",
      "Epoch 2740/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 68.9436 - mae: 69.6267 - val_loss: 1262.9103 - val_mae: 1263.6033\n",
      "Epoch 2741/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 66.9110 - mae: 67.5895 - val_loss: 1354.2833 - val_mae: 1354.9750\n",
      "Epoch 2742/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 63.3114 - mae: 63.9905 - val_loss: 1370.8685 - val_mae: 1371.5607\n",
      "Epoch 2743/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 66.5725 - mae: 67.2568 - val_loss: 1449.0916 - val_mae: 1449.7848\n",
      "Epoch 2744/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 63.5232 - mae: 64.2040 - val_loss: 1362.7905 - val_mae: 1363.4827\n",
      "Epoch 2745/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 70.4424 - mae: 71.1210 - val_loss: 1480.0590 - val_mae: 1480.7517\n",
      "Epoch 2746/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 77.2836 - mae: 77.9638 - val_loss: 1414.2416 - val_mae: 1414.9346\n",
      "Epoch 2747/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 77.5571 - mae: 78.2375 - val_loss: 1430.1395 - val_mae: 1430.8328\n",
      "Epoch 2748/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 69.8389 - mae: 70.5173 - val_loss: 1383.2340 - val_mae: 1383.9271\n",
      "Epoch 2749/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 58.5557 - mae: 59.2350 - val_loss: 1409.4553 - val_mae: 1410.1477\n",
      "Epoch 2750/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 69.2295 - mae: 69.9074 - val_loss: 1305.5627 - val_mae: 1306.2560\n",
      "Epoch 2751/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 58.4199 - mae: 59.0942 - val_loss: 1322.5399 - val_mae: 1323.2329\n",
      "Epoch 2752/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 59.0458 - mae: 59.7240 - val_loss: 1326.4042 - val_mae: 1327.0970\n",
      "Epoch 2753/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 62.8939 - mae: 63.5726 - val_loss: 1353.7729 - val_mae: 1354.4659\n",
      "Epoch 2754/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 71.3051 - mae: 71.9819 - val_loss: 1246.2252 - val_mae: 1246.9185\n",
      "Epoch 2755/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 71.0196 - mae: 71.6994 - val_loss: 1309.7941 - val_mae: 1310.4863\n",
      "Epoch 2756/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 69.0980 - mae: 69.7790 - val_loss: 1537.5270 - val_mae: 1538.2190\n",
      "Epoch 2757/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 76.1454 - mae: 76.8247 - val_loss: 1573.5835 - val_mae: 1574.2754\n",
      "Epoch 2758/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 67.6615 - mae: 68.3402 - val_loss: 1429.2637 - val_mae: 1429.9568\n",
      "Epoch 2759/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 65.9043 - mae: 66.5830 - val_loss: 1408.4904 - val_mae: 1409.1836\n",
      "Epoch 2760/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 69.4845 - mae: 70.1599 - val_loss: 1425.8558 - val_mae: 1426.5480\n",
      "Epoch 2761/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 79.1060 - mae: 79.7879 - val_loss: 1392.2874 - val_mae: 1392.9803\n",
      "Epoch 2762/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 72.1311 - mae: 72.8098 - val_loss: 1395.5952 - val_mae: 1396.2886\n",
      "Epoch 2763/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 76.2608 - mae: 76.9422 - val_loss: 1383.0508 - val_mae: 1383.7438\n",
      "Epoch 2764/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 75.8326 - mae: 76.5157 - val_loss: 1295.6710 - val_mae: 1296.3628\n",
      "Epoch 2765/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 68.3624 - mae: 69.0434 - val_loss: 1316.9264 - val_mae: 1317.6193\n",
      "Epoch 2766/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 63.6717 - mae: 64.3518 - val_loss: 1356.9557 - val_mae: 1357.6479\n",
      "Epoch 2767/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 59.2706 - mae: 59.9481 - val_loss: 1392.4075 - val_mae: 1393.0979\n",
      "Epoch 2768/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 68.2471 - mae: 68.9227 - val_loss: 1368.8954 - val_mae: 1369.5884\n",
      "Epoch 2769/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 68.8922 - mae: 69.5724 - val_loss: 1431.1719 - val_mae: 1431.8640\n",
      "Epoch 2770/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 63.8882 - mae: 64.5649 - val_loss: 1434.9806 - val_mae: 1435.6737\n",
      "Epoch 2771/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 58.5666 - mae: 59.2471 - val_loss: 1378.5190 - val_mae: 1379.2113\n",
      "Epoch 2772/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 63.1483 - mae: 63.8287 - val_loss: 1420.3363 - val_mae: 1421.0288\n",
      "Epoch 2773/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 63.3579 - mae: 64.0326 - val_loss: 1269.2057 - val_mae: 1269.8988\n",
      "Epoch 2774/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 73.2482 - mae: 73.9333 - val_loss: 1359.9944 - val_mae: 1360.6873\n",
      "Epoch 2775/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 71.4196 - mae: 72.1006 - val_loss: 1300.2424 - val_mae: 1300.9354\n",
      "Epoch 2776/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 70.0656 - mae: 70.7434 - val_loss: 1423.9510 - val_mae: 1424.6431\n",
      "Epoch 2777/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 59.1922 - mae: 59.8661 - val_loss: 1282.8459 - val_mae: 1283.5391\n",
      "Epoch 2778/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 63.9746 - mae: 64.6551 - val_loss: 1438.6376 - val_mae: 1439.3308\n",
      "Epoch 2779/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 61.3510 - mae: 62.0306 - val_loss: 1321.6603 - val_mae: 1322.3531\n",
      "Epoch 2780/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 62.2752 - mae: 62.9562 - val_loss: 1351.2816 - val_mae: 1351.9750\n",
      "Epoch 2781/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 64.3428 - mae: 65.0222 - val_loss: 1345.9480 - val_mae: 1346.6411\n",
      "Epoch 2782/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 65.7067 - mae: 66.3880 - val_loss: 1278.6770 - val_mae: 1279.3688\n",
      "Epoch 2783/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 65.3798 - mae: 66.0606 - val_loss: 1357.0592 - val_mae: 1357.7518\n",
      "Epoch 2784/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 66.8855 - mae: 67.5651 - val_loss: 1346.5237 - val_mae: 1347.2169\n",
      "Epoch 2785/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 70.4570 - mae: 71.1369 - val_loss: 1326.6399 - val_mae: 1327.3331\n",
      "Epoch 2786/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 71.5732 - mae: 72.2528 - val_loss: 1359.6111 - val_mae: 1360.3040\n",
      "Epoch 2787/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 68.7871 - mae: 69.4660 - val_loss: 1392.8845 - val_mae: 1393.5778\n",
      "Epoch 2788/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 64.4377 - mae: 65.1164 - val_loss: 1292.4669 - val_mae: 1293.1599\n",
      "Epoch 2789/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 63.0349 - mae: 63.7174 - val_loss: 1343.0005 - val_mae: 1343.6936\n",
      "Epoch 2790/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 61.1060 - mae: 61.7845 - val_loss: 1331.0095 - val_mae: 1331.7029\n",
      "Epoch 2791/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 65.4774 - mae: 66.1583 - val_loss: 1304.9656 - val_mae: 1305.6586\n",
      "Epoch 2792/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 67.4874 - mae: 68.1674 - val_loss: 1286.9299 - val_mae: 1287.6230\n",
      "Epoch 2793/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 72.9116 - mae: 73.5942 - val_loss: 1304.6912 - val_mae: 1305.3842\n",
      "Epoch 2794/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 68.0631 - mae: 68.7439 - val_loss: 1359.3234 - val_mae: 1360.0166\n",
      "Epoch 2795/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 66.4464 - mae: 67.1284 - val_loss: 1300.5415 - val_mae: 1301.2345\n",
      "Epoch 2796/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 62.0566 - mae: 62.7399 - val_loss: 1267.7708 - val_mae: 1268.4636\n",
      "Epoch 2797/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 67.2801 - mae: 67.9562 - val_loss: 1363.0391 - val_mae: 1363.7322\n",
      "Epoch 2798/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 68.9068 - mae: 69.5883 - val_loss: 1412.8247 - val_mae: 1413.5179\n",
      "Epoch 2799/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 60.7320 - mae: 61.4119 - val_loss: 1287.1519 - val_mae: 1287.8451\n",
      "Epoch 2800/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 59.6020 - mae: 60.2766 - val_loss: 1341.5221 - val_mae: 1342.2152\n",
      "Epoch 2801/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 70.5718 - mae: 71.2542 - val_loss: 1334.6193 - val_mae: 1335.3125\n",
      "Epoch 2802/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 61.9217 - mae: 62.6057 - val_loss: 1310.5061 - val_mae: 1311.1991\n",
      "Epoch 2803/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 63.7650 - mae: 64.4430 - val_loss: 1401.8568 - val_mae: 1402.5499\n",
      "Epoch 2804/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 61.3176 - mae: 61.9945 - val_loss: 1397.6842 - val_mae: 1398.3774\n",
      "Epoch 2805/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 58.7895 - mae: 59.4656 - val_loss: 1353.4851 - val_mae: 1354.1781\n",
      "Epoch 2806/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 71.1082 - mae: 71.7888 - val_loss: 1398.3660 - val_mae: 1399.0590\n",
      "Epoch 2807/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 65.9087 - mae: 66.5906 - val_loss: 1249.9917 - val_mae: 1250.6840\n",
      "Epoch 2808/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 72.7907 - mae: 73.4729 - val_loss: 1331.7372 - val_mae: 1332.4304\n",
      "Epoch 2809/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 70.6552 - mae: 71.3340 - val_loss: 1299.0635 - val_mae: 1299.7565\n",
      "Epoch 2810/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 64.6647 - mae: 65.3445 - val_loss: 1363.4938 - val_mae: 1364.1870\n",
      "Epoch 2811/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 64.4967 - mae: 65.1787 - val_loss: 1357.2000 - val_mae: 1357.8928\n",
      "Epoch 2812/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 59.7476 - mae: 60.4269 - val_loss: 1377.5798 - val_mae: 1378.2725\n",
      "Epoch 2813/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 62.1285 - mae: 62.8065 - val_loss: 1384.2761 - val_mae: 1384.9684\n",
      "Epoch 2814/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 62.1643 - mae: 62.8418 - val_loss: 1310.5616 - val_mae: 1311.2548\n",
      "Epoch 2815/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 67.2802 - mae: 67.9623 - val_loss: 1472.0165 - val_mae: 1472.7098\n",
      "Epoch 2816/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 66.9242 - mae: 67.6039 - val_loss: 1370.6826 - val_mae: 1371.3744\n",
      "Epoch 2817/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 60.7298 - mae: 61.4069 - val_loss: 1357.1733 - val_mae: 1357.8666\n",
      "Epoch 2818/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 58.6079 - mae: 59.2818 - val_loss: 1375.3633 - val_mae: 1376.0544\n",
      "Epoch 2819/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 62.2238 - mae: 62.9002 - val_loss: 1443.9634 - val_mae: 1444.6560\n",
      "Epoch 2820/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 62.5968 - mae: 63.2747 - val_loss: 1296.0874 - val_mae: 1296.7795\n",
      "Epoch 2821/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 68.5220 - mae: 69.2048 - val_loss: 1363.9352 - val_mae: 1364.6283\n",
      "Epoch 2822/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 65.0855 - mae: 65.7653 - val_loss: 1335.0929 - val_mae: 1335.7860\n",
      "Epoch 2823/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 73.8891 - mae: 74.5682 - val_loss: 1327.1920 - val_mae: 1327.8851\n",
      "Epoch 2824/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 74.0449 - mae: 74.7261 - val_loss: 1359.1591 - val_mae: 1359.8517\n",
      "Epoch 2825/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 64.5277 - mae: 65.2105 - val_loss: 1470.9805 - val_mae: 1471.6735\n",
      "Epoch 2826/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 73.6828 - mae: 74.3647 - val_loss: 1458.7551 - val_mae: 1459.4480\n",
      "Epoch 2827/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 61.2936 - mae: 61.9741 - val_loss: 1310.8047 - val_mae: 1311.4979\n",
      "Epoch 2828/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 64.9936 - mae: 65.6739 - val_loss: 1258.1771 - val_mae: 1258.8702\n",
      "Epoch 2829/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 65.5645 - mae: 66.2400 - val_loss: 1390.6195 - val_mae: 1391.3127\n",
      "Epoch 2830/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 57.2977 - mae: 57.9718 - val_loss: 1275.2429 - val_mae: 1275.9360\n",
      "Epoch 2831/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 75.7411 - mae: 76.4199 - val_loss: 1428.6595 - val_mae: 1429.3523\n",
      "Epoch 2832/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 73.8852 - mae: 74.5662 - val_loss: 1412.8380 - val_mae: 1413.5309\n",
      "Epoch 2833/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 60.9987 - mae: 61.6820 - val_loss: 1328.2556 - val_mae: 1328.9473\n",
      "Epoch 2834/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 68.2046 - mae: 68.8888 - val_loss: 1366.0168 - val_mae: 1366.7095\n",
      "Epoch 2835/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 68.8557 - mae: 69.5368 - val_loss: 1311.6619 - val_mae: 1312.3551\n",
      "Epoch 2836/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 62.4472 - mae: 63.1264 - val_loss: 1442.4098 - val_mae: 1443.1030\n",
      "Epoch 2837/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 61.1446 - mae: 61.8232 - val_loss: 1409.7592 - val_mae: 1410.4524\n",
      "Epoch 2838/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 64.0893 - mae: 64.7674 - val_loss: 1348.8436 - val_mae: 1349.5366\n",
      "Epoch 2839/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 59.4356 - mae: 60.1111 - val_loss: 1259.2825 - val_mae: 1259.9755\n",
      "Epoch 2840/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 64.0399 - mae: 64.7163 - val_loss: 1329.2706 - val_mae: 1329.9637\n",
      "Epoch 2841/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 64.7705 - mae: 65.4509 - val_loss: 1290.2074 - val_mae: 1290.8999\n",
      "Epoch 2842/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 65.6081 - mae: 66.2895 - val_loss: 1335.9453 - val_mae: 1336.6385\n",
      "Epoch 2843/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 63.3451 - mae: 64.0220 - val_loss: 1393.1171 - val_mae: 1393.8099\n",
      "Epoch 2844/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 58.6142 - mae: 59.2923 - val_loss: 1304.5625 - val_mae: 1305.2549\n",
      "Epoch 2845/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 65.4714 - mae: 66.1514 - val_loss: 1397.4198 - val_mae: 1398.1132\n",
      "Epoch 2846/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 61.9082 - mae: 62.5878 - val_loss: 1311.9830 - val_mae: 1312.6761\n",
      "Epoch 2847/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 62.0361 - mae: 62.7134 - val_loss: 1359.8433 - val_mae: 1360.5360\n",
      "Epoch 2848/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 71.0537 - mae: 71.7347 - val_loss: 1345.9282 - val_mae: 1346.6213\n",
      "Epoch 2849/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 72.2885 - mae: 72.9715 - val_loss: 1253.5863 - val_mae: 1254.2795\n",
      "Epoch 2850/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 64.7158 - mae: 65.3963 - val_loss: 1214.2515 - val_mae: 1214.9425\n",
      "Epoch 2851/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 70.4975 - mae: 71.1784 - val_loss: 1320.0992 - val_mae: 1320.7920\n",
      "Epoch 2852/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 64.5268 - mae: 65.2055 - val_loss: 1262.5267 - val_mae: 1263.2195\n",
      "Epoch 2853/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 61.1448 - mae: 61.8223 - val_loss: 1334.1593 - val_mae: 1334.8522\n",
      "Epoch 2854/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 60.5066 - mae: 61.1815 - val_loss: 1284.3835 - val_mae: 1285.0767\n",
      "Epoch 2855/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 59.0116 - mae: 59.6878 - val_loss: 1415.6349 - val_mae: 1416.3275\n",
      "Epoch 2856/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 61.1890 - mae: 61.8700 - val_loss: 1326.1357 - val_mae: 1326.8290\n",
      "Epoch 2857/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 68.9113 - mae: 69.5910 - val_loss: 1328.4177 - val_mae: 1329.1099\n",
      "Epoch 2858/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 66.8689 - mae: 67.5482 - val_loss: 1294.5896 - val_mae: 1295.2822\n",
      "Epoch 2859/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 61.2094 - mae: 61.8875 - val_loss: 1399.3457 - val_mae: 1400.0388\n",
      "Epoch 2860/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 71.1054 - mae: 71.7850 - val_loss: 1314.0593 - val_mae: 1314.7524\n",
      "Epoch 2861/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 63.8040 - mae: 64.4851 - val_loss: 1377.2307 - val_mae: 1377.9238\n",
      "Epoch 2862/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 58.9695 - mae: 59.6504 - val_loss: 1343.8800 - val_mae: 1344.5730\n",
      "Epoch 2863/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 65.9280 - mae: 66.6067 - val_loss: 1264.4528 - val_mae: 1265.1440\n",
      "Epoch 2864/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 62.6680 - mae: 63.3490 - val_loss: 1288.7599 - val_mae: 1289.4531\n",
      "Epoch 2865/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 65.6099 - mae: 66.2933 - val_loss: 1332.4830 - val_mae: 1333.1761\n",
      "Epoch 2866/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 66.8797 - mae: 67.5636 - val_loss: 1353.8329 - val_mae: 1354.5262\n",
      "Epoch 2867/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 62.8562 - mae: 63.5361 - val_loss: 1277.0769 - val_mae: 1277.7693\n",
      "Epoch 2868/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 73.5312 - mae: 74.2172 - val_loss: 1299.4235 - val_mae: 1300.1168\n",
      "Epoch 2869/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 58.5021 - mae: 59.1781 - val_loss: 1375.2803 - val_mae: 1375.9729\n",
      "Epoch 2870/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 68.6377 - mae: 69.3153 - val_loss: 1353.0907 - val_mae: 1353.7837\n",
      "Epoch 2871/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 60.3100 - mae: 60.9891 - val_loss: 1389.5328 - val_mae: 1390.2261\n",
      "Epoch 2872/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 62.3838 - mae: 63.0604 - val_loss: 1270.3065 - val_mae: 1270.9990\n",
      "Epoch 2873/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 58.5793 - mae: 59.2577 - val_loss: 1457.9017 - val_mae: 1458.5950\n",
      "Epoch 2874/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 63.9166 - mae: 64.5920 - val_loss: 1382.0040 - val_mae: 1382.6958\n",
      "Epoch 2875/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 69.5927 - mae: 70.2717 - val_loss: 1304.9490 - val_mae: 1305.6421\n",
      "Epoch 2876/5000\n",
      "46/46 [==============================] - 2s 46ms/step - loss: 64.9317 - mae: 65.6130 - val_loss: 1290.5366 - val_mae: 1291.2299\n",
      "Epoch 2877/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 58.3629 - mae: 59.0428 - val_loss: 1323.9531 - val_mae: 1324.6453\n",
      "Epoch 2878/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 60.2670 - mae: 60.9475 - val_loss: 1368.5259 - val_mae: 1369.2185\n",
      "Epoch 2879/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 62.4374 - mae: 63.1150 - val_loss: 1343.5884 - val_mae: 1344.2812\n",
      "Epoch 2880/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 61.1721 - mae: 61.8526 - val_loss: 1402.6807 - val_mae: 1403.3738\n",
      "Epoch 2881/5000\n",
      "46/46 [==============================] - 2s 32ms/step - loss: 68.5103 - mae: 69.1900 - val_loss: 1361.7068 - val_mae: 1362.3999\n",
      "Epoch 2882/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 71.9174 - mae: 72.5980 - val_loss: 1308.8606 - val_mae: 1309.5540\n",
      "Epoch 2883/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 83.5462 - mae: 84.2292 - val_loss: 1447.4492 - val_mae: 1448.1427\n",
      "Epoch 2884/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 65.8182 - mae: 66.4967 - val_loss: 1354.7063 - val_mae: 1355.3992\n",
      "Epoch 2885/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 58.3989 - mae: 59.0746 - val_loss: 1355.8333 - val_mae: 1356.5260\n",
      "Epoch 2886/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 62.0268 - mae: 62.7013 - val_loss: 1429.8014 - val_mae: 1430.4938\n",
      "Epoch 2887/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 74.7887 - mae: 75.4668 - val_loss: 1434.4343 - val_mae: 1435.1262\n",
      "Epoch 2888/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 65.2638 - mae: 65.9435 - val_loss: 1450.2531 - val_mae: 1450.9459\n",
      "Epoch 2889/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 65.2469 - mae: 65.9243 - val_loss: 1349.1702 - val_mae: 1349.8630\n",
      "Epoch 2890/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 64.3787 - mae: 65.0551 - val_loss: 1393.7029 - val_mae: 1394.3960\n",
      "Epoch 2891/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 62.9136 - mae: 63.5912 - val_loss: 1480.1268 - val_mae: 1480.8201\n",
      "Epoch 2892/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 72.0999 - mae: 72.7781 - val_loss: 1259.8209 - val_mae: 1260.5142\n",
      "Epoch 2893/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 60.2222 - mae: 60.9000 - val_loss: 1307.0750 - val_mae: 1307.7666\n",
      "Epoch 2894/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 63.6763 - mae: 64.3578 - val_loss: 1373.5048 - val_mae: 1374.1978\n",
      "Epoch 2895/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 60.6349 - mae: 61.3132 - val_loss: 1365.6399 - val_mae: 1366.3328\n",
      "Epoch 2896/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 70.3973 - mae: 71.0774 - val_loss: 1401.3069 - val_mae: 1402.0000\n",
      "Epoch 2897/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 67.5633 - mae: 68.2421 - val_loss: 1539.0288 - val_mae: 1539.7213\n",
      "Epoch 2898/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 64.6086 - mae: 65.2868 - val_loss: 1451.9396 - val_mae: 1452.6322\n",
      "Epoch 2899/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 64.7058 - mae: 65.3827 - val_loss: 1342.7871 - val_mae: 1343.4794\n",
      "Epoch 2900/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 62.9740 - mae: 63.6516 - val_loss: 1368.1080 - val_mae: 1368.8013\n",
      "Epoch 2901/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 57.5789 - mae: 58.2609 - val_loss: 1292.8535 - val_mae: 1293.5457\n",
      "Epoch 2902/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 59.0577 - mae: 59.7381 - val_loss: 1363.7703 - val_mae: 1364.4635\n",
      "Epoch 2903/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 68.1304 - mae: 68.8066 - val_loss: 1459.4725 - val_mae: 1460.1656\n",
      "Epoch 2904/5000\n",
      "46/46 [==============================] - 2s 44ms/step - loss: 71.2168 - mae: 71.8935 - val_loss: 1384.6257 - val_mae: 1385.3185\n",
      "Epoch 2905/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 65.4926 - mae: 66.1735 - val_loss: 1316.2354 - val_mae: 1316.9270\n",
      "Epoch 2906/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 63.3455 - mae: 64.0241 - val_loss: 1347.8613 - val_mae: 1348.5546\n",
      "Epoch 2907/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 60.3819 - mae: 61.0601 - val_loss: 1309.3580 - val_mae: 1310.0487\n",
      "Epoch 2908/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 69.6658 - mae: 70.3481 - val_loss: 1377.2195 - val_mae: 1377.9114\n",
      "Epoch 2909/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 62.5172 - mae: 63.1952 - val_loss: 1301.7104 - val_mae: 1302.4039\n",
      "Epoch 2910/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 66.4194 - mae: 67.0974 - val_loss: 1350.9154 - val_mae: 1351.6072\n",
      "Epoch 2911/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 64.6729 - mae: 65.3532 - val_loss: 1417.4066 - val_mae: 1418.0999\n",
      "Epoch 2912/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 57.7059 - mae: 58.3816 - val_loss: 1310.8555 - val_mae: 1311.5486\n",
      "Epoch 2913/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 60.6192 - mae: 61.2994 - val_loss: 1291.2388 - val_mae: 1291.9320\n",
      "Epoch 2914/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 59.9444 - mae: 60.6206 - val_loss: 1341.6652 - val_mae: 1342.3583\n",
      "Epoch 2915/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 61.4621 - mae: 62.1403 - val_loss: 1371.8905 - val_mae: 1372.5828\n",
      "Epoch 2916/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 60.9519 - mae: 61.6298 - val_loss: 1450.5605 - val_mae: 1451.2537\n",
      "Epoch 2917/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 65.1890 - mae: 65.8688 - val_loss: 1353.6327 - val_mae: 1354.3248\n",
      "Epoch 2918/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 60.3968 - mae: 61.0746 - val_loss: 1443.2351 - val_mae: 1443.9282\n",
      "Epoch 2919/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 61.5020 - mae: 62.1782 - val_loss: 1372.7028 - val_mae: 1373.3959\n",
      "Epoch 2920/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 67.8635 - mae: 68.5412 - val_loss: 1320.3666 - val_mae: 1321.0596\n",
      "Epoch 2921/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 66.3614 - mae: 67.0396 - val_loss: 1327.9000 - val_mae: 1328.5931\n",
      "Epoch 2922/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 59.3902 - mae: 60.0698 - val_loss: 1330.9861 - val_mae: 1331.6792\n",
      "Epoch 2923/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 77.9274 - mae: 78.6085 - val_loss: 1372.4503 - val_mae: 1373.1436\n",
      "Epoch 2924/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 66.9211 - mae: 67.5996 - val_loss: 1372.5037 - val_mae: 1373.1953\n",
      "Epoch 2925/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 61.3770 - mae: 62.0552 - val_loss: 1445.3439 - val_mae: 1446.0366\n",
      "Epoch 2926/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 64.1626 - mae: 64.8390 - val_loss: 1439.7526 - val_mae: 1440.4452\n",
      "Epoch 2927/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 65.7604 - mae: 66.4418 - val_loss: 1427.7518 - val_mae: 1428.4448\n",
      "Epoch 2928/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 70.7515 - mae: 71.4324 - val_loss: 1342.2877 - val_mae: 1342.9785\n",
      "Epoch 2929/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 65.7269 - mae: 66.4056 - val_loss: 1417.6554 - val_mae: 1418.3484\n",
      "Epoch 2930/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 85.6652 - mae: 86.3464 - val_loss: 1361.8613 - val_mae: 1362.5547\n",
      "Epoch 2931/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 64.4442 - mae: 65.1253 - val_loss: 1346.9034 - val_mae: 1347.5956\n",
      "Epoch 2932/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 65.9482 - mae: 66.6279 - val_loss: 1312.8729 - val_mae: 1313.5660\n",
      "Epoch 2933/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 63.3326 - mae: 64.0115 - val_loss: 1426.3574 - val_mae: 1427.0508\n",
      "Epoch 2934/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 66.3590 - mae: 67.0334 - val_loss: 1360.3060 - val_mae: 1360.9987\n",
      "Epoch 2935/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 60.4530 - mae: 61.1248 - val_loss: 1312.8859 - val_mae: 1313.5791\n",
      "Epoch 2936/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 63.7271 - mae: 64.4023 - val_loss: 1436.7323 - val_mae: 1437.4255\n",
      "Epoch 2937/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 69.1845 - mae: 69.8645 - val_loss: 1379.7988 - val_mae: 1380.4922\n",
      "Epoch 2938/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 75.1071 - mae: 75.7882 - val_loss: 1423.9042 - val_mae: 1424.5972\n",
      "Epoch 2939/5000\n",
      "17/46 [==========>...................] - ETA: 0s - loss: 67.6077 - mae: 68.2832"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mregressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tensorplustorch/lib/python3.10/site-packages/keras/wrappers/scikit_learn.py:175\u001b[0m, in \u001b[0;36mBaseWrapper.fit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/keras/wrappers/scikit_learn.py?line=171'>172</a>\u001b[0m fit_args \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilter_sk_params(Sequential\u001b[39m.\u001b[39mfit))\n\u001b[1;32m    <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/keras/wrappers/scikit_learn.py?line=172'>173</a>\u001b[0m fit_args\u001b[39m.\u001b[39mupdate(kwargs)\n\u001b[0;32m--> <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/keras/wrappers/scikit_learn.py?line=174'>175</a>\u001b[0m history \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mfit(x, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_args)\n\u001b[1;32m    <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/keras/wrappers/scikit_learn.py?line=176'>177</a>\u001b[0m \u001b[39mreturn\u001b[39;00m history\n",
      "File \u001b[0;32m~/.conda/envs/tensorplustorch/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/keras/utils/traceback_utils.py?line=62'>63</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/keras/utils/traceback_utils.py?line=63'>64</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/tensorplustorch/lib/python3.10/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/keras/engine/training.py?line=1555'>1556</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/keras/engine/training.py?line=1556'>1557</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/keras/engine/training.py?line=1557'>1558</a>\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/keras/engine/training.py?line=1560'>1561</a>\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/keras/engine/training.py?line=1561'>1562</a>\u001b[0m ):\n\u001b[1;32m   <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/keras/engine/training.py?line=1562'>1563</a>\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/keras/engine/training.py?line=1563'>1564</a>\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/keras/engine/training.py?line=1564'>1565</a>\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/keras/engine/training.py?line=1565'>1566</a>\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py?line=147'>148</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py?line=148'>149</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py?line=149'>150</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py?line=150'>151</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py?line=151'>152</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=911'>912</a>\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=913'>914</a>\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=914'>915</a>\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=916'>917</a>\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=917'>918</a>\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=943'>944</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=944'>945</a>\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=945'>946</a>\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=946'>947</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=947'>948</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=948'>949</a>\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=949'>950</a>\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=950'>951</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=2492'>2493</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=2493'>2494</a>\u001b[0m   (graph_function,\n\u001b[1;32m   <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=2494'>2495</a>\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=2495'>2496</a>\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=2496'>2497</a>\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=1857'>1858</a>\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=1858'>1859</a>\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=1859'>1860</a>\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=1860'>1861</a>\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=1861'>1862</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=1862'>1863</a>\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=1863'>1864</a>\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=1864'>1865</a>\u001b[0m     args,\n\u001b[1;32m   <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=1865'>1866</a>\u001b[0m     possible_gradient_type,\n\u001b[1;32m   <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=1866'>1867</a>\u001b[0m     executing_eagerly)\n\u001b[1;32m   <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=1867'>1868</a>\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=496'>497</a>\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=497'>498</a>\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=498'>499</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=499'>500</a>\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=500'>501</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=501'>502</a>\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=502'>503</a>\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=503'>504</a>\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=504'>505</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=505'>506</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=506'>507</a>\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=507'>508</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=510'>511</a>\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=511'>512</a>\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/execute.py?line=51'>52</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/execute.py?line=52'>53</a>\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/execute.py?line=53'>54</a>\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/execute.py?line=54'>55</a>\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/execute.py?line=55'>56</a>\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     <a href='file:///home/spectre/.conda/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/execute.py?line=56'>57</a>\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "regressor.fit(X_train,Y_train,validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2dd3a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0f090b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 127us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9966428561796283"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for check\n",
    "y_train_pred=regressor.predict(X_train)\n",
    "r2_score(Y_train, y_train_pred) #training score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e757ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred = regressor.predict(y_val)\n",
    "r2=r2_score(y_test,y_val_pred) #validation score/ r^2\n",
    "print(f'r2:{r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ef1c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2_oos\n",
    "def r2_oos(ret, pred):\n",
    "    sum_of_sq_res = np.nansum(np.power((ret-pred), 2))\n",
    "    sum_of_sq_total = np.nansum(np.power(ret, 2))\n",
    "    \n",
    "    return 1-sum_of_sq_res/sum_of_sq_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8969a6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_oos = r2_oos(y_val, y_val_pred)\n",
    "print(f'r2_oos:{r2_oos}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b87143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9293.30533975775"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae=mean_absolute_error(y_val,y_val_pred) #mae\n",
    "print(f'mae:{mae}')\n",
    "\n",
    "rmse=np.sqrt(mean_squared_error(Y_val,y_val_pred)) #rmse\n",
    "print(f'rmse:{rmse}')\n",
    "\n",
    "mape=mean_absolute_percentage_error(y_val,y_val_pred) #mape\n",
    "print(f'mape:{mape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb36caf",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5641115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546/546 [==============================] - 0s 83us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-06-01</th>\n",
       "      <td>33731.0</td>\n",
       "      <td>34394.320312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-02</th>\n",
       "      <td>33285.0</td>\n",
       "      <td>34062.855469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-03</th>\n",
       "      <td>34298.0</td>\n",
       "      <td>33482.195312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-04</th>\n",
       "      <td>35271.0</td>\n",
       "      <td>34105.074219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-05</th>\n",
       "      <td>34100.0</td>\n",
       "      <td>33992.710938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>24159.792969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>23844.636719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>23678.773438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>23188.109375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>22679.246094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>546 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             y_test        y_pred\n",
       "Date                             \n",
       "2021-06-01  33731.0  34394.320312\n",
       "2021-06-02  33285.0  34062.855469\n",
       "2021-06-03  34298.0  33482.195312\n",
       "2021-06-04  35271.0  34105.074219\n",
       "2021-06-05  34100.0  33992.710938\n",
       "...             ...           ...\n",
       "2022-11-24      NaN  24159.792969\n",
       "2022-11-25      NaN  23844.636719\n",
       "2022-11-26      NaN  23678.773438\n",
       "2022-11-27      NaN  23188.109375\n",
       "2022-11-28      NaN  22679.246094\n",
       "\n",
       "[546 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=regressor.predict(X_test)\n",
    "pre_df = pd.DataFrame(zip(Y_test,y_pred),columns=['y_test','y_pred'])\n",
    "pre_df.index = Y_test.index\n",
    "pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4bd2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df['pred_returns'] = pre_df['y_pred'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b94aa95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>pred_returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-06-01</th>\n",
       "      <td>33731.0</td>\n",
       "      <td>34394.320312</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-02</th>\n",
       "      <td>33285.0</td>\n",
       "      <td>34062.855469</td>\n",
       "      <td>-0.009637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-03</th>\n",
       "      <td>34298.0</td>\n",
       "      <td>33482.195312</td>\n",
       "      <td>-0.017047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-04</th>\n",
       "      <td>35271.0</td>\n",
       "      <td>34105.074219</td>\n",
       "      <td>0.018603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-05</th>\n",
       "      <td>34100.0</td>\n",
       "      <td>33992.710938</td>\n",
       "      <td>-0.003295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>24159.792969</td>\n",
       "      <td>-0.012123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>23844.636719</td>\n",
       "      <td>-0.013045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>23678.773438</td>\n",
       "      <td>-0.006956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>23188.109375</td>\n",
       "      <td>-0.020722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>22679.246094</td>\n",
       "      <td>-0.021945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>546 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             y_test        y_pred  pred_returns\n",
       "Date                                           \n",
       "2021-06-01  33731.0  34394.320312           NaN\n",
       "2021-06-02  33285.0  34062.855469     -0.009637\n",
       "2021-06-03  34298.0  33482.195312     -0.017047\n",
       "2021-06-04  35271.0  34105.074219      0.018603\n",
       "2021-06-05  34100.0  33992.710938     -0.003295\n",
       "...             ...           ...           ...\n",
       "2022-11-24      NaN  24159.792969     -0.012123\n",
       "2022-11-25      NaN  23844.636719     -0.013045\n",
       "2022-11-26      NaN  23678.773438     -0.006956\n",
       "2022-11-27      NaN  23188.109375     -0.020722\n",
       "2022-11-28      NaN  22679.246094     -0.021945\n",
       "\n",
       "[546 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9674c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(zip(Y_test,y_pred),columns=['y_test','y_pred']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732393ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df.to_csv(\"eth_NN.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12ff04f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cef2f18be2d5386e4fd1bc286dcc88d3ee7aeec39a7ac2ade023409b7b9d5f98"
  },
  "kernelspec": {
   "display_name": "Python 3.10.8 ('tensorplustorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
