{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5b918ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/liuxuyang/opt/anaconda3/envs/mytensorflow/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from datetime import datetime\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n",
    "from keras import losses\n",
    "from keras.constraints import maxnorm\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_validate\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from collections import Counter\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import randint\n",
    "import shap\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, accuracy_score, f1_score, classification_report\n",
    "import pickle\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from commons import mean_absolute_percentage_error\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, roc_auc_score\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import Audio\n",
    "#sound_file = 'beep.wav'\n",
    "from keras.callbacks import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936c43d9",
   "metadata": {},
   "source": [
    "### Bitcoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9701878d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "regs = pd.read_csv(\"btc_reg.csv\")\n",
    "btc = pd.read_csv(\"btc_Data.csv\")\n",
    "for i in range(len(btc['Date'])):\n",
    "    btc['Date'][i]  =  datetime.strptime(btc['Date'][i], '%Y/%m/%d')\n",
    "\n",
    "btc = btc.set_index(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1dbeb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "btcData = btc[regs.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "755494b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "btcData['returns'] = btcData['priceUSD'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec97a8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = btcData.drop(columns=['priceUSD'])\n",
    "Data = Data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64a4e819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>difficulty14mom</th>\n",
       "      <th>difficulty30mom</th>\n",
       "      <th>difficulty3rsi</th>\n",
       "      <th>difficulty90mom</th>\n",
       "      <th>difficulty90sma</th>\n",
       "      <th>hashrate90std</th>\n",
       "      <th>hashrate90var</th>\n",
       "      <th>median_transaction_fee90rocUSD</th>\n",
       "      <th>median_transaction_feeUSD</th>\n",
       "      <th>mining_profitability</th>\n",
       "      <th>...</th>\n",
       "      <th>price90wmaUSD</th>\n",
       "      <th>sentinusd90emaUSD</th>\n",
       "      <th>sentinusdUSD</th>\n",
       "      <th>top100cap</th>\n",
       "      <th>top100cap14trx</th>\n",
       "      <th>top100cap30rsi</th>\n",
       "      <th>top100cap30trx</th>\n",
       "      <th>top100cap90rsi</th>\n",
       "      <th>top100cap90trx</th>\n",
       "      <th>returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-07-18</th>\n",
       "      <td>1.621380e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.737230e+02</td>\n",
       "      <td>2.055500e+01</td>\n",
       "      <td>8.384389e+08</td>\n",
       "      <td>1.757449e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>401834.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.612000e+03</td>\n",
       "      <td>25.782</td>\n",
       "      <td>0.139</td>\n",
       "      <td>71.191</td>\n",
       "      <td>0.308</td>\n",
       "      <td>79.756</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-19</th>\n",
       "      <td>1.621380e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.737230e+02</td>\n",
       "      <td>2.248600e+01</td>\n",
       "      <td>8.819952e+08</td>\n",
       "      <td>1.944789e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>481473.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.047000e+03</td>\n",
       "      <td>25.685</td>\n",
       "      <td>0.123</td>\n",
       "      <td>68.863</td>\n",
       "      <td>0.300</td>\n",
       "      <td>78.999</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.183196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-20</th>\n",
       "      <td>1.584060e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.735570e+02</td>\n",
       "      <td>2.441400e+01</td>\n",
       "      <td>9.280542e+08</td>\n",
       "      <td>2.153212e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>431831.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.341000e+03</td>\n",
       "      <td>25.602</td>\n",
       "      <td>0.107</td>\n",
       "      <td>66.923</td>\n",
       "      <td>0.292</td>\n",
       "      <td>78.355</td>\n",
       "      <td>0.544</td>\n",
       "      <td>-0.088475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-21</th>\n",
       "      <td>1.580420e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.700790e+02</td>\n",
       "      <td>2.630400e+01</td>\n",
       "      <td>9.761949e+08</td>\n",
       "      <td>2.382391e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>460783.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.122000e+03</td>\n",
       "      <td>25.638</td>\n",
       "      <td>0.092</td>\n",
       "      <td>67.334</td>\n",
       "      <td>0.284</td>\n",
       "      <td>78.431</td>\n",
       "      <td>0.541</td>\n",
       "      <td>-0.020434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-22</th>\n",
       "      <td>1.580420e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.700790e+02</td>\n",
       "      <td>2.819400e+01</td>\n",
       "      <td>1.019474e+09</td>\n",
       "      <td>2.598318e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>334641.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.570000e+03</td>\n",
       "      <td>25.604</td>\n",
       "      <td>0.078</td>\n",
       "      <td>66.526</td>\n",
       "      <td>0.275</td>\n",
       "      <td>78.166</td>\n",
       "      <td>0.537</td>\n",
       "      <td>-0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-24</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.384949e+13</td>\n",
       "      <td>4.612156e+19</td>\n",
       "      <td>5.317997e+38</td>\n",
       "      <td>165.380</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.0577</td>\n",
       "      <td>...</td>\n",
       "      <td>18820.0</td>\n",
       "      <td>28544682257</td>\n",
       "      <td>6.114577e+09</td>\n",
       "      <td>15.236</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>40.118</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>45.140</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.008075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-25</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.394503e+13</td>\n",
       "      <td>4.542600e+19</td>\n",
       "      <td>5.158804e+38</td>\n",
       "      <td>214.464</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.0541</td>\n",
       "      <td>...</td>\n",
       "      <td>18759.0</td>\n",
       "      <td>28076113671</td>\n",
       "      <td>7.224812e+09</td>\n",
       "      <td>15.231</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>39.880</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>45.047</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.006565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-26</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.404058e+13</td>\n",
       "      <td>4.624135e+19</td>\n",
       "      <td>5.345655e+38</td>\n",
       "      <td>255.004</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.0571</td>\n",
       "      <td>...</td>\n",
       "      <td>18701.0</td>\n",
       "      <td>27566714445</td>\n",
       "      <td>4.898449e+09</td>\n",
       "      <td>15.233</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>40.069</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>45.106</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.004001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-27</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.413612e+13</td>\n",
       "      <td>4.576227e+19</td>\n",
       "      <td>5.235464e+38</td>\n",
       "      <td>113.430</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.0618</td>\n",
       "      <td>...</td>\n",
       "      <td>18642.0</td>\n",
       "      <td>27053515674</td>\n",
       "      <td>4.216170e+09</td>\n",
       "      <td>15.259</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>41.833</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>45.656</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.423166e+13</td>\n",
       "      <td>4.552126e+19</td>\n",
       "      <td>5.180463e+38</td>\n",
       "      <td>104.294</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.0592</td>\n",
       "      <td>...</td>\n",
       "      <td>18578.0</td>\n",
       "      <td>26631320968</td>\n",
       "      <td>7.843657e+09</td>\n",
       "      <td>15.219</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>39.965</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>44.957</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.019828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4517 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            difficulty14mom  difficulty30mom  difficulty3rsi  difficulty90mom  \\\n",
       "Date                                                                            \n",
       "2010-07-18     1.621380e+02     1.641620e+02          100.00     1.737230e+02   \n",
       "2010-07-19     1.621380e+02     1.641620e+02          100.00     1.737230e+02   \n",
       "2010-07-20     1.584060e+02     1.641620e+02          100.00     1.735570e+02   \n",
       "2010-07-21     1.580420e+02     1.641620e+02          100.00     1.700790e+02   \n",
       "2010-07-22     1.580420e+02     1.641620e+02          100.00     1.700790e+02   \n",
       "...                     ...              ...             ...              ...   \n",
       "2022-11-24     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-25     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-26     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-27     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-28     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "\n",
       "            difficulty90sma  hashrate90std  hashrate90var  \\\n",
       "Date                                                        \n",
       "2010-07-18     2.055500e+01   8.384389e+08   1.757449e+17   \n",
       "2010-07-19     2.248600e+01   8.819952e+08   1.944789e+17   \n",
       "2010-07-20     2.441400e+01   9.280542e+08   2.153212e+17   \n",
       "2010-07-21     2.630400e+01   9.761949e+08   2.382391e+17   \n",
       "2010-07-22     2.819400e+01   1.019474e+09   2.598318e+17   \n",
       "...                     ...            ...            ...   \n",
       "2022-11-24     3.384949e+13   4.612156e+19   5.317997e+38   \n",
       "2022-11-25     3.394503e+13   4.542600e+19   5.158804e+38   \n",
       "2022-11-26     3.404058e+13   4.624135e+19   5.345655e+38   \n",
       "2022-11-27     3.413612e+13   4.576227e+19   5.235464e+38   \n",
       "2022-11-28     3.423166e+13   4.552126e+19   5.180463e+38   \n",
       "\n",
       "            median_transaction_fee90rocUSD  median_transaction_feeUSD  \\\n",
       "Date                                                                    \n",
       "2010-07-18                           0.000                      0.000   \n",
       "2010-07-19                           0.000                      0.000   \n",
       "2010-07-20                           0.000                      0.000   \n",
       "2010-07-21                           0.000                      0.000   \n",
       "2010-07-22                           0.000                      0.000   \n",
       "...                                    ...                        ...   \n",
       "2022-11-24                         165.380                      0.818   \n",
       "2022-11-25                         214.464                      0.812   \n",
       "2022-11-26                         255.004                      0.809   \n",
       "2022-11-27                         113.430                      0.619   \n",
       "2022-11-28                         104.294                      0.622   \n",
       "\n",
       "            mining_profitability  ...  price90wmaUSD  sentinusd90emaUSD  \\\n",
       "Date                              ...                                     \n",
       "2010-07-18           401834.0000  ...            0.0                  0   \n",
       "2010-07-19           481473.0000  ...            0.0                  0   \n",
       "2010-07-20           431831.0000  ...            0.0                  0   \n",
       "2010-07-21           460783.0000  ...            0.0                  0   \n",
       "2010-07-22           334641.0000  ...            0.0                  0   \n",
       "...                          ...  ...            ...                ...   \n",
       "2022-11-24                0.0577  ...        18820.0        28544682257   \n",
       "2022-11-25                0.0541  ...        18759.0        28076113671   \n",
       "2022-11-26                0.0571  ...        18701.0        27566714445   \n",
       "2022-11-27                0.0618  ...        18642.0        27053515674   \n",
       "2022-11-28                0.0592  ...        18578.0        26631320968   \n",
       "\n",
       "            sentinusdUSD  top100cap  top100cap14trx  top100cap30rsi  \\\n",
       "Date                                                                  \n",
       "2010-07-18  2.612000e+03     25.782           0.139          71.191   \n",
       "2010-07-19  4.047000e+03     25.685           0.123          68.863   \n",
       "2010-07-20  2.341000e+03     25.602           0.107          66.923   \n",
       "2010-07-21  2.122000e+03     25.638           0.092          67.334   \n",
       "2010-07-22  2.570000e+03     25.604           0.078          66.526   \n",
       "...                  ...        ...             ...             ...   \n",
       "2022-11-24  6.114577e+09     15.236          -0.080          40.118   \n",
       "2022-11-25  7.224812e+09     15.231          -0.075          39.880   \n",
       "2022-11-26  4.898449e+09     15.233          -0.070          40.069   \n",
       "2022-11-27  4.216170e+09     15.259          -0.065          41.833   \n",
       "2022-11-28  7.843657e+09     15.219          -0.061          39.965   \n",
       "\n",
       "            top100cap30trx  top100cap90rsi  top100cap90trx   returns  \n",
       "Date                                                                  \n",
       "2010-07-18           0.308          79.756           0.550  0.466667  \n",
       "2010-07-19           0.300          78.999           0.547  0.183196  \n",
       "2010-07-20           0.292          78.355           0.544 -0.088475  \n",
       "2010-07-21           0.284          78.431           0.541 -0.020434  \n",
       "2010-07-22           0.275          78.166           0.537 -0.153846  \n",
       "...                    ...             ...             ...       ...  \n",
       "2022-11-24          -0.042          45.140           0.001  0.008075  \n",
       "2022-11-25          -0.043          45.047           0.001 -0.006565  \n",
       "2022-11-26          -0.043          45.106           0.001  0.004001  \n",
       "2022-11-27          -0.044          45.656           0.000 -0.001147  \n",
       "2022-11-28          -0.044          44.957           0.000 -0.019828  \n",
       "\n",
       "[4517 rows x 32 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfcc2185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>difficulty14mom</th>\n",
       "      <th>difficulty30mom</th>\n",
       "      <th>difficulty3rsi</th>\n",
       "      <th>difficulty90mom</th>\n",
       "      <th>difficulty90sma</th>\n",
       "      <th>hashrate90std</th>\n",
       "      <th>hashrate90var</th>\n",
       "      <th>median_transaction_fee90rocUSD</th>\n",
       "      <th>median_transaction_feeUSD</th>\n",
       "      <th>mining_profitability</th>\n",
       "      <th>...</th>\n",
       "      <th>price90wmaUSD</th>\n",
       "      <th>sentinusd90emaUSD</th>\n",
       "      <th>sentinusdUSD</th>\n",
       "      <th>top100cap</th>\n",
       "      <th>top100cap14trx</th>\n",
       "      <th>top100cap30rsi</th>\n",
       "      <th>top100cap30trx</th>\n",
       "      <th>top100cap90rsi</th>\n",
       "      <th>top100cap90trx</th>\n",
       "      <th>returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-07-18</th>\n",
       "      <td>1.621380e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.737230e+02</td>\n",
       "      <td>2.055500e+01</td>\n",
       "      <td>8.384389e+08</td>\n",
       "      <td>1.757449e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>401834.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.612000e+03</td>\n",
       "      <td>25.782</td>\n",
       "      <td>0.139</td>\n",
       "      <td>71.191</td>\n",
       "      <td>0.308</td>\n",
       "      <td>79.756</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-19</th>\n",
       "      <td>1.621380e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.737230e+02</td>\n",
       "      <td>2.248600e+01</td>\n",
       "      <td>8.819952e+08</td>\n",
       "      <td>1.944789e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>481473.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.047000e+03</td>\n",
       "      <td>25.685</td>\n",
       "      <td>0.123</td>\n",
       "      <td>68.863</td>\n",
       "      <td>0.300</td>\n",
       "      <td>78.999</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.183196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-20</th>\n",
       "      <td>1.584060e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.735570e+02</td>\n",
       "      <td>2.441400e+01</td>\n",
       "      <td>9.280542e+08</td>\n",
       "      <td>2.153212e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>431831.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.341000e+03</td>\n",
       "      <td>25.602</td>\n",
       "      <td>0.107</td>\n",
       "      <td>66.923</td>\n",
       "      <td>0.292</td>\n",
       "      <td>78.355</td>\n",
       "      <td>0.544</td>\n",
       "      <td>-0.088475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-21</th>\n",
       "      <td>1.580420e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.700790e+02</td>\n",
       "      <td>2.630400e+01</td>\n",
       "      <td>9.761949e+08</td>\n",
       "      <td>2.382391e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>460783.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.122000e+03</td>\n",
       "      <td>25.638</td>\n",
       "      <td>0.092</td>\n",
       "      <td>67.334</td>\n",
       "      <td>0.284</td>\n",
       "      <td>78.431</td>\n",
       "      <td>0.541</td>\n",
       "      <td>-0.020434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-22</th>\n",
       "      <td>1.580420e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.700790e+02</td>\n",
       "      <td>2.819400e+01</td>\n",
       "      <td>1.019474e+09</td>\n",
       "      <td>2.598318e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>334641.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.570000e+03</td>\n",
       "      <td>25.604</td>\n",
       "      <td>0.078</td>\n",
       "      <td>66.526</td>\n",
       "      <td>0.275</td>\n",
       "      <td>78.166</td>\n",
       "      <td>0.537</td>\n",
       "      <td>-0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-24</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.384949e+13</td>\n",
       "      <td>4.612156e+19</td>\n",
       "      <td>5.317997e+38</td>\n",
       "      <td>165.380</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.0577</td>\n",
       "      <td>...</td>\n",
       "      <td>18820.0</td>\n",
       "      <td>28544682257</td>\n",
       "      <td>6.114577e+09</td>\n",
       "      <td>15.236</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>40.118</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>45.140</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.008075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-25</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.394503e+13</td>\n",
       "      <td>4.542600e+19</td>\n",
       "      <td>5.158804e+38</td>\n",
       "      <td>214.464</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.0541</td>\n",
       "      <td>...</td>\n",
       "      <td>18759.0</td>\n",
       "      <td>28076113671</td>\n",
       "      <td>7.224812e+09</td>\n",
       "      <td>15.231</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>39.880</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>45.047</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.006565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-26</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.404058e+13</td>\n",
       "      <td>4.624135e+19</td>\n",
       "      <td>5.345655e+38</td>\n",
       "      <td>255.004</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.0571</td>\n",
       "      <td>...</td>\n",
       "      <td>18701.0</td>\n",
       "      <td>27566714445</td>\n",
       "      <td>4.898449e+09</td>\n",
       "      <td>15.233</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>40.069</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>45.106</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.004001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-27</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.413612e+13</td>\n",
       "      <td>4.576227e+19</td>\n",
       "      <td>5.235464e+38</td>\n",
       "      <td>113.430</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.0618</td>\n",
       "      <td>...</td>\n",
       "      <td>18642.0</td>\n",
       "      <td>27053515674</td>\n",
       "      <td>4.216170e+09</td>\n",
       "      <td>15.259</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>41.833</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>45.656</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.423166e+13</td>\n",
       "      <td>4.552126e+19</td>\n",
       "      <td>5.180463e+38</td>\n",
       "      <td>104.294</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.0592</td>\n",
       "      <td>...</td>\n",
       "      <td>18578.0</td>\n",
       "      <td>26631320968</td>\n",
       "      <td>7.843657e+09</td>\n",
       "      <td>15.219</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>39.965</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>44.957</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.019828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4517 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            difficulty14mom  difficulty30mom  difficulty3rsi  difficulty90mom  \\\n",
       "Date                                                                            \n",
       "2010-07-18     1.621380e+02     1.641620e+02          100.00     1.737230e+02   \n",
       "2010-07-19     1.621380e+02     1.641620e+02          100.00     1.737230e+02   \n",
       "2010-07-20     1.584060e+02     1.641620e+02          100.00     1.735570e+02   \n",
       "2010-07-21     1.580420e+02     1.641620e+02          100.00     1.700790e+02   \n",
       "2010-07-22     1.580420e+02     1.641620e+02          100.00     1.700790e+02   \n",
       "...                     ...              ...             ...              ...   \n",
       "2022-11-24     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-25     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-26     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-27     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-28     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "\n",
       "            difficulty90sma  hashrate90std  hashrate90var  \\\n",
       "Date                                                        \n",
       "2010-07-18     2.055500e+01   8.384389e+08   1.757449e+17   \n",
       "2010-07-19     2.248600e+01   8.819952e+08   1.944789e+17   \n",
       "2010-07-20     2.441400e+01   9.280542e+08   2.153212e+17   \n",
       "2010-07-21     2.630400e+01   9.761949e+08   2.382391e+17   \n",
       "2010-07-22     2.819400e+01   1.019474e+09   2.598318e+17   \n",
       "...                     ...            ...            ...   \n",
       "2022-11-24     3.384949e+13   4.612156e+19   5.317997e+38   \n",
       "2022-11-25     3.394503e+13   4.542600e+19   5.158804e+38   \n",
       "2022-11-26     3.404058e+13   4.624135e+19   5.345655e+38   \n",
       "2022-11-27     3.413612e+13   4.576227e+19   5.235464e+38   \n",
       "2022-11-28     3.423166e+13   4.552126e+19   5.180463e+38   \n",
       "\n",
       "            median_transaction_fee90rocUSD  median_transaction_feeUSD  \\\n",
       "Date                                                                    \n",
       "2010-07-18                           0.000                      0.000   \n",
       "2010-07-19                           0.000                      0.000   \n",
       "2010-07-20                           0.000                      0.000   \n",
       "2010-07-21                           0.000                      0.000   \n",
       "2010-07-22                           0.000                      0.000   \n",
       "...                                    ...                        ...   \n",
       "2022-11-24                         165.380                      0.818   \n",
       "2022-11-25                         214.464                      0.812   \n",
       "2022-11-26                         255.004                      0.809   \n",
       "2022-11-27                         113.430                      0.619   \n",
       "2022-11-28                         104.294                      0.622   \n",
       "\n",
       "            mining_profitability  ...  price90wmaUSD  sentinusd90emaUSD  \\\n",
       "Date                              ...                                     \n",
       "2010-07-18           401834.0000  ...            0.0                  0   \n",
       "2010-07-19           481473.0000  ...            0.0                  0   \n",
       "2010-07-20           431831.0000  ...            0.0                  0   \n",
       "2010-07-21           460783.0000  ...            0.0                  0   \n",
       "2010-07-22           334641.0000  ...            0.0                  0   \n",
       "...                          ...  ...            ...                ...   \n",
       "2022-11-24                0.0577  ...        18820.0        28544682257   \n",
       "2022-11-25                0.0541  ...        18759.0        28076113671   \n",
       "2022-11-26                0.0571  ...        18701.0        27566714445   \n",
       "2022-11-27                0.0618  ...        18642.0        27053515674   \n",
       "2022-11-28                0.0592  ...        18578.0        26631320968   \n",
       "\n",
       "            sentinusdUSD  top100cap  top100cap14trx  top100cap30rsi  \\\n",
       "Date                                                                  \n",
       "2010-07-18  2.612000e+03     25.782           0.139          71.191   \n",
       "2010-07-19  4.047000e+03     25.685           0.123          68.863   \n",
       "2010-07-20  2.341000e+03     25.602           0.107          66.923   \n",
       "2010-07-21  2.122000e+03     25.638           0.092          67.334   \n",
       "2010-07-22  2.570000e+03     25.604           0.078          66.526   \n",
       "...                  ...        ...             ...             ...   \n",
       "2022-11-24  6.114577e+09     15.236          -0.080          40.118   \n",
       "2022-11-25  7.224812e+09     15.231          -0.075          39.880   \n",
       "2022-11-26  4.898449e+09     15.233          -0.070          40.069   \n",
       "2022-11-27  4.216170e+09     15.259          -0.065          41.833   \n",
       "2022-11-28  7.843657e+09     15.219          -0.061          39.965   \n",
       "\n",
       "            top100cap30trx  top100cap90rsi  top100cap90trx   returns  \n",
       "Date                                                                  \n",
       "2010-07-18           0.308          79.756           0.550  0.466667  \n",
       "2010-07-19           0.300          78.999           0.547  0.183196  \n",
       "2010-07-20           0.292          78.355           0.544 -0.088475  \n",
       "2010-07-21           0.284          78.431           0.541 -0.020434  \n",
       "2010-07-22           0.275          78.166           0.537 -0.153846  \n",
       "...                    ...             ...             ...       ...  \n",
       "2022-11-24          -0.042          45.140           0.001  0.008075  \n",
       "2022-11-25          -0.043          45.047           0.001 -0.006565  \n",
       "2022-11-26          -0.043          45.106           0.001  0.004001  \n",
       "2022-11-27          -0.044          45.656           0.000 -0.001147  \n",
       "2022-11-28          -0.044          44.957           0.000 -0.019828  \n",
       "\n",
       "[4517 rows x 32 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "620e34fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>difficulty14mom</th>\n",
       "      <th>difficulty30mom</th>\n",
       "      <th>difficulty3rsi</th>\n",
       "      <th>difficulty90mom</th>\n",
       "      <th>difficulty90sma</th>\n",
       "      <th>hashrate90std</th>\n",
       "      <th>hashrate90var</th>\n",
       "      <th>median_transaction_fee90rocUSD</th>\n",
       "      <th>median_transaction_feeUSD</th>\n",
       "      <th>mining_profitability</th>\n",
       "      <th>...</th>\n",
       "      <th>price90wmaUSD</th>\n",
       "      <th>sentinusd90emaUSD</th>\n",
       "      <th>sentinusdUSD</th>\n",
       "      <th>top100cap</th>\n",
       "      <th>top100cap14trx</th>\n",
       "      <th>top100cap30rsi</th>\n",
       "      <th>top100cap30trx</th>\n",
       "      <th>top100cap90rsi</th>\n",
       "      <th>top100cap90trx</th>\n",
       "      <th>returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-07-18</th>\n",
       "      <td>1.621380e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.737230e+02</td>\n",
       "      <td>2.055500e+01</td>\n",
       "      <td>8.384389e+08</td>\n",
       "      <td>1.757449e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>401834.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.612000e+03</td>\n",
       "      <td>25.782</td>\n",
       "      <td>0.139</td>\n",
       "      <td>71.191</td>\n",
       "      <td>0.308</td>\n",
       "      <td>79.756</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-19</th>\n",
       "      <td>1.621380e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.737230e+02</td>\n",
       "      <td>2.248600e+01</td>\n",
       "      <td>8.819952e+08</td>\n",
       "      <td>1.944789e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>481473.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.047000e+03</td>\n",
       "      <td>25.685</td>\n",
       "      <td>0.123</td>\n",
       "      <td>68.863</td>\n",
       "      <td>0.300</td>\n",
       "      <td>78.999</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.183196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-20</th>\n",
       "      <td>1.584060e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.735570e+02</td>\n",
       "      <td>2.441400e+01</td>\n",
       "      <td>9.280542e+08</td>\n",
       "      <td>2.153212e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>431831.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.341000e+03</td>\n",
       "      <td>25.602</td>\n",
       "      <td>0.107</td>\n",
       "      <td>66.923</td>\n",
       "      <td>0.292</td>\n",
       "      <td>78.355</td>\n",
       "      <td>0.544</td>\n",
       "      <td>-0.088475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-21</th>\n",
       "      <td>1.580420e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.700790e+02</td>\n",
       "      <td>2.630400e+01</td>\n",
       "      <td>9.761949e+08</td>\n",
       "      <td>2.382391e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>460783.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.122000e+03</td>\n",
       "      <td>25.638</td>\n",
       "      <td>0.092</td>\n",
       "      <td>67.334</td>\n",
       "      <td>0.284</td>\n",
       "      <td>78.431</td>\n",
       "      <td>0.541</td>\n",
       "      <td>-0.020434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-22</th>\n",
       "      <td>1.580420e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.700790e+02</td>\n",
       "      <td>2.819400e+01</td>\n",
       "      <td>1.019474e+09</td>\n",
       "      <td>2.598318e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>334641.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.570000e+03</td>\n",
       "      <td>25.604</td>\n",
       "      <td>0.078</td>\n",
       "      <td>66.526</td>\n",
       "      <td>0.275</td>\n",
       "      <td>78.166</td>\n",
       "      <td>0.537</td>\n",
       "      <td>-0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-24</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.384949e+13</td>\n",
       "      <td>4.612156e+19</td>\n",
       "      <td>5.317997e+38</td>\n",
       "      <td>165.380</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.0577</td>\n",
       "      <td>...</td>\n",
       "      <td>18820.0</td>\n",
       "      <td>28544682257</td>\n",
       "      <td>6.114577e+09</td>\n",
       "      <td>15.236</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>40.118</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>45.140</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.008075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-25</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.394503e+13</td>\n",
       "      <td>4.542600e+19</td>\n",
       "      <td>5.158804e+38</td>\n",
       "      <td>214.464</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.0541</td>\n",
       "      <td>...</td>\n",
       "      <td>18759.0</td>\n",
       "      <td>28076113671</td>\n",
       "      <td>7.224812e+09</td>\n",
       "      <td>15.231</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>39.880</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>45.047</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.006565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-26</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.404058e+13</td>\n",
       "      <td>4.624135e+19</td>\n",
       "      <td>5.345655e+38</td>\n",
       "      <td>255.004</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.0571</td>\n",
       "      <td>...</td>\n",
       "      <td>18701.0</td>\n",
       "      <td>27566714445</td>\n",
       "      <td>4.898449e+09</td>\n",
       "      <td>15.233</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>40.069</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>45.106</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.004001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-27</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.413612e+13</td>\n",
       "      <td>4.576227e+19</td>\n",
       "      <td>5.235464e+38</td>\n",
       "      <td>113.430</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.0618</td>\n",
       "      <td>...</td>\n",
       "      <td>18642.0</td>\n",
       "      <td>27053515674</td>\n",
       "      <td>4.216170e+09</td>\n",
       "      <td>15.259</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>41.833</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>45.656</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.423166e+13</td>\n",
       "      <td>4.552126e+19</td>\n",
       "      <td>5.180463e+38</td>\n",
       "      <td>104.294</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.0592</td>\n",
       "      <td>...</td>\n",
       "      <td>18578.0</td>\n",
       "      <td>26631320968</td>\n",
       "      <td>7.843657e+09</td>\n",
       "      <td>15.219</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>39.965</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>44.957</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.019828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4517 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            difficulty14mom  difficulty30mom  difficulty3rsi  difficulty90mom  \\\n",
       "Date                                                                            \n",
       "2010-07-18     1.621380e+02     1.641620e+02          100.00     1.737230e+02   \n",
       "2010-07-19     1.621380e+02     1.641620e+02          100.00     1.737230e+02   \n",
       "2010-07-20     1.584060e+02     1.641620e+02          100.00     1.735570e+02   \n",
       "2010-07-21     1.580420e+02     1.641620e+02          100.00     1.700790e+02   \n",
       "2010-07-22     1.580420e+02     1.641620e+02          100.00     1.700790e+02   \n",
       "...                     ...              ...             ...              ...   \n",
       "2022-11-24     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-25     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-26     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-27     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-28     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "\n",
       "            difficulty90sma  hashrate90std  hashrate90var  \\\n",
       "Date                                                        \n",
       "2010-07-18     2.055500e+01   8.384389e+08   1.757449e+17   \n",
       "2010-07-19     2.248600e+01   8.819952e+08   1.944789e+17   \n",
       "2010-07-20     2.441400e+01   9.280542e+08   2.153212e+17   \n",
       "2010-07-21     2.630400e+01   9.761949e+08   2.382391e+17   \n",
       "2010-07-22     2.819400e+01   1.019474e+09   2.598318e+17   \n",
       "...                     ...            ...            ...   \n",
       "2022-11-24     3.384949e+13   4.612156e+19   5.317997e+38   \n",
       "2022-11-25     3.394503e+13   4.542600e+19   5.158804e+38   \n",
       "2022-11-26     3.404058e+13   4.624135e+19   5.345655e+38   \n",
       "2022-11-27     3.413612e+13   4.576227e+19   5.235464e+38   \n",
       "2022-11-28     3.423166e+13   4.552126e+19   5.180463e+38   \n",
       "\n",
       "            median_transaction_fee90rocUSD  median_transaction_feeUSD  \\\n",
       "Date                                                                    \n",
       "2010-07-18                           0.000                      0.000   \n",
       "2010-07-19                           0.000                      0.000   \n",
       "2010-07-20                           0.000                      0.000   \n",
       "2010-07-21                           0.000                      0.000   \n",
       "2010-07-22                           0.000                      0.000   \n",
       "...                                    ...                        ...   \n",
       "2022-11-24                         165.380                      0.818   \n",
       "2022-11-25                         214.464                      0.812   \n",
       "2022-11-26                         255.004                      0.809   \n",
       "2022-11-27                         113.430                      0.619   \n",
       "2022-11-28                         104.294                      0.622   \n",
       "\n",
       "            mining_profitability  ...  price90wmaUSD  sentinusd90emaUSD  \\\n",
       "Date                              ...                                     \n",
       "2010-07-18           401834.0000  ...            0.0                  0   \n",
       "2010-07-19           481473.0000  ...            0.0                  0   \n",
       "2010-07-20           431831.0000  ...            0.0                  0   \n",
       "2010-07-21           460783.0000  ...            0.0                  0   \n",
       "2010-07-22           334641.0000  ...            0.0                  0   \n",
       "...                          ...  ...            ...                ...   \n",
       "2022-11-24                0.0577  ...        18820.0        28544682257   \n",
       "2022-11-25                0.0541  ...        18759.0        28076113671   \n",
       "2022-11-26                0.0571  ...        18701.0        27566714445   \n",
       "2022-11-27                0.0618  ...        18642.0        27053515674   \n",
       "2022-11-28                0.0592  ...        18578.0        26631320968   \n",
       "\n",
       "            sentinusdUSD  top100cap  top100cap14trx  top100cap30rsi  \\\n",
       "Date                                                                  \n",
       "2010-07-18  2.612000e+03     25.782           0.139          71.191   \n",
       "2010-07-19  4.047000e+03     25.685           0.123          68.863   \n",
       "2010-07-20  2.341000e+03     25.602           0.107          66.923   \n",
       "2010-07-21  2.122000e+03     25.638           0.092          67.334   \n",
       "2010-07-22  2.570000e+03     25.604           0.078          66.526   \n",
       "...                  ...        ...             ...             ...   \n",
       "2022-11-24  6.114577e+09     15.236          -0.080          40.118   \n",
       "2022-11-25  7.224812e+09     15.231          -0.075          39.880   \n",
       "2022-11-26  4.898449e+09     15.233          -0.070          40.069   \n",
       "2022-11-27  4.216170e+09     15.259          -0.065          41.833   \n",
       "2022-11-28  7.843657e+09     15.219          -0.061          39.965   \n",
       "\n",
       "            top100cap30trx  top100cap90rsi  top100cap90trx   returns  \n",
       "Date                                                                  \n",
       "2010-07-18           0.308          79.756           0.550  0.466667  \n",
       "2010-07-19           0.300          78.999           0.547  0.183196  \n",
       "2010-07-20           0.292          78.355           0.544 -0.088475  \n",
       "2010-07-21           0.284          78.431           0.541 -0.020434  \n",
       "2010-07-22           0.275          78.166           0.537 -0.153846  \n",
       "...                    ...             ...             ...       ...  \n",
       "2022-11-24          -0.042          45.140           0.001  0.008075  \n",
       "2022-11-25          -0.043          45.047           0.001 -0.006565  \n",
       "2022-11-26          -0.043          45.106           0.001  0.004001  \n",
       "2022-11-27          -0.044          45.656           0.000 -0.001147  \n",
       "2022-11-28          -0.044          44.957           0.000 -0.019828  \n",
       "\n",
       "[4517 rows x 32 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b1c6545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide X and Y\n",
    "X = Data.iloc[:,0:]\n",
    "#Y = Data['returns']   # 用returns的话就用这一行，然后把下一行comment掉\n",
    "Y = btcData['priceUSD'][1:] # 反之亦然"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cf7e632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>difficulty14mom</th>\n",
       "      <th>difficulty30mom</th>\n",
       "      <th>difficulty3rsi</th>\n",
       "      <th>difficulty90mom</th>\n",
       "      <th>difficulty90sma</th>\n",
       "      <th>hashrate90std</th>\n",
       "      <th>hashrate90var</th>\n",
       "      <th>median_transaction_fee90rocUSD</th>\n",
       "      <th>median_transaction_feeUSD</th>\n",
       "      <th>mining_profitability</th>\n",
       "      <th>...</th>\n",
       "      <th>price90wmaUSD</th>\n",
       "      <th>sentinusd90emaUSD</th>\n",
       "      <th>sentinusdUSD</th>\n",
       "      <th>top100cap</th>\n",
       "      <th>top100cap14trx</th>\n",
       "      <th>top100cap30rsi</th>\n",
       "      <th>top100cap30trx</th>\n",
       "      <th>top100cap90rsi</th>\n",
       "      <th>top100cap90trx</th>\n",
       "      <th>returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-07-18</th>\n",
       "      <td>162.138</td>\n",
       "      <td>164.162</td>\n",
       "      <td>100.0</td>\n",
       "      <td>173.723</td>\n",
       "      <td>20.555</td>\n",
       "      <td>838438881.0</td>\n",
       "      <td>1.757449e+17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>401834.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2612.0</td>\n",
       "      <td>25.782</td>\n",
       "      <td>0.139</td>\n",
       "      <td>71.191</td>\n",
       "      <td>0.308</td>\n",
       "      <td>79.756</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-19</th>\n",
       "      <td>162.138</td>\n",
       "      <td>164.162</td>\n",
       "      <td>100.0</td>\n",
       "      <td>173.723</td>\n",
       "      <td>22.486</td>\n",
       "      <td>881995244.0</td>\n",
       "      <td>1.944789e+17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>481473.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4047.0</td>\n",
       "      <td>25.685</td>\n",
       "      <td>0.123</td>\n",
       "      <td>68.863</td>\n",
       "      <td>0.300</td>\n",
       "      <td>78.999</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.183196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-20</th>\n",
       "      <td>158.406</td>\n",
       "      <td>164.162</td>\n",
       "      <td>100.0</td>\n",
       "      <td>173.557</td>\n",
       "      <td>24.414</td>\n",
       "      <td>928054231.0</td>\n",
       "      <td>2.153212e+17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>431831.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2341.0</td>\n",
       "      <td>25.602</td>\n",
       "      <td>0.107</td>\n",
       "      <td>66.923</td>\n",
       "      <td>0.292</td>\n",
       "      <td>78.355</td>\n",
       "      <td>0.544</td>\n",
       "      <td>-0.088475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            difficulty14mom  difficulty30mom  difficulty3rsi  difficulty90mom  \\\n",
       "Date                                                                            \n",
       "2010-07-18          162.138          164.162           100.0          173.723   \n",
       "2010-07-19          162.138          164.162           100.0          173.723   \n",
       "2010-07-20          158.406          164.162           100.0          173.557   \n",
       "\n",
       "            difficulty90sma  hashrate90std  hashrate90var  \\\n",
       "Date                                                        \n",
       "2010-07-18           20.555    838438881.0   1.757449e+17   \n",
       "2010-07-19           22.486    881995244.0   1.944789e+17   \n",
       "2010-07-20           24.414    928054231.0   2.153212e+17   \n",
       "\n",
       "            median_transaction_fee90rocUSD  median_transaction_feeUSD  \\\n",
       "Date                                                                    \n",
       "2010-07-18                             0.0                        0.0   \n",
       "2010-07-19                             0.0                        0.0   \n",
       "2010-07-20                             0.0                        0.0   \n",
       "\n",
       "            mining_profitability  ...  price90wmaUSD  sentinusd90emaUSD  \\\n",
       "Date                              ...                                     \n",
       "2010-07-18              401834.0  ...            0.0                  0   \n",
       "2010-07-19              481473.0  ...            0.0                  0   \n",
       "2010-07-20              431831.0  ...            0.0                  0   \n",
       "\n",
       "            sentinusdUSD  top100cap  top100cap14trx  top100cap30rsi  \\\n",
       "Date                                                                  \n",
       "2010-07-18        2612.0     25.782           0.139          71.191   \n",
       "2010-07-19        4047.0     25.685           0.123          68.863   \n",
       "2010-07-20        2341.0     25.602           0.107          66.923   \n",
       "\n",
       "            top100cap30trx  top100cap90rsi  top100cap90trx   returns  \n",
       "Date                                                                  \n",
       "2010-07-18           0.308          79.756           0.550  0.466667  \n",
       "2010-07-19           0.300          78.999           0.547  0.183196  \n",
       "2010-07-20           0.292          78.355           0.544 -0.088475  \n",
       "\n",
       "[3 rows x 32 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3f91933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into three data sets\n",
    "X_train = X['2016-01-01':'2019-12-31']\n",
    "X_val = X['2020-01-01':'2021-05-31']\n",
    "X_test = X['2021-06-01':'2023-01-01']\n",
    "\n",
    "Y_train = Y['2016-01-01':'2019-12-31']\n",
    "Y_val = Y['2020-01-01':'2021-05-31']\n",
    "Y_test = Y['2021-06-01':'2023-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67ec6582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............ (step 1 of 2) Processing mixmax, total=   0.0s\n",
      "[Pipeline] ............ (step 2 of 2) Processing robust, total=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('mixmax', MinMaxScaler()), ['robust', RobustScaler()]],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators=[]\n",
    "estimators.append(['mixmax',MinMaxScaler()])\n",
    "estimators.append(['robust',RobustScaler()])\n",
    "scale=Pipeline(estimators,verbose=True)\n",
    "scale.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c88aa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=scale.transform(X_train)\n",
    "X_test=scale.transform(X_test)\n",
    "X_val = scale.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8fa51da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02287341",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "# define neural network model\n",
    "shape=X.shape[1]\n",
    "def sequential_model(initializer='normal', activation='relu', neurons=300, NUM_FEATURES=shape):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_shape=(NUM_FEATURES,), kernel_initializer=initializer, activation=activation))\n",
    "    model.add(Dense(neurons, activation=activation))\n",
    "    model.add(Dense(neurons, activation=activation))\n",
    "    model.add(Dense(1, activation=activation))\n",
    "    # Compile model\n",
    "    adam=keras.optimizers.adam(lr=lr_schedule(0), amsgrad=True)\n",
    "    #sgd=keras.optimizers.SGD(learning_rate=0.08, momentum=0.9, nesterov=False)\n",
    "    model.compile(loss='logcosh', optimizer=adam, metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "970b5237",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mcp_save = ModelCheckpoint('trained_models/ANN_reg_seven_new.hdf5', save_best_only=True, monitor='val_loss', mode='auto')\n",
    "#earlyStopping = EarlyStopping(monitor='val_loss', patience=100,verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c03d28b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor=KerasRegressor(build_fn=sequential_model,epochs=1000,verbose=1, shuffle=True,use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdf7e872",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 18:31:13.493144: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-30 18:31:13.495664: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Train on 1461 samples, validate on 517 samples\n",
      "Epoch 1/1000\n",
      "1461/1461 [==============================] - 1s 769us/step - loss: 4435.6594 - mae: 4436.3521 - val_loss: 17741.4674 - val_mae: 17742.1602\n",
      "Epoch 2/1000\n",
      "1461/1461 [==============================] - 0s 293us/step - loss: 2509.5925 - mae: 2510.2861 - val_loss: 3962.5571 - val_mae: 3963.2500\n",
      "Epoch 3/1000\n",
      "1461/1461 [==============================] - 0s 281us/step - loss: 1025.6292 - mae: 1026.3217 - val_loss: 3288.3453 - val_mae: 3289.0386\n",
      "Epoch 4/1000\n",
      "1461/1461 [==============================] - 0s 291us/step - loss: 803.9454 - mae: 804.6374 - val_loss: 2546.8519 - val_mae: 2547.5452\n",
      "Epoch 5/1000\n",
      "1461/1461 [==============================] - 0s 303us/step - loss: 651.2163 - mae: 651.9087 - val_loss: 2213.3741 - val_mae: 2214.0667\n",
      "Epoch 6/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 551.6183 - mae: 552.3109 - val_loss: 1764.6005 - val_mae: 1765.2937\n",
      "Epoch 7/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 464.7897 - mae: 465.4817 - val_loss: 2337.6756 - val_mae: 2338.3687\n",
      "Epoch 8/1000\n",
      "1461/1461 [==============================] - 0s 291us/step - loss: 488.0146 - mae: 488.7074 - val_loss: 1449.9075 - val_mae: 1450.5995\n",
      "Epoch 9/1000\n",
      "1461/1461 [==============================] - 0s 289us/step - loss: 379.6305 - mae: 380.3226 - val_loss: 2288.0295 - val_mae: 2288.7224\n",
      "Epoch 10/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 336.8334 - mae: 337.5256 - val_loss: 1528.8941 - val_mae: 1529.5867\n",
      "Epoch 11/1000\n",
      "1461/1461 [==============================] - 0s 283us/step - loss: 307.4300 - mae: 308.1220 - val_loss: 1658.4363 - val_mae: 1659.1296\n",
      "Epoch 12/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 293.2779 - mae: 293.9688 - val_loss: 1620.2622 - val_mae: 1620.9553\n",
      "Epoch 13/1000\n",
      "1461/1461 [==============================] - 0s 281us/step - loss: 257.7935 - mae: 258.4853 - val_loss: 1565.5668 - val_mae: 1566.2598\n",
      "Epoch 14/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 239.8867 - mae: 240.5765 - val_loss: 1456.6449 - val_mae: 1457.3383\n",
      "Epoch 15/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 239.7188 - mae: 240.4100 - val_loss: 1479.3790 - val_mae: 1480.0723\n",
      "Epoch 16/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 228.4817 - mae: 229.1722 - val_loss: 2044.2369 - val_mae: 2044.9302\n",
      "Epoch 17/1000\n",
      "1461/1461 [==============================] - 0s 281us/step - loss: 211.4841 - mae: 212.1747 - val_loss: 1932.1317 - val_mae: 1932.8247\n",
      "Epoch 18/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 184.9726 - mae: 185.6634 - val_loss: 1901.1164 - val_mae: 1901.8093\n",
      "Epoch 19/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 196.4719 - mae: 197.1640 - val_loss: 1547.3584 - val_mae: 1548.0507\n",
      "Epoch 20/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 202.1197 - mae: 202.8086 - val_loss: 1968.2266 - val_mae: 1968.9198\n",
      "Epoch 21/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 159.9432 - mae: 160.6327 - val_loss: 1436.8531 - val_mae: 1437.5461\n",
      "Epoch 22/1000\n",
      "1461/1461 [==============================] - 0s 294us/step - loss: 164.1752 - mae: 164.8669 - val_loss: 1730.4907 - val_mae: 1731.1838\n",
      "Epoch 23/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 175.8829 - mae: 176.5727 - val_loss: 2224.2047 - val_mae: 2224.8979\n",
      "Epoch 24/1000\n",
      "1461/1461 [==============================] - 0s 288us/step - loss: 155.2709 - mae: 155.9606 - val_loss: 1694.1164 - val_mae: 1694.8091\n",
      "Epoch 25/1000\n",
      "1461/1461 [==============================] - 0s 295us/step - loss: 143.7960 - mae: 144.4855 - val_loss: 1760.1293 - val_mae: 1760.8225\n",
      "Epoch 26/1000\n",
      "1461/1461 [==============================] - 0s 292us/step - loss: 177.7468 - mae: 178.4366 - val_loss: 2006.4270 - val_mae: 2007.1204\n",
      "Epoch 27/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 149.3243 - mae: 150.0152 - val_loss: 1557.8673 - val_mae: 1558.5598\n",
      "Epoch 28/1000\n",
      "1461/1461 [==============================] - 0s 325us/step - loss: 140.8282 - mae: 141.5189 - val_loss: 1621.1507 - val_mae: 1621.8439\n",
      "Epoch 29/1000\n",
      "1461/1461 [==============================] - 0s 287us/step - loss: 141.6161 - mae: 142.3056 - val_loss: 1407.3207 - val_mae: 1408.0137\n",
      "Epoch 30/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 137.4418 - mae: 138.1305 - val_loss: 2150.2685 - val_mae: 2150.9619\n",
      "Epoch 31/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 173.8657 - mae: 174.5556 - val_loss: 2089.4760 - val_mae: 2090.1694\n",
      "Epoch 32/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 123.7306 - mae: 124.4207 - val_loss: 1974.0359 - val_mae: 1974.7291\n",
      "Epoch 33/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 149.6953 - mae: 150.3849 - val_loss: 1898.9356 - val_mae: 1899.6288\n",
      "Epoch 34/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 127.0936 - mae: 127.7835 - val_loss: 1776.3439 - val_mae: 1777.0361\n",
      "Epoch 35/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 184.2869 - mae: 184.9771 - val_loss: 1459.2089 - val_mae: 1459.9014\n",
      "Epoch 36/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 137.9777 - mae: 138.6662 - val_loss: 1682.2954 - val_mae: 1682.9886\n",
      "Epoch 37/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 158.2012 - mae: 158.8902 - val_loss: 1546.3510 - val_mae: 1547.0442\n",
      "Epoch 38/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 113.8638 - mae: 114.5522 - val_loss: 1684.2931 - val_mae: 1684.9855\n",
      "Epoch 39/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 119.0014 - mae: 119.6899 - val_loss: 1763.6648 - val_mae: 1764.3580\n",
      "Epoch 40/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 120.3331 - mae: 121.0226 - val_loss: 1804.7192 - val_mae: 1805.4121\n",
      "Epoch 41/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 119.2577 - mae: 119.9484 - val_loss: 1694.1243 - val_mae: 1694.8177\n",
      "Epoch 42/1000\n",
      "1461/1461 [==============================] - 0s 281us/step - loss: 114.1321 - mae: 114.8207 - val_loss: 1776.2092 - val_mae: 1776.9026\n",
      "Epoch 43/1000\n",
      "1461/1461 [==============================] - 0s 299us/step - loss: 129.7234 - mae: 130.4125 - val_loss: 1470.0643 - val_mae: 1470.7574\n",
      "Epoch 44/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 124.6797 - mae: 125.3689 - val_loss: 1492.8447 - val_mae: 1493.5372\n",
      "Epoch 45/1000\n",
      "1461/1461 [==============================] - 0s 289us/step - loss: 117.0237 - mae: 117.7108 - val_loss: 1519.0209 - val_mae: 1519.7141\n",
      "Epoch 46/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 119.2554 - mae: 119.9431 - val_loss: 1323.8296 - val_mae: 1324.5227\n",
      "Epoch 47/1000\n",
      "1461/1461 [==============================] - 0s 292us/step - loss: 100.4540 - mae: 101.1429 - val_loss: 1509.3607 - val_mae: 1510.0540\n",
      "Epoch 48/1000\n",
      "1461/1461 [==============================] - 0s 287us/step - loss: 121.1774 - mae: 121.8666 - val_loss: 1556.1310 - val_mae: 1556.8242\n",
      "Epoch 49/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 106.8854 - mae: 107.5725 - val_loss: 1246.4229 - val_mae: 1247.1152\n",
      "Epoch 50/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 100.8440 - mae: 101.5333 - val_loss: 1632.1219 - val_mae: 1632.8142\n",
      "Epoch 51/1000\n",
      "1461/1461 [==============================] - 0s 313us/step - loss: 120.0463 - mae: 120.7340 - val_loss: 1206.3360 - val_mae: 1207.0273\n",
      "Epoch 52/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 96.0022 - mae: 96.6897 - val_loss: 1791.5032 - val_mae: 1792.1964\n",
      "Epoch 53/1000\n",
      "1461/1461 [==============================] - 0s 283us/step - loss: 106.2842 - mae: 106.9714 - val_loss: 1361.2710 - val_mae: 1361.9642\n",
      "Epoch 54/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 97.5452 - mae: 98.2339 - val_loss: 1412.9366 - val_mae: 1413.6299\n",
      "Epoch 55/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 273us/step - loss: 94.0208 - mae: 94.7082 - val_loss: 1486.4351 - val_mae: 1487.1282\n",
      "Epoch 56/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 93.1843 - mae: 93.8724 - val_loss: 1384.4102 - val_mae: 1385.1029\n",
      "Epoch 57/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 96.9344 - mae: 97.6240 - val_loss: 1476.2079 - val_mae: 1476.9010\n",
      "Epoch 58/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 93.7953 - mae: 94.4830 - val_loss: 1560.8756 - val_mae: 1561.5687\n",
      "Epoch 59/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 98.6268 - mae: 99.3147 - val_loss: 1709.3722 - val_mae: 1710.0654\n",
      "Epoch 60/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 95.3853 - mae: 96.0736 - val_loss: 1458.5039 - val_mae: 1459.1970\n",
      "Epoch 61/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 91.4514 - mae: 92.1391 - val_loss: 1531.9527 - val_mae: 1532.6459\n",
      "Epoch 62/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 102.0144 - mae: 102.7018 - val_loss: 1661.6504 - val_mae: 1662.3436\n",
      "Epoch 63/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 92.0116 - mae: 92.6983 - val_loss: 1493.6019 - val_mae: 1494.2949\n",
      "Epoch 64/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 102.9025 - mae: 103.5900 - val_loss: 1332.2560 - val_mae: 1332.9492\n",
      "Epoch 65/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 107.7004 - mae: 108.3891 - val_loss: 1240.0826 - val_mae: 1240.7753\n",
      "Epoch 66/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 92.6321 - mae: 93.3205 - val_loss: 1449.8049 - val_mae: 1450.4979\n",
      "Epoch 67/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 94.4487 - mae: 95.1376 - val_loss: 1384.9556 - val_mae: 1385.6484\n",
      "Epoch 68/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 89.4394 - mae: 90.1275 - val_loss: 1651.6404 - val_mae: 1652.3336\n",
      "Epoch 69/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 91.6516 - mae: 92.3387 - val_loss: 1496.9472 - val_mae: 1497.6398\n",
      "Epoch 70/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 81.2281 - mae: 81.9159 - val_loss: 1498.1333 - val_mae: 1498.8263\n",
      "Epoch 71/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 93.3959 - mae: 94.0831 - val_loss: 1165.7801 - val_mae: 1166.4731\n",
      "Epoch 72/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 96.4913 - mae: 97.1790 - val_loss: 1596.7072 - val_mae: 1597.4003\n",
      "Epoch 73/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 107.2224 - mae: 107.9114 - val_loss: 1111.9963 - val_mae: 1112.6895\n",
      "Epoch 74/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 108.7870 - mae: 109.4757 - val_loss: 1466.7801 - val_mae: 1467.4724\n",
      "Epoch 75/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 84.9226 - mae: 85.6079 - val_loss: 1498.6499 - val_mae: 1499.3424\n",
      "Epoch 76/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 83.0154 - mae: 83.7034 - val_loss: 1443.0022 - val_mae: 1443.6952\n",
      "Epoch 77/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 93.8161 - mae: 94.5038 - val_loss: 1260.2127 - val_mae: 1260.9059\n",
      "Epoch 78/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 79.4929 - mae: 80.1807 - val_loss: 1244.3425 - val_mae: 1245.0349\n",
      "Epoch 79/1000\n",
      "1461/1461 [==============================] - 0s 281us/step - loss: 106.9038 - mae: 107.5924 - val_loss: 1475.8220 - val_mae: 1476.5153\n",
      "Epoch 80/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 70.4622 - mae: 71.1496 - val_loss: 1487.9513 - val_mae: 1488.6445\n",
      "Epoch 81/1000\n",
      "1461/1461 [==============================] - 0s 288us/step - loss: 81.8294 - mae: 82.5160 - val_loss: 1447.3559 - val_mae: 1448.0490\n",
      "Epoch 82/1000\n",
      "1461/1461 [==============================] - 0s 324us/step - loss: 101.0076 - mae: 101.6944 - val_loss: 1433.2691 - val_mae: 1433.9623\n",
      "Epoch 83/1000\n",
      "1461/1461 [==============================] - 1s 343us/step - loss: 95.5580 - mae: 96.2472 - val_loss: 1200.2877 - val_mae: 1200.9810\n",
      "Epoch 84/1000\n",
      "1461/1461 [==============================] - 0s 300us/step - loss: 80.3867 - mae: 81.0748 - val_loss: 1304.6833 - val_mae: 1305.3766\n",
      "Epoch 85/1000\n",
      "1461/1461 [==============================] - 0s 287us/step - loss: 83.7810 - mae: 84.4664 - val_loss: 1554.5725 - val_mae: 1555.2656\n",
      "Epoch 86/1000\n",
      "1461/1461 [==============================] - 0s 297us/step - loss: 94.3853 - mae: 95.0708 - val_loss: 1230.9700 - val_mae: 1231.6631\n",
      "Epoch 87/1000\n",
      "1461/1461 [==============================] - 0s 299us/step - loss: 82.9030 - mae: 83.5897 - val_loss: 1322.6954 - val_mae: 1323.3884\n",
      "Epoch 88/1000\n",
      "1461/1461 [==============================] - 0s 303us/step - loss: 86.0515 - mae: 86.7375 - val_loss: 1382.6292 - val_mae: 1383.3214\n",
      "Epoch 89/1000\n",
      "1461/1461 [==============================] - 0s 295us/step - loss: 92.5601 - mae: 93.2473 - val_loss: 1390.4035 - val_mae: 1391.0967\n",
      "Epoch 90/1000\n",
      "1461/1461 [==============================] - 0s 301us/step - loss: 78.1923 - mae: 78.8781 - val_loss: 1363.9955 - val_mae: 1364.6877\n",
      "Epoch 91/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 78.5777 - mae: 79.2635 - val_loss: 1537.4169 - val_mae: 1538.1100\n",
      "Epoch 92/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 70.6777 - mae: 71.3622 - val_loss: 1448.5720 - val_mae: 1449.2645\n",
      "Epoch 93/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 73.8074 - mae: 74.4960 - val_loss: 1256.8834 - val_mae: 1257.5767\n",
      "Epoch 94/1000\n",
      "1461/1461 [==============================] - 0s 281us/step - loss: 92.2930 - mae: 92.9808 - val_loss: 1530.8271 - val_mae: 1531.5203\n",
      "Epoch 95/1000\n",
      "1461/1461 [==============================] - 0s 291us/step - loss: 81.1018 - mae: 81.7893 - val_loss: 1402.2151 - val_mae: 1402.9073\n",
      "Epoch 96/1000\n",
      "1461/1461 [==============================] - 0s 290us/step - loss: 77.7277 - mae: 78.4141 - val_loss: 1489.0965 - val_mae: 1489.7897\n",
      "Epoch 97/1000\n",
      "1461/1461 [==============================] - 0s 288us/step - loss: 70.4939 - mae: 71.1810 - val_loss: 1409.2048 - val_mae: 1409.8975\n",
      "Epoch 98/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 69.7837 - mae: 70.4697 - val_loss: 1326.6385 - val_mae: 1327.3315\n",
      "Epoch 99/1000\n",
      "1461/1461 [==============================] - 0s 303us/step - loss: 87.5238 - mae: 88.2112 - val_loss: 1750.8738 - val_mae: 1751.5673\n",
      "Epoch 100/1000\n",
      "1461/1461 [==============================] - 0s 290us/step - loss: 95.4215 - mae: 96.1083 - val_loss: 1668.8625 - val_mae: 1669.5557\n",
      "Epoch 101/1000\n",
      "1461/1461 [==============================] - 0s 320us/step - loss: 115.2267 - mae: 115.9151 - val_loss: 1261.1604 - val_mae: 1261.8538\n",
      "Epoch 102/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 85.6444 - mae: 86.3316 - val_loss: 1393.7681 - val_mae: 1394.4611\n",
      "Epoch 103/1000\n",
      "1461/1461 [==============================] - 0s 292us/step - loss: 71.8448 - mae: 72.5296 - val_loss: 1319.8226 - val_mae: 1320.5153\n",
      "Epoch 104/1000\n",
      "1461/1461 [==============================] - 0s 307us/step - loss: 88.9504 - mae: 89.6360 - val_loss: 1394.6962 - val_mae: 1395.3892\n",
      "Epoch 105/1000\n",
      "1461/1461 [==============================] - 0s 301us/step - loss: 76.8518 - mae: 77.5364 - val_loss: 1386.1550 - val_mae: 1386.8480\n",
      "Epoch 106/1000\n",
      "1461/1461 [==============================] - 0s 291us/step - loss: 90.5845 - mae: 91.2703 - val_loss: 1429.4752 - val_mae: 1430.1686\n",
      "Epoch 107/1000\n",
      "1461/1461 [==============================] - 0s 303us/step - loss: 77.2566 - mae: 77.9430 - val_loss: 1400.1727 - val_mae: 1400.8658\n",
      "Epoch 108/1000\n",
      "1461/1461 [==============================] - 0s 321us/step - loss: 78.3788 - mae: 79.0664 - val_loss: 1502.3114 - val_mae: 1503.0048\n",
      "Epoch 109/1000\n",
      "1461/1461 [==============================] - 0s 334us/step - loss: 78.5309 - mae: 79.2145 - val_loss: 1145.3208 - val_mae: 1146.0140\n",
      "Epoch 110/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 322us/step - loss: 77.8943 - mae: 78.5791 - val_loss: 1409.7504 - val_mae: 1410.4436\n",
      "Epoch 111/1000\n",
      "1461/1461 [==============================] - 0s 327us/step - loss: 70.3982 - mae: 71.0818 - val_loss: 1371.1930 - val_mae: 1371.8861\n",
      "Epoch 112/1000\n",
      "1461/1461 [==============================] - 0s 319us/step - loss: 67.7717 - mae: 68.4559 - val_loss: 1508.9881 - val_mae: 1509.6813\n",
      "Epoch 113/1000\n",
      "1461/1461 [==============================] - 0s 313us/step - loss: 78.1155 - mae: 78.8017 - val_loss: 1138.8972 - val_mae: 1139.5895\n",
      "Epoch 114/1000\n",
      "1461/1461 [==============================] - 0s 295us/step - loss: 92.1516 - mae: 92.8390 - val_loss: 1668.4405 - val_mae: 1669.1337\n",
      "Epoch 115/1000\n",
      "1461/1461 [==============================] - 0s 288us/step - loss: 93.0013 - mae: 93.6872 - val_loss: 1879.1275 - val_mae: 1879.8208\n",
      "Epoch 116/1000\n",
      "1461/1461 [==============================] - 0s 303us/step - loss: 96.2120 - mae: 96.8993 - val_loss: 1417.2382 - val_mae: 1417.9310\n",
      "Epoch 117/1000\n",
      "1461/1461 [==============================] - 0s 288us/step - loss: 74.9026 - mae: 75.5858 - val_loss: 1760.2620 - val_mae: 1760.9552\n",
      "Epoch 118/1000\n",
      "1461/1461 [==============================] - 1s 367us/step - loss: 69.1627 - mae: 69.8488 - val_loss: 1292.8452 - val_mae: 1293.5381\n",
      "Epoch 119/1000\n",
      "1461/1461 [==============================] - 1s 362us/step - loss: 70.8927 - mae: 71.5813 - val_loss: 1657.2363 - val_mae: 1657.9297\n",
      "Epoch 120/1000\n",
      "1461/1461 [==============================] - 1s 389us/step - loss: 64.6255 - mae: 65.3093 - val_loss: 1530.5347 - val_mae: 1531.2278\n",
      "Epoch 121/1000\n",
      "1461/1461 [==============================] - 1s 392us/step - loss: 79.5631 - mae: 80.2467 - val_loss: 1431.9795 - val_mae: 1432.6729\n",
      "Epoch 122/1000\n",
      "1461/1461 [==============================] - 0s 304us/step - loss: 85.1361 - mae: 85.8215 - val_loss: 1685.5705 - val_mae: 1686.2637\n",
      "Epoch 123/1000\n",
      "1461/1461 [==============================] - 0s 335us/step - loss: 83.6185 - mae: 84.3040 - val_loss: 1681.9695 - val_mae: 1682.6626\n",
      "Epoch 124/1000\n",
      "1461/1461 [==============================] - 0s 319us/step - loss: 75.9651 - mae: 76.6529 - val_loss: 1572.3008 - val_mae: 1572.9939\n",
      "Epoch 125/1000\n",
      "1461/1461 [==============================] - 0s 314us/step - loss: 71.0900 - mae: 71.7762 - val_loss: 1441.5152 - val_mae: 1442.2084\n",
      "Epoch 126/1000\n",
      "1461/1461 [==============================] - 0s 331us/step - loss: 72.3107 - mae: 72.9956 - val_loss: 1157.2014 - val_mae: 1157.8944\n",
      "Epoch 127/1000\n",
      "1461/1461 [==============================] - 0s 297us/step - loss: 71.5178 - mae: 72.2012 - val_loss: 1517.5153 - val_mae: 1518.2085\n",
      "Epoch 128/1000\n",
      "1461/1461 [==============================] - 0s 293us/step - loss: 59.5956 - mae: 60.2819 - val_loss: 1521.3739 - val_mae: 1522.0671\n",
      "Epoch 129/1000\n",
      "1461/1461 [==============================] - 0s 298us/step - loss: 69.8483 - mae: 70.5357 - val_loss: 1648.0624 - val_mae: 1648.7556\n",
      "Epoch 130/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 71.6626 - mae: 72.3482 - val_loss: 1405.3019 - val_mae: 1405.9949\n",
      "Epoch 131/1000\n",
      "1461/1461 [==============================] - 0s 301us/step - loss: 72.9103 - mae: 73.5982 - val_loss: 1524.8655 - val_mae: 1525.5587\n",
      "Epoch 132/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 85.6350 - mae: 86.3200 - val_loss: 1780.9287 - val_mae: 1781.6217\n",
      "Epoch 133/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 72.9494 - mae: 73.6340 - val_loss: 1556.1524 - val_mae: 1556.8455\n",
      "Epoch 134/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 70.8802 - mae: 71.5666 - val_loss: 1125.4771 - val_mae: 1126.1697\n",
      "Epoch 135/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 71.7273 - mae: 72.4136 - val_loss: 1231.4224 - val_mae: 1232.1157\n",
      "Epoch 136/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 68.7286 - mae: 69.4141 - val_loss: 1510.8676 - val_mae: 1511.5607\n",
      "Epoch 137/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 64.2391 - mae: 64.9243 - val_loss: 1753.7377 - val_mae: 1754.4308\n",
      "Epoch 138/1000\n",
      "1461/1461 [==============================] - 0s 295us/step - loss: 63.2650 - mae: 63.9480 - val_loss: 1290.4071 - val_mae: 1291.1003\n",
      "Epoch 139/1000\n",
      "1461/1461 [==============================] - 0s 314us/step - loss: 72.5736 - mae: 73.2605 - val_loss: 1336.5342 - val_mae: 1337.2275\n",
      "Epoch 140/1000\n",
      "1461/1461 [==============================] - 0s 325us/step - loss: 64.0245 - mae: 64.7092 - val_loss: 1309.7402 - val_mae: 1310.4332\n",
      "Epoch 141/1000\n",
      "1461/1461 [==============================] - 0s 312us/step - loss: 79.2454 - mae: 79.9337 - val_loss: 1528.1477 - val_mae: 1528.8409\n",
      "Epoch 142/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 79.7095 - mae: 80.3944 - val_loss: 1150.1699 - val_mae: 1150.8632\n",
      "Epoch 143/1000\n",
      "1461/1461 [==============================] - 0s 291us/step - loss: 71.8138 - mae: 72.5013 - val_loss: 1546.1083 - val_mae: 1546.8015\n",
      "Epoch 144/1000\n",
      "1461/1461 [==============================] - 0s 300us/step - loss: 70.5759 - mae: 71.2623 - val_loss: 1360.5898 - val_mae: 1361.2831\n",
      "Epoch 145/1000\n",
      "1461/1461 [==============================] - 0s 290us/step - loss: 61.9383 - mae: 62.6207 - val_loss: 1292.6236 - val_mae: 1293.3157\n",
      "Epoch 146/1000\n",
      "1461/1461 [==============================] - 0s 288us/step - loss: 62.9123 - mae: 63.5998 - val_loss: 1474.5550 - val_mae: 1475.2483\n",
      "Epoch 147/1000\n",
      "1461/1461 [==============================] - 0s 298us/step - loss: 74.2950 - mae: 74.9812 - val_loss: 1470.4775 - val_mae: 1471.1707\n",
      "Epoch 148/1000\n",
      "1461/1461 [==============================] - 0s 296us/step - loss: 65.8030 - mae: 66.4867 - val_loss: 1392.9980 - val_mae: 1393.6913\n",
      "Epoch 149/1000\n",
      "1461/1461 [==============================] - 0s 292us/step - loss: 60.5333 - mae: 61.2156 - val_loss: 1561.1022 - val_mae: 1561.7954\n",
      "Epoch 150/1000\n",
      "1461/1461 [==============================] - 0s 308us/step - loss: 66.4657 - mae: 67.1512 - val_loss: 1349.6999 - val_mae: 1350.3932\n",
      "Epoch 151/1000\n",
      "1461/1461 [==============================] - 0s 293us/step - loss: 77.4079 - mae: 78.0931 - val_loss: 1324.7366 - val_mae: 1325.4298\n",
      "Epoch 152/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 85.7038 - mae: 86.3890 - val_loss: 1763.6524 - val_mae: 1764.3456\n",
      "Epoch 153/1000\n",
      "1461/1461 [==============================] - 0s 322us/step - loss: 72.8156 - mae: 73.5002 - val_loss: 1359.4759 - val_mae: 1360.1689\n",
      "Epoch 154/1000\n",
      "1461/1461 [==============================] - 0s 309us/step - loss: 72.4067 - mae: 73.0915 - val_loss: 1045.9493 - val_mae: 1046.6425\n",
      "Epoch 155/1000\n",
      "1461/1461 [==============================] - 0s 311us/step - loss: 67.9497 - mae: 68.6327 - val_loss: 1605.6089 - val_mae: 1606.3022\n",
      "Epoch 156/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 80.2200 - mae: 80.9057 - val_loss: 1135.0402 - val_mae: 1135.7324\n",
      "Epoch 157/1000\n",
      "1461/1461 [==============================] - 0s 299us/step - loss: 69.5598 - mae: 70.2444 - val_loss: 1537.7590 - val_mae: 1538.4523\n",
      "Epoch 158/1000\n",
      "1461/1461 [==============================] - 0s 300us/step - loss: 57.9550 - mae: 58.6391 - val_loss: 1298.6237 - val_mae: 1299.3168\n",
      "Epoch 159/1000\n",
      "1461/1461 [==============================] - 0s 296us/step - loss: 87.5778 - mae: 88.2635 - val_loss: 1675.7419 - val_mae: 1676.4351\n",
      "Epoch 160/1000\n",
      "1461/1461 [==============================] - 0s 296us/step - loss: 85.0538 - mae: 85.7407 - val_loss: 1432.6216 - val_mae: 1433.3149\n",
      "Epoch 161/1000\n",
      "1461/1461 [==============================] - 0s 283us/step - loss: 65.3143 - mae: 66.0037 - val_loss: 1292.4103 - val_mae: 1293.1024\n",
      "Epoch 162/1000\n",
      "1461/1461 [==============================] - 0s 292us/step - loss: 66.9221 - mae: 67.6084 - val_loss: 1565.0703 - val_mae: 1565.7637\n",
      "Epoch 163/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 78.5014 - mae: 79.1868 - val_loss: 1316.5897 - val_mae: 1317.2827\n",
      "Epoch 164/1000\n",
      "1461/1461 [==============================] - 0s 283us/step - loss: 63.7492 - mae: 64.4299 - val_loss: 1493.4693 - val_mae: 1494.1624\n",
      "Epoch 165/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 280us/step - loss: 68.3289 - mae: 69.0101 - val_loss: 1415.3064 - val_mae: 1415.9996\n",
      "Epoch 166/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 63.3351 - mae: 64.0194 - val_loss: 1415.0333 - val_mae: 1415.7255\n",
      "Epoch 167/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 64.8395 - mae: 65.5233 - val_loss: 1467.8732 - val_mae: 1468.5665\n",
      "Epoch 168/1000\n",
      "1461/1461 [==============================] - 0s 287us/step - loss: 57.9558 - mae: 58.6415 - val_loss: 1463.1362 - val_mae: 1463.8293\n",
      "Epoch 169/1000\n",
      "1461/1461 [==============================] - 0s 292us/step - loss: 53.7421 - mae: 54.4253 - val_loss: 1278.4746 - val_mae: 1279.1678\n",
      "Epoch 170/1000\n",
      "1461/1461 [==============================] - 0s 334us/step - loss: 68.7162 - mae: 69.4027 - val_loss: 1414.6370 - val_mae: 1415.3303\n",
      "Epoch 171/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 59.6297 - mae: 60.3110 - val_loss: 1404.6085 - val_mae: 1405.3018\n",
      "Epoch 172/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 80.2273 - mae: 80.9136 - val_loss: 1191.4956 - val_mae: 1192.1888\n",
      "Epoch 173/1000\n",
      "1461/1461 [==============================] - 0s 283us/step - loss: 57.9744 - mae: 58.6581 - val_loss: 1668.9634 - val_mae: 1669.6566\n",
      "Epoch 174/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 68.8763 - mae: 69.5616 - val_loss: 1343.3064 - val_mae: 1343.9995\n",
      "Epoch 175/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 72.8350 - mae: 73.5190 - val_loss: 1340.5755 - val_mae: 1341.2687\n",
      "Epoch 176/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 58.2187 - mae: 58.9043 - val_loss: 1494.7527 - val_mae: 1495.4458\n",
      "Epoch 177/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 58.0365 - mae: 58.7204 - val_loss: 1231.4727 - val_mae: 1232.1659\n",
      "Epoch 178/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 77.5224 - mae: 78.2080 - val_loss: 1203.4365 - val_mae: 1204.1293\n",
      "Epoch 179/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 67.6268 - mae: 68.3127 - val_loss: 1585.8004 - val_mae: 1586.4937\n",
      "Epoch 180/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 57.8472 - mae: 58.5351 - val_loss: 1419.9233 - val_mae: 1420.6166\n",
      "Epoch 181/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 77.5258 - mae: 78.2108 - val_loss: 1283.4467 - val_mae: 1284.1399\n",
      "Epoch 182/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 71.7153 - mae: 72.3995 - val_loss: 1631.0922 - val_mae: 1631.7854\n",
      "Epoch 183/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 71.6713 - mae: 72.3563 - val_loss: 1272.9749 - val_mae: 1273.6681\n",
      "Epoch 184/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 59.5622 - mae: 60.2463 - val_loss: 1357.1086 - val_mae: 1357.8016\n",
      "Epoch 185/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 60.4936 - mae: 61.1795 - val_loss: 1525.6802 - val_mae: 1526.3735\n",
      "Epoch 186/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 61.5558 - mae: 62.2393 - val_loss: 1481.4711 - val_mae: 1482.1642\n",
      "Epoch 187/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 57.4911 - mae: 58.1747 - val_loss: 1472.5822 - val_mae: 1473.2754\n",
      "Epoch 188/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 50.2253 - mae: 50.9081 - val_loss: 1563.5676 - val_mae: 1564.2610\n",
      "Epoch 189/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 62.4053 - mae: 63.0917 - val_loss: 1450.3029 - val_mae: 1450.9960\n",
      "Epoch 190/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 69.0301 - mae: 69.7103 - val_loss: 1399.2630 - val_mae: 1399.9562\n",
      "Epoch 191/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 63.8636 - mae: 64.5479 - val_loss: 1471.0984 - val_mae: 1471.7917\n",
      "Epoch 192/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 56.9916 - mae: 57.6752 - val_loss: 1309.8343 - val_mae: 1310.5275\n",
      "Epoch 193/1000\n",
      "1461/1461 [==============================] - 0s 295us/step - loss: 57.2048 - mae: 57.8898 - val_loss: 1386.6583 - val_mae: 1387.3514\n",
      "Epoch 194/1000\n",
      "1461/1461 [==============================] - 0s 281us/step - loss: 69.4595 - mae: 70.1413 - val_loss: 1439.7064 - val_mae: 1440.3998\n",
      "Epoch 195/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 52.7259 - mae: 53.4088 - val_loss: 1549.1780 - val_mae: 1549.8710\n",
      "Epoch 196/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 54.9179 - mae: 55.5988 - val_loss: 1368.6922 - val_mae: 1369.3855\n",
      "Epoch 197/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 79.6551 - mae: 80.3396 - val_loss: 1460.8893 - val_mae: 1461.5825\n",
      "Epoch 198/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 69.2748 - mae: 69.9562 - val_loss: 1204.4372 - val_mae: 1205.1299\n",
      "Epoch 199/1000\n",
      "1461/1461 [==============================] - 0s 281us/step - loss: 71.8459 - mae: 72.5298 - val_loss: 1806.3249 - val_mae: 1807.0179\n",
      "Epoch 200/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 84.0711 - mae: 84.7567 - val_loss: 1623.8723 - val_mae: 1624.5656\n",
      "Epoch 201/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 58.4370 - mae: 59.1196 - val_loss: 1340.6374 - val_mae: 1341.3308\n",
      "Epoch 202/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 54.0658 - mae: 54.7479 - val_loss: 1609.8823 - val_mae: 1610.5754\n",
      "Epoch 203/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 64.2481 - mae: 64.9336 - val_loss: 1289.3913 - val_mae: 1290.0845\n",
      "Epoch 204/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 57.6672 - mae: 58.3499 - val_loss: 1447.6215 - val_mae: 1448.3148\n",
      "Epoch 205/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 57.3146 - mae: 58.0002 - val_loss: 1526.4912 - val_mae: 1527.1843\n",
      "Epoch 206/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 68.8553 - mae: 69.5380 - val_loss: 1554.0212 - val_mae: 1554.7145\n",
      "Epoch 207/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 64.8497 - mae: 65.5295 - val_loss: 1362.0948 - val_mae: 1362.7881\n",
      "Epoch 208/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 51.1722 - mae: 51.8527 - val_loss: 1447.0727 - val_mae: 1447.7659\n",
      "Epoch 209/1000\n",
      "1461/1461 [==============================] - 0s 288us/step - loss: 61.1911 - mae: 61.8769 - val_loss: 1484.7680 - val_mae: 1485.4611\n",
      "Epoch 210/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 81.6825 - mae: 82.3673 - val_loss: 1244.1940 - val_mae: 1244.8872\n",
      "Epoch 211/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 76.0451 - mae: 76.7322 - val_loss: 1265.9561 - val_mae: 1266.6493\n",
      "Epoch 212/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 54.6869 - mae: 55.3694 - val_loss: 1140.1029 - val_mae: 1140.7960\n",
      "Epoch 213/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 79.3307 - mae: 80.0134 - val_loss: 1488.7866 - val_mae: 1489.4797\n",
      "Epoch 214/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 55.1543 - mae: 55.8372 - val_loss: 1268.5059 - val_mae: 1269.1991\n",
      "Epoch 215/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 66.5532 - mae: 67.2369 - val_loss: 1315.4959 - val_mae: 1316.1892\n",
      "Epoch 216/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 57.5482 - mae: 58.2334 - val_loss: 1294.1809 - val_mae: 1294.8741\n",
      "Epoch 217/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 58.9903 - mae: 59.6695 - val_loss: 1426.2324 - val_mae: 1426.9257\n",
      "Epoch 218/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 52.1235 - mae: 52.8053 - val_loss: 1694.2351 - val_mae: 1694.9281\n",
      "Epoch 219/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 61.4404 - mae: 62.1232 - val_loss: 1581.1072 - val_mae: 1581.8007\n",
      "Epoch 220/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 271us/step - loss: 65.0166 - mae: 65.6993 - val_loss: 1642.9504 - val_mae: 1643.6437\n",
      "Epoch 221/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 80.4466 - mae: 81.1317 - val_loss: 1415.7783 - val_mae: 1416.4714\n",
      "Epoch 222/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 84.1464 - mae: 84.8337 - val_loss: 1213.8732 - val_mae: 1214.5664\n",
      "Epoch 223/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 89.9013 - mae: 90.5882 - val_loss: 1527.2052 - val_mae: 1527.8984\n",
      "Epoch 224/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 57.7289 - mae: 58.4123 - val_loss: 1274.9903 - val_mae: 1275.6835\n",
      "Epoch 225/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 56.3967 - mae: 57.0802 - val_loss: 1576.3804 - val_mae: 1577.0736\n",
      "Epoch 226/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 75.3827 - mae: 76.0659 - val_loss: 1532.7007 - val_mae: 1533.3939\n",
      "Epoch 227/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 59.8408 - mae: 60.5267 - val_loss: 1545.3655 - val_mae: 1546.0587\n",
      "Epoch 228/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 65.5695 - mae: 66.2558 - val_loss: 1666.5913 - val_mae: 1667.2844\n",
      "Epoch 229/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 58.3008 - mae: 58.9843 - val_loss: 1452.1278 - val_mae: 1452.8210\n",
      "Epoch 230/1000\n",
      "1461/1461 [==============================] - 0s 299us/step - loss: 57.1667 - mae: 57.8492 - val_loss: 1677.6409 - val_mae: 1678.3340\n",
      "Epoch 231/1000\n",
      "1461/1461 [==============================] - 0s 288us/step - loss: 59.4433 - mae: 60.1241 - val_loss: 1415.5534 - val_mae: 1416.2466\n",
      "Epoch 232/1000\n",
      "1461/1461 [==============================] - 0s 300us/step - loss: 52.6771 - mae: 53.3568 - val_loss: 1256.2500 - val_mae: 1256.9434\n",
      "Epoch 233/1000\n",
      "1461/1461 [==============================] - 0s 308us/step - loss: 57.3085 - mae: 57.9919 - val_loss: 1782.8413 - val_mae: 1783.5345\n",
      "Epoch 234/1000\n",
      "1461/1461 [==============================] - 0s 306us/step - loss: 72.1340 - mae: 72.8187 - val_loss: 1483.0640 - val_mae: 1483.7573\n",
      "Epoch 235/1000\n",
      "1461/1461 [==============================] - 0s 298us/step - loss: 68.8214 - mae: 69.5064 - val_loss: 1213.1126 - val_mae: 1213.8054\n",
      "Epoch 236/1000\n",
      "1461/1461 [==============================] - 0s 300us/step - loss: 49.3474 - mae: 50.0316 - val_loss: 1300.2434 - val_mae: 1300.9365\n",
      "Epoch 237/1000\n",
      "1461/1461 [==============================] - 0s 291us/step - loss: 62.9437 - mae: 63.6262 - val_loss: 1509.5931 - val_mae: 1510.2863\n",
      "Epoch 238/1000\n",
      "1461/1461 [==============================] - 0s 288us/step - loss: 61.3089 - mae: 61.9916 - val_loss: 1404.7960 - val_mae: 1405.4888\n",
      "Epoch 239/1000\n",
      "1461/1461 [==============================] - 0s 302us/step - loss: 60.6426 - mae: 61.3270 - val_loss: 1533.2259 - val_mae: 1533.9191\n",
      "Epoch 240/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 52.8842 - mae: 53.5655 - val_loss: 1644.0383 - val_mae: 1644.7316\n",
      "Epoch 241/1000\n",
      "1461/1461 [==============================] - 0s 301us/step - loss: 52.8380 - mae: 53.5204 - val_loss: 1349.9784 - val_mae: 1350.6716\n",
      "Epoch 242/1000\n",
      "1461/1461 [==============================] - 0s 287us/step - loss: 53.2271 - mae: 53.9121 - val_loss: 1224.3953 - val_mae: 1225.0880\n",
      "Epoch 243/1000\n",
      "1461/1461 [==============================] - 0s 293us/step - loss: 64.4713 - mae: 65.1518 - val_loss: 1424.1553 - val_mae: 1424.8480\n",
      "Epoch 244/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 45.7881 - mae: 46.4716 - val_loss: 1256.7567 - val_mae: 1257.4497\n",
      "Epoch 245/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 61.6856 - mae: 62.3708 - val_loss: 1674.2243 - val_mae: 1674.9177\n",
      "Epoch 246/1000\n",
      "1461/1461 [==============================] - 0s 288us/step - loss: 66.4916 - mae: 67.1796 - val_loss: 1499.4372 - val_mae: 1500.1305\n",
      "Epoch 247/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 65.9154 - mae: 66.6003 - val_loss: 1538.4286 - val_mae: 1539.1218\n",
      "Epoch 248/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 56.9001 - mae: 57.5818 - val_loss: 1156.3134 - val_mae: 1157.0065\n",
      "Epoch 249/1000\n",
      "1461/1461 [==============================] - 0s 302us/step - loss: 66.2455 - mae: 66.9301 - val_loss: 1312.4404 - val_mae: 1313.1337\n",
      "Epoch 250/1000\n",
      "1461/1461 [==============================] - 0s 288us/step - loss: 59.7836 - mae: 60.4649 - val_loss: 1425.3913 - val_mae: 1426.0831\n",
      "Epoch 251/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 66.5259 - mae: 67.2098 - val_loss: 1297.1157 - val_mae: 1297.8088\n",
      "Epoch 252/1000\n",
      "1461/1461 [==============================] - 0s 312us/step - loss: 47.2689 - mae: 47.9501 - val_loss: 1593.2277 - val_mae: 1593.9209\n",
      "Epoch 253/1000\n",
      "1461/1461 [==============================] - 0s 287us/step - loss: 50.3925 - mae: 51.0734 - val_loss: 1473.9537 - val_mae: 1474.6470\n",
      "Epoch 254/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 57.7068 - mae: 58.3900 - val_loss: 1436.5141 - val_mae: 1437.2070\n",
      "Epoch 255/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 62.0616 - mae: 62.7462 - val_loss: 1383.6588 - val_mae: 1384.3521\n",
      "Epoch 256/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 48.3102 - mae: 48.9911 - val_loss: 1494.1986 - val_mae: 1494.8920\n",
      "Epoch 257/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 60.0114 - mae: 60.6977 - val_loss: 1381.8538 - val_mae: 1382.5470\n",
      "Epoch 258/1000\n",
      "1461/1461 [==============================] - 0s 302us/step - loss: 52.2029 - mae: 52.8851 - val_loss: 1512.6978 - val_mae: 1513.3911\n",
      "Epoch 259/1000\n",
      "1461/1461 [==============================] - 0s 291us/step - loss: 56.8551 - mae: 57.5409 - val_loss: 1376.9560 - val_mae: 1377.6492\n",
      "Epoch 260/1000\n",
      "1461/1461 [==============================] - 0s 291us/step - loss: 51.6250 - mae: 52.3042 - val_loss: 1496.6695 - val_mae: 1497.3628\n",
      "Epoch 261/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 60.6663 - mae: 61.3482 - val_loss: 1059.0991 - val_mae: 1059.7922\n",
      "Epoch 262/1000\n",
      "1461/1461 [==============================] - 0s 291us/step - loss: 54.3456 - mae: 55.0275 - val_loss: 1250.0440 - val_mae: 1250.7372\n",
      "Epoch 263/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 57.7890 - mae: 58.4702 - val_loss: 1420.4842 - val_mae: 1421.1774\n",
      "Epoch 264/1000\n",
      "1461/1461 [==============================] - 0s 288us/step - loss: 65.5916 - mae: 66.2737 - val_loss: 1241.2100 - val_mae: 1241.9032\n",
      "Epoch 265/1000\n",
      "1461/1461 [==============================] - 0s 294us/step - loss: 46.5061 - mae: 47.1850 - val_loss: 1581.7754 - val_mae: 1582.4686\n",
      "Epoch 266/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 63.0090 - mae: 63.6942 - val_loss: 1418.9702 - val_mae: 1419.6635\n",
      "Epoch 267/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 55.4070 - mae: 56.0868 - val_loss: 1394.0198 - val_mae: 1394.7131\n",
      "Epoch 268/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 65.3414 - mae: 66.0249 - val_loss: 1452.0926 - val_mae: 1452.7858\n",
      "Epoch 269/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 62.7624 - mae: 63.4497 - val_loss: 1373.2515 - val_mae: 1373.9446\n",
      "Epoch 270/1000\n",
      "1461/1461 [==============================] - 0s 309us/step - loss: 59.2594 - mae: 59.9432 - val_loss: 1058.6612 - val_mae: 1059.3528\n",
      "Epoch 271/1000\n",
      "1461/1461 [==============================] - 0s 301us/step - loss: 78.4731 - mae: 79.1602 - val_loss: 1023.3229 - val_mae: 1024.0154\n",
      "Epoch 272/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 72.1676 - mae: 72.8539 - val_loss: 1385.9356 - val_mae: 1386.6288\n",
      "Epoch 273/1000\n",
      "1461/1461 [==============================] - 0s 290us/step - loss: 55.7584 - mae: 56.4433 - val_loss: 1677.9896 - val_mae: 1678.6829\n",
      "Epoch 274/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 58.8856 - mae: 59.5678 - val_loss: 1702.5324 - val_mae: 1703.2256\n",
      "Epoch 275/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 287us/step - loss: 79.8527 - mae: 80.5374 - val_loss: 1192.0423 - val_mae: 1192.7354\n",
      "Epoch 276/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 49.4040 - mae: 50.0872 - val_loss: 1352.8448 - val_mae: 1353.5381\n",
      "Epoch 277/1000\n",
      "1461/1461 [==============================] - 0s 291us/step - loss: 49.1680 - mae: 49.8483 - val_loss: 1752.3609 - val_mae: 1753.0543\n",
      "Epoch 278/1000\n",
      "1461/1461 [==============================] - 0s 297us/step - loss: 64.6116 - mae: 65.2950 - val_loss: 933.0294 - val_mae: 933.7214\n",
      "Epoch 279/1000\n",
      "1461/1461 [==============================] - 0s 330us/step - loss: 81.0807 - mae: 81.7670 - val_loss: 816.1627 - val_mae: 816.8549\n",
      "Epoch 280/1000\n",
      "1461/1461 [==============================] - 0s 289us/step - loss: 59.4880 - mae: 60.1711 - val_loss: 1400.9426 - val_mae: 1401.6357\n",
      "Epoch 281/1000\n",
      "1461/1461 [==============================] - 0s 298us/step - loss: 44.8546 - mae: 45.5325 - val_loss: 1606.6156 - val_mae: 1607.3087\n",
      "Epoch 282/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 58.2880 - mae: 58.9685 - val_loss: 1273.4455 - val_mae: 1274.1387\n",
      "Epoch 283/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 65.1666 - mae: 65.8506 - val_loss: 1393.6905 - val_mae: 1394.3838\n",
      "Epoch 284/1000\n",
      "1461/1461 [==============================] - 0s 289us/step - loss: 66.2365 - mae: 66.9212 - val_loss: 1560.3662 - val_mae: 1561.0594\n",
      "Epoch 285/1000\n",
      "1461/1461 [==============================] - 0s 312us/step - loss: 56.4405 - mae: 57.1252 - val_loss: 1448.4354 - val_mae: 1449.1287\n",
      "Epoch 286/1000\n",
      "1461/1461 [==============================] - 0s 299us/step - loss: 52.9499 - mae: 53.6346 - val_loss: 1408.0766 - val_mae: 1408.7697\n",
      "Epoch 287/1000\n",
      "1461/1461 [==============================] - 0s 291us/step - loss: 49.2313 - mae: 49.9161 - val_loss: 1388.9902 - val_mae: 1389.6832\n",
      "Epoch 288/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 57.1575 - mae: 57.8374 - val_loss: 1654.5648 - val_mae: 1655.2578\n",
      "Epoch 289/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 64.8330 - mae: 65.5157 - val_loss: 1318.2553 - val_mae: 1318.9485\n",
      "Epoch 290/1000\n",
      "1461/1461 [==============================] - 0s 290us/step - loss: 66.9156 - mae: 67.6014 - val_loss: 1331.1099 - val_mae: 1331.8032\n",
      "Epoch 291/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 47.8804 - mae: 48.5627 - val_loss: 1531.6051 - val_mae: 1532.2982\n",
      "Epoch 292/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 49.7663 - mae: 50.4501 - val_loss: 1565.7435 - val_mae: 1566.4368\n",
      "Epoch 293/1000\n",
      "1461/1461 [==============================] - 0s 292us/step - loss: 58.8517 - mae: 59.5319 - val_loss: 1294.2578 - val_mae: 1294.9507\n",
      "Epoch 294/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 44.9543 - mae: 45.6347 - val_loss: 1412.2191 - val_mae: 1412.9124\n",
      "Epoch 295/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 60.6768 - mae: 61.3599 - val_loss: 1642.5691 - val_mae: 1643.2623\n",
      "Epoch 296/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 61.3959 - mae: 62.0769 - val_loss: 1075.1666 - val_mae: 1075.8599\n",
      "Epoch 297/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 56.8243 - mae: 57.5101 - val_loss: 1311.9209 - val_mae: 1312.6140\n",
      "Epoch 298/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 53.3377 - mae: 54.0194 - val_loss: 1329.8519 - val_mae: 1330.5449\n",
      "Epoch 299/1000\n",
      "1461/1461 [==============================] - 0s 288us/step - loss: 57.7000 - mae: 58.3831 - val_loss: 1450.8540 - val_mae: 1451.5472\n",
      "Epoch 300/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 64.4873 - mae: 65.1679 - val_loss: 1485.8972 - val_mae: 1486.5902\n",
      "Epoch 301/1000\n",
      "1461/1461 [==============================] - 0s 283us/step - loss: 64.4750 - mae: 65.1602 - val_loss: 1358.9798 - val_mae: 1359.6720\n",
      "Epoch 302/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 68.9888 - mae: 69.6710 - val_loss: 1363.5073 - val_mae: 1364.2004\n",
      "Epoch 303/1000\n",
      "1461/1461 [==============================] - 0s 291us/step - loss: 71.1216 - mae: 71.8073 - val_loss: 1176.6916 - val_mae: 1177.3846\n",
      "Epoch 304/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 60.3661 - mae: 61.0490 - val_loss: 1241.8231 - val_mae: 1242.5156\n",
      "Epoch 305/1000\n",
      "1461/1461 [==============================] - 0s 292us/step - loss: 56.7504 - mae: 57.4342 - val_loss: 1569.8774 - val_mae: 1570.5706\n",
      "Epoch 306/1000\n",
      "1461/1461 [==============================] - 0s 294us/step - loss: 53.9291 - mae: 54.6092 - val_loss: 1294.1696 - val_mae: 1294.8622\n",
      "Epoch 307/1000\n",
      "1461/1461 [==============================] - 0s 293us/step - loss: 56.3457 - mae: 57.0295 - val_loss: 1162.9556 - val_mae: 1163.6487\n",
      "Epoch 308/1000\n",
      "1461/1461 [==============================] - 1s 357us/step - loss: 51.6922 - mae: 52.3723 - val_loss: 1302.1306 - val_mae: 1302.8239\n",
      "Epoch 309/1000\n",
      "1461/1461 [==============================] - 0s 298us/step - loss: 56.2996 - mae: 56.9818 - val_loss: 1614.3999 - val_mae: 1615.0931\n",
      "Epoch 310/1000\n",
      "1461/1461 [==============================] - 0s 291us/step - loss: 46.8970 - mae: 47.5795 - val_loss: 1356.5944 - val_mae: 1357.2877\n",
      "Epoch 311/1000\n",
      "1461/1461 [==============================] - 0s 289us/step - loss: 49.9677 - mae: 50.6481 - val_loss: 1570.7236 - val_mae: 1571.4166\n",
      "Epoch 312/1000\n",
      "1461/1461 [==============================] - 0s 334us/step - loss: 55.1124 - mae: 55.7931 - val_loss: 1574.8857 - val_mae: 1575.5789\n",
      "Epoch 313/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 63.2337 - mae: 63.9148 - val_loss: 1284.9518 - val_mae: 1285.6449\n",
      "Epoch 314/1000\n",
      "1461/1461 [==============================] - 0s 290us/step - loss: 57.8516 - mae: 58.5318 - val_loss: 1524.4763 - val_mae: 1525.1694\n",
      "Epoch 315/1000\n",
      "1461/1461 [==============================] - 0s 315us/step - loss: 61.9987 - mae: 62.6791 - val_loss: 1374.0148 - val_mae: 1374.7079\n",
      "Epoch 316/1000\n",
      "1461/1461 [==============================] - 0s 326us/step - loss: 58.4321 - mae: 59.1157 - val_loss: 1469.2086 - val_mae: 1469.9017\n",
      "Epoch 317/1000\n",
      "1461/1461 [==============================] - 1s 347us/step - loss: 52.2897 - mae: 52.9692 - val_loss: 1486.0340 - val_mae: 1486.7273\n",
      "Epoch 318/1000\n",
      "1461/1461 [==============================] - 0s 320us/step - loss: 57.8301 - mae: 58.5142 - val_loss: 1594.1022 - val_mae: 1594.7954\n",
      "Epoch 319/1000\n",
      "1461/1461 [==============================] - 0s 324us/step - loss: 58.9097 - mae: 59.5926 - val_loss: 1605.9393 - val_mae: 1606.6326\n",
      "Epoch 320/1000\n",
      "1461/1461 [==============================] - 0s 289us/step - loss: 71.9592 - mae: 72.6425 - val_loss: 916.0990 - val_mae: 916.7919\n",
      "Epoch 321/1000\n",
      "1461/1461 [==============================] - 0s 311us/step - loss: 64.0484 - mae: 64.7330 - val_loss: 1444.3523 - val_mae: 1445.0443\n",
      "Epoch 322/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 55.6519 - mae: 56.3332 - val_loss: 1471.7327 - val_mae: 1472.4261\n",
      "Epoch 323/1000\n",
      "1461/1461 [==============================] - 0s 293us/step - loss: 57.5976 - mae: 58.2826 - val_loss: 1553.2627 - val_mae: 1553.9556\n",
      "Epoch 324/1000\n",
      "1461/1461 [==============================] - 0s 297us/step - loss: 50.9537 - mae: 51.6309 - val_loss: 1319.5532 - val_mae: 1320.2463\n",
      "Epoch 325/1000\n",
      "1461/1461 [==============================] - 0s 289us/step - loss: 43.6737 - mae: 44.3531 - val_loss: 1275.8592 - val_mae: 1276.5525\n",
      "Epoch 326/1000\n",
      "1461/1461 [==============================] - 0s 287us/step - loss: 53.6602 - mae: 54.3421 - val_loss: 1149.5404 - val_mae: 1150.2335\n",
      "Epoch 327/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 58.7869 - mae: 59.4676 - val_loss: 1421.2879 - val_mae: 1421.9811\n",
      "Epoch 328/1000\n",
      "1461/1461 [==============================] - 0s 291us/step - loss: 47.3452 - mae: 48.0291 - val_loss: 1257.5437 - val_mae: 1258.2368\n",
      "Epoch 329/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 50.3949 - mae: 51.0783 - val_loss: 1567.0078 - val_mae: 1567.7009\n",
      "Epoch 330/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 288us/step - loss: 53.2668 - mae: 53.9511 - val_loss: 1454.0414 - val_mae: 1454.7345\n",
      "Epoch 331/1000\n",
      "1461/1461 [==============================] - 0s 287us/step - loss: 52.3820 - mae: 53.0661 - val_loss: 1585.9272 - val_mae: 1586.6204\n",
      "Epoch 332/1000\n",
      "1461/1461 [==============================] - 0s 281us/step - loss: 47.9285 - mae: 48.6115 - val_loss: 1286.9823 - val_mae: 1287.6750\n",
      "Epoch 333/1000\n",
      "1461/1461 [==============================] - 0s 293us/step - loss: 50.3744 - mae: 51.0550 - val_loss: 1078.2824 - val_mae: 1078.9756\n",
      "Epoch 334/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 93.0707 - mae: 93.7554 - val_loss: 965.7344 - val_mae: 966.4267\n",
      "Epoch 335/1000\n",
      "1461/1461 [==============================] - 0s 292us/step - loss: 53.0775 - mae: 53.7612 - val_loss: 1483.1994 - val_mae: 1483.8926\n",
      "Epoch 336/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 58.1129 - mae: 58.7940 - val_loss: 1294.2262 - val_mae: 1294.9188\n",
      "Epoch 337/1000\n",
      "1461/1461 [==============================] - 0s 287us/step - loss: 65.4403 - mae: 66.1221 - val_loss: 1225.4792 - val_mae: 1226.1716\n",
      "Epoch 338/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 51.2785 - mae: 51.9611 - val_loss: 1591.6943 - val_mae: 1592.3876\n",
      "Epoch 339/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 63.9966 - mae: 64.6808 - val_loss: 1378.9058 - val_mae: 1379.5991\n",
      "Epoch 340/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 53.0419 - mae: 53.7258 - val_loss: 1378.0306 - val_mae: 1378.7238\n",
      "Epoch 341/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 46.3186 - mae: 47.0007 - val_loss: 1485.3721 - val_mae: 1486.0653\n",
      "Epoch 342/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 44.5302 - mae: 45.2135 - val_loss: 1376.6112 - val_mae: 1377.3044\n",
      "Epoch 343/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 57.2979 - mae: 57.9795 - val_loss: 1359.0141 - val_mae: 1359.7070\n",
      "Epoch 344/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 42.8995 - mae: 43.5776 - val_loss: 1342.2910 - val_mae: 1342.9841\n",
      "Epoch 345/1000\n",
      "1461/1461 [==============================] - 0s 298us/step - loss: 46.9904 - mae: 47.6736 - val_loss: 1369.8793 - val_mae: 1370.5720\n",
      "Epoch 346/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 45.6714 - mae: 46.3530 - val_loss: 1473.9198 - val_mae: 1474.6130\n",
      "Epoch 347/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 43.2301 - mae: 43.9131 - val_loss: 1437.8223 - val_mae: 1438.5156\n",
      "Epoch 348/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 45.8510 - mae: 46.5268 - val_loss: 1303.1928 - val_mae: 1303.8860\n",
      "Epoch 349/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 50.6020 - mae: 51.2841 - val_loss: 1351.0183 - val_mae: 1351.7115\n",
      "Epoch 350/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 60.2860 - mae: 60.9709 - val_loss: 1335.2448 - val_mae: 1335.9377\n",
      "Epoch 351/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 43.1866 - mae: 43.8657 - val_loss: 1151.9420 - val_mae: 1152.6351\n",
      "Epoch 352/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 49.8984 - mae: 50.5843 - val_loss: 1321.7567 - val_mae: 1322.4498\n",
      "Epoch 353/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 59.1288 - mae: 59.8103 - val_loss: 1527.4316 - val_mae: 1528.1249\n",
      "Epoch 354/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 49.0607 - mae: 49.7439 - val_loss: 1669.8230 - val_mae: 1670.5162\n",
      "Epoch 355/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 53.4467 - mae: 54.1304 - val_loss: 1569.8756 - val_mae: 1570.5687\n",
      "Epoch 356/1000\n",
      "1461/1461 [==============================] - 0s 283us/step - loss: 45.7676 - mae: 46.4487 - val_loss: 1226.5250 - val_mae: 1227.2172\n",
      "Epoch 357/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 43.0857 - mae: 43.7658 - val_loss: 1274.0674 - val_mae: 1274.7605\n",
      "Epoch 358/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 55.4065 - mae: 56.0904 - val_loss: 1548.9459 - val_mae: 1549.6390\n",
      "Epoch 359/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 48.6111 - mae: 49.2928 - val_loss: 1432.4228 - val_mae: 1433.1161\n",
      "Epoch 360/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 73.0931 - mae: 73.7758 - val_loss: 1394.8647 - val_mae: 1395.5580\n",
      "Epoch 361/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 57.6809 - mae: 58.3658 - val_loss: 1387.5917 - val_mae: 1388.2848\n",
      "Epoch 362/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 55.8879 - mae: 56.5725 - val_loss: 1859.8156 - val_mae: 1860.5087\n",
      "Epoch 363/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 65.9057 - mae: 66.5918 - val_loss: 1394.5859 - val_mae: 1395.2783\n",
      "Epoch 364/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 56.0135 - mae: 56.6918 - val_loss: 1484.0062 - val_mae: 1484.6993\n",
      "Epoch 365/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 50.2682 - mae: 50.9537 - val_loss: 1332.9866 - val_mae: 1333.6797\n",
      "Epoch 366/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 39.8328 - mae: 40.5094 - val_loss: 1305.7130 - val_mae: 1306.4062\n",
      "Epoch 367/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 49.8939 - mae: 50.5773 - val_loss: 1403.1959 - val_mae: 1403.8890\n",
      "Epoch 368/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 47.6147 - mae: 48.2992 - val_loss: 1325.8163 - val_mae: 1326.5094\n",
      "Epoch 369/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 62.1616 - mae: 62.8443 - val_loss: 1351.9687 - val_mae: 1352.6620\n",
      "Epoch 370/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 53.1473 - mae: 53.8273 - val_loss: 1713.8553 - val_mae: 1714.5485\n",
      "Epoch 371/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 61.8727 - mae: 62.5593 - val_loss: 1287.4850 - val_mae: 1288.1774\n",
      "Epoch 372/1000\n",
      "1461/1461 [==============================] - 0s 281us/step - loss: 67.2242 - mae: 67.9057 - val_loss: 1233.2891 - val_mae: 1233.9822\n",
      "Epoch 373/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 49.7663 - mae: 50.4471 - val_loss: 1343.1137 - val_mae: 1343.8062\n",
      "Epoch 374/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 51.7563 - mae: 52.4404 - val_loss: 1334.3709 - val_mae: 1335.0627\n",
      "Epoch 375/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 40.0572 - mae: 40.7365 - val_loss: 1361.9066 - val_mae: 1362.5997\n",
      "Epoch 376/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 53.0403 - mae: 53.7182 - val_loss: 1013.9561 - val_mae: 1014.6492\n",
      "Epoch 377/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 60.1109 - mae: 60.7944 - val_loss: 1342.7645 - val_mae: 1343.4574\n",
      "Epoch 378/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 60.0923 - mae: 60.7773 - val_loss: 1377.0575 - val_mae: 1377.7507\n",
      "Epoch 379/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 56.8070 - mae: 57.4855 - val_loss: 1446.7115 - val_mae: 1447.4048\n",
      "Epoch 380/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 57.1564 - mae: 57.8417 - val_loss: 1247.7061 - val_mae: 1248.3993\n",
      "Epoch 381/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 52.7216 - mae: 53.4028 - val_loss: 1378.1672 - val_mae: 1378.8602\n",
      "Epoch 382/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 43.9404 - mae: 44.6180 - val_loss: 1221.2501 - val_mae: 1221.9420\n",
      "Epoch 383/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 49.8243 - mae: 50.5041 - val_loss: 1155.6125 - val_mae: 1156.3057\n",
      "Epoch 384/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 52.9144 - mae: 53.5962 - val_loss: 1406.9854 - val_mae: 1407.6786\n",
      "Epoch 385/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 296us/step - loss: 44.8371 - mae: 45.5199 - val_loss: 1505.6525 - val_mae: 1506.3456\n",
      "Epoch 386/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 38.8704 - mae: 39.5504 - val_loss: 1348.8232 - val_mae: 1349.5164\n",
      "Epoch 387/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 42.7762 - mae: 43.4564 - val_loss: 1430.0150 - val_mae: 1430.7081\n",
      "Epoch 388/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 54.5678 - mae: 55.2492 - val_loss: 1366.8610 - val_mae: 1367.5535\n",
      "Epoch 389/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 53.6613 - mae: 54.3438 - val_loss: 1073.8214 - val_mae: 1074.5145\n",
      "Epoch 390/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 57.1525 - mae: 57.8365 - val_loss: 1300.9508 - val_mae: 1301.6440\n",
      "Epoch 391/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 47.9235 - mae: 48.6064 - val_loss: 1620.8771 - val_mae: 1621.5702\n",
      "Epoch 392/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 44.1794 - mae: 44.8593 - val_loss: 1422.0766 - val_mae: 1422.7699\n",
      "Epoch 393/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 46.9340 - mae: 47.6162 - val_loss: 1318.0019 - val_mae: 1318.6951\n",
      "Epoch 394/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 56.6755 - mae: 57.3595 - val_loss: 1444.3726 - val_mae: 1445.0658\n",
      "Epoch 395/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 52.4055 - mae: 53.0888 - val_loss: 1256.6733 - val_mae: 1257.3655\n",
      "Epoch 396/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 54.0522 - mae: 54.7328 - val_loss: 1450.7757 - val_mae: 1451.4690\n",
      "Epoch 397/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 39.4469 - mae: 40.1288 - val_loss: 1183.3531 - val_mae: 1184.0463\n",
      "Epoch 398/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 48.2585 - mae: 48.9408 - val_loss: 1531.7945 - val_mae: 1532.4877\n",
      "Epoch 399/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 51.3151 - mae: 51.9943 - val_loss: 1249.3111 - val_mae: 1250.0044\n",
      "Epoch 400/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 59.8261 - mae: 60.5089 - val_loss: 1502.6094 - val_mae: 1503.3027\n",
      "Epoch 401/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 49.7283 - mae: 50.4100 - val_loss: 1430.7432 - val_mae: 1431.4360\n",
      "Epoch 402/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 52.3592 - mae: 53.0381 - val_loss: 1531.5821 - val_mae: 1532.2747\n",
      "Epoch 403/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 43.0318 - mae: 43.7095 - val_loss: 1381.9073 - val_mae: 1382.6005\n",
      "Epoch 404/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 54.9578 - mae: 55.6404 - val_loss: 1315.2833 - val_mae: 1315.9760\n",
      "Epoch 405/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 52.8411 - mae: 53.5244 - val_loss: 1316.0650 - val_mae: 1316.7582\n",
      "Epoch 406/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 44.4979 - mae: 45.1806 - val_loss: 1451.2966 - val_mae: 1451.9899\n",
      "Epoch 407/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 48.0652 - mae: 48.7476 - val_loss: 1207.4857 - val_mae: 1208.1788\n",
      "Epoch 408/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 57.1414 - mae: 57.8219 - val_loss: 1585.2219 - val_mae: 1585.9146\n",
      "Epoch 409/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 62.1494 - mae: 62.8344 - val_loss: 1699.6144 - val_mae: 1700.3075\n",
      "Epoch 410/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 52.1046 - mae: 52.7872 - val_loss: 1531.9329 - val_mae: 1532.6260\n",
      "Epoch 411/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 45.7645 - mae: 46.4451 - val_loss: 1323.8475 - val_mae: 1324.5406\n",
      "Epoch 412/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 42.3892 - mae: 43.0682 - val_loss: 1444.2633 - val_mae: 1444.9565\n",
      "Epoch 413/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 43.9829 - mae: 44.6655 - val_loss: 1345.8516 - val_mae: 1346.5448\n",
      "Epoch 414/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 45.4409 - mae: 46.1250 - val_loss: 1420.4047 - val_mae: 1421.0979\n",
      "Epoch 415/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 84.6464 - mae: 85.3314 - val_loss: 1448.4130 - val_mae: 1449.1062\n",
      "Epoch 416/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 83.7530 - mae: 84.4378 - val_loss: 1519.3838 - val_mae: 1520.0770\n",
      "Epoch 417/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 56.6985 - mae: 57.3824 - val_loss: 1600.4369 - val_mae: 1601.1302\n",
      "Epoch 418/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 51.2543 - mae: 51.9369 - val_loss: 1552.8654 - val_mae: 1553.5586\n",
      "Epoch 419/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 60.2598 - mae: 60.9438 - val_loss: 1216.4316 - val_mae: 1217.1246\n",
      "Epoch 420/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 53.7536 - mae: 54.4381 - val_loss: 1355.6470 - val_mae: 1356.3402\n",
      "Epoch 421/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 46.4465 - mae: 47.1276 - val_loss: 1526.8630 - val_mae: 1527.5563\n",
      "Epoch 422/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 47.1201 - mae: 47.8031 - val_loss: 1444.7955 - val_mae: 1445.4889\n",
      "Epoch 423/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 46.9798 - mae: 47.6616 - val_loss: 1248.4317 - val_mae: 1249.1248\n",
      "Epoch 424/1000\n",
      "1461/1461 [==============================] - 0s 281us/step - loss: 44.1953 - mae: 44.8747 - val_loss: 1220.4899 - val_mae: 1221.1831\n",
      "Epoch 425/1000\n",
      "1461/1461 [==============================] - 0s 294us/step - loss: 55.3043 - mae: 55.9878 - val_loss: 1456.8222 - val_mae: 1457.5154\n",
      "Epoch 426/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 45.7789 - mae: 46.4595 - val_loss: 1435.6896 - val_mae: 1436.3828\n",
      "Epoch 427/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 50.0044 - mae: 50.6883 - val_loss: 1653.0252 - val_mae: 1653.7185\n",
      "Epoch 428/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 53.5744 - mae: 54.2550 - val_loss: 1449.7405 - val_mae: 1450.4337\n",
      "Epoch 429/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 53.8639 - mae: 54.5469 - val_loss: 1405.7336 - val_mae: 1406.4268\n",
      "Epoch 430/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 47.0530 - mae: 47.7340 - val_loss: 1449.9720 - val_mae: 1450.6652\n",
      "Epoch 431/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 50.9359 - mae: 51.6149 - val_loss: 1530.8633 - val_mae: 1531.5565\n",
      "Epoch 432/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 47.0389 - mae: 47.7217 - val_loss: 1343.3722 - val_mae: 1344.0656\n",
      "Epoch 433/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 50.1131 - mae: 50.7918 - val_loss: 1198.1804 - val_mae: 1198.8735\n",
      "Epoch 434/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 57.3813 - mae: 58.0637 - val_loss: 1251.4640 - val_mae: 1252.1571\n",
      "Epoch 435/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 47.4646 - mae: 48.1452 - val_loss: 1622.1529 - val_mae: 1622.8461\n",
      "Epoch 436/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 43.1278 - mae: 43.8082 - val_loss: 1584.2904 - val_mae: 1584.9835\n",
      "Epoch 437/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 43.2526 - mae: 43.9325 - val_loss: 1215.7641 - val_mae: 1216.4569\n",
      "Epoch 438/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 55.1860 - mae: 55.8713 - val_loss: 1349.0376 - val_mae: 1349.7308\n",
      "Epoch 439/1000\n",
      "1461/1461 [==============================] - 0s 283us/step - loss: 52.5591 - mae: 53.2409 - val_loss: 1697.3532 - val_mae: 1698.0464\n",
      "Epoch 440/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 275us/step - loss: 48.2973 - mae: 48.9745 - val_loss: 1556.5710 - val_mae: 1557.2643\n",
      "Epoch 441/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 46.5459 - mae: 47.2259 - val_loss: 1574.3008 - val_mae: 1574.9939\n",
      "Epoch 442/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 54.0677 - mae: 54.7513 - val_loss: 1351.9932 - val_mae: 1352.6863\n",
      "Epoch 443/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 53.5391 - mae: 54.2232 - val_loss: 1435.3296 - val_mae: 1436.0229\n",
      "Epoch 444/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 46.9495 - mae: 47.6299 - val_loss: 1608.0868 - val_mae: 1608.7800\n",
      "Epoch 445/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 56.0720 - mae: 56.7551 - val_loss: 1400.1884 - val_mae: 1400.8815\n",
      "Epoch 446/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 42.7529 - mae: 43.4283 - val_loss: 1430.4374 - val_mae: 1431.1306\n",
      "Epoch 447/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 48.3607 - mae: 49.0436 - val_loss: 1386.6184 - val_mae: 1387.3115\n",
      "Epoch 448/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 53.9161 - mae: 54.5985 - val_loss: 1534.5885 - val_mae: 1535.2812\n",
      "Epoch 449/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 57.1355 - mae: 57.8168 - val_loss: 1348.0425 - val_mae: 1348.7343\n",
      "Epoch 450/1000\n",
      "1461/1461 [==============================] - 0s 292us/step - loss: 47.4801 - mae: 48.1637 - val_loss: 1438.3371 - val_mae: 1439.0300\n",
      "Epoch 451/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 39.3505 - mae: 40.0309 - val_loss: 1535.2921 - val_mae: 1535.9851\n",
      "Epoch 452/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 48.1778 - mae: 48.8608 - val_loss: 1394.1638 - val_mae: 1394.8569\n",
      "Epoch 453/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 68.6457 - mae: 69.3273 - val_loss: 1336.0179 - val_mae: 1336.7106\n",
      "Epoch 454/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 54.5833 - mae: 55.2620 - val_loss: 1478.0557 - val_mae: 1478.7488\n",
      "Epoch 455/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 44.5332 - mae: 45.2158 - val_loss: 1508.1234 - val_mae: 1508.8168\n",
      "Epoch 456/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 45.1013 - mae: 45.7810 - val_loss: 1902.5523 - val_mae: 1903.2455\n",
      "Epoch 457/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 51.6261 - mae: 52.3086 - val_loss: 1309.3088 - val_mae: 1310.0020\n",
      "Epoch 458/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 48.5559 - mae: 49.2348 - val_loss: 1422.9250 - val_mae: 1423.6182\n",
      "Epoch 459/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 51.6823 - mae: 52.3632 - val_loss: 1392.6853 - val_mae: 1393.3785\n",
      "Epoch 460/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 41.4826 - mae: 42.1603 - val_loss: 1592.6794 - val_mae: 1593.3726\n",
      "Epoch 461/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 43.8358 - mae: 44.5141 - val_loss: 1608.9981 - val_mae: 1609.6915\n",
      "Epoch 462/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 53.8115 - mae: 54.4969 - val_loss: 1214.3752 - val_mae: 1215.0681\n",
      "Epoch 463/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 48.3735 - mae: 49.0567 - val_loss: 1271.4892 - val_mae: 1272.1820\n",
      "Epoch 464/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 54.3070 - mae: 54.9918 - val_loss: 1353.4630 - val_mae: 1354.1562\n",
      "Epoch 465/1000\n",
      "1461/1461 [==============================] - 0s 292us/step - loss: 52.1350 - mae: 52.8140 - val_loss: 1501.6243 - val_mae: 1502.3175\n",
      "Epoch 466/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 46.5349 - mae: 47.2144 - val_loss: 1438.0545 - val_mae: 1438.7477\n",
      "Epoch 467/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 45.5851 - mae: 46.2682 - val_loss: 1485.9055 - val_mae: 1486.5986\n",
      "Epoch 468/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 52.7220 - mae: 53.4060 - val_loss: 1717.0125 - val_mae: 1717.7059\n",
      "Epoch 469/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 49.8086 - mae: 50.4913 - val_loss: 1389.9568 - val_mae: 1390.6499\n",
      "Epoch 470/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 39.0833 - mae: 39.7648 - val_loss: 1190.4691 - val_mae: 1191.1621\n",
      "Epoch 471/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 50.5192 - mae: 51.1990 - val_loss: 1572.5007 - val_mae: 1573.1938\n",
      "Epoch 472/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 49.6991 - mae: 50.3810 - val_loss: 1390.4391 - val_mae: 1391.1321\n",
      "Epoch 473/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 38.4263 - mae: 39.1093 - val_loss: 1429.1705 - val_mae: 1429.8633\n",
      "Epoch 474/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 49.2341 - mae: 49.9160 - val_loss: 1278.9585 - val_mae: 1279.6517\n",
      "Epoch 475/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 46.1774 - mae: 46.8594 - val_loss: 1294.0515 - val_mae: 1294.7435\n",
      "Epoch 476/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 40.9527 - mae: 41.6348 - val_loss: 1303.1996 - val_mae: 1303.8928\n",
      "Epoch 477/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 56.8529 - mae: 57.5338 - val_loss: 1268.2297 - val_mae: 1268.9229\n",
      "Epoch 478/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 52.9840 - mae: 53.6672 - val_loss: 1561.5230 - val_mae: 1562.2162\n",
      "Epoch 479/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 38.3357 - mae: 39.0147 - val_loss: 1458.8617 - val_mae: 1459.5551\n",
      "Epoch 480/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 45.0127 - mae: 45.6956 - val_loss: 1381.0518 - val_mae: 1381.7448\n",
      "Epoch 481/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 52.6089 - mae: 53.2921 - val_loss: 1367.5089 - val_mae: 1368.2021\n",
      "Epoch 482/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 69.1558 - mae: 69.8415 - val_loss: 1606.6814 - val_mae: 1607.3746\n",
      "Epoch 483/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 55.3027 - mae: 55.9844 - val_loss: 1331.1935 - val_mae: 1331.8867\n",
      "Epoch 484/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 66.7113 - mae: 67.3958 - val_loss: 1348.1887 - val_mae: 1348.8817\n",
      "Epoch 485/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 53.2558 - mae: 53.9383 - val_loss: 1598.9339 - val_mae: 1599.6272\n",
      "Epoch 486/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 41.1423 - mae: 41.8207 - val_loss: 1403.3650 - val_mae: 1404.0582\n",
      "Epoch 487/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 54.4427 - mae: 55.1256 - val_loss: 1421.2622 - val_mae: 1421.9553\n",
      "Epoch 488/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 46.8018 - mae: 47.4853 - val_loss: 1514.7758 - val_mae: 1515.4689\n",
      "Epoch 489/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 43.5620 - mae: 44.2470 - val_loss: 1391.4987 - val_mae: 1392.1919\n",
      "Epoch 490/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 41.7660 - mae: 42.4482 - val_loss: 1328.9556 - val_mae: 1329.6487\n",
      "Epoch 491/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 54.3620 - mae: 55.0437 - val_loss: 1335.1758 - val_mae: 1335.8688\n",
      "Epoch 492/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 45.9322 - mae: 46.6155 - val_loss: 1401.0318 - val_mae: 1401.7251\n",
      "Epoch 493/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 48.8904 - mae: 49.5751 - val_loss: 1439.3051 - val_mae: 1439.9983\n",
      "Epoch 494/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 55.2076 - mae: 55.8892 - val_loss: 1213.6313 - val_mae: 1214.3243\n",
      "Epoch 495/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 268us/step - loss: 40.7178 - mae: 41.3969 - val_loss: 1451.6900 - val_mae: 1452.3832\n",
      "Epoch 496/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 47.5252 - mae: 48.2086 - val_loss: 1322.4173 - val_mae: 1323.1105\n",
      "Epoch 497/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 54.1272 - mae: 54.8077 - val_loss: 1737.9103 - val_mae: 1738.6027\n",
      "Epoch 498/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 49.8568 - mae: 50.5379 - val_loss: 1333.8839 - val_mae: 1334.5771\n",
      "Epoch 499/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 48.4418 - mae: 49.1214 - val_loss: 1505.1316 - val_mae: 1505.8247\n",
      "Epoch 500/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 41.3382 - mae: 42.0201 - val_loss: 1612.2580 - val_mae: 1612.9512\n",
      "Epoch 501/1000\n",
      "1461/1461 [==============================] - 0s 281us/step - loss: 47.3599 - mae: 48.0402 - val_loss: 1579.1585 - val_mae: 1579.8518\n",
      "Epoch 502/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 74.9895 - mae: 75.6752 - val_loss: 1424.7643 - val_mae: 1425.4574\n",
      "Epoch 503/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 68.4529 - mae: 69.1392 - val_loss: 1177.5789 - val_mae: 1178.2719\n",
      "Epoch 504/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 54.1545 - mae: 54.8370 - val_loss: 1349.8399 - val_mae: 1350.5330\n",
      "Epoch 505/1000\n",
      "1461/1461 [==============================] - 0s 288us/step - loss: 44.7470 - mae: 45.4300 - val_loss: 1380.6998 - val_mae: 1381.3932\n",
      "Epoch 506/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 41.5064 - mae: 42.1863 - val_loss: 1477.9160 - val_mae: 1478.6091\n",
      "Epoch 507/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 41.7466 - mae: 42.4242 - val_loss: 1214.1067 - val_mae: 1214.8000\n",
      "Epoch 508/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 40.4088 - mae: 41.0850 - val_loss: 1449.5359 - val_mae: 1450.2292\n",
      "Epoch 509/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 55.6784 - mae: 56.3638 - val_loss: 1450.9539 - val_mae: 1451.6470\n",
      "Epoch 510/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 41.9274 - mae: 42.6085 - val_loss: 1442.4390 - val_mae: 1443.1322\n",
      "Epoch 511/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 46.0709 - mae: 46.7541 - val_loss: 1583.9275 - val_mae: 1584.6207\n",
      "Epoch 512/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 46.8538 - mae: 47.5351 - val_loss: 1386.3710 - val_mae: 1387.0642\n",
      "Epoch 513/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 45.9651 - mae: 46.6458 - val_loss: 1424.5423 - val_mae: 1425.2356\n",
      "Epoch 514/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 48.7328 - mae: 49.4127 - val_loss: 1038.9850 - val_mae: 1039.6777\n",
      "Epoch 515/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 51.2990 - mae: 51.9775 - val_loss: 1582.5468 - val_mae: 1583.2388\n",
      "Epoch 516/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 71.1452 - mae: 71.8278 - val_loss: 1557.1984 - val_mae: 1557.8904\n",
      "Epoch 517/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 42.6020 - mae: 43.2786 - val_loss: 1315.6370 - val_mae: 1316.3302\n",
      "Epoch 518/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 49.0349 - mae: 49.7197 - val_loss: 1449.3095 - val_mae: 1450.0027\n",
      "Epoch 519/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 36.8192 - mae: 37.4962 - val_loss: 1462.2433 - val_mae: 1462.9366\n",
      "Epoch 520/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 38.6134 - mae: 39.2955 - val_loss: 1492.7603 - val_mae: 1493.4535\n",
      "Epoch 521/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 44.0316 - mae: 44.7091 - val_loss: 1287.1157 - val_mae: 1287.8088\n",
      "Epoch 522/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 56.5419 - mae: 57.2251 - val_loss: 1419.8292 - val_mae: 1420.5225\n",
      "Epoch 523/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 49.9483 - mae: 50.6275 - val_loss: 1453.4898 - val_mae: 1454.1829\n",
      "Epoch 524/1000\n",
      "1461/1461 [==============================] - 0s 316us/step - loss: 45.4535 - mae: 46.1346 - val_loss: 1429.1165 - val_mae: 1429.8097\n",
      "Epoch 525/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 39.8189 - mae: 40.4977 - val_loss: 1704.4839 - val_mae: 1705.1771\n",
      "Epoch 526/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 44.2257 - mae: 44.9067 - val_loss: 1480.0874 - val_mae: 1480.7805\n",
      "Epoch 527/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 44.3283 - mae: 45.0083 - val_loss: 1535.4518 - val_mae: 1536.1449\n",
      "Epoch 528/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 59.7659 - mae: 60.4484 - val_loss: 1428.4350 - val_mae: 1429.1283\n",
      "Epoch 529/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 53.6370 - mae: 54.3201 - val_loss: 1468.4125 - val_mae: 1469.1057\n",
      "Epoch 530/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 37.5221 - mae: 38.2010 - val_loss: 1298.2863 - val_mae: 1298.9790\n",
      "Epoch 531/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 46.8049 - mae: 47.4819 - val_loss: 1335.8211 - val_mae: 1336.5143\n",
      "Epoch 532/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 46.8619 - mae: 47.5434 - val_loss: 1432.0283 - val_mae: 1432.7213\n",
      "Epoch 533/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 54.4637 - mae: 55.1398 - val_loss: 1479.2681 - val_mae: 1479.9613\n",
      "Epoch 534/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 59.9081 - mae: 60.5894 - val_loss: 1273.9844 - val_mae: 1274.6776\n",
      "Epoch 535/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 54.9779 - mae: 55.6594 - val_loss: 1530.1128 - val_mae: 1530.8059\n",
      "Epoch 536/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 48.0648 - mae: 48.7454 - val_loss: 1474.7443 - val_mae: 1475.4374\n",
      "Epoch 537/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 55.3091 - mae: 55.9943 - val_loss: 1567.2454 - val_mae: 1567.9386\n",
      "Epoch 538/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 45.7854 - mae: 46.4670 - val_loss: 1203.0070 - val_mae: 1203.7002\n",
      "Epoch 539/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 42.1694 - mae: 42.8495 - val_loss: 1533.7280 - val_mae: 1534.4211\n",
      "Epoch 540/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 47.3651 - mae: 48.0451 - val_loss: 1115.1338 - val_mae: 1115.8263\n",
      "Epoch 541/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 63.5512 - mae: 64.2345 - val_loss: 1130.7020 - val_mae: 1131.3951\n",
      "Epoch 542/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 48.1557 - mae: 48.8373 - val_loss: 1647.0817 - val_mae: 1647.7737\n",
      "Epoch 543/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 46.6573 - mae: 47.3371 - val_loss: 1407.7627 - val_mae: 1408.4553\n",
      "Epoch 544/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 40.9014 - mae: 41.5803 - val_loss: 1479.2849 - val_mae: 1479.9781\n",
      "Epoch 545/1000\n",
      "1461/1461 [==============================] - 0s 292us/step - loss: 47.9325 - mae: 48.6170 - val_loss: 1400.3768 - val_mae: 1401.0695\n",
      "Epoch 546/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 40.5363 - mae: 41.2149 - val_loss: 1223.0072 - val_mae: 1223.7001\n",
      "Epoch 547/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 46.4751 - mae: 47.1537 - val_loss: 1258.9381 - val_mae: 1259.6312\n",
      "Epoch 548/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 49.3378 - mae: 50.0164 - val_loss: 1503.9967 - val_mae: 1504.6899\n",
      "Epoch 549/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 39.0183 - mae: 39.6969 - val_loss: 1448.2165 - val_mae: 1448.9097\n",
      "Epoch 550/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 271us/step - loss: 51.8968 - mae: 52.5753 - val_loss: 1534.8953 - val_mae: 1535.5885\n",
      "Epoch 551/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 42.9746 - mae: 43.6554 - val_loss: 1181.9900 - val_mae: 1182.6831\n",
      "Epoch 552/1000\n",
      "1461/1461 [==============================] - 0s 294us/step - loss: 47.6654 - mae: 48.3470 - val_loss: 1386.2874 - val_mae: 1386.9806\n",
      "Epoch 553/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 39.5461 - mae: 40.2230 - val_loss: 1323.2321 - val_mae: 1323.9253\n",
      "Epoch 554/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 40.4807 - mae: 41.1654 - val_loss: 1378.9260 - val_mae: 1379.6191\n",
      "Epoch 555/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 45.1929 - mae: 45.8736 - val_loss: 1430.8745 - val_mae: 1431.5667\n",
      "Epoch 556/1000\n",
      "1461/1461 [==============================] - 0s 290us/step - loss: 41.4515 - mae: 42.1312 - val_loss: 1452.4545 - val_mae: 1453.1476\n",
      "Epoch 557/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 45.0081 - mae: 45.6890 - val_loss: 1149.6709 - val_mae: 1150.3641\n",
      "Epoch 558/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 49.6163 - mae: 50.2992 - val_loss: 1336.8566 - val_mae: 1337.5499\n",
      "Epoch 559/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 39.8817 - mae: 40.5633 - val_loss: 1324.1342 - val_mae: 1324.8274\n",
      "Epoch 560/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 37.6254 - mae: 38.3025 - val_loss: 1393.1863 - val_mae: 1393.8793\n",
      "Epoch 561/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 45.0987 - mae: 45.7806 - val_loss: 1503.1794 - val_mae: 1503.8726\n",
      "Epoch 562/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 50.4086 - mae: 51.0925 - val_loss: 1543.5398 - val_mae: 1544.2330\n",
      "Epoch 563/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 43.8290 - mae: 44.5108 - val_loss: 1294.0328 - val_mae: 1294.7261\n",
      "Epoch 564/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 49.7243 - mae: 50.4070 - val_loss: 1359.0172 - val_mae: 1359.7102\n",
      "Epoch 565/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 48.2563 - mae: 48.9353 - val_loss: 1558.0354 - val_mae: 1558.7286\n",
      "Epoch 566/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 50.4300 - mae: 51.1083 - val_loss: 1553.6070 - val_mae: 1554.2994\n",
      "Epoch 567/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 41.7123 - mae: 42.3856 - val_loss: 1377.9724 - val_mae: 1378.6655\n",
      "Epoch 568/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 40.7354 - mae: 41.4125 - val_loss: 1354.0652 - val_mae: 1354.7582\n",
      "Epoch 569/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 41.1715 - mae: 41.8512 - val_loss: 1468.1572 - val_mae: 1468.8505\n",
      "Epoch 570/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 39.9256 - mae: 40.6054 - val_loss: 1569.1099 - val_mae: 1569.8031\n",
      "Epoch 571/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 45.3008 - mae: 45.9799 - val_loss: 1417.3887 - val_mae: 1418.0817\n",
      "Epoch 572/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 42.5005 - mae: 43.1791 - val_loss: 1322.8747 - val_mae: 1323.5679\n",
      "Epoch 573/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 41.4238 - mae: 42.1060 - val_loss: 1191.0406 - val_mae: 1191.7332\n",
      "Epoch 574/1000\n",
      "1461/1461 [==============================] - 0s 305us/step - loss: 44.3694 - mae: 45.0484 - val_loss: 1606.9691 - val_mae: 1607.6622\n",
      "Epoch 575/1000\n",
      "1461/1461 [==============================] - 0s 281us/step - loss: 48.4594 - mae: 49.1385 - val_loss: 1650.0497 - val_mae: 1650.7428\n",
      "Epoch 576/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 64.8079 - mae: 65.4942 - val_loss: 1378.0713 - val_mae: 1378.7640\n",
      "Epoch 577/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 49.9521 - mae: 50.6361 - val_loss: 1423.3489 - val_mae: 1424.0421\n",
      "Epoch 578/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 53.7527 - mae: 54.4377 - val_loss: 1422.6222 - val_mae: 1423.3154\n",
      "Epoch 579/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 48.4489 - mae: 49.1299 - val_loss: 1684.3624 - val_mae: 1685.0557\n",
      "Epoch 580/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 46.0288 - mae: 46.7110 - val_loss: 1468.0986 - val_mae: 1468.7919\n",
      "Epoch 581/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 42.2553 - mae: 42.9331 - val_loss: 1413.9050 - val_mae: 1414.5981\n",
      "Epoch 582/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 50.5883 - mae: 51.2711 - val_loss: 1580.9576 - val_mae: 1581.6509\n",
      "Epoch 583/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 38.9870 - mae: 39.6684 - val_loss: 1263.7962 - val_mae: 1264.4894\n",
      "Epoch 584/1000\n",
      "1461/1461 [==============================] - 0s 290us/step - loss: 47.0594 - mae: 47.7385 - val_loss: 1604.1521 - val_mae: 1604.8444\n",
      "Epoch 585/1000\n",
      "1461/1461 [==============================] - 0s 281us/step - loss: 48.3367 - mae: 49.0217 - val_loss: 1350.3921 - val_mae: 1351.0840\n",
      "Epoch 586/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 38.0243 - mae: 38.7017 - val_loss: 1397.8693 - val_mae: 1398.5625\n",
      "Epoch 587/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 36.9271 - mae: 37.6061 - val_loss: 1445.7820 - val_mae: 1446.4752\n",
      "Epoch 588/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 43.9291 - mae: 44.6123 - val_loss: 1374.9036 - val_mae: 1375.5970\n",
      "Epoch 589/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 59.8187 - mae: 60.4999 - val_loss: 1438.2703 - val_mae: 1438.9636\n",
      "Epoch 590/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 38.3331 - mae: 39.0104 - val_loss: 1314.4701 - val_mae: 1315.1631\n",
      "Epoch 591/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 38.9733 - mae: 39.6550 - val_loss: 1300.5083 - val_mae: 1301.2015\n",
      "Epoch 592/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 44.1008 - mae: 44.7810 - val_loss: 1405.5254 - val_mae: 1406.2178\n",
      "Epoch 593/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 55.5910 - mae: 56.2731 - val_loss: 1305.2494 - val_mae: 1305.9420\n",
      "Epoch 594/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 49.2625 - mae: 49.9437 - val_loss: 1604.6557 - val_mae: 1605.3490\n",
      "Epoch 595/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 42.5784 - mae: 43.2586 - val_loss: 1688.0304 - val_mae: 1688.7235\n",
      "Epoch 596/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 55.0780 - mae: 55.7598 - val_loss: 1537.8094 - val_mae: 1538.5026\n",
      "Epoch 597/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 39.1045 - mae: 39.7832 - val_loss: 1405.7702 - val_mae: 1406.4634\n",
      "Epoch 598/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 48.0774 - mae: 48.7585 - val_loss: 1190.3547 - val_mae: 1191.0479\n",
      "Epoch 599/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 44.4395 - mae: 45.1214 - val_loss: 1360.7142 - val_mae: 1361.4073\n",
      "Epoch 600/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 43.3834 - mae: 44.0651 - val_loss: 1609.1831 - val_mae: 1609.8756\n",
      "Epoch 601/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 39.5906 - mae: 40.2675 - val_loss: 1375.5917 - val_mae: 1376.2849\n",
      "Epoch 602/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 39.6220 - mae: 40.3026 - val_loss: 1158.4924 - val_mae: 1159.1855\n",
      "Epoch 603/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 44.1459 - mae: 44.8287 - val_loss: 1385.1502 - val_mae: 1385.8433\n",
      "Epoch 604/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 44.1346 - mae: 44.8178 - val_loss: 1354.1125 - val_mae: 1354.8044\n",
      "Epoch 605/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 291us/step - loss: 43.2786 - mae: 43.9573 - val_loss: 1323.8246 - val_mae: 1324.5179\n",
      "Epoch 606/1000\n",
      "1461/1461 [==============================] - 0s 321us/step - loss: 40.5867 - mae: 41.2649 - val_loss: 1180.6646 - val_mae: 1181.3578\n",
      "Epoch 607/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 51.1649 - mae: 51.8440 - val_loss: 1362.2084 - val_mae: 1362.9016\n",
      "Epoch 608/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 43.9912 - mae: 44.6705 - val_loss: 1304.3828 - val_mae: 1305.0760\n",
      "Epoch 609/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 41.9405 - mae: 42.6169 - val_loss: 1600.7054 - val_mae: 1601.3979\n",
      "Epoch 610/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 44.0052 - mae: 44.6856 - val_loss: 1372.2482 - val_mae: 1372.9415\n",
      "Epoch 611/1000\n",
      "1461/1461 [==============================] - 0s 289us/step - loss: 51.3690 - mae: 52.0476 - val_loss: 1391.7407 - val_mae: 1392.4337\n",
      "Epoch 612/1000\n",
      "1461/1461 [==============================] - 0s 290us/step - loss: 42.6467 - mae: 43.3264 - val_loss: 1378.1315 - val_mae: 1378.8246\n",
      "Epoch 613/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 44.4332 - mae: 45.1130 - val_loss: 1539.6297 - val_mae: 1540.3228\n",
      "Epoch 614/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 37.8570 - mae: 38.5316 - val_loss: 1187.8686 - val_mae: 1188.5618\n",
      "Epoch 615/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 42.8708 - mae: 43.5510 - val_loss: 1341.6298 - val_mae: 1342.3229\n",
      "Epoch 616/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 54.4843 - mae: 55.1646 - val_loss: 1573.5163 - val_mae: 1574.2095\n",
      "Epoch 617/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 53.3669 - mae: 54.0477 - val_loss: 1142.3035 - val_mae: 1142.9965\n",
      "Epoch 618/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 51.9170 - mae: 52.5992 - val_loss: 1381.8217 - val_mae: 1382.5149\n",
      "Epoch 619/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 51.1255 - mae: 51.8090 - val_loss: 1135.5141 - val_mae: 1136.2072\n",
      "Epoch 620/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 44.2752 - mae: 44.9590 - val_loss: 1513.4343 - val_mae: 1514.1276\n",
      "Epoch 621/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 41.1121 - mae: 41.7904 - val_loss: 1523.7185 - val_mae: 1524.4117\n",
      "Epoch 622/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 46.2720 - mae: 46.9473 - val_loss: 1405.3759 - val_mae: 1406.0690\n",
      "Epoch 623/1000\n",
      "1461/1461 [==============================] - 0s 291us/step - loss: 40.3842 - mae: 41.0637 - val_loss: 1296.1718 - val_mae: 1296.8649\n",
      "Epoch 624/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 42.9536 - mae: 43.6365 - val_loss: 1417.5557 - val_mae: 1418.2489\n",
      "Epoch 625/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 57.0985 - mae: 57.7809 - val_loss: 1854.1504 - val_mae: 1854.8435\n",
      "Epoch 626/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 62.8077 - mae: 63.4896 - val_loss: 1292.2597 - val_mae: 1292.9528\n",
      "Epoch 627/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 43.8246 - mae: 44.5030 - val_loss: 1512.0601 - val_mae: 1512.7534\n",
      "Epoch 628/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 46.2760 - mae: 46.9620 - val_loss: 1356.0703 - val_mae: 1356.7633\n",
      "Epoch 629/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 43.3517 - mae: 44.0304 - val_loss: 1229.3969 - val_mae: 1230.0901\n",
      "Epoch 630/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 39.3799 - mae: 40.0576 - val_loss: 1384.3022 - val_mae: 1384.9954\n",
      "Epoch 631/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 49.3972 - mae: 50.0762 - val_loss: 1416.2729 - val_mae: 1416.9662\n",
      "Epoch 632/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 47.5097 - mae: 48.1908 - val_loss: 1286.6711 - val_mae: 1287.3643\n",
      "Epoch 633/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 58.9865 - mae: 59.6690 - val_loss: 1086.4316 - val_mae: 1087.1248\n",
      "Epoch 634/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 49.7163 - mae: 50.4000 - val_loss: 1343.8739 - val_mae: 1344.5665\n",
      "Epoch 635/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 45.4568 - mae: 46.1378 - val_loss: 1376.6999 - val_mae: 1377.3931\n",
      "Epoch 636/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 43.5399 - mae: 44.2188 - val_loss: 1243.9954 - val_mae: 1244.6886\n",
      "Epoch 637/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 56.2422 - mae: 56.9211 - val_loss: 1337.3295 - val_mae: 1338.0227\n",
      "Epoch 638/1000\n",
      "1461/1461 [==============================] - 0s 281us/step - loss: 42.1679 - mae: 42.8440 - val_loss: 1317.6500 - val_mae: 1318.3433\n",
      "Epoch 639/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 42.6858 - mae: 43.3633 - val_loss: 1621.6638 - val_mae: 1622.3568\n",
      "Epoch 640/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 48.3051 - mae: 48.9870 - val_loss: 1281.8588 - val_mae: 1282.5520\n",
      "Epoch 641/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 51.3052 - mae: 51.9884 - val_loss: 1508.2519 - val_mae: 1508.9443\n",
      "Epoch 642/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 47.8247 - mae: 48.5060 - val_loss: 1422.3446 - val_mae: 1423.0378\n",
      "Epoch 643/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 39.0594 - mae: 39.7393 - val_loss: 1348.2826 - val_mae: 1348.9758\n",
      "Epoch 644/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 38.9335 - mae: 39.6111 - val_loss: 1281.3546 - val_mae: 1282.0479\n",
      "Epoch 645/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 41.7514 - mae: 42.4292 - val_loss: 1495.5986 - val_mae: 1496.2905\n",
      "Epoch 646/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 35.4393 - mae: 36.1149 - val_loss: 1197.8785 - val_mae: 1198.5701\n",
      "Epoch 647/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 44.6177 - mae: 45.2965 - val_loss: 1526.8291 - val_mae: 1527.5222\n",
      "Epoch 648/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 43.8473 - mae: 44.5289 - val_loss: 1178.6747 - val_mae: 1179.3678\n",
      "Epoch 649/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 41.6125 - mae: 42.2950 - val_loss: 1578.2650 - val_mae: 1578.9581\n",
      "Epoch 650/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 43.3197 - mae: 43.9953 - val_loss: 1467.1661 - val_mae: 1467.8591\n",
      "Epoch 651/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 37.6227 - mae: 38.3024 - val_loss: 1383.9124 - val_mae: 1384.6044\n",
      "Epoch 652/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 48.2028 - mae: 48.8835 - val_loss: 1334.4714 - val_mae: 1335.1639\n",
      "Epoch 653/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 52.1233 - mae: 52.8042 - val_loss: 1221.9921 - val_mae: 1222.6852\n",
      "Epoch 654/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 39.2786 - mae: 39.9575 - val_loss: 1331.9175 - val_mae: 1332.6107\n",
      "Epoch 655/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 36.4922 - mae: 37.1673 - val_loss: 1457.8783 - val_mae: 1458.5714\n",
      "Epoch 656/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 41.1495 - mae: 41.8299 - val_loss: 1268.9931 - val_mae: 1269.6860\n",
      "Epoch 657/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 49.4275 - mae: 50.1118 - val_loss: 1513.1758 - val_mae: 1513.8689\n",
      "Epoch 658/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 42.3665 - mae: 43.0484 - val_loss: 1293.5990 - val_mae: 1294.2922\n",
      "Epoch 659/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 51.9259 - mae: 52.6049 - val_loss: 1425.4763 - val_mae: 1426.1694\n",
      "Epoch 660/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 267us/step - loss: 44.5856 - mae: 45.2686 - val_loss: 1503.7743 - val_mae: 1504.4678\n",
      "Epoch 661/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 46.5961 - mae: 47.2780 - val_loss: 1376.5300 - val_mae: 1377.2231\n",
      "Epoch 662/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 51.8040 - mae: 52.4850 - val_loss: 1329.4654 - val_mae: 1330.1584\n",
      "Epoch 663/1000\n",
      "1461/1461 [==============================] - 0s 297us/step - loss: 46.8774 - mae: 47.5586 - val_loss: 1216.6735 - val_mae: 1217.3666\n",
      "Epoch 664/1000\n",
      "1461/1461 [==============================] - 0s 281us/step - loss: 43.2123 - mae: 43.8957 - val_loss: 1048.3337 - val_mae: 1049.0253\n",
      "Epoch 665/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 48.3119 - mae: 48.9925 - val_loss: 1477.5016 - val_mae: 1478.1948\n",
      "Epoch 666/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 44.8187 - mae: 45.4986 - val_loss: 1374.8642 - val_mae: 1375.5574\n",
      "Epoch 667/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 38.0735 - mae: 38.7552 - val_loss: 1610.1875 - val_mae: 1610.8806\n",
      "Epoch 668/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 55.1603 - mae: 55.8415 - val_loss: 1462.4195 - val_mae: 1463.1127\n",
      "Epoch 669/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 37.1053 - mae: 37.7825 - val_loss: 1462.0060 - val_mae: 1462.6993\n",
      "Epoch 670/1000\n",
      "1461/1461 [==============================] - 0s 290us/step - loss: 38.1731 - mae: 38.8507 - val_loss: 1365.1648 - val_mae: 1365.8580\n",
      "Epoch 671/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 48.7643 - mae: 49.4450 - val_loss: 1455.6818 - val_mae: 1456.3750\n",
      "Epoch 672/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 53.3161 - mae: 53.9977 - val_loss: 1216.8901 - val_mae: 1217.5833\n",
      "Epoch 673/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 51.8931 - mae: 52.5773 - val_loss: 1495.4825 - val_mae: 1496.1757\n",
      "Epoch 674/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 39.5013 - mae: 40.1837 - val_loss: 1553.1831 - val_mae: 1553.8762\n",
      "Epoch 675/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 39.6896 - mae: 40.3627 - val_loss: 1302.7519 - val_mae: 1303.4451\n",
      "Epoch 676/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 41.4367 - mae: 42.1160 - val_loss: 1411.4254 - val_mae: 1412.1188\n",
      "Epoch 677/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 41.7052 - mae: 42.3849 - val_loss: 1308.6967 - val_mae: 1309.3899\n",
      "Epoch 678/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 49.2649 - mae: 49.9491 - val_loss: 1513.1239 - val_mae: 1513.8171\n",
      "Epoch 679/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 45.4685 - mae: 46.1480 - val_loss: 1466.3092 - val_mae: 1467.0024\n",
      "Epoch 680/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 35.4739 - mae: 36.1493 - val_loss: 1293.2843 - val_mae: 1293.9767\n",
      "Epoch 681/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 36.3069 - mae: 36.9829 - val_loss: 1374.2129 - val_mae: 1374.9062\n",
      "Epoch 682/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 50.6416 - mae: 51.3185 - val_loss: 1482.9881 - val_mae: 1483.6814\n",
      "Epoch 683/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 39.7963 - mae: 40.4742 - val_loss: 1333.3179 - val_mae: 1334.0111\n",
      "Epoch 684/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 40.0404 - mae: 40.7170 - val_loss: 1356.9927 - val_mae: 1357.6859\n",
      "Epoch 685/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 42.9017 - mae: 43.5805 - val_loss: 1328.9830 - val_mae: 1329.6761\n",
      "Epoch 686/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 40.7749 - mae: 41.4555 - val_loss: 1159.9384 - val_mae: 1160.6313\n",
      "Epoch 687/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 50.5592 - mae: 51.2359 - val_loss: 1477.2388 - val_mae: 1477.9320\n",
      "Epoch 688/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 42.5096 - mae: 43.1875 - val_loss: 1427.5494 - val_mae: 1428.2427\n",
      "Epoch 689/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 42.4053 - mae: 43.0807 - val_loss: 1331.6293 - val_mae: 1332.3224\n",
      "Epoch 690/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 37.4476 - mae: 38.1255 - val_loss: 1250.6475 - val_mae: 1251.3408\n",
      "Epoch 691/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 39.9708 - mae: 40.6474 - val_loss: 1404.4880 - val_mae: 1405.1812\n",
      "Epoch 692/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 39.7037 - mae: 40.3815 - val_loss: 1316.0434 - val_mae: 1316.7366\n",
      "Epoch 693/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 43.6182 - mae: 44.2951 - val_loss: 1355.3103 - val_mae: 1356.0034\n",
      "Epoch 694/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 34.9065 - mae: 35.5784 - val_loss: 1238.9201 - val_mae: 1239.6134\n",
      "Epoch 695/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 53.5130 - mae: 54.1927 - val_loss: 1074.1008 - val_mae: 1074.7939\n",
      "Epoch 696/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 51.0830 - mae: 51.7617 - val_loss: 1561.0406 - val_mae: 1561.7336\n",
      "Epoch 697/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 39.8844 - mae: 40.5662 - val_loss: 1238.7981 - val_mae: 1239.4913\n",
      "Epoch 698/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 45.5031 - mae: 46.1812 - val_loss: 1530.1759 - val_mae: 1530.8690\n",
      "Epoch 699/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 41.2994 - mae: 41.9740 - val_loss: 1555.2141 - val_mae: 1555.9072\n",
      "Epoch 700/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 47.3738 - mae: 48.0536 - val_loss: 1039.0316 - val_mae: 1039.7246\n",
      "Epoch 701/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 46.9692 - mae: 47.6505 - val_loss: 1374.2178 - val_mae: 1374.9109\n",
      "Epoch 702/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 39.7013 - mae: 40.3797 - val_loss: 1521.4190 - val_mae: 1522.1122\n",
      "Epoch 703/1000\n",
      "1461/1461 [==============================] - 0s 298us/step - loss: 52.6692 - mae: 53.3495 - val_loss: 1287.1024 - val_mae: 1287.7955\n",
      "Epoch 704/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 37.9729 - mae: 38.6483 - val_loss: 1207.0495 - val_mae: 1207.7428\n",
      "Epoch 705/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 53.5858 - mae: 54.2643 - val_loss: 1286.5788 - val_mae: 1287.2717\n",
      "Epoch 706/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 34.1284 - mae: 34.8053 - val_loss: 1244.7789 - val_mae: 1245.4719\n",
      "Epoch 707/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 40.1559 - mae: 40.8337 - val_loss: 1141.5983 - val_mae: 1142.2916\n",
      "Epoch 708/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 48.1557 - mae: 48.8396 - val_loss: 1308.8242 - val_mae: 1309.5175\n",
      "Epoch 709/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 36.5195 - mae: 37.1983 - val_loss: 1515.0780 - val_mae: 1515.7712\n",
      "Epoch 710/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 38.7423 - mae: 39.4168 - val_loss: 1362.2186 - val_mae: 1362.9117\n",
      "Epoch 711/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 38.5748 - mae: 39.2551 - val_loss: 1320.4270 - val_mae: 1321.1201\n",
      "Epoch 712/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 45.0265 - mae: 45.7070 - val_loss: 1336.5222 - val_mae: 1337.2155\n",
      "Epoch 713/1000\n",
      "1461/1461 [==============================] - 0s 290us/step - loss: 35.9935 - mae: 36.6709 - val_loss: 1458.1032 - val_mae: 1458.7964\n",
      "Epoch 714/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 36.4498 - mae: 37.1263 - val_loss: 1274.4369 - val_mae: 1275.1302\n",
      "Epoch 715/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 275us/step - loss: 33.8273 - mae: 34.5005 - val_loss: 1427.4244 - val_mae: 1428.1174\n",
      "Epoch 716/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 37.7391 - mae: 38.4200 - val_loss: 1290.1908 - val_mae: 1290.8837\n",
      "Epoch 717/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 49.3149 - mae: 49.9967 - val_loss: 1254.0777 - val_mae: 1254.7695\n",
      "Epoch 718/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 37.6947 - mae: 38.3702 - val_loss: 1527.6277 - val_mae: 1528.3209\n",
      "Epoch 719/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 54.7440 - mae: 55.4255 - val_loss: 1201.3904 - val_mae: 1202.0836\n",
      "Epoch 720/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 44.2260 - mae: 44.9063 - val_loss: 1437.5833 - val_mae: 1438.2764\n",
      "Epoch 721/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 35.3559 - mae: 36.0320 - val_loss: 1269.3190 - val_mae: 1270.0121\n",
      "Epoch 722/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 48.8023 - mae: 49.4830 - val_loss: 1036.4706 - val_mae: 1037.1626\n",
      "Epoch 723/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 45.7082 - mae: 46.3859 - val_loss: 1262.4220 - val_mae: 1263.1152\n",
      "Epoch 724/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 43.8770 - mae: 44.5564 - val_loss: 1352.5059 - val_mae: 1353.1991\n",
      "Epoch 725/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 36.2898 - mae: 36.9708 - val_loss: 1466.8765 - val_mae: 1467.5697\n",
      "Epoch 726/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 46.2207 - mae: 46.9027 - val_loss: 1207.3656 - val_mae: 1208.0588\n",
      "Epoch 727/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 48.2870 - mae: 48.9676 - val_loss: 1464.5803 - val_mae: 1465.2736\n",
      "Epoch 728/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 39.5970 - mae: 40.2788 - val_loss: 1303.5323 - val_mae: 1304.2253\n",
      "Epoch 729/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 40.5543 - mae: 41.2382 - val_loss: 1384.0742 - val_mae: 1384.7675\n",
      "Epoch 730/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 38.3382 - mae: 39.0190 - val_loss: 1201.5890 - val_mae: 1202.2822\n",
      "Epoch 731/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 36.9094 - mae: 37.5864 - val_loss: 1328.1323 - val_mae: 1328.8254\n",
      "Epoch 732/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 34.9427 - mae: 35.6189 - val_loss: 1267.1308 - val_mae: 1267.8240\n",
      "Epoch 733/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 34.6407 - mae: 35.3221 - val_loss: 1538.0145 - val_mae: 1538.7076\n",
      "Epoch 734/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 39.6298 - mae: 40.3097 - val_loss: 1299.1886 - val_mae: 1299.8817\n",
      "Epoch 735/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 34.3837 - mae: 35.0595 - val_loss: 1241.9084 - val_mae: 1242.6017\n",
      "Epoch 736/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 38.7404 - mae: 39.4143 - val_loss: 1349.3300 - val_mae: 1350.0231\n",
      "Epoch 737/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 40.6959 - mae: 41.3742 - val_loss: 1416.8683 - val_mae: 1417.5616\n",
      "Epoch 738/1000\n",
      "1461/1461 [==============================] - 0s 281us/step - loss: 55.3797 - mae: 56.0585 - val_loss: 1591.1631 - val_mae: 1591.8563\n",
      "Epoch 739/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 57.3519 - mae: 58.0375 - val_loss: 1608.5423 - val_mae: 1609.2356\n",
      "Epoch 740/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 55.2758 - mae: 55.9600 - val_loss: 1249.5132 - val_mae: 1250.2063\n",
      "Epoch 741/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 48.1811 - mae: 48.8612 - val_loss: 1450.2744 - val_mae: 1450.9675\n",
      "Epoch 742/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 37.6069 - mae: 38.2831 - val_loss: 1121.7401 - val_mae: 1122.4332\n",
      "Epoch 743/1000\n",
      "1461/1461 [==============================] - 0s 300us/step - loss: 38.8546 - mae: 39.5298 - val_loss: 1440.1311 - val_mae: 1440.8242\n",
      "Epoch 744/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 43.0508 - mae: 43.7257 - val_loss: 1343.3055 - val_mae: 1343.9987\n",
      "Epoch 745/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 33.8323 - mae: 34.5065 - val_loss: 1459.9263 - val_mae: 1460.6195\n",
      "Epoch 746/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 39.9102 - mae: 40.5914 - val_loss: 1227.7780 - val_mae: 1228.4713\n",
      "Epoch 747/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 45.7197 - mae: 46.3979 - val_loss: 1346.4036 - val_mae: 1347.0967\n",
      "Epoch 748/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 39.3395 - mae: 40.0171 - val_loss: 1232.2093 - val_mae: 1232.9025\n",
      "Epoch 749/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 42.4440 - mae: 43.1224 - val_loss: 1512.1649 - val_mae: 1512.8580\n",
      "Epoch 750/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 53.4051 - mae: 54.0805 - val_loss: 1482.3034 - val_mae: 1482.9966\n",
      "Epoch 751/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 41.3328 - mae: 42.0126 - val_loss: 1274.6628 - val_mae: 1275.3562\n",
      "Epoch 752/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 42.9208 - mae: 43.6025 - val_loss: 1357.9442 - val_mae: 1358.6367\n",
      "Epoch 753/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 42.2726 - mae: 42.9491 - val_loss: 1399.3500 - val_mae: 1400.0435\n",
      "Epoch 754/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 46.3527 - mae: 47.0339 - val_loss: 1499.9716 - val_mae: 1500.6649\n",
      "Epoch 755/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 36.4579 - mae: 37.1352 - val_loss: 1416.0207 - val_mae: 1416.7140\n",
      "Epoch 756/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 33.9365 - mae: 34.6133 - val_loss: 1511.1852 - val_mae: 1511.8783\n",
      "Epoch 757/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 34.7838 - mae: 35.4575 - val_loss: 1409.3317 - val_mae: 1410.0248\n",
      "Epoch 758/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 52.5492 - mae: 53.2271 - val_loss: 1469.2479 - val_mae: 1469.9413\n",
      "Epoch 759/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 45.0931 - mae: 45.7708 - val_loss: 1422.2314 - val_mae: 1422.9247\n",
      "Epoch 760/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 41.3865 - mae: 42.0622 - val_loss: 1038.8343 - val_mae: 1039.5266\n",
      "Epoch 761/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 49.4034 - mae: 50.0842 - val_loss: 1325.4551 - val_mae: 1326.1484\n",
      "Epoch 762/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 40.1431 - mae: 40.8224 - val_loss: 1328.0519 - val_mae: 1328.7451\n",
      "Epoch 763/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 43.0811 - mae: 43.7590 - val_loss: 1197.4365 - val_mae: 1198.1298\n",
      "Epoch 764/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 41.8895 - mae: 42.5698 - val_loss: 1401.5327 - val_mae: 1402.2261\n",
      "Epoch 765/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 37.4414 - mae: 38.1206 - val_loss: 1075.3340 - val_mae: 1076.0258\n",
      "Epoch 766/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 47.0851 - mae: 47.7667 - val_loss: 1382.4600 - val_mae: 1383.1533\n",
      "Epoch 767/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 46.3152 - mae: 46.9923 - val_loss: 1423.2282 - val_mae: 1423.9202\n",
      "Epoch 768/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 37.5070 - mae: 38.1853 - val_loss: 1338.7390 - val_mae: 1339.4321\n",
      "Epoch 769/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 58.9110 - mae: 59.5962 - val_loss: 1195.1796 - val_mae: 1195.8728\n",
      "Epoch 770/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 274us/step - loss: 40.2651 - mae: 40.9436 - val_loss: 1278.9960 - val_mae: 1279.6893\n",
      "Epoch 771/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 40.0771 - mae: 40.7554 - val_loss: 1565.9137 - val_mae: 1566.6069\n",
      "Epoch 772/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 49.8939 - mae: 50.5758 - val_loss: 1326.3683 - val_mae: 1327.0614\n",
      "Epoch 773/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 34.0500 - mae: 34.7280 - val_loss: 1267.3958 - val_mae: 1268.0889\n",
      "Epoch 774/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 46.1713 - mae: 46.8480 - val_loss: 1582.0753 - val_mae: 1582.7686\n",
      "Epoch 775/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 42.8630 - mae: 43.5421 - val_loss: 1430.9723 - val_mae: 1431.6656\n",
      "Epoch 776/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 51.2062 - mae: 51.8855 - val_loss: 1237.2795 - val_mae: 1237.9724\n",
      "Epoch 777/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 48.2491 - mae: 48.9249 - val_loss: 1412.5523 - val_mae: 1413.2454\n",
      "Epoch 778/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 45.9082 - mae: 46.5902 - val_loss: 1382.7153 - val_mae: 1383.4082\n",
      "Epoch 779/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 44.0041 - mae: 44.6853 - val_loss: 1503.3807 - val_mae: 1504.0740\n",
      "Epoch 780/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 39.6006 - mae: 40.2825 - val_loss: 1222.7160 - val_mae: 1223.4077\n",
      "Epoch 781/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 49.1458 - mae: 49.8272 - val_loss: 1143.1461 - val_mae: 1143.8394\n",
      "Epoch 782/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 42.0468 - mae: 42.7209 - val_loss: 1242.5366 - val_mae: 1243.2291\n",
      "Epoch 783/1000\n",
      "1461/1461 [==============================] - 0s 293us/step - loss: 38.5189 - mae: 39.1976 - val_loss: 1566.4002 - val_mae: 1567.0934\n",
      "Epoch 784/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 46.2271 - mae: 46.9069 - val_loss: 1683.3263 - val_mae: 1684.0194\n",
      "Epoch 785/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 58.4878 - mae: 59.1702 - val_loss: 1293.4549 - val_mae: 1294.1479\n",
      "Epoch 786/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 45.0750 - mae: 45.7536 - val_loss: 1317.7866 - val_mae: 1318.4795\n",
      "Epoch 787/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 36.2168 - mae: 36.8932 - val_loss: 1324.9099 - val_mae: 1325.6023\n",
      "Epoch 788/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 42.2644 - mae: 42.9415 - val_loss: 1169.6398 - val_mae: 1170.3326\n",
      "Epoch 789/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 43.4061 - mae: 44.0851 - val_loss: 1268.5311 - val_mae: 1269.2241\n",
      "Epoch 790/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 59.8456 - mae: 60.5309 - val_loss: 1215.1735 - val_mae: 1215.8663\n",
      "Epoch 791/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 36.8564 - mae: 37.5335 - val_loss: 1331.0516 - val_mae: 1331.7448\n",
      "Epoch 792/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 33.4200 - mae: 34.0930 - val_loss: 1381.6107 - val_mae: 1382.3040\n",
      "Epoch 793/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 34.0408 - mae: 34.7182 - val_loss: 1257.5090 - val_mae: 1258.2023\n",
      "Epoch 794/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 35.1021 - mae: 35.7802 - val_loss: 1440.8049 - val_mae: 1441.4980\n",
      "Epoch 795/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 38.2014 - mae: 38.8747 - val_loss: 1555.4451 - val_mae: 1556.1383\n",
      "Epoch 796/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 53.1282 - mae: 53.8100 - val_loss: 1458.2052 - val_mae: 1458.8982\n",
      "Epoch 797/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 37.8101 - mae: 38.4883 - val_loss: 1518.8365 - val_mae: 1519.5298\n",
      "Epoch 798/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 38.5752 - mae: 39.2572 - val_loss: 1389.1601 - val_mae: 1389.8533\n",
      "Epoch 799/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 33.5913 - mae: 34.2688 - val_loss: 1473.7933 - val_mae: 1474.4865\n",
      "Epoch 800/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 44.0107 - mae: 44.6891 - val_loss: 1101.7750 - val_mae: 1102.4681\n",
      "Epoch 801/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 39.6595 - mae: 40.3372 - val_loss: 1513.7944 - val_mae: 1514.4877\n",
      "Epoch 802/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 34.4736 - mae: 35.1503 - val_loss: 1314.8581 - val_mae: 1315.5513\n",
      "Epoch 803/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 37.6198 - mae: 38.2996 - val_loss: 1176.8365 - val_mae: 1177.5295\n",
      "Epoch 804/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 40.9877 - mae: 41.6648 - val_loss: 1300.8187 - val_mae: 1301.5120\n",
      "Epoch 805/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 37.6101 - mae: 38.2863 - val_loss: 1277.0832 - val_mae: 1277.7765\n",
      "Epoch 806/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 49.4174 - mae: 50.1030 - val_loss: 1417.5672 - val_mae: 1418.2604\n",
      "Epoch 807/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 41.5367 - mae: 42.2132 - val_loss: 1223.9577 - val_mae: 1224.6508\n",
      "Epoch 808/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 33.9086 - mae: 34.5849 - val_loss: 1252.4590 - val_mae: 1253.1522\n",
      "Epoch 809/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 39.0966 - mae: 39.7765 - val_loss: 1552.3440 - val_mae: 1553.0372\n",
      "Epoch 810/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 38.4623 - mae: 39.1432 - val_loss: 1559.9669 - val_mae: 1560.6600\n",
      "Epoch 811/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 34.0261 - mae: 34.6946 - val_loss: 1418.1003 - val_mae: 1418.7936\n",
      "Epoch 812/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 40.1830 - mae: 40.8611 - val_loss: 1242.8819 - val_mae: 1243.5750\n",
      "Epoch 813/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 40.2755 - mae: 40.9562 - val_loss: 1257.3812 - val_mae: 1258.0737\n",
      "Epoch 814/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 42.4925 - mae: 43.1756 - val_loss: 1279.1810 - val_mae: 1279.8741\n",
      "Epoch 815/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 37.6713 - mae: 38.3503 - val_loss: 1534.0347 - val_mae: 1534.7280\n",
      "Epoch 816/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 38.9445 - mae: 39.6216 - val_loss: 1228.1473 - val_mae: 1228.8406\n",
      "Epoch 817/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 39.5820 - mae: 40.2615 - val_loss: 1305.0333 - val_mae: 1305.7266\n",
      "Epoch 818/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 36.5772 - mae: 37.2538 - val_loss: 1268.7915 - val_mae: 1269.4840\n",
      "Epoch 819/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 48.1895 - mae: 48.8709 - val_loss: 1325.1840 - val_mae: 1325.8772\n",
      "Epoch 820/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 42.4833 - mae: 43.1612 - val_loss: 1333.5912 - val_mae: 1334.2843\n",
      "Epoch 821/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 35.5332 - mae: 36.2097 - val_loss: 1316.5968 - val_mae: 1317.2900\n",
      "Epoch 822/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 46.1363 - mae: 46.8166 - val_loss: 1672.8012 - val_mae: 1673.4941\n",
      "Epoch 823/1000\n",
      "1461/1461 [==============================] - 0s 306us/step - loss: 43.6125 - mae: 44.2942 - val_loss: 1349.4679 - val_mae: 1350.1611\n",
      "Epoch 824/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 36.9251 - mae: 37.6039 - val_loss: 1456.1425 - val_mae: 1456.8357\n",
      "Epoch 825/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 271us/step - loss: 50.2406 - mae: 50.9179 - val_loss: 1197.3811 - val_mae: 1198.0745\n",
      "Epoch 826/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 47.2687 - mae: 47.9513 - val_loss: 1599.9716 - val_mae: 1600.6648\n",
      "Epoch 827/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 41.8654 - mae: 42.5464 - val_loss: 1441.9498 - val_mae: 1442.6432\n",
      "Epoch 828/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 36.5747 - mae: 37.2538 - val_loss: 1234.0635 - val_mae: 1234.7567\n",
      "Epoch 829/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 39.5116 - mae: 40.1894 - val_loss: 1478.6085 - val_mae: 1479.3018\n",
      "Epoch 830/1000\n",
      "1461/1461 [==============================] - 1s 360us/step - loss: 44.7420 - mae: 45.4223 - val_loss: 1063.4117 - val_mae: 1064.1049\n",
      "Epoch 831/1000\n",
      "1461/1461 [==============================] - 0s 307us/step - loss: 46.9673 - mae: 47.6491 - val_loss: 1315.9043 - val_mae: 1316.5974\n",
      "Epoch 832/1000\n",
      "1461/1461 [==============================] - 0s 293us/step - loss: 36.8177 - mae: 37.4911 - val_loss: 1206.9170 - val_mae: 1207.6102\n",
      "Epoch 833/1000\n",
      "1461/1461 [==============================] - 0s 283us/step - loss: 54.0135 - mae: 54.6969 - val_loss: 1558.8470 - val_mae: 1559.5402\n",
      "Epoch 834/1000\n",
      "1461/1461 [==============================] - 0s 324us/step - loss: 50.3404 - mae: 51.0204 - val_loss: 1195.2851 - val_mae: 1195.9783\n",
      "Epoch 835/1000\n",
      "1461/1461 [==============================] - 0s 311us/step - loss: 42.8100 - mae: 43.4899 - val_loss: 1508.8521 - val_mae: 1509.5453\n",
      "Epoch 836/1000\n",
      "1461/1461 [==============================] - 0s 297us/step - loss: 43.8702 - mae: 44.5520 - val_loss: 1316.6419 - val_mae: 1317.3351\n",
      "Epoch 837/1000\n",
      "1461/1461 [==============================] - 0s 302us/step - loss: 40.9993 - mae: 41.6746 - val_loss: 1410.3310 - val_mae: 1411.0242\n",
      "Epoch 838/1000\n",
      "1461/1461 [==============================] - 1s 352us/step - loss: 47.9043 - mae: 48.5847 - val_loss: 1209.3145 - val_mae: 1210.0076\n",
      "Epoch 839/1000\n",
      "1461/1461 [==============================] - 0s 291us/step - loss: 53.9098 - mae: 54.5915 - val_loss: 1494.5014 - val_mae: 1495.1946\n",
      "Epoch 840/1000\n",
      "1461/1461 [==============================] - 0s 287us/step - loss: 39.9487 - mae: 40.6256 - val_loss: 1549.9574 - val_mae: 1550.6499\n",
      "Epoch 841/1000\n",
      "1461/1461 [==============================] - 0s 302us/step - loss: 37.6074 - mae: 38.2887 - val_loss: 1439.0164 - val_mae: 1439.7097\n",
      "Epoch 842/1000\n",
      "1461/1461 [==============================] - 0s 283us/step - loss: 40.7896 - mae: 41.4753 - val_loss: 1355.4691 - val_mae: 1356.1622\n",
      "Epoch 843/1000\n",
      "1461/1461 [==============================] - 0s 296us/step - loss: 37.8026 - mae: 38.4853 - val_loss: 1382.3048 - val_mae: 1382.9980\n",
      "Epoch 844/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 35.5618 - mae: 36.2375 - val_loss: 1365.5272 - val_mae: 1366.2205\n",
      "Epoch 845/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 46.6108 - mae: 47.2950 - val_loss: 1374.8380 - val_mae: 1375.5311\n",
      "Epoch 846/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 38.7609 - mae: 39.4388 - val_loss: 1451.2741 - val_mae: 1451.9675\n",
      "Epoch 847/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 47.4732 - mae: 48.1554 - val_loss: 1353.4644 - val_mae: 1354.1576\n",
      "Epoch 848/1000\n",
      "1461/1461 [==============================] - 0s 281us/step - loss: 44.6286 - mae: 45.3094 - val_loss: 1409.5338 - val_mae: 1410.2269\n",
      "Epoch 849/1000\n",
      "1461/1461 [==============================] - 0s 294us/step - loss: 36.8264 - mae: 37.5021 - val_loss: 1421.4369 - val_mae: 1422.1302\n",
      "Epoch 850/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 39.0112 - mae: 39.6875 - val_loss: 1501.6055 - val_mae: 1502.2986\n",
      "Epoch 851/1000\n",
      "1461/1461 [==============================] - 0s 283us/step - loss: 39.3003 - mae: 39.9785 - val_loss: 1392.4949 - val_mae: 1393.1881\n",
      "Epoch 852/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 37.0076 - mae: 37.6852 - val_loss: 1337.8717 - val_mae: 1338.5649\n",
      "Epoch 853/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 42.5390 - mae: 43.2170 - val_loss: 1274.3848 - val_mae: 1275.0775\n",
      "Epoch 854/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 37.6403 - mae: 38.3198 - val_loss: 1493.0960 - val_mae: 1493.7892\n",
      "Epoch 855/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 44.9946 - mae: 45.6739 - val_loss: 1162.6806 - val_mae: 1163.3737\n",
      "Epoch 856/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 37.2369 - mae: 37.9162 - val_loss: 1112.1438 - val_mae: 1112.8369\n",
      "Epoch 857/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 41.2775 - mae: 41.9582 - val_loss: 1444.8651 - val_mae: 1445.5583\n",
      "Epoch 858/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 38.9200 - mae: 39.5999 - val_loss: 1476.6089 - val_mae: 1477.3021\n",
      "Epoch 859/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 39.0114 - mae: 39.6874 - val_loss: 1358.0383 - val_mae: 1358.7314\n",
      "Epoch 860/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 36.3962 - mae: 37.0720 - val_loss: 1411.3346 - val_mae: 1412.0278\n",
      "Epoch 861/1000\n",
      "1461/1461 [==============================] - 0s 304us/step - loss: 43.0658 - mae: 43.7436 - val_loss: 1534.5278 - val_mae: 1535.2208\n",
      "Epoch 862/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 33.0121 - mae: 33.6834 - val_loss: 1195.2733 - val_mae: 1195.9666\n",
      "Epoch 863/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 47.1170 - mae: 47.7976 - val_loss: 1505.1220 - val_mae: 1505.8153\n",
      "Epoch 864/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 38.6598 - mae: 39.3393 - val_loss: 1455.2720 - val_mae: 1455.9653\n",
      "Epoch 865/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 40.4990 - mae: 41.1767 - val_loss: 1285.3726 - val_mae: 1286.0657\n",
      "Epoch 866/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 46.0122 - mae: 46.6897 - val_loss: 1336.3728 - val_mae: 1337.0660\n",
      "Epoch 867/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 44.5672 - mae: 45.2499 - val_loss: 1485.7232 - val_mae: 1486.4164\n",
      "Epoch 868/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 35.8154 - mae: 36.4960 - val_loss: 1327.5553 - val_mae: 1328.2484\n",
      "Epoch 869/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 34.0248 - mae: 34.7032 - val_loss: 1336.2839 - val_mae: 1336.9767\n",
      "Epoch 870/1000\n",
      "1461/1461 [==============================] - 0s 297us/step - loss: 34.3475 - mae: 35.0273 - val_loss: 1376.4326 - val_mae: 1377.1257\n",
      "Epoch 871/1000\n",
      "1461/1461 [==============================] - 0s 292us/step - loss: 44.3513 - mae: 45.0312 - val_loss: 1159.5617 - val_mae: 1160.2550\n",
      "Epoch 872/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 45.6676 - mae: 46.3504 - val_loss: 1444.1413 - val_mae: 1444.8344\n",
      "Epoch 873/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 34.3995 - mae: 35.0803 - val_loss: 1304.1760 - val_mae: 1304.8690\n",
      "Epoch 874/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 37.1965 - mae: 37.8743 - val_loss: 1386.4566 - val_mae: 1387.1495\n",
      "Epoch 875/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 44.8522 - mae: 45.5308 - val_loss: 1495.9144 - val_mae: 1496.6077\n",
      "Epoch 876/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 40.7807 - mae: 41.4623 - val_loss: 1338.0647 - val_mae: 1338.7578\n",
      "Epoch 877/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 37.6820 - mae: 38.3646 - val_loss: 1443.4368 - val_mae: 1444.1301\n",
      "Epoch 878/1000\n",
      "1461/1461 [==============================] - 0s 289us/step - loss: 34.9919 - mae: 35.6753 - val_loss: 1458.5793 - val_mae: 1459.2725\n",
      "Epoch 879/1000\n",
      "1461/1461 [==============================] - 0s 310us/step - loss: 35.9810 - mae: 36.6636 - val_loss: 1468.4696 - val_mae: 1469.1628\n",
      "Epoch 880/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 313us/step - loss: 42.4603 - mae: 43.1430 - val_loss: 1375.8592 - val_mae: 1376.5516\n",
      "Epoch 881/1000\n",
      "1461/1461 [==============================] - 0s 310us/step - loss: 41.0841 - mae: 41.7682 - val_loss: 1326.1650 - val_mae: 1326.8580\n",
      "Epoch 882/1000\n",
      "1461/1461 [==============================] - 0s 296us/step - loss: 37.9660 - mae: 38.6417 - val_loss: 1274.2612 - val_mae: 1274.9539\n",
      "Epoch 883/1000\n",
      "1461/1461 [==============================] - 0s 316us/step - loss: 47.6545 - mae: 48.3341 - val_loss: 1374.1083 - val_mae: 1374.8015\n",
      "Epoch 884/1000\n",
      "1461/1461 [==============================] - 0s 324us/step - loss: 39.0321 - mae: 39.7177 - val_loss: 1183.2575 - val_mae: 1183.9504\n",
      "Epoch 885/1000\n",
      "1461/1461 [==============================] - 0s 305us/step - loss: 34.3422 - mae: 35.0220 - val_loss: 1427.0526 - val_mae: 1427.7457\n",
      "Epoch 886/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 37.0525 - mae: 37.7311 - val_loss: 1350.1308 - val_mae: 1350.8240\n",
      "Epoch 887/1000\n",
      "1461/1461 [==============================] - 0s 289us/step - loss: 32.6050 - mae: 33.2769 - val_loss: 1402.0094 - val_mae: 1402.7026\n",
      "Epoch 888/1000\n",
      "1461/1461 [==============================] - 0s 297us/step - loss: 39.9927 - mae: 40.6696 - val_loss: 1478.3246 - val_mae: 1479.0178\n",
      "Epoch 889/1000\n",
      "1461/1461 [==============================] - 0s 295us/step - loss: 35.5579 - mae: 36.2378 - val_loss: 1283.6660 - val_mae: 1284.3591\n",
      "Epoch 890/1000\n",
      "1461/1461 [==============================] - 0s 304us/step - loss: 37.2167 - mae: 37.8984 - val_loss: 1326.3800 - val_mae: 1327.0731\n",
      "Epoch 891/1000\n",
      "1461/1461 [==============================] - 0s 299us/step - loss: 34.7177 - mae: 35.3938 - val_loss: 1458.1488 - val_mae: 1458.8420\n",
      "Epoch 892/1000\n",
      "1461/1461 [==============================] - 0s 289us/step - loss: 37.9370 - mae: 38.6181 - val_loss: 1362.0736 - val_mae: 1362.7662\n",
      "Epoch 893/1000\n",
      "1461/1461 [==============================] - 0s 295us/step - loss: 35.4663 - mae: 36.1428 - val_loss: 1376.6540 - val_mae: 1377.3470\n",
      "Epoch 894/1000\n",
      "1461/1461 [==============================] - 0s 304us/step - loss: 39.6285 - mae: 40.3112 - val_loss: 1401.7758 - val_mae: 1402.4690\n",
      "Epoch 895/1000\n",
      "1461/1461 [==============================] - 1s 348us/step - loss: 49.9840 - mae: 50.6649 - val_loss: 971.0778 - val_mae: 971.7711\n",
      "Epoch 896/1000\n",
      "1461/1461 [==============================] - 0s 308us/step - loss: 51.5652 - mae: 52.2419 - val_loss: 1435.8978 - val_mae: 1436.5909\n",
      "Epoch 897/1000\n",
      "1461/1461 [==============================] - 0s 288us/step - loss: 47.1825 - mae: 47.8647 - val_loss: 1057.8986 - val_mae: 1058.5918\n",
      "Epoch 898/1000\n",
      "1461/1461 [==============================] - 0s 338us/step - loss: 52.5937 - mae: 53.2767 - val_loss: 1581.4098 - val_mae: 1582.1031\n",
      "Epoch 899/1000\n",
      "1461/1461 [==============================] - 0s 291us/step - loss: 42.4098 - mae: 43.0925 - val_loss: 1453.9170 - val_mae: 1454.6101\n",
      "Epoch 900/1000\n",
      "1461/1461 [==============================] - 0s 288us/step - loss: 38.4945 - mae: 39.1697 - val_loss: 1213.7250 - val_mae: 1214.4175\n",
      "Epoch 901/1000\n",
      "1461/1461 [==============================] - 0s 290us/step - loss: 36.3342 - mae: 37.0115 - val_loss: 1367.7433 - val_mae: 1368.4364\n",
      "Epoch 902/1000\n",
      "1461/1461 [==============================] - 0s 290us/step - loss: 36.2887 - mae: 36.9663 - val_loss: 1165.3273 - val_mae: 1166.0204\n",
      "Epoch 903/1000\n",
      "1461/1461 [==============================] - 0s 296us/step - loss: 38.3302 - mae: 39.0080 - val_loss: 1198.7382 - val_mae: 1199.4313\n",
      "Epoch 904/1000\n",
      "1461/1461 [==============================] - 0s 294us/step - loss: 37.6671 - mae: 38.3444 - val_loss: 1410.5798 - val_mae: 1411.2731\n",
      "Epoch 905/1000\n",
      "1461/1461 [==============================] - 0s 290us/step - loss: 38.8304 - mae: 39.5084 - val_loss: 1337.0081 - val_mae: 1337.7003\n",
      "Epoch 906/1000\n",
      "1461/1461 [==============================] - 0s 304us/step - loss: 42.0541 - mae: 42.7311 - val_loss: 1386.7444 - val_mae: 1387.4376\n",
      "Epoch 907/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 33.3193 - mae: 33.9951 - val_loss: 1492.0809 - val_mae: 1492.7740\n",
      "Epoch 908/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 38.1022 - mae: 38.7766 - val_loss: 1384.5443 - val_mae: 1385.2374\n",
      "Epoch 909/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 38.1786 - mae: 38.8546 - val_loss: 1459.9851 - val_mae: 1460.6774\n",
      "Epoch 910/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 45.2188 - mae: 45.8979 - val_loss: 1624.9529 - val_mae: 1625.6461\n",
      "Epoch 911/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 43.1430 - mae: 43.8195 - val_loss: 1214.0686 - val_mae: 1214.7618\n",
      "Epoch 912/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 31.0034 - mae: 31.6794 - val_loss: 1363.1187 - val_mae: 1363.8118\n",
      "Epoch 913/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 37.5304 - mae: 38.2061 - val_loss: 1192.8226 - val_mae: 1193.5157\n",
      "Epoch 914/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 45.2697 - mae: 45.9489 - val_loss: 1177.5758 - val_mae: 1178.2690\n",
      "Epoch 915/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 38.7749 - mae: 39.4549 - val_loss: 1303.7864 - val_mae: 1304.4796\n",
      "Epoch 916/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 38.9749 - mae: 39.6551 - val_loss: 1471.5480 - val_mae: 1472.2413\n",
      "Epoch 917/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 47.4584 - mae: 48.1414 - val_loss: 1327.2973 - val_mae: 1327.9905\n",
      "Epoch 918/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 39.1409 - mae: 39.8159 - val_loss: 1377.9852 - val_mae: 1378.6785\n",
      "Epoch 919/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 38.0579 - mae: 38.7349 - val_loss: 1430.0635 - val_mae: 1430.7566\n",
      "Epoch 920/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 37.6911 - mae: 38.3710 - val_loss: 1352.8282 - val_mae: 1353.5214\n",
      "Epoch 921/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 36.3650 - mae: 37.0466 - val_loss: 1044.3913 - val_mae: 1045.0845\n",
      "Epoch 922/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 44.7167 - mae: 45.3993 - val_loss: 1207.6269 - val_mae: 1208.3196\n",
      "Epoch 923/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 50.0533 - mae: 50.7325 - val_loss: 1290.0659 - val_mae: 1290.7589\n",
      "Epoch 924/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 33.8206 - mae: 34.4922 - val_loss: 1348.6970 - val_mae: 1349.3904\n",
      "Epoch 925/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 37.3865 - mae: 38.0614 - val_loss: 1307.2339 - val_mae: 1307.9270\n",
      "Epoch 926/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 35.8962 - mae: 36.5721 - val_loss: 1233.3249 - val_mae: 1234.0181\n",
      "Epoch 927/1000\n",
      "1461/1461 [==============================] - 0s 302us/step - loss: 40.2145 - mae: 40.8933 - val_loss: 1385.6622 - val_mae: 1386.3551\n",
      "Epoch 928/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 51.4447 - mae: 52.1262 - val_loss: 1344.7120 - val_mae: 1345.4052\n",
      "Epoch 929/1000\n",
      "1461/1461 [==============================] - 0s 319us/step - loss: 32.3021 - mae: 32.9797 - val_loss: 1262.9906 - val_mae: 1263.6827\n",
      "Epoch 930/1000\n",
      "1461/1461 [==============================] - 0s 302us/step - loss: 30.3497 - mae: 31.0264 - val_loss: 1388.2081 - val_mae: 1388.9012\n",
      "Epoch 931/1000\n",
      "1461/1461 [==============================] - 0s 325us/step - loss: 36.1110 - mae: 36.7838 - val_loss: 1414.5181 - val_mae: 1415.2113\n",
      "Epoch 932/1000\n",
      "1461/1461 [==============================] - 0s 298us/step - loss: 38.8976 - mae: 39.5751 - val_loss: 1208.3228 - val_mae: 1209.0160\n",
      "Epoch 933/1000\n",
      "1461/1461 [==============================] - 0s 294us/step - loss: 35.2413 - mae: 35.9209 - val_loss: 1339.6972 - val_mae: 1340.3904\n",
      "Epoch 934/1000\n",
      "1461/1461 [==============================] - 0s 298us/step - loss: 34.2442 - mae: 34.9180 - val_loss: 1205.4460 - val_mae: 1206.1390\n",
      "Epoch 935/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 279us/step - loss: 38.7398 - mae: 39.4195 - val_loss: 1309.2010 - val_mae: 1309.8940\n",
      "Epoch 936/1000\n",
      "1461/1461 [==============================] - 0s 299us/step - loss: 40.9871 - mae: 41.6686 - val_loss: 1462.9260 - val_mae: 1463.6189\n",
      "Epoch 937/1000\n",
      "1461/1461 [==============================] - 0s 295us/step - loss: 43.1689 - mae: 43.8495 - val_loss: 1471.9942 - val_mae: 1472.6874\n",
      "Epoch 938/1000\n",
      "1461/1461 [==============================] - 0s 295us/step - loss: 36.1865 - mae: 36.8651 - val_loss: 1396.9228 - val_mae: 1397.6161\n",
      "Epoch 939/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 43.1847 - mae: 43.8614 - val_loss: 1469.0230 - val_mae: 1469.7163\n",
      "Epoch 940/1000\n",
      "1461/1461 [==============================] - 0s 283us/step - loss: 44.3342 - mae: 45.0136 - val_loss: 1344.6473 - val_mae: 1345.3406\n",
      "Epoch 941/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 34.3126 - mae: 34.9933 - val_loss: 1309.9265 - val_mae: 1310.6195\n",
      "Epoch 942/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 40.4591 - mae: 41.1350 - val_loss: 1261.1074 - val_mae: 1261.8005\n",
      "Epoch 943/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 32.7952 - mae: 33.4695 - val_loss: 1276.9621 - val_mae: 1277.6554\n",
      "Epoch 944/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 36.4350 - mae: 37.1093 - val_loss: 1309.8481 - val_mae: 1310.5413\n",
      "Epoch 945/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 33.7514 - mae: 34.4252 - val_loss: 1377.0600 - val_mae: 1377.7533\n",
      "Epoch 946/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 35.5600 - mae: 36.2351 - val_loss: 1375.0379 - val_mae: 1375.7312\n",
      "Epoch 947/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 47.8397 - mae: 48.5216 - val_loss: 1248.7561 - val_mae: 1249.4493\n",
      "Epoch 948/1000\n",
      "1461/1461 [==============================] - 0s 289us/step - loss: 38.2903 - mae: 38.9690 - val_loss: 1238.1238 - val_mae: 1238.8171\n",
      "Epoch 949/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 40.3207 - mae: 41.0024 - val_loss: 1250.7571 - val_mae: 1251.4504\n",
      "Epoch 950/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 40.9168 - mae: 41.5972 - val_loss: 1469.2077 - val_mae: 1469.9009\n",
      "Epoch 951/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 37.6435 - mae: 38.3186 - val_loss: 1090.4493 - val_mae: 1091.1426\n",
      "Epoch 952/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 38.7170 - mae: 39.3971 - val_loss: 1424.8723 - val_mae: 1425.5656\n",
      "Epoch 953/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 34.4482 - mae: 35.1255 - val_loss: 1519.0383 - val_mae: 1519.7314\n",
      "Epoch 954/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 55.0391 - mae: 55.7224 - val_loss: 1228.0775 - val_mae: 1228.7705\n",
      "Epoch 955/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 38.5613 - mae: 39.2424 - val_loss: 1375.9869 - val_mae: 1376.6801\n",
      "Epoch 956/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 41.1815 - mae: 41.8662 - val_loss: 1440.3831 - val_mae: 1441.0764\n",
      "Epoch 957/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 34.1659 - mae: 34.8443 - val_loss: 1397.7619 - val_mae: 1398.4550\n",
      "Epoch 958/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 41.7602 - mae: 42.4363 - val_loss: 1275.6744 - val_mae: 1276.3676\n",
      "Epoch 959/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 38.9506 - mae: 39.6262 - val_loss: 1278.9684 - val_mae: 1279.6615\n",
      "Epoch 960/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 39.6336 - mae: 40.3113 - val_loss: 1371.2622 - val_mae: 1371.9554\n",
      "Epoch 961/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 40.8420 - mae: 41.5176 - val_loss: 1394.9969 - val_mae: 1395.6888\n",
      "Epoch 962/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 37.2482 - mae: 37.9289 - val_loss: 1352.8141 - val_mae: 1353.5072\n",
      "Epoch 963/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 39.8380 - mae: 40.5177 - val_loss: 1169.7049 - val_mae: 1170.3979\n",
      "Epoch 964/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 36.3279 - mae: 37.0005 - val_loss: 1380.6840 - val_mae: 1381.3772\n",
      "Epoch 965/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 41.0564 - mae: 41.7349 - val_loss: 1396.4948 - val_mae: 1397.1880\n",
      "Epoch 966/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 31.7465 - mae: 32.4220 - val_loss: 1419.1749 - val_mae: 1419.8682\n",
      "Epoch 967/1000\n",
      "1461/1461 [==============================] - 0s 281us/step - loss: 43.9662 - mae: 44.6507 - val_loss: 1371.6296 - val_mae: 1372.3228\n",
      "Epoch 968/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 42.9530 - mae: 43.6353 - val_loss: 1352.1186 - val_mae: 1352.8118\n",
      "Epoch 969/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 32.2094 - mae: 32.8855 - val_loss: 1283.0311 - val_mae: 1283.7241\n",
      "Epoch 970/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 43.2715 - mae: 43.9484 - val_loss: 1521.0608 - val_mae: 1521.7540\n",
      "Epoch 971/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 40.8407 - mae: 41.5186 - val_loss: 1462.7498 - val_mae: 1463.4431\n",
      "Epoch 972/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 35.3101 - mae: 35.9898 - val_loss: 1207.4766 - val_mae: 1208.1697\n",
      "Epoch 973/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 42.3297 - mae: 43.0051 - val_loss: 1356.2522 - val_mae: 1356.9454\n",
      "Epoch 974/1000\n",
      "1461/1461 [==============================] - 0s 296us/step - loss: 67.9997 - mae: 68.6828 - val_loss: 1486.2788 - val_mae: 1486.9719\n",
      "Epoch 975/1000\n",
      "1461/1461 [==============================] - 0s 302us/step - loss: 48.6932 - mae: 49.3759 - val_loss: 1272.3165 - val_mae: 1273.0094\n",
      "Epoch 976/1000\n",
      "1461/1461 [==============================] - 0s 311us/step - loss: 34.9140 - mae: 35.5904 - val_loss: 1459.6546 - val_mae: 1460.3479\n",
      "Epoch 977/1000\n",
      "1461/1461 [==============================] - 0s 287us/step - loss: 48.6176 - mae: 49.2950 - val_loss: 1220.0068 - val_mae: 1220.6987\n",
      "Epoch 978/1000\n",
      "1461/1461 [==============================] - 0s 288us/step - loss: 37.0978 - mae: 37.7788 - val_loss: 1265.3554 - val_mae: 1266.0486\n",
      "Epoch 979/1000\n",
      "1461/1461 [==============================] - 0s 291us/step - loss: 37.6156 - mae: 38.2939 - val_loss: 1292.1775 - val_mae: 1292.8706\n",
      "Epoch 980/1000\n",
      "1461/1461 [==============================] - 0s 290us/step - loss: 35.4710 - mae: 36.1494 - val_loss: 1293.6651 - val_mae: 1294.3583\n",
      "Epoch 981/1000\n",
      "1461/1461 [==============================] - 0s 290us/step - loss: 43.3436 - mae: 44.0232 - val_loss: 1370.7081 - val_mae: 1371.4014\n",
      "Epoch 982/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 30.5765 - mae: 31.2529 - val_loss: 1193.1501 - val_mae: 1193.8434\n",
      "Epoch 983/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 47.8512 - mae: 48.5317 - val_loss: 1323.8858 - val_mae: 1324.5790\n",
      "Epoch 984/1000\n",
      "1461/1461 [==============================] - 0s 289us/step - loss: 36.6420 - mae: 37.3207 - val_loss: 1395.5743 - val_mae: 1396.2675\n",
      "Epoch 985/1000\n",
      "1461/1461 [==============================] - 0s 289us/step - loss: 34.5656 - mae: 35.2382 - val_loss: 1434.7634 - val_mae: 1435.4565\n",
      "Epoch 986/1000\n",
      "1461/1461 [==============================] - 0s 288us/step - loss: 54.6062 - mae: 55.2815 - val_loss: 1536.6488 - val_mae: 1537.3419\n",
      "Epoch 987/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 39.1771 - mae: 39.8636 - val_loss: 1245.1221 - val_mae: 1245.8152\n",
      "Epoch 988/1000\n",
      "1461/1461 [==============================] - 0s 283us/step - loss: 39.0914 - mae: 39.7740 - val_loss: 1212.9194 - val_mae: 1213.6127\n",
      "Epoch 989/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 47.9745 - mae: 48.6554 - val_loss: 1338.9395 - val_mae: 1339.6327\n",
      "Epoch 990/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 289us/step - loss: 33.8155 - mae: 34.4922 - val_loss: 1436.1645 - val_mae: 1436.8577\n",
      "Epoch 991/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 34.9559 - mae: 35.6332 - val_loss: 1083.1705 - val_mae: 1083.8634\n",
      "Epoch 992/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 36.2472 - mae: 36.9271 - val_loss: 1236.7911 - val_mae: 1237.4833\n",
      "Epoch 993/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 37.6843 - mae: 38.3642 - val_loss: 1423.9780 - val_mae: 1424.6711\n",
      "Epoch 994/1000\n",
      "1461/1461 [==============================] - 0s 281us/step - loss: 44.8838 - mae: 45.5664 - val_loss: 1230.8368 - val_mae: 1231.5300\n",
      "Epoch 995/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 32.9053 - mae: 33.5841 - val_loss: 1372.9817 - val_mae: 1373.6750\n",
      "Epoch 996/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 28.7059 - mae: 29.3797 - val_loss: 1260.8399 - val_mae: 1261.5316\n",
      "Epoch 997/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 33.7508 - mae: 34.4333 - val_loss: 1399.6518 - val_mae: 1400.3450\n",
      "Epoch 998/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 37.1034 - mae: 37.7794 - val_loss: 1417.3838 - val_mae: 1418.0770\n",
      "Epoch 999/1000\n",
      "1461/1461 [==============================] - 0s 281us/step - loss: 49.0072 - mae: 49.6840 - val_loss: 1409.9664 - val_mae: 1410.6597\n",
      "Epoch 1000/1000\n",
      "1461/1461 [==============================] - 0s 281us/step - loss: 32.3500 - mae: 33.0235 - val_loss: 1429.0800 - val_mae: 1429.7732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fe88a4bff50>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(X_train,Y_train,validation_data=(X_val, Y_val))\n",
    "#Audio(sound_file,autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78fdbade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 160us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.999740456503254"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for check\n",
    "y_train_pred=regressor.predict(X_train)\n",
    "r2_score(Y_train, y_train_pred) #training score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0435993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546/546 [==============================] - 0s 99us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-06-01</th>\n",
       "      <td>36599.0</td>\n",
       "      <td>40073.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-02</th>\n",
       "      <td>37295.0</td>\n",
       "      <td>40686.402344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-03</th>\n",
       "      <td>38539.0</td>\n",
       "      <td>41667.679688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-04</th>\n",
       "      <td>37117.0</td>\n",
       "      <td>41084.648438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-05</th>\n",
       "      <td>36607.0</td>\n",
       "      <td>40739.804688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-24</th>\n",
       "      <td>16604.0</td>\n",
       "      <td>21682.167969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-25</th>\n",
       "      <td>16495.0</td>\n",
       "      <td>21614.537109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-26</th>\n",
       "      <td>16561.0</td>\n",
       "      <td>21784.324219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-27</th>\n",
       "      <td>16542.0</td>\n",
       "      <td>21721.552734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28</th>\n",
       "      <td>16214.0</td>\n",
       "      <td>21436.666016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>546 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             y_test        y_pred\n",
       "Date                             \n",
       "2021-06-01  36599.0  40073.562500\n",
       "2021-06-02  37295.0  40686.402344\n",
       "2021-06-03  38539.0  41667.679688\n",
       "2021-06-04  37117.0  41084.648438\n",
       "2021-06-05  36607.0  40739.804688\n",
       "...             ...           ...\n",
       "2022-11-24  16604.0  21682.167969\n",
       "2022-11-25  16495.0  21614.537109\n",
       "2022-11-26  16561.0  21784.324219\n",
       "2022-11-27  16542.0  21721.552734\n",
       "2022-11-28  16214.0  21436.666016\n",
       "\n",
       "[546 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=regressor.predict(X_test)\n",
    "pre_df = pd.DataFrame(zip(Y_test,y_pred),columns=['y_test','y_pred'])\n",
    "pre_df.index = Y_test.index\n",
    "pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f767c6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8980494760128862"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2=r2_score(Y_test,y_pred) #testing score/ r^2\n",
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cfc7e6a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4203.585744761503"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse=np.sqrt(mean_squared_error(Y_test,y_pred)) #rmse\n",
    "rmse#太特么大了，感觉数据集划分有问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e4bb7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df['pred_returns'] = pre_df['y_pred'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e607b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>pred_returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-06-01</th>\n",
       "      <td>36599.0</td>\n",
       "      <td>40073.562500</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-02</th>\n",
       "      <td>37295.0</td>\n",
       "      <td>40686.402344</td>\n",
       "      <td>0.015293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-03</th>\n",
       "      <td>38539.0</td>\n",
       "      <td>41667.679688</td>\n",
       "      <td>0.024118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-04</th>\n",
       "      <td>37117.0</td>\n",
       "      <td>41084.648438</td>\n",
       "      <td>-0.013992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-05</th>\n",
       "      <td>36607.0</td>\n",
       "      <td>40739.804688</td>\n",
       "      <td>-0.008393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-24</th>\n",
       "      <td>16604.0</td>\n",
       "      <td>21682.167969</td>\n",
       "      <td>0.007150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-25</th>\n",
       "      <td>16495.0</td>\n",
       "      <td>21614.537109</td>\n",
       "      <td>-0.003119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-26</th>\n",
       "      <td>16561.0</td>\n",
       "      <td>21784.324219</td>\n",
       "      <td>0.007855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-27</th>\n",
       "      <td>16542.0</td>\n",
       "      <td>21721.552734</td>\n",
       "      <td>-0.002882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28</th>\n",
       "      <td>16214.0</td>\n",
       "      <td>21436.666016</td>\n",
       "      <td>-0.013115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>546 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             y_test        y_pred  pred_returns\n",
       "Date                                           \n",
       "2021-06-01  36599.0  40073.562500           NaN\n",
       "2021-06-02  37295.0  40686.402344      0.015293\n",
       "2021-06-03  38539.0  41667.679688      0.024118\n",
       "2021-06-04  37117.0  41084.648438     -0.013992\n",
       "2021-06-05  36607.0  40739.804688     -0.008393\n",
       "...             ...           ...           ...\n",
       "2022-11-24  16604.0  21682.167969      0.007150\n",
       "2022-11-25  16495.0  21614.537109     -0.003119\n",
       "2022-11-26  16561.0  21784.324219      0.007855\n",
       "2022-11-27  16542.0  21721.552734     -0.002882\n",
       "2022-11-28  16214.0  21436.666016     -0.013115\n",
       "\n",
       "[546 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63572f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(zip(Y_test,y_pred),columns=['y_test','y_pred']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77be8854",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df.to_csv(\"btc_NN.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
