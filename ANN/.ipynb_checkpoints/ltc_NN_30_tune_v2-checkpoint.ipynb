{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fcb3f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 00:37:01.201390: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# conda install keras-tuner\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_validate\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from collections import Counter\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import randint\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, accuracy_score, f1_score, classification_report\n",
    "# import pickle\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import *\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ded3682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535a3fac",
   "metadata": {},
   "source": [
    "### ltc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1467391e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1732882/1097672958.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ltc['Date'][i]  =  datetime.strptime(ltc['Date'][i], '%Y/%m/%d')\n"
     ]
    }
   ],
   "source": [
    "regs = pd.read_csv(\"../Data/train_ltc_selected_features.csv\")\n",
    "ltc = pd.read_csv(\"../Data/litecoin_Data.csv\")\n",
    "for i in range(len(ltc['Date'])):\n",
    "    ltc['Date'][i]  =  datetime.strptime(ltc['Date'][i], '%Y/%m/%d')\n",
    "\n",
    "ltc = ltc.set_index(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d665a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ltcData = ltc[regs.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8227b01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1732882/1118086751.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ltcData['returns'] = ltcData['priceUSD'].pct_change()\n"
     ]
    }
   ],
   "source": [
    "ltcData['returns'] = ltcData['priceUSD'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0204bfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = ltcData.drop(columns=['priceUSD'])\n",
    "Data = Data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a926d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activeaddresses30std</th>\n",
       "      <th>activeaddresses30var</th>\n",
       "      <th>activeaddresses7trx</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>difficulty30mom</th>\n",
       "      <th>difficulty30trx</th>\n",
       "      <th>difficulty90mom</th>\n",
       "      <th>difficulty90roc</th>\n",
       "      <th>difficulty90rsi</th>\n",
       "      <th>difficulty90trx</th>\n",
       "      <th>...</th>\n",
       "      <th>sentinusd30emaUSD</th>\n",
       "      <th>sentinusd30smaUSD</th>\n",
       "      <th>sentinusd30wmaUSD</th>\n",
       "      <th>sentinusd90emaUSD</th>\n",
       "      <th>sentinusd90varUSD</th>\n",
       "      <th>sentinusd90wmaUSD</th>\n",
       "      <th>sentinusdUSD</th>\n",
       "      <th>top100cap</th>\n",
       "      <th>transactionvalueUSD</th>\n",
       "      <th>returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-02</th>\n",
       "      <td>7666</td>\n",
       "      <td>14691594</td>\n",
       "      <td>0.296</td>\n",
       "      <td>51718</td>\n",
       "      <td>2875.000</td>\n",
       "      <td>0.146</td>\n",
       "      <td>3694.0</td>\n",
       "      <td>7.693</td>\n",
       "      <td>55.143</td>\n",
       "      <td>0.073</td>\n",
       "      <td>...</td>\n",
       "      <td>13119990</td>\n",
       "      <td>14447549</td>\n",
       "      <td>13084361</td>\n",
       "      <td>15062343</td>\n",
       "      <td>9.207781e+13</td>\n",
       "      <td>15267519</td>\n",
       "      <td>6145265</td>\n",
       "      <td>49.907</td>\n",
       "      <td>1299.0</td>\n",
       "      <td>-0.004836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-03</th>\n",
       "      <td>7553</td>\n",
       "      <td>14263642</td>\n",
       "      <td>0.531</td>\n",
       "      <td>50653</td>\n",
       "      <td>1810.000</td>\n",
       "      <td>0.152</td>\n",
       "      <td>2764.0</td>\n",
       "      <td>5.771</td>\n",
       "      <td>53.920</td>\n",
       "      <td>0.074</td>\n",
       "      <td>...</td>\n",
       "      <td>12661408</td>\n",
       "      <td>14247155</td>\n",
       "      <td>12540130</td>\n",
       "      <td>14863434</td>\n",
       "      <td>9.237091e+13</td>\n",
       "      <td>15058214</td>\n",
       "      <td>6011974</td>\n",
       "      <td>49.914</td>\n",
       "      <td>1650.0</td>\n",
       "      <td>-0.008005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-04</th>\n",
       "      <td>6990</td>\n",
       "      <td>12215979</td>\n",
       "      <td>0.492</td>\n",
       "      <td>50158</td>\n",
       "      <td>1315.000</td>\n",
       "      <td>0.156</td>\n",
       "      <td>2268.0</td>\n",
       "      <td>4.737</td>\n",
       "      <td>53.364</td>\n",
       "      <td>0.074</td>\n",
       "      <td>...</td>\n",
       "      <td>12160118</td>\n",
       "      <td>13934412</td>\n",
       "      <td>11936534</td>\n",
       "      <td>14644269</td>\n",
       "      <td>9.336173e+13</td>\n",
       "      <td>14824650</td>\n",
       "      <td>4891418</td>\n",
       "      <td>49.963</td>\n",
       "      <td>1112.0</td>\n",
       "      <td>0.004323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-05</th>\n",
       "      <td>7207</td>\n",
       "      <td>12983796</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>50158</td>\n",
       "      <td>436.688</td>\n",
       "      <td>0.159</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>3.574</td>\n",
       "      <td>53.364</td>\n",
       "      <td>0.075</td>\n",
       "      <td>...</td>\n",
       "      <td>11733962</td>\n",
       "      <td>13588124</td>\n",
       "      <td>11395907</td>\n",
       "      <td>14444498</td>\n",
       "      <td>9.368327e+13</td>\n",
       "      <td>14607077</td>\n",
       "      <td>5554693</td>\n",
       "      <td>49.976</td>\n",
       "      <td>1505.0</td>\n",
       "      <td>-0.004878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-06</th>\n",
       "      <td>7368</td>\n",
       "      <td>13572232</td>\n",
       "      <td>-0.950</td>\n",
       "      <td>51299</td>\n",
       "      <td>1184.000</td>\n",
       "      <td>0.161</td>\n",
       "      <td>1473.0</td>\n",
       "      <td>2.956</td>\n",
       "      <td>54.471</td>\n",
       "      <td>0.075</td>\n",
       "      <td>...</td>\n",
       "      <td>11652530</td>\n",
       "      <td>13295257</td>\n",
       "      <td>11194853</td>\n",
       "      <td>14357186</td>\n",
       "      <td>9.312956e+13</td>\n",
       "      <td>14497960</td>\n",
       "      <td>10471777</td>\n",
       "      <td>49.916</td>\n",
       "      <td>3469.0</td>\n",
       "      <td>-0.004902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-24</th>\n",
       "      <td>55298</td>\n",
       "      <td>764459612</td>\n",
       "      <td>-0.329</td>\n",
       "      <td>19352505</td>\n",
       "      <td>2570167.000</td>\n",
       "      <td>0.274</td>\n",
       "      <td>3935133.0</td>\n",
       "      <td>25.524</td>\n",
       "      <td>61.809</td>\n",
       "      <td>0.095</td>\n",
       "      <td>...</td>\n",
       "      <td>1038289555</td>\n",
       "      <td>1236990379</td>\n",
       "      <td>1125469305</td>\n",
       "      <td>906646434</td>\n",
       "      <td>7.982758e+17</td>\n",
       "      <td>1088633253</td>\n",
       "      <td>361588723</td>\n",
       "      <td>44.670</td>\n",
       "      <td>30828.0</td>\n",
       "      <td>-0.001286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-25</th>\n",
       "      <td>55854</td>\n",
       "      <td>779921846</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>19996996</td>\n",
       "      <td>3214659.000</td>\n",
       "      <td>0.279</td>\n",
       "      <td>4316584.0</td>\n",
       "      <td>27.529</td>\n",
       "      <td>63.623</td>\n",
       "      <td>0.097</td>\n",
       "      <td>...</td>\n",
       "      <td>990339490</td>\n",
       "      <td>1228280282</td>\n",
       "      <td>1064699832</td>\n",
       "      <td>893205052</td>\n",
       "      <td>7.936632e+17</td>\n",
       "      <td>1076450910</td>\n",
       "      <td>295063546</td>\n",
       "      <td>44.667</td>\n",
       "      <td>37548.0</td>\n",
       "      <td>-0.023929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-26</th>\n",
       "      <td>55376</td>\n",
       "      <td>766634614</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>19996996</td>\n",
       "      <td>3214659.000</td>\n",
       "      <td>0.284</td>\n",
       "      <td>4316584.0</td>\n",
       "      <td>27.529</td>\n",
       "      <td>63.623</td>\n",
       "      <td>0.099</td>\n",
       "      <td>...</td>\n",
       "      <td>939695376</td>\n",
       "      <td>1214390963</td>\n",
       "      <td>998704699</td>\n",
       "      <td>878087484</td>\n",
       "      <td>7.902356e+17</td>\n",
       "      <td>1062224912</td>\n",
       "      <td>205355717</td>\n",
       "      <td>44.746</td>\n",
       "      <td>35847.0</td>\n",
       "      <td>0.012488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-27</th>\n",
       "      <td>55508</td>\n",
       "      <td>770273876</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>19996996</td>\n",
       "      <td>2274388.000</td>\n",
       "      <td>0.289</td>\n",
       "      <td>4316584.0</td>\n",
       "      <td>27.529</td>\n",
       "      <td>63.623</td>\n",
       "      <td>0.101</td>\n",
       "      <td>...</td>\n",
       "      <td>890366945</td>\n",
       "      <td>1190592377</td>\n",
       "      <td>931653972</td>\n",
       "      <td>862637313</td>\n",
       "      <td>7.872458e+17</td>\n",
       "      <td>1047283908</td>\n",
       "      <td>175104696</td>\n",
       "      <td>44.739</td>\n",
       "      <td>23536.0</td>\n",
       "      <td>0.001146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28</th>\n",
       "      <td>54692</td>\n",
       "      <td>747797946</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>19429695</td>\n",
       "      <td>1678156.000</td>\n",
       "      <td>0.294</td>\n",
       "      <td>4598305.0</td>\n",
       "      <td>31.004</td>\n",
       "      <td>60.986</td>\n",
       "      <td>0.103</td>\n",
       "      <td>...</td>\n",
       "      <td>845359022</td>\n",
       "      <td>1183566854</td>\n",
       "      <td>867276666</td>\n",
       "      <td>847914386</td>\n",
       "      <td>7.839838e+17</td>\n",
       "      <td>1032687824</td>\n",
       "      <td>192744138</td>\n",
       "      <td>44.742</td>\n",
       "      <td>24643.0</td>\n",
       "      <td>-0.054550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2523 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            activeaddresses30std  activeaddresses30var  activeaddresses7trx  \\\n",
       "Date                                                                          \n",
       "2016-01-02                  7666              14691594                0.296   \n",
       "2016-01-03                  7553              14263642                0.531   \n",
       "2016-01-04                  6990              12215979                0.492   \n",
       "2016-01-05                  7207              12983796               -0.176   \n",
       "2016-01-06                  7368              13572232               -0.950   \n",
       "...                          ...                   ...                  ...   \n",
       "2022-11-24                 55298             764459612               -0.329   \n",
       "2022-11-25                 55854             779921846               -0.097   \n",
       "2022-11-26                 55376             766634614               -0.101   \n",
       "2022-11-27                 55508             770273876               -0.228   \n",
       "2022-11-28                 54692             747797946               -0.210   \n",
       "\n",
       "            difficulty  difficulty30mom  difficulty30trx  difficulty90mom  \\\n",
       "Date                                                                        \n",
       "2016-01-02       51718         2875.000            0.146           3694.0   \n",
       "2016-01-03       50653         1810.000            0.152           2764.0   \n",
       "2016-01-04       50158         1315.000            0.156           2268.0   \n",
       "2016-01-05       50158          436.688            0.159           1731.0   \n",
       "2016-01-06       51299         1184.000            0.161           1473.0   \n",
       "...                ...              ...              ...              ...   \n",
       "2022-11-24    19352505      2570167.000            0.274        3935133.0   \n",
       "2022-11-25    19996996      3214659.000            0.279        4316584.0   \n",
       "2022-11-26    19996996      3214659.000            0.284        4316584.0   \n",
       "2022-11-27    19996996      2274388.000            0.289        4316584.0   \n",
       "2022-11-28    19429695      1678156.000            0.294        4598305.0   \n",
       "\n",
       "            difficulty90roc  difficulty90rsi  difficulty90trx  ...  \\\n",
       "Date                                                           ...   \n",
       "2016-01-02            7.693           55.143            0.073  ...   \n",
       "2016-01-03            5.771           53.920            0.074  ...   \n",
       "2016-01-04            4.737           53.364            0.074  ...   \n",
       "2016-01-05            3.574           53.364            0.075  ...   \n",
       "2016-01-06            2.956           54.471            0.075  ...   \n",
       "...                     ...              ...              ...  ...   \n",
       "2022-11-24           25.524           61.809            0.095  ...   \n",
       "2022-11-25           27.529           63.623            0.097  ...   \n",
       "2022-11-26           27.529           63.623            0.099  ...   \n",
       "2022-11-27           27.529           63.623            0.101  ...   \n",
       "2022-11-28           31.004           60.986            0.103  ...   \n",
       "\n",
       "            sentinusd30emaUSD  sentinusd30smaUSD  sentinusd30wmaUSD  \\\n",
       "Date                                                                  \n",
       "2016-01-02           13119990           14447549           13084361   \n",
       "2016-01-03           12661408           14247155           12540130   \n",
       "2016-01-04           12160118           13934412           11936534   \n",
       "2016-01-05           11733962           13588124           11395907   \n",
       "2016-01-06           11652530           13295257           11194853   \n",
       "...                       ...                ...                ...   \n",
       "2022-11-24         1038289555         1236990379         1125469305   \n",
       "2022-11-25          990339490         1228280282         1064699832   \n",
       "2022-11-26          939695376         1214390963          998704699   \n",
       "2022-11-27          890366945         1190592377          931653972   \n",
       "2022-11-28          845359022         1183566854          867276666   \n",
       "\n",
       "            sentinusd90emaUSD  sentinusd90varUSD  sentinusd90wmaUSD  \\\n",
       "Date                                                                  \n",
       "2016-01-02           15062343       9.207781e+13           15267519   \n",
       "2016-01-03           14863434       9.237091e+13           15058214   \n",
       "2016-01-04           14644269       9.336173e+13           14824650   \n",
       "2016-01-05           14444498       9.368327e+13           14607077   \n",
       "2016-01-06           14357186       9.312956e+13           14497960   \n",
       "...                       ...                ...                ...   \n",
       "2022-11-24          906646434       7.982758e+17         1088633253   \n",
       "2022-11-25          893205052       7.936632e+17         1076450910   \n",
       "2022-11-26          878087484       7.902356e+17         1062224912   \n",
       "2022-11-27          862637313       7.872458e+17         1047283908   \n",
       "2022-11-28          847914386       7.839838e+17         1032687824   \n",
       "\n",
       "            sentinusdUSD  top100cap  transactionvalueUSD   returns  \n",
       "Date                                                                \n",
       "2016-01-02       6145265     49.907               1299.0 -0.004836  \n",
       "2016-01-03       6011974     49.914               1650.0 -0.008005  \n",
       "2016-01-04       4891418     49.963               1112.0  0.004323  \n",
       "2016-01-05       5554693     49.976               1505.0 -0.004878  \n",
       "2016-01-06      10471777     49.916               3469.0 -0.004902  \n",
       "...                  ...        ...                  ...       ...  \n",
       "2022-11-24     361588723     44.670              30828.0 -0.001286  \n",
       "2022-11-25     295063546     44.667              37548.0 -0.023929  \n",
       "2022-11-26     205355717     44.746              35847.0  0.012488  \n",
       "2022-11-27     175104696     44.739              23536.0  0.001146  \n",
       "2022-11-28     192744138     44.742              24643.0 -0.054550  \n",
       "\n",
       "[2523 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5d4f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide X and Y\n",
    "X = Data.iloc[:,0:]\n",
    "#Y = Data['returns']   # 用returns的话就用这一行，然后把下一行comment掉\n",
    "Y = ltcData['priceUSD'].shift(-30)[1:] # 反之亦然"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb3275b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activeaddresses30std</th>\n",
       "      <th>activeaddresses30var</th>\n",
       "      <th>activeaddresses7trx</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>difficulty30mom</th>\n",
       "      <th>difficulty30trx</th>\n",
       "      <th>difficulty90mom</th>\n",
       "      <th>difficulty90roc</th>\n",
       "      <th>difficulty90rsi</th>\n",
       "      <th>difficulty90trx</th>\n",
       "      <th>...</th>\n",
       "      <th>sentinusd30emaUSD</th>\n",
       "      <th>sentinusd30smaUSD</th>\n",
       "      <th>sentinusd30wmaUSD</th>\n",
       "      <th>sentinusd90emaUSD</th>\n",
       "      <th>sentinusd90varUSD</th>\n",
       "      <th>sentinusd90wmaUSD</th>\n",
       "      <th>sentinusdUSD</th>\n",
       "      <th>top100cap</th>\n",
       "      <th>transactionvalueUSD</th>\n",
       "      <th>returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-02</th>\n",
       "      <td>7666</td>\n",
       "      <td>14691594</td>\n",
       "      <td>0.296</td>\n",
       "      <td>51718</td>\n",
       "      <td>2875.0</td>\n",
       "      <td>0.146</td>\n",
       "      <td>3694.0</td>\n",
       "      <td>7.693</td>\n",
       "      <td>55.143</td>\n",
       "      <td>0.073</td>\n",
       "      <td>...</td>\n",
       "      <td>13119990</td>\n",
       "      <td>14447549</td>\n",
       "      <td>13084361</td>\n",
       "      <td>15062343</td>\n",
       "      <td>9.207781e+13</td>\n",
       "      <td>15267519</td>\n",
       "      <td>6145265</td>\n",
       "      <td>49.907</td>\n",
       "      <td>1299.0</td>\n",
       "      <td>-0.004836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-03</th>\n",
       "      <td>7553</td>\n",
       "      <td>14263642</td>\n",
       "      <td>0.531</td>\n",
       "      <td>50653</td>\n",
       "      <td>1810.0</td>\n",
       "      <td>0.152</td>\n",
       "      <td>2764.0</td>\n",
       "      <td>5.771</td>\n",
       "      <td>53.920</td>\n",
       "      <td>0.074</td>\n",
       "      <td>...</td>\n",
       "      <td>12661408</td>\n",
       "      <td>14247155</td>\n",
       "      <td>12540130</td>\n",
       "      <td>14863434</td>\n",
       "      <td>9.237091e+13</td>\n",
       "      <td>15058214</td>\n",
       "      <td>6011974</td>\n",
       "      <td>49.914</td>\n",
       "      <td>1650.0</td>\n",
       "      <td>-0.008005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-04</th>\n",
       "      <td>6990</td>\n",
       "      <td>12215979</td>\n",
       "      <td>0.492</td>\n",
       "      <td>50158</td>\n",
       "      <td>1315.0</td>\n",
       "      <td>0.156</td>\n",
       "      <td>2268.0</td>\n",
       "      <td>4.737</td>\n",
       "      <td>53.364</td>\n",
       "      <td>0.074</td>\n",
       "      <td>...</td>\n",
       "      <td>12160118</td>\n",
       "      <td>13934412</td>\n",
       "      <td>11936534</td>\n",
       "      <td>14644269</td>\n",
       "      <td>9.336173e+13</td>\n",
       "      <td>14824650</td>\n",
       "      <td>4891418</td>\n",
       "      <td>49.963</td>\n",
       "      <td>1112.0</td>\n",
       "      <td>0.004323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            activeaddresses30std  activeaddresses30var  activeaddresses7trx  \\\n",
       "Date                                                                          \n",
       "2016-01-02                  7666              14691594                0.296   \n",
       "2016-01-03                  7553              14263642                0.531   \n",
       "2016-01-04                  6990              12215979                0.492   \n",
       "\n",
       "            difficulty  difficulty30mom  difficulty30trx  difficulty90mom  \\\n",
       "Date                                                                        \n",
       "2016-01-02       51718           2875.0            0.146           3694.0   \n",
       "2016-01-03       50653           1810.0            0.152           2764.0   \n",
       "2016-01-04       50158           1315.0            0.156           2268.0   \n",
       "\n",
       "            difficulty90roc  difficulty90rsi  difficulty90trx  ...  \\\n",
       "Date                                                           ...   \n",
       "2016-01-02            7.693           55.143            0.073  ...   \n",
       "2016-01-03            5.771           53.920            0.074  ...   \n",
       "2016-01-04            4.737           53.364            0.074  ...   \n",
       "\n",
       "            sentinusd30emaUSD  sentinusd30smaUSD  sentinusd30wmaUSD  \\\n",
       "Date                                                                  \n",
       "2016-01-02           13119990           14447549           13084361   \n",
       "2016-01-03           12661408           14247155           12540130   \n",
       "2016-01-04           12160118           13934412           11936534   \n",
       "\n",
       "            sentinusd90emaUSD  sentinusd90varUSD  sentinusd90wmaUSD  \\\n",
       "Date                                                                  \n",
       "2016-01-02           15062343       9.207781e+13           15267519   \n",
       "2016-01-03           14863434       9.237091e+13           15058214   \n",
       "2016-01-04           14644269       9.336173e+13           14824650   \n",
       "\n",
       "            sentinusdUSD  top100cap  transactionvalueUSD   returns  \n",
       "Date                                                                \n",
       "2016-01-02       6145265     49.907               1299.0 -0.004836  \n",
       "2016-01-03       6011974     49.914               1650.0 -0.008005  \n",
       "2016-01-04       4891418     49.963               1112.0  0.004323  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd589ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into three data sets\n",
    "X_train = X['2016-01-01':'2019-12-31']\n",
    "X_val = X['2020-01-01':'2021-05-31']\n",
    "X_test = X['2021-06-01':'2023-01-01']\n",
    "\n",
    "Y_train = Y['2016-01-01':'2019-12-31']\n",
    "Y_val = Y['2020-01-01':'2021-05-31']\n",
    "Y_test = Y['2021-06-01':'2023-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e5e9fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............ (step 1 of 2) Processing mixmax, total=   0.0s\n",
      "[Pipeline] ............ (step 2 of 2) Processing robust, total=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;mixmax&#x27;, MinMaxScaler()), [&#x27;robust&#x27;, RobustScaler()]],\n",
       "         verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;mixmax&#x27;, MinMaxScaler()), [&#x27;robust&#x27;, RobustScaler()]],\n",
       "         verbose=True)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RobustScaler</label><div class=\"sk-toggleable__content\"><pre>RobustScaler()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('mixmax', MinMaxScaler()), ['robust', RobustScaler()]],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators=[]\n",
    "estimators.append(['mixmax',MinMaxScaler()])\n",
    "estimators.append(['robust',RobustScaler()])\n",
    "scale=Pipeline(estimators,verbose=True)\n",
    "scale.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6fc0859",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=scale.transform(X_train)\n",
    "X_test=scale.transform(X_test)\n",
    "X_val = scale.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8a84d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f6916ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define neural network model\n",
    "shape=X.shape[1]\n",
    "def build_model(hp, initializer='normal', activation='relu', NUM_FEATURES=shape):\n",
    "    # create model\n",
    "    hp_units1 = hp.Int('units1', min_value=32, max_value=512, step=32)\n",
    "    hp_units2 = hp.Int('units2', min_value=32, max_value=512, step=32)\n",
    "    hp_units3 = hp.Int('units3', min_value=32, max_value=512, step=32)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hp_units1, input_shape=(NUM_FEATURES,), kernel_initializer=initializer, activation=activation))\n",
    "    model.add(Dense(hp_units2, activation=activation))\n",
    "    model.add(Dense(hp_units3, activation=activation))\n",
    "    model.add(Dense(1, activation=activation))\n",
    "    # Compile model\n",
    "    adam=keras.optimizers.Adam(lr=lr_schedule(0), amsgrad=True)\n",
    "    #sgd=keras.optimizers.SGD(learning_rate=0.08, momentum=0.9, nesterov=False)\n",
    "    model.compile(loss='logcosh', optimizer=adam, metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112c8df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project ltc_tune/ANN_TUNE/oracle.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 00:37:02.588698: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-04 00:37:02.589583: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "/home/spectre/anaconda3/envs/tensorplustorch/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "320               |?                 |units1\n",
      "96                |?                 |units2\n",
      "224               |?                 |units3\n",
      "\n",
      "Learning rate:  0.001\n",
      "Epoch 1/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 41.0121 - mae: 41.6608 - val_loss: 59.3365 - val_mae: 60.0265\n",
      "Epoch 2/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 18.7665 - mae: 19.3692 - val_loss: 37.1064 - val_mae: 37.7835\n",
      "Epoch 3/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 14.2480 - mae: 14.8451 - val_loss: 35.7648 - val_mae: 36.4473\n",
      "Epoch 4/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 12.4674 - mae: 13.0726 - val_loss: 35.7027 - val_mae: 36.3824\n",
      "Epoch 5/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 11.3453 - mae: 11.9384 - val_loss: 43.2255 - val_mae: 43.9091\n",
      "Epoch 6/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 10.8889 - mae: 11.4840 - val_loss: 40.6132 - val_mae: 41.2910\n",
      "Epoch 7/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 9.7100 - mae: 10.2944 - val_loss: 41.3841 - val_mae: 42.0613\n",
      "Epoch 8/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 8.6611 - mae: 9.2339 - val_loss: 41.5371 - val_mae: 42.2169\n",
      "Epoch 9/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 7.7131 - mae: 8.2862 - val_loss: 41.8789 - val_mae: 42.5634\n",
      "Epoch 10/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 7.2510 - mae: 7.8124 - val_loss: 43.2537 - val_mae: 43.9392\n",
      "Epoch 11/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 6.9745 - mae: 7.5388 - val_loss: 38.7944 - val_mae: 39.4808\n",
      "Epoch 12/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 6.5305 - mae: 7.1043 - val_loss: 41.8659 - val_mae: 42.5524\n",
      "Epoch 13/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 5.9708 - mae: 6.5263 - val_loss: 43.2100 - val_mae: 43.8974\n",
      "Epoch 14/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 5.8052 - mae: 6.3553 - val_loss: 43.6455 - val_mae: 44.3334\n",
      "Epoch 15/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 5.0777 - mae: 5.6374 - val_loss: 43.7815 - val_mae: 44.4677\n",
      "Epoch 16/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 5.1166 - mae: 5.6780 - val_loss: 43.5391 - val_mae: 44.2292\n",
      "Epoch 17/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 5.4456 - mae: 6.0030 - val_loss: 47.6847 - val_mae: 48.3681\n",
      "Epoch 18/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 5.3995 - mae: 5.9423 - val_loss: 42.7849 - val_mae: 43.4746\n",
      "Epoch 19/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 4.9307 - mae: 5.4771 - val_loss: 47.6007 - val_mae: 48.2845\n",
      "Epoch 20/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 5.0034 - mae: 5.5512 - val_loss: 47.1504 - val_mae: 47.8349\n",
      "Epoch 21/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 4.2790 - mae: 4.8200 - val_loss: 45.9095 - val_mae: 46.5951\n",
      "Epoch 22/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 4.5443 - mae: 5.0970 - val_loss: 46.6911 - val_mae: 47.3788\n",
      "Epoch 23/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 4.0481 - mae: 4.5787 - val_loss: 45.0129 - val_mae: 45.7005\n",
      "Epoch 24/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 4.0571 - mae: 4.6046 - val_loss: 45.9713 - val_mae: 46.6575\n",
      "Epoch 25/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 3.9220 - mae: 4.4468 - val_loss: 46.6601 - val_mae: 47.3436\n",
      "Epoch 26/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 3.9077 - mae: 4.4368 - val_loss: 44.5480 - val_mae: 45.2380\n",
      "Epoch 27/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 3.6018 - mae: 4.1404 - val_loss: 46.7631 - val_mae: 47.4518\n",
      "Epoch 28/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 3.7492 - mae: 4.2826 - val_loss: 47.6490 - val_mae: 48.3346\n",
      "Epoch 29/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 3.5724 - mae: 4.1030 - val_loss: 45.6610 - val_mae: 46.3502\n",
      "Epoch 30/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 3.6953 - mae: 4.2146 - val_loss: 45.7790 - val_mae: 46.4657\n",
      "Epoch 31/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 3.3941 - mae: 3.9030 - val_loss: 45.6363 - val_mae: 46.3228\n",
      "Epoch 32/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 3.9706 - mae: 4.4887 - val_loss: 45.9431 - val_mae: 46.6227\n",
      "Epoch 33/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 3.5963 - mae: 4.1210 - val_loss: 46.2402 - val_mae: 46.9231\n",
      "Epoch 34/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 3.8761 - mae: 4.4032 - val_loss: 45.6800 - val_mae: 46.3625\n",
      "Epoch 35/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 3.2598 - mae: 3.7690 - val_loss: 44.7781 - val_mae: 45.4651\n",
      "Epoch 36/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 3.4875 - mae: 4.0120 - val_loss: 46.0167 - val_mae: 46.6971\n",
      "Epoch 37/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 3.3751 - mae: 3.8841 - val_loss: 45.0088 - val_mae: 45.6896\n",
      "Epoch 38/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 3.3248 - mae: 3.8378 - val_loss: 46.8452 - val_mae: 47.5254\n",
      "Epoch 39/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 3.4241 - mae: 3.9526 - val_loss: 44.7497 - val_mae: 45.4340\n",
      "Epoch 40/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 3.1427 - mae: 3.6534 - val_loss: 44.7933 - val_mae: 45.4814\n",
      "Epoch 41/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 3.5337 - mae: 4.0533 - val_loss: 46.5114 - val_mae: 47.1924\n",
      "Epoch 42/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 3.2488 - mae: 3.7551 - val_loss: 45.2865 - val_mae: 45.9753\n",
      "Epoch 43/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.9896 - mae: 3.4982 - val_loss: 47.0844 - val_mae: 47.7665\n",
      "Epoch 44/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 3.2555 - mae: 3.7634 - val_loss: 45.7753 - val_mae: 46.4572\n",
      "Epoch 45/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 3.2324 - mae: 3.7474 - val_loss: 45.0072 - val_mae: 45.6825\n",
      "Epoch 46/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 3.2843 - mae: 3.7897 - val_loss: 45.5091 - val_mae: 46.1883\n",
      "Epoch 47/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 3.1240 - mae: 3.6253 - val_loss: 44.2298 - val_mae: 44.9178\n",
      "Epoch 48/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 3.1651 - mae: 3.6798 - val_loss: 45.9464 - val_mae: 46.6232\n",
      "Epoch 49/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 3.6920 - mae: 4.2016 - val_loss: 45.6630 - val_mae: 46.3488\n",
      "Epoch 50/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 3.0161 - mae: 3.5250 - val_loss: 46.5468 - val_mae: 47.2357\n",
      "Epoch 51/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 3.1789 - mae: 3.6887 - val_loss: 44.7736 - val_mae: 45.4601\n",
      "Epoch 52/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 3.0134 - mae: 3.5207 - val_loss: 44.6870 - val_mae: 45.3729\n",
      "Epoch 53/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.8388 - mae: 3.3360 - val_loss: 44.8109 - val_mae: 45.4967\n",
      "Epoch 54/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 2.7662 - mae: 3.2579 - val_loss: 46.3855 - val_mae: 47.0685\n",
      "Epoch 55/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.8043 - mae: 3.2877 - val_loss: 46.7008 - val_mae: 47.3803\n",
      "Epoch 56/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 2.8553 - mae: 3.3473 - val_loss: 47.1466 - val_mae: 47.8322\n",
      "Epoch 57/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 3.0443 - mae: 3.5451 - val_loss: 46.5444 - val_mae: 47.2207\n",
      "Epoch 58/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 2.8569 - mae: 3.3593 - val_loss: 46.0106 - val_mae: 46.6919\n",
      "Epoch 59/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 2.6864 - mae: 3.1760 - val_loss: 46.8341 - val_mae: 47.5153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.9237 - mae: 3.4154 - val_loss: 45.1634 - val_mae: 45.8451\n",
      "Epoch 61/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.7035 - mae: 3.2027 - val_loss: 45.3524 - val_mae: 46.0406\n",
      "Epoch 62/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.8630 - mae: 3.3619 - val_loss: 46.1550 - val_mae: 46.8370\n",
      "Epoch 63/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.5393 - mae: 3.0237 - val_loss: 46.1282 - val_mae: 46.8105\n",
      "Epoch 64/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.7713 - mae: 3.2634 - val_loss: 46.5099 - val_mae: 47.1907\n",
      "Epoch 65/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 2.7064 - mae: 3.2033 - val_loss: 46.4892 - val_mae: 47.1721\n",
      "Epoch 66/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.4972 - mae: 2.9786 - val_loss: 47.9023 - val_mae: 48.5827\n",
      "Epoch 67/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 2.7652 - mae: 3.2563 - val_loss: 46.7481 - val_mae: 47.4309\n",
      "Epoch 68/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 2.7084 - mae: 3.2020 - val_loss: 46.7625 - val_mae: 47.4391\n",
      "Epoch 69/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 2.4642 - mae: 2.9437 - val_loss: 47.0455 - val_mae: 47.7281\n",
      "Epoch 70/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 2.7918 - mae: 3.2945 - val_loss: 45.5374 - val_mae: 46.2137\n",
      "Epoch 71/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.6770 - mae: 3.1659 - val_loss: 46.2756 - val_mae: 46.9611\n",
      "Epoch 72/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 2.7819 - mae: 3.2691 - val_loss: 44.6969 - val_mae: 45.3817\n",
      "Epoch 73/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 3.1083 - mae: 3.6175 - val_loss: 45.4284 - val_mae: 46.1110\n",
      "Epoch 74/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 3.0387 - mae: 3.5297 - val_loss: 44.9287 - val_mae: 45.6117\n",
      "Epoch 75/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.5693 - mae: 3.0616 - val_loss: 45.5875 - val_mae: 46.2692\n",
      "Epoch 76/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 2.8533 - mae: 3.3434 - val_loss: 47.2112 - val_mae: 47.8948\n",
      "Epoch 77/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 2.6419 - mae: 3.1443 - val_loss: 46.3315 - val_mae: 47.0124\n",
      "Epoch 78/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 2.7356 - mae: 3.2224 - val_loss: 45.5381 - val_mae: 46.2172\n",
      "Epoch 79/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.9947 - mae: 3.4783 - val_loss: 45.6523 - val_mae: 46.3332\n",
      "Epoch 80/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 2.6569 - mae: 3.1264 - val_loss: 44.7716 - val_mae: 45.4558\n",
      "Epoch 81/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 2.1965 - mae: 2.6595 - val_loss: 46.2470 - val_mae: 46.9286\n",
      "Epoch 82/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.2456 - mae: 2.7141 - val_loss: 45.2369 - val_mae: 45.9154\n",
      "Epoch 83/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 2.4979 - mae: 2.9836 - val_loss: 46.6165 - val_mae: 47.2969\n",
      "Epoch 84/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.5165 - mae: 2.9982 - val_loss: 45.7767 - val_mae: 46.4552\n",
      "Epoch 85/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 2.4340 - mae: 2.8925 - val_loss: 45.4129 - val_mae: 46.0920\n",
      "Epoch 86/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 2.4553 - mae: 2.9270 - val_loss: 45.5414 - val_mae: 46.2214\n",
      "Epoch 87/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 2.3680 - mae: 2.8465 - val_loss: 45.4606 - val_mae: 46.1399\n",
      "Epoch 88/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.3687 - mae: 2.8375 - val_loss: 45.8973 - val_mae: 46.5792\n",
      "Epoch 89/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 2.4892 - mae: 2.9530 - val_loss: 46.0165 - val_mae: 46.6949\n",
      "Epoch 90/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 2.3167 - mae: 2.7963 - val_loss: 46.5447 - val_mae: 47.2209\n",
      "Epoch 91/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 2.3461 - mae: 2.8095 - val_loss: 47.1358 - val_mae: 47.8168\n",
      "Epoch 92/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 2.4294 - mae: 2.9161 - val_loss: 45.8598 - val_mae: 46.5376\n",
      "Epoch 93/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 2.8700 - mae: 3.3698 - val_loss: 45.0492 - val_mae: 45.7239\n",
      "Epoch 94/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.5750 - mae: 3.0599 - val_loss: 46.1836 - val_mae: 46.8678\n",
      "Epoch 95/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.3923 - mae: 2.8778 - val_loss: 45.8321 - val_mae: 46.5098\n",
      "Epoch 96/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.5360 - mae: 3.0169 - val_loss: 46.9354 - val_mae: 47.6173\n",
      "Epoch 97/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 2.3602 - mae: 2.8349 - val_loss: 46.8134 - val_mae: 47.4981\n",
      "Epoch 98/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 2.4983 - mae: 2.9811 - val_loss: 45.7192 - val_mae: 46.4071\n",
      "Epoch 99/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 2.1682 - mae: 2.6305 - val_loss: 44.7216 - val_mae: 45.4068\n",
      "Epoch 100/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.1332 - mae: 2.5895 - val_loss: 46.3893 - val_mae: 47.0718\n",
      "Epoch 101/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.0590 - mae: 2.5169 - val_loss: 45.7402 - val_mae: 46.4224\n",
      "Epoch 102/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 2.1213 - mae: 2.5760 - val_loss: 46.0040 - val_mae: 46.6815\n",
      "Epoch 103/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 2.1030 - mae: 2.5559 - val_loss: 46.7228 - val_mae: 47.4044\n",
      "Epoch 104/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.3612 - mae: 2.8309 - val_loss: 45.8893 - val_mae: 46.5709\n",
      "Epoch 105/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 2.3214 - mae: 2.7856 - val_loss: 45.2843 - val_mae: 45.9666\n",
      "Epoch 106/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.2707 - mae: 2.7339 - val_loss: 46.7913 - val_mae: 47.4724\n",
      "Epoch 107/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.0288 - mae: 2.4782 - val_loss: 45.1116 - val_mae: 45.7876\n",
      "Epoch 108/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 2.1149 - mae: 2.5900 - val_loss: 45.2584 - val_mae: 45.9366\n",
      "Epoch 109/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.5717 - mae: 3.0474 - val_loss: 45.7674 - val_mae: 46.4478\n",
      "Epoch 110/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.4114 - mae: 2.8714 - val_loss: 46.1290 - val_mae: 46.8159\n",
      "Epoch 111/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 2.3548 - mae: 2.8199 - val_loss: 46.2742 - val_mae: 46.9521\n",
      "Epoch 112/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.2509 - mae: 2.7280 - val_loss: 45.9901 - val_mae: 46.6680\n",
      "Epoch 113/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.9951 - mae: 2.4527 - val_loss: 45.8179 - val_mae: 46.4974\n",
      "Epoch 114/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 2.5775 - mae: 3.0463 - val_loss: 43.0229 - val_mae: 43.7053\n",
      "Epoch 115/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.1127 - mae: 2.5817 - val_loss: 45.0839 - val_mae: 45.7694\n",
      "Epoch 116/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.0528 - mae: 2.5141 - val_loss: 46.6358 - val_mae: 47.3157\n",
      "Epoch 117/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 2.0792 - mae: 2.5435 - val_loss: 45.5012 - val_mae: 46.1831\n",
      "Epoch 118/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.0373 - mae: 2.4957 - val_loss: 46.4585 - val_mae: 47.1351\n",
      "Epoch 119/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.1240 - mae: 2.5781 - val_loss: 45.4894 - val_mae: 46.1706\n",
      "Epoch 120/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 4ms/step - loss: 2.2271 - mae: 2.6928 - val_loss: 46.0197 - val_mae: 46.6988\n",
      "Epoch 121/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.9834 - mae: 2.4337 - val_loss: 45.5129 - val_mae: 46.1957\n",
      "Epoch 122/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 2.2375 - mae: 2.7104 - val_loss: 45.3712 - val_mae: 46.0525\n",
      "Epoch 123/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 2.4770 - mae: 2.9551 - val_loss: 46.4609 - val_mae: 47.1382\n",
      "Epoch 124/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.9629 - mae: 2.4176 - val_loss: 45.6768 - val_mae: 46.3532\n",
      "Epoch 125/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.4739 - mae: 2.9549 - val_loss: 46.8414 - val_mae: 47.5177\n",
      "Epoch 126/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 2.7257 - mae: 3.2066 - val_loss: 46.7248 - val_mae: 47.4035\n",
      "Epoch 127/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.3031 - mae: 2.7711 - val_loss: 45.9932 - val_mae: 46.6715\n",
      "Epoch 128/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.9875 - mae: 2.4362 - val_loss: 45.5304 - val_mae: 46.2155\n",
      "Epoch 129/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 2.0670 - mae: 2.5158 - val_loss: 46.5412 - val_mae: 47.2241\n",
      "Epoch 130/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.2283 - mae: 2.6904 - val_loss: 45.4439 - val_mae: 46.1279\n",
      "Epoch 131/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.9759 - mae: 2.4235 - val_loss: 47.7065 - val_mae: 48.3796\n",
      "Epoch 132/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.2322 - mae: 2.7025 - val_loss: 47.5877 - val_mae: 48.2703\n",
      "Epoch 133/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.9816 - mae: 2.4394 - val_loss: 46.6824 - val_mae: 47.3689\n",
      "Epoch 134/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 2.0368 - mae: 2.4901 - val_loss: 46.6427 - val_mae: 47.3222\n",
      "Epoch 135/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.7286 - mae: 3.2035 - val_loss: 46.9583 - val_mae: 47.6409\n",
      "Epoch 136/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.3604 - mae: 2.8268 - val_loss: 47.4858 - val_mae: 48.1635\n",
      "Epoch 137/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.1175 - mae: 2.5819 - val_loss: 46.3922 - val_mae: 47.0739\n",
      "Epoch 138/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.9086 - mae: 2.3618 - val_loss: 47.2851 - val_mae: 47.9649\n",
      "Epoch 139/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.0537 - mae: 2.5081 - val_loss: 45.8087 - val_mae: 46.4889\n",
      "Epoch 140/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.0738 - mae: 2.5462 - val_loss: 46.6394 - val_mae: 47.3191\n",
      "Epoch 141/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 2.2241 - mae: 2.6867 - val_loss: 46.1312 - val_mae: 46.8140\n",
      "Epoch 142/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.9298 - mae: 2.3767 - val_loss: 45.0373 - val_mae: 45.7204\n",
      "Epoch 143/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.9684 - mae: 2.4213 - val_loss: 47.0845 - val_mae: 47.7647\n",
      "Epoch 144/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.1628 - mae: 2.6177 - val_loss: 45.8990 - val_mae: 46.5831\n",
      "Epoch 145/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.2088 - mae: 2.6861 - val_loss: 48.2428 - val_mae: 48.9213\n",
      "Epoch 146/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.3481 - mae: 2.8171 - val_loss: 45.5693 - val_mae: 46.2514\n",
      "Epoch 147/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.9690 - mae: 2.4232 - val_loss: 45.8530 - val_mae: 46.5337\n",
      "Epoch 148/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.8524 - mae: 2.3028 - val_loss: 47.1198 - val_mae: 47.8019\n",
      "Epoch 149/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.7911 - mae: 2.2389 - val_loss: 46.2256 - val_mae: 46.9038\n",
      "Epoch 150/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.7845 - mae: 2.2187 - val_loss: 46.0437 - val_mae: 46.7254\n",
      "Epoch 151/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.9042 - mae: 2.3617 - val_loss: 45.9817 - val_mae: 46.6634\n",
      "Epoch 152/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 1.9731 - mae: 2.4184 - val_loss: 45.8460 - val_mae: 46.5252\n",
      "Epoch 153/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.0773 - mae: 2.5342 - val_loss: 46.7276 - val_mae: 47.4029\n",
      "Epoch 154/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.1009 - mae: 2.5704 - val_loss: 46.7695 - val_mae: 47.4519\n",
      "Epoch 155/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.8590 - mae: 2.3122 - val_loss: 46.1910 - val_mae: 46.8718\n",
      "Epoch 156/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.7407 - mae: 2.1804 - val_loss: 46.7365 - val_mae: 47.4170\n",
      "Epoch 157/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.6150 - mae: 2.0412 - val_loss: 46.4582 - val_mae: 47.1356\n",
      "Epoch 158/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.8073 - mae: 2.2517 - val_loss: 44.9883 - val_mae: 45.6676\n",
      "Epoch 159/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.3107 - mae: 2.7907 - val_loss: 46.5956 - val_mae: 47.2773\n",
      "Epoch 160/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.8272 - mae: 2.2738 - val_loss: 46.0776 - val_mae: 46.7599\n",
      "Epoch 161/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1.7725 - mae: 2.2153 - val_loss: 46.0066 - val_mae: 46.6885\n",
      "Epoch 162/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.9443 - mae: 2.3913 - val_loss: 46.9772 - val_mae: 47.6617\n",
      "Epoch 163/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 2.2107 - mae: 2.6698 - val_loss: 45.5965 - val_mae: 46.2776\n",
      "Epoch 164/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 2.1097 - mae: 2.5861 - val_loss: 46.9707 - val_mae: 47.6540\n",
      "Epoch 165/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.7323 - mae: 2.1725 - val_loss: 45.5621 - val_mae: 46.2436\n",
      "Epoch 166/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.7747 - mae: 2.2126 - val_loss: 46.5717 - val_mae: 47.2541\n",
      "Epoch 167/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.7787 - mae: 2.2258 - val_loss: 45.0874 - val_mae: 45.7707\n",
      "Epoch 168/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.7777 - mae: 2.2164 - val_loss: 46.5089 - val_mae: 47.1911\n",
      "Epoch 169/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.7464 - mae: 2.1843 - val_loss: 47.5728 - val_mae: 48.2551\n",
      "Epoch 170/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.8470 - mae: 2.2898 - val_loss: 47.3413 - val_mae: 48.0212\n",
      "Epoch 171/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.8993 - mae: 2.3411 - val_loss: 45.7193 - val_mae: 46.4025\n",
      "Epoch 172/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.7454 - mae: 2.1864 - val_loss: 45.4752 - val_mae: 46.1585\n",
      "Epoch 173/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.7160 - mae: 2.1517 - val_loss: 45.1632 - val_mae: 45.8445\n",
      "Epoch 174/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.9442 - mae: 2.3844 - val_loss: 46.1604 - val_mae: 46.8415\n",
      "Epoch 175/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.5980 - mae: 2.0431 - val_loss: 46.3036 - val_mae: 46.9845\n",
      "Epoch 176/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1.7082 - mae: 2.1432 - val_loss: 44.7312 - val_mae: 45.4150\n",
      "Epoch 177/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.7240 - mae: 2.1561 - val_loss: 45.6110 - val_mae: 46.2926\n",
      "Epoch 178/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.9536 - mae: 2.4090 - val_loss: 46.4444 - val_mae: 47.1287\n",
      "Epoch 179/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1.8885 - mae: 2.3305 - val_loss: 45.4385 - val_mae: 46.1216\n",
      "Epoch 180/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 3ms/step - loss: 1.7396 - mae: 2.1811 - val_loss: 46.4824 - val_mae: 47.1623\n",
      "Epoch 181/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.5587 - mae: 1.9747 - val_loss: 47.0573 - val_mae: 47.7408\n",
      "Epoch 182/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1.9858 - mae: 2.4372 - val_loss: 45.9508 - val_mae: 46.6318\n",
      "Epoch 183/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.5840 - mae: 2.0097 - val_loss: 46.5781 - val_mae: 47.2580\n",
      "Epoch 184/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.7671 - mae: 2.2173 - val_loss: 45.8359 - val_mae: 46.5157\n",
      "Epoch 185/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.6552 - mae: 2.0790 - val_loss: 46.3956 - val_mae: 47.0744\n",
      "Epoch 186/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.6311 - mae: 2.0627 - val_loss: 45.7800 - val_mae: 46.4629\n",
      "Epoch 187/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.7296 - mae: 2.1602 - val_loss: 46.7834 - val_mae: 47.4622\n",
      "Epoch 188/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.8855 - mae: 2.3264 - val_loss: 46.2209 - val_mae: 46.9010\n",
      "Epoch 189/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.6833 - mae: 2.1160 - val_loss: 46.0309 - val_mae: 46.7098\n",
      "Epoch 190/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.6448 - mae: 2.0734 - val_loss: 45.1409 - val_mae: 45.8245\n",
      "Epoch 191/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.6843 - mae: 2.1139 - val_loss: 46.5130 - val_mae: 47.1914\n",
      "Epoch 192/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.7512 - mae: 2.1864 - val_loss: 46.1446 - val_mae: 46.8272\n",
      "Epoch 193/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.4816 - mae: 1.9032 - val_loss: 46.4850 - val_mae: 47.1643\n",
      "Epoch 194/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.6326 - mae: 2.0733 - val_loss: 45.8707 - val_mae: 46.5517\n",
      "Epoch 195/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.6937 - mae: 2.1249 - val_loss: 46.1565 - val_mae: 46.8348\n",
      "Epoch 196/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.6297 - mae: 2.0708 - val_loss: 48.0229 - val_mae: 48.7093\n",
      "Epoch 197/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.8094 - mae: 2.2404 - val_loss: 45.3404 - val_mae: 46.0212\n",
      "Epoch 198/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.7931 - mae: 2.2373 - val_loss: 46.5397 - val_mae: 47.2197\n",
      "Epoch 199/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.5491 - mae: 1.9760 - val_loss: 46.3048 - val_mae: 46.9821\n",
      "Epoch 200/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.4618 - mae: 1.8927 - val_loss: 46.3061 - val_mae: 46.9834\n",
      "Epoch 201/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.5209 - mae: 1.9509 - val_loss: 46.2911 - val_mae: 46.9748\n",
      "Epoch 202/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.5151 - mae: 1.9495 - val_loss: 47.5815 - val_mae: 48.2600\n",
      "Epoch 203/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.7145 - mae: 2.1579 - val_loss: 44.6814 - val_mae: 45.3650\n",
      "Epoch 204/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.5060 - mae: 1.9294 - val_loss: 46.6325 - val_mae: 47.3126\n",
      "Epoch 205/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1.6875 - mae: 2.1112 - val_loss: 47.2797 - val_mae: 47.9631\n",
      "Epoch 206/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.6169 - mae: 2.0534 - val_loss: 47.3759 - val_mae: 48.0566\n",
      "Epoch 207/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.6976 - mae: 2.1368 - val_loss: 46.6749 - val_mae: 47.3554\n",
      "Epoch 208/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1.5555 - mae: 1.9756 - val_loss: 45.8571 - val_mae: 46.5376\n",
      "Epoch 209/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.6878 - mae: 2.1356 - val_loss: 47.0698 - val_mae: 47.7541\n",
      "Epoch 210/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1.6346 - mae: 2.0704 - val_loss: 45.2701 - val_mae: 45.9501\n",
      "Epoch 211/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.4706 - mae: 1.8890 - val_loss: 46.3613 - val_mae: 47.0423\n",
      "Epoch 212/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.7871 - mae: 2.2235 - val_loss: 45.6575 - val_mae: 46.3391\n",
      "Epoch 213/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.6333 - mae: 2.0640 - val_loss: 46.3660 - val_mae: 47.0489\n",
      "Epoch 214/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.6407 - mae: 2.0737 - val_loss: 46.9329 - val_mae: 47.6138\n",
      "Epoch 215/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.4781 - mae: 1.9144 - val_loss: 46.4205 - val_mae: 47.1025\n",
      "Epoch 216/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.6544 - mae: 2.0823 - val_loss: 47.7358 - val_mae: 48.4132\n",
      "Epoch 217/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.6953 - mae: 2.1352 - val_loss: 46.4541 - val_mae: 47.1362\n",
      "Epoch 218/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.4527 - mae: 1.8720 - val_loss: 45.8239 - val_mae: 46.5067\n",
      "Epoch 219/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.5654 - mae: 1.9910 - val_loss: 46.7954 - val_mae: 47.4761\n",
      "Epoch 220/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.6162 - mae: 2.0641 - val_loss: 46.3043 - val_mae: 46.9824\n",
      "Epoch 221/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.4917 - mae: 1.9128 - val_loss: 46.6500 - val_mae: 47.3298\n",
      "Epoch 222/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1.5921 - mae: 2.0182 - val_loss: 46.0219 - val_mae: 46.7043\n",
      "Epoch 223/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.4818 - mae: 1.9136 - val_loss: 46.8072 - val_mae: 47.4916\n",
      "Epoch 224/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.7696 - mae: 2.2121 - val_loss: 45.9489 - val_mae: 46.6289\n",
      "Epoch 225/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.7346 - mae: 2.1708 - val_loss: 46.5537 - val_mae: 47.2351\n",
      "Epoch 226/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.5769 - mae: 1.9976 - val_loss: 46.0200 - val_mae: 46.6996\n",
      "Epoch 227/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.4825 - mae: 1.8943 - val_loss: 46.7088 - val_mae: 47.3905\n",
      "Epoch 228/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.6026 - mae: 2.0317 - val_loss: 46.1327 - val_mae: 46.8133\n",
      "Epoch 229/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.6251 - mae: 2.0631 - val_loss: 46.5080 - val_mae: 47.1922\n",
      "Epoch 230/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.5438 - mae: 1.9566 - val_loss: 47.1811 - val_mae: 47.8633\n",
      "Epoch 231/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.4221 - mae: 1.8479 - val_loss: 46.5956 - val_mae: 47.2786\n",
      "Epoch 232/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.7706 - mae: 2.2140 - val_loss: 46.0989 - val_mae: 46.7772\n",
      "Epoch 233/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.4846 - mae: 1.9041 - val_loss: 46.3368 - val_mae: 47.0151\n",
      "Epoch 234/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.5098 - mae: 1.9235 - val_loss: 46.0449 - val_mae: 46.7248\n",
      "Epoch 235/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.5385 - mae: 1.9670 - val_loss: 46.2688 - val_mae: 46.9519\n",
      "Epoch 236/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1.6112 - mae: 2.0477 - val_loss: 45.8317 - val_mae: 46.5141\n",
      "Epoch 237/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.5871 - mae: 2.0145 - val_loss: 46.7619 - val_mae: 47.4437\n",
      "Epoch 238/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.4062 - mae: 1.8112 - val_loss: 47.0085 - val_mae: 47.6878\n",
      "Epoch 239/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.5770 - mae: 2.0149 - val_loss: 45.7020 - val_mae: 46.3805\n",
      "Epoch 240/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 4ms/step - loss: 1.3914 - mae: 1.8128 - val_loss: 45.7983 - val_mae: 46.4816\n",
      "Epoch 241/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.4938 - mae: 1.9166 - val_loss: 45.2581 - val_mae: 45.9379\n",
      "Epoch 242/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.4694 - mae: 1.8948 - val_loss: 45.4190 - val_mae: 46.0982\n",
      "Epoch 243/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.5616 - mae: 1.9855 - val_loss: 46.1298 - val_mae: 46.8097\n",
      "Epoch 244/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.4442 - mae: 1.8600 - val_loss: 47.2849 - val_mae: 47.9660\n",
      "Epoch 245/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.3364 - mae: 1.7536 - val_loss: 46.5095 - val_mae: 47.1897\n",
      "Epoch 246/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.4390 - mae: 1.8572 - val_loss: 46.3823 - val_mae: 47.0629\n",
      "Epoch 247/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.4231 - mae: 1.8488 - val_loss: 45.6395 - val_mae: 46.3217\n",
      "Epoch 248/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.4464 - mae: 1.8668 - val_loss: 47.0814 - val_mae: 47.7610\n",
      "Epoch 249/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.3733 - mae: 1.7916 - val_loss: 45.7369 - val_mae: 46.4211\n",
      "Epoch 250/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.6342 - mae: 2.0705 - val_loss: 45.4283 - val_mae: 46.1139\n",
      "Epoch 251/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.4898 - mae: 1.9167 - val_loss: 47.9362 - val_mae: 48.6182\n",
      "Epoch 252/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.7068 - mae: 2.1393 - val_loss: 45.3994 - val_mae: 46.0844\n",
      "Epoch 253/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.4386 - mae: 1.8571 - val_loss: 46.3770 - val_mae: 47.0574\n",
      "Epoch 254/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.5827 - mae: 2.0014 - val_loss: 44.7930 - val_mae: 45.4738\n",
      "Epoch 255/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.4736 - mae: 1.8978 - val_loss: 46.3254 - val_mae: 47.0104\n",
      "Epoch 256/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.4007 - mae: 1.8176 - val_loss: 46.5960 - val_mae: 47.2779\n",
      "Epoch 257/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.4027 - mae: 1.8188 - val_loss: 46.2488 - val_mae: 46.9310\n",
      "Epoch 258/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.5298 - mae: 1.9632 - val_loss: 48.0364 - val_mae: 48.7195\n",
      "Epoch 259/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.4857 - mae: 1.8988 - val_loss: 45.4935 - val_mae: 46.1761\n",
      "Epoch 260/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.3503 - mae: 1.7689 - val_loss: 45.6703 - val_mae: 46.3516\n",
      "Epoch 261/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.4499 - mae: 1.8752 - val_loss: 46.3167 - val_mae: 47.0019\n",
      "Epoch 262/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.5703 - mae: 1.9996 - val_loss: 46.5004 - val_mae: 47.1808\n",
      "Epoch 263/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.5336 - mae: 1.9539 - val_loss: 45.9799 - val_mae: 46.6620\n",
      "Epoch 264/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.3619 - mae: 1.7798 - val_loss: 45.9039 - val_mae: 46.5838\n",
      "Epoch 265/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.5739 - mae: 2.0058 - val_loss: 46.2004 - val_mae: 46.8873\n",
      "Epoch 266/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.4884 - mae: 1.9159 - val_loss: 45.8369 - val_mae: 46.5190\n",
      "Epoch 267/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.5143 - mae: 1.9348 - val_loss: 46.9487 - val_mae: 47.6311\n",
      "Epoch 268/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.5831 - mae: 2.0159 - val_loss: 45.7192 - val_mae: 46.4063\n",
      "Epoch 269/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.2060 - mae: 1.6072 - val_loss: 45.7341 - val_mae: 46.4203\n",
      "Epoch 270/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.3179 - mae: 1.7427 - val_loss: 46.3924 - val_mae: 47.0695\n",
      "Epoch 271/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.3111 - mae: 1.7253 - val_loss: 46.5708 - val_mae: 47.2531\n",
      "Epoch 272/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.4217 - mae: 1.8442 - val_loss: 46.1300 - val_mae: 46.8122\n",
      "Epoch 273/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.4716 - mae: 1.8865 - val_loss: 47.3249 - val_mae: 48.0043\n",
      "Epoch 274/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.3491 - mae: 1.7640 - val_loss: 46.5976 - val_mae: 47.2793\n",
      "Epoch 275/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.3508 - mae: 1.7623 - val_loss: 46.4743 - val_mae: 47.1555\n",
      "Epoch 276/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.1448 - mae: 1.5494 - val_loss: 46.9639 - val_mae: 47.6471\n",
      "Epoch 277/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.3373 - mae: 1.7473 - val_loss: 46.7803 - val_mae: 47.4608\n",
      "Epoch 278/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.3987 - mae: 1.8119 - val_loss: 46.3755 - val_mae: 47.0532\n",
      "Epoch 279/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.4891 - mae: 1.9161 - val_loss: 46.8866 - val_mae: 47.5656\n",
      "Epoch 280/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.2677 - mae: 1.6791 - val_loss: 46.5159 - val_mae: 47.1951\n",
      "Epoch 281/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.5368 - mae: 1.9547 - val_loss: 45.8427 - val_mae: 46.5259\n",
      "Epoch 282/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.2912 - mae: 1.7119 - val_loss: 46.2107 - val_mae: 46.8956\n",
      "Epoch 283/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1.5041 - mae: 1.9313 - val_loss: 46.6872 - val_mae: 47.3687\n",
      "Epoch 284/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.2383 - mae: 1.6504 - val_loss: 46.3190 - val_mae: 47.0016\n",
      "Epoch 285/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.3468 - mae: 1.7703 - val_loss: 46.6826 - val_mae: 47.3621\n",
      "Epoch 286/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1.3689 - mae: 1.7903 - val_loss: 45.8187 - val_mae: 46.5022\n",
      "Epoch 287/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1.2706 - mae: 1.6725 - val_loss: 46.9890 - val_mae: 47.6716\n",
      "Epoch 288/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.3032 - mae: 1.7137 - val_loss: 46.4124 - val_mae: 47.0977\n",
      "Epoch 289/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1.4372 - mae: 1.8546 - val_loss: 46.2454 - val_mae: 46.9234\n",
      "Epoch 290/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.7024 - mae: 2.1340 - val_loss: 46.1097 - val_mae: 46.7922\n",
      "Epoch 291/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.6878 - mae: 2.1302 - val_loss: 46.1177 - val_mae: 46.7999\n",
      "Epoch 292/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.2397 - mae: 1.6471 - val_loss: 46.9454 - val_mae: 47.6258\n",
      "Epoch 293/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.3534 - mae: 1.7710 - val_loss: 47.1363 - val_mae: 47.8206\n",
      "Epoch 294/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.3354 - mae: 1.7556 - val_loss: 46.0446 - val_mae: 46.7290\n",
      "Epoch 295/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.2380 - mae: 1.6497 - val_loss: 46.4410 - val_mae: 47.1209\n",
      "Epoch 296/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1.2623 - mae: 1.6623 - val_loss: 45.8181 - val_mae: 46.5000\n",
      "Epoch 297/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.2088 - mae: 1.6144 - val_loss: 45.3784 - val_mae: 46.0607\n",
      "Epoch 298/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1.2458 - mae: 1.6679 - val_loss: 46.4416 - val_mae: 47.1246\n",
      "Epoch 299/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1.2913 - mae: 1.7092 - val_loss: 45.6662 - val_mae: 46.3520\n",
      "Epoch 300/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 4ms/step - loss: 1.4273 - mae: 1.8531 - val_loss: 45.7895 - val_mae: 46.4710\n",
      "Epoch 301/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.5652 - mae: 1.9949 - val_loss: 46.8475 - val_mae: 47.5263\n",
      "Epoch 302/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.3565 - mae: 1.7731 - val_loss: 46.3476 - val_mae: 47.0340\n",
      "Epoch 303/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.2663 - mae: 1.6689 - val_loss: 46.9403 - val_mae: 47.6230\n",
      "Epoch 304/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.2067 - mae: 1.6220 - val_loss: 46.9435 - val_mae: 47.6260\n",
      "Epoch 305/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.4886 - mae: 1.9131 - val_loss: 46.4090 - val_mae: 47.0920\n",
      "Epoch 306/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.3043 - mae: 1.7157 - val_loss: 45.5204 - val_mae: 46.2050\n",
      "Epoch 307/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.7353 - mae: 2.1755 - val_loss: 47.8571 - val_mae: 48.5399\n",
      "Epoch 308/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.1993 - mae: 1.5984 - val_loss: 46.8040 - val_mae: 47.4866\n",
      "Epoch 309/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.3003 - mae: 1.7018 - val_loss: 46.8127 - val_mae: 47.4950\n",
      "Epoch 310/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.1421 - mae: 1.5412 - val_loss: 46.3887 - val_mae: 47.0749\n",
      "Epoch 311/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.2025 - mae: 1.6043 - val_loss: 47.2199 - val_mae: 47.9046\n",
      "Epoch 312/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.7089 - mae: 2.1533 - val_loss: 45.8491 - val_mae: 46.5366\n",
      "Epoch 313/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.2580 - mae: 1.6779 - val_loss: 46.5030 - val_mae: 47.1818\n",
      "Epoch 314/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.2662 - mae: 1.6861 - val_loss: 46.2369 - val_mae: 46.9188\n",
      "Epoch 315/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.3065 - mae: 1.7124 - val_loss: 46.7751 - val_mae: 47.4581\n",
      "Epoch 316/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.3206 - mae: 1.7380 - val_loss: 46.3448 - val_mae: 47.0299\n",
      "Epoch 317/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.5061 - mae: 1.9402 - val_loss: 47.3894 - val_mae: 48.0708\n",
      "Epoch 318/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.3018 - mae: 1.7172 - val_loss: 47.4488 - val_mae: 48.1328\n",
      "Epoch 319/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.2857 - mae: 1.7091 - val_loss: 47.5641 - val_mae: 48.2496\n",
      "Epoch 320/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.5840 - mae: 2.0316 - val_loss: 47.1898 - val_mae: 47.8714\n",
      "Epoch 321/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.2130 - mae: 1.6246 - val_loss: 46.6116 - val_mae: 47.2966\n",
      "Epoch 322/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.1704 - mae: 1.5736 - val_loss: 46.1713 - val_mae: 46.8524\n",
      "Epoch 323/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.0951 - mae: 1.4862 - val_loss: 47.4146 - val_mae: 48.0988\n",
      "Epoch 324/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.2617 - mae: 1.6738 - val_loss: 45.9434 - val_mae: 46.6299\n",
      "Epoch 325/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.3106 - mae: 1.7234 - val_loss: 46.8040 - val_mae: 47.4854\n",
      "Epoch 326/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.3480 - mae: 1.7853 - val_loss: 47.4572 - val_mae: 48.1411\n",
      "Epoch 327/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.2687 - mae: 1.6794 - val_loss: 46.0178 - val_mae: 46.7003\n",
      "Epoch 328/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.2581 - mae: 1.6673 - val_loss: 46.9892 - val_mae: 47.6709\n",
      "Epoch 329/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.1821 - mae: 1.5949 - val_loss: 47.2278 - val_mae: 47.9127\n",
      "Epoch 330/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1.2322 - mae: 1.6454 - val_loss: 45.5102 - val_mae: 46.1956\n",
      "Epoch 331/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.3322 - mae: 1.7492 - val_loss: 46.2849 - val_mae: 46.9646\n",
      "Epoch 332/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.3561 - mae: 1.7750 - val_loss: 45.9234 - val_mae: 46.6094\n",
      "Epoch 333/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.3176 - mae: 1.7259 - val_loss: 46.7062 - val_mae: 47.3911\n",
      "Epoch 334/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.0471 - mae: 1.4421 - val_loss: 45.8729 - val_mae: 46.5549\n",
      "Epoch 335/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.1975 - mae: 1.6022 - val_loss: 45.8592 - val_mae: 46.5424\n",
      "Epoch 336/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.1441 - mae: 1.5523 - val_loss: 46.3357 - val_mae: 47.0156\n",
      "Epoch 337/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.3356 - mae: 1.7349 - val_loss: 46.3952 - val_mae: 47.0824\n",
      "Epoch 338/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1.2516 - mae: 1.6493 - val_loss: 46.4256 - val_mae: 47.1098\n",
      "Epoch 339/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.2167 - mae: 1.6174 - val_loss: 46.5080 - val_mae: 47.1873\n",
      "Epoch 340/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.2835 - mae: 1.7041 - val_loss: 46.7530 - val_mae: 47.4352\n",
      "Epoch 341/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.0945 - mae: 1.4859 - val_loss: 45.6057 - val_mae: 46.2879\n",
      "Epoch 342/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.3143 - mae: 1.7243 - val_loss: 46.2002 - val_mae: 46.8843\n",
      "Epoch 343/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.3464 - mae: 1.7597 - val_loss: 47.1961 - val_mae: 47.8787\n",
      "Epoch 344/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.2026 - mae: 1.6058 - val_loss: 46.5841 - val_mae: 47.2645\n",
      "Epoch 345/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.4291 - mae: 1.8560 - val_loss: 45.9401 - val_mae: 46.6235\n",
      "Epoch 346/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1.0689 - mae: 1.4582 - val_loss: 46.6794 - val_mae: 47.3609\n",
      "Epoch 347/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1.3254 - mae: 1.7443 - val_loss: 46.2591 - val_mae: 46.9406\n",
      "Epoch 348/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.4802 - mae: 1.9174 - val_loss: 44.9664 - val_mae: 45.6498\n",
      "Epoch 349/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.2609 - mae: 1.6752 - val_loss: 47.3968 - val_mae: 48.0806\n",
      "Epoch 350/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.2225 - mae: 1.6376 - val_loss: 45.7654 - val_mae: 46.4507\n",
      "Epoch 351/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.2307 - mae: 1.6357 - val_loss: 46.9835 - val_mae: 47.6659\n",
      "Epoch 352/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.3733 - mae: 1.7822 - val_loss: 46.1458 - val_mae: 46.8283\n",
      "Epoch 353/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.0711 - mae: 1.4614 - val_loss: 46.1403 - val_mae: 46.8227\n",
      "Epoch 354/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.2816 - mae: 1.6905 - val_loss: 47.3291 - val_mae: 48.0109\n",
      "Epoch 355/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.0863 - mae: 1.4841 - val_loss: 46.3438 - val_mae: 47.0240\n",
      "Epoch 356/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.3168 - mae: 1.7346 - val_loss: 45.6400 - val_mae: 46.3253\n",
      "Epoch 357/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.5444 - mae: 1.9685 - val_loss: 45.5900 - val_mae: 46.2732\n",
      "Epoch 358/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1.1211 - mae: 1.5146 - val_loss: 47.3147 - val_mae: 47.9983\n",
      "Epoch 359/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.2265 - mae: 1.6380 - val_loss: 46.5056 - val_mae: 47.1917\n",
      "Epoch 360/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 5ms/step - loss: 1.2200 - mae: 1.6146 - val_loss: 47.0793 - val_mae: 47.7665\n",
      "Epoch 361/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.1932 - mae: 1.6007 - val_loss: 46.2298 - val_mae: 46.9137\n",
      "Epoch 362/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1.0993 - mae: 1.4939 - val_loss: 46.2796 - val_mae: 46.9630\n",
      "Epoch 363/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.1079 - mae: 1.4981 - val_loss: 46.8492 - val_mae: 47.5325\n",
      "Epoch 364/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.9921 - mae: 1.3705 - val_loss: 47.2001 - val_mae: 47.8824\n",
      "Epoch 365/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.0317 - mae: 1.4148 - val_loss: 46.5041 - val_mae: 47.1855\n",
      "Epoch 366/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.1679 - mae: 1.5731 - val_loss: 46.4552 - val_mae: 47.1353\n",
      "Epoch 367/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.1217 - mae: 1.5254 - val_loss: 46.5015 - val_mae: 47.1859\n",
      "Epoch 368/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.2972 - mae: 1.6953 - val_loss: 46.3141 - val_mae: 46.9961\n",
      "Epoch 369/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1.2254 - mae: 1.6331 - val_loss: 46.4902 - val_mae: 47.1744\n",
      "Epoch 370/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1.1060 - mae: 1.5037 - val_loss: 46.6686 - val_mae: 47.3537\n",
      "Epoch 371/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1.1279 - mae: 1.5355 - val_loss: 45.8828 - val_mae: 46.5650\n",
      "Epoch 372/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1.1161 - mae: 1.5140 - val_loss: 46.8933 - val_mae: 47.5770\n",
      "Epoch 373/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1.3169 - mae: 1.7324 - val_loss: 46.3623 - val_mae: 47.0466\n",
      "Epoch 374/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1.1048 - mae: 1.4916 - val_loss: 46.4822 - val_mae: 47.1660\n",
      "Epoch 375/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1.1027 - mae: 1.4993 - val_loss: 46.4291 - val_mae: 47.1142\n",
      "Epoch 376/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.2919 - mae: 1.7150 - val_loss: 46.7012 - val_mae: 47.3847\n",
      "Epoch 377/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.2943 - mae: 1.7096 - val_loss: 47.1897 - val_mae: 47.8725\n",
      "Epoch 378/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1.0942 - mae: 1.4882 - val_loss: 47.3057 - val_mae: 47.9903\n",
      "Epoch 379/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.1341 - mae: 1.5194 - val_loss: 46.7955 - val_mae: 47.4763\n",
      "Epoch 380/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.0674 - mae: 1.4551 - val_loss: 47.8149 - val_mae: 48.4991\n",
      "Epoch 381/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.3630 - mae: 1.7889 - val_loss: 45.5472 - val_mae: 46.2330\n",
      "Epoch 382/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.1682 - mae: 1.5700 - val_loss: 46.5978 - val_mae: 47.2819\n",
      "Epoch 383/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.0969 - mae: 1.4912 - val_loss: 45.9841 - val_mae: 46.6642\n",
      "Epoch 384/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.1658 - mae: 1.5489 - val_loss: 46.8187 - val_mae: 47.5013\n",
      "Epoch 385/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.1992 - mae: 1.5980 - val_loss: 45.7008 - val_mae: 46.3868\n",
      "Epoch 386/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.0875 - mae: 1.4722 - val_loss: 45.6707 - val_mae: 46.3520\n",
      "Epoch 387/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.4902 - mae: 1.9175 - val_loss: 48.2129 - val_mae: 48.9001\n",
      "Epoch 388/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.4041 - mae: 1.8161 - val_loss: 47.3680 - val_mae: 48.0531\n",
      "Epoch 389/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.2482 - mae: 1.6657 - val_loss: 45.7519 - val_mae: 46.4330\n",
      "Epoch 390/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.0712 - mae: 1.4652 - val_loss: 46.8299 - val_mae: 47.5128\n",
      "Epoch 391/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.1727 - mae: 1.5640 - val_loss: 46.1494 - val_mae: 46.8299\n",
      "Epoch 392/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 1.0704 - mae: 1.4648 - val_loss: 46.0980 - val_mae: 46.7843\n",
      "Epoch 393/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.0047 - mae: 1.3835 - val_loss: 45.5331 - val_mae: 46.2195\n",
      "Epoch 394/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.1655 - mae: 1.5600 - val_loss: 45.7070 - val_mae: 46.3917\n",
      "Epoch 395/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.3138 - mae: 1.7251 - val_loss: 45.8420 - val_mae: 46.5227\n",
      "Epoch 396/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.0391 - mae: 1.4386 - val_loss: 46.9469 - val_mae: 47.6269\n",
      "Epoch 397/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.0434 - mae: 1.4308 - val_loss: 47.1666 - val_mae: 47.8482\n",
      "Epoch 398/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.1712 - mae: 1.5624 - val_loss: 46.9678 - val_mae: 47.6497\n",
      "Epoch 399/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.9953 - mae: 1.3910 - val_loss: 47.1442 - val_mae: 47.8252\n",
      "Epoch 400/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.1311 - mae: 1.5363 - val_loss: 46.2799 - val_mae: 46.9607\n",
      "Epoch 401/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.0937 - mae: 1.4964 - val_loss: 46.6800 - val_mae: 47.3634\n",
      "Epoch 402/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.9256 - mae: 1.3040 - val_loss: 47.2140 - val_mae: 47.8961\n",
      "Epoch 403/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 1.1992 - mae: 1.6023 - val_loss: 46.6203 - val_mae: 47.3050\n",
      "Epoch 404/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.1794 - mae: 1.5816 - val_loss: 45.7035 - val_mae: 46.3849\n",
      "Epoch 405/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.9238 - mae: 1.2955 - val_loss: 47.5227 - val_mae: 48.2045\n",
      "Epoch 406/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.9449 - mae: 1.3189 - val_loss: 46.2499 - val_mae: 46.9320\n",
      "Epoch 407/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.1404 - mae: 1.5370 - val_loss: 46.1023 - val_mae: 46.7846\n",
      "Epoch 408/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 1.0816 - mae: 1.4881 - val_loss: 46.8701 - val_mae: 47.5493\n",
      "Epoch 409/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 1.0377 - mae: 1.4328 - val_loss: 46.0948 - val_mae: 46.7788\n",
      "Epoch 410/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.0993 - mae: 1.4951 - val_loss: 46.2940 - val_mae: 46.9736\n",
      "Epoch 411/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.1812 - mae: 1.5860 - val_loss: 45.9788 - val_mae: 46.6557\n",
      "Epoch 412/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.0495 - mae: 1.4520 - val_loss: 46.6917 - val_mae: 47.3769\n",
      "Epoch 413/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.1172 - mae: 1.5251 - val_loss: 46.2377 - val_mae: 46.9192\n",
      "Epoch 414/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.0627 - mae: 1.4604 - val_loss: 47.2297 - val_mae: 47.9106\n",
      "Epoch 415/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.9659 - mae: 1.3453 - val_loss: 46.6367 - val_mae: 47.3185\n",
      "Epoch 416/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.1410 - mae: 1.5348 - val_loss: 46.6434 - val_mae: 47.3263\n",
      "Epoch 417/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.9397 - mae: 1.3312 - val_loss: 46.4667 - val_mae: 47.1476\n",
      "Epoch 418/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.1478 - mae: 1.5376 - val_loss: 46.5630 - val_mae: 47.2480\n",
      "Epoch 419/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.9758 - mae: 1.3703 - val_loss: 46.6834 - val_mae: 47.3649\n",
      "Epoch 420/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 4ms/step - loss: 1.1246 - mae: 1.5041 - val_loss: 46.1375 - val_mae: 46.8187\n",
      "Epoch 421/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.9329 - mae: 1.3161 - val_loss: 47.6487 - val_mae: 48.3318\n",
      "Epoch 422/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.0343 - mae: 1.4183 - val_loss: 46.6877 - val_mae: 47.3655\n",
      "Epoch 423/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.0541 - mae: 1.4307 - val_loss: 46.3434 - val_mae: 47.0256\n",
      "Epoch 424/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1.1606 - mae: 1.5557 - val_loss: 47.3563 - val_mae: 48.0390\n",
      "Epoch 425/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1.0858 - mae: 1.4817 - val_loss: 46.6532 - val_mae: 47.3310\n",
      "Epoch 426/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.9669 - mae: 1.3597 - val_loss: 46.4851 - val_mae: 47.1678\n",
      "Epoch 427/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1.0064 - mae: 1.4043 - val_loss: 46.8875 - val_mae: 47.5683\n",
      "Epoch 428/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.0040 - mae: 1.3830 - val_loss: 45.7604 - val_mae: 46.4389\n",
      "Epoch 429/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.0080 - mae: 1.4052 - val_loss: 46.7628 - val_mae: 47.4454\n",
      "Epoch 430/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.0990 - mae: 1.4976 - val_loss: 46.2386 - val_mae: 46.9180\n",
      "Epoch 431/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.0302 - mae: 1.4248 - val_loss: 46.6573 - val_mae: 47.3381\n",
      "Epoch 432/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.8546 - mae: 1.2284 - val_loss: 46.6492 - val_mae: 47.3305\n",
      "Epoch 433/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.0206 - mae: 1.4150 - val_loss: 47.0469 - val_mae: 47.7311\n",
      "Epoch 434/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.0954 - mae: 1.4908 - val_loss: 45.9914 - val_mae: 46.6735\n",
      "Epoch 435/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.9709 - mae: 1.3437 - val_loss: 47.5913 - val_mae: 48.2737\n",
      "Epoch 436/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.0026 - mae: 1.3933 - val_loss: 47.2786 - val_mae: 47.9603\n",
      "Epoch 437/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.9381 - mae: 1.3192 - val_loss: 47.1386 - val_mae: 47.8194\n",
      "Epoch 438/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.1714 - mae: 1.5751 - val_loss: 46.2247 - val_mae: 46.9038\n",
      "Epoch 439/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.9491 - mae: 1.3310 - val_loss: 46.0784 - val_mae: 46.7609\n",
      "Epoch 440/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.1633 - mae: 1.5573 - val_loss: 46.3086 - val_mae: 46.9911\n",
      "Epoch 441/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.1714 - mae: 1.5642 - val_loss: 46.0900 - val_mae: 46.7677\n",
      "Epoch 442/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.9872 - mae: 1.3676 - val_loss: 46.1881 - val_mae: 46.8678\n",
      "Epoch 443/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.1478 - mae: 1.5445 - val_loss: 46.0664 - val_mae: 46.7493\n",
      "Epoch 444/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.9972 - mae: 1.3892 - val_loss: 46.7638 - val_mae: 47.4493\n",
      "Epoch 445/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.9618 - mae: 1.3433 - val_loss: 45.9000 - val_mae: 46.5824\n",
      "Epoch 446/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.0990 - mae: 1.4702 - val_loss: 46.9343 - val_mae: 47.6156\n",
      "Epoch 447/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.8940 - mae: 1.2766 - val_loss: 46.1018 - val_mae: 46.7852\n",
      "Epoch 448/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.1596 - mae: 1.5743 - val_loss: 46.6877 - val_mae: 47.3683\n",
      "Epoch 449/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.0441 - mae: 1.4294 - val_loss: 47.4055 - val_mae: 48.0877\n",
      "Epoch 450/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.1530 - mae: 1.5490 - val_loss: 46.7696 - val_mae: 47.4526\n",
      "Epoch 451/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.2914 - mae: 1.7155 - val_loss: 46.6277 - val_mae: 47.3074\n",
      "Epoch 452/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.1306 - mae: 1.5404 - val_loss: 46.2954 - val_mae: 46.9747\n",
      "Epoch 453/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.1714 - mae: 1.5762 - val_loss: 46.6277 - val_mae: 47.3072\n",
      "Epoch 454/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.0509 - mae: 1.4351 - val_loss: 47.5150 - val_mae: 48.1957\n",
      "Epoch 455/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.0935 - mae: 1.4924 - val_loss: 47.4150 - val_mae: 48.0944\n",
      "Epoch 456/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.0165 - mae: 1.4041 - val_loss: 47.1554 - val_mae: 47.8379\n",
      "Epoch 457/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.8974 - mae: 1.2667 - val_loss: 46.0035 - val_mae: 46.6843\n",
      "Epoch 458/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.1037 - mae: 1.5055 - val_loss: 46.1829 - val_mae: 46.8646\n",
      "Epoch 459/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.3177 - mae: 1.7402 - val_loss: 46.7142 - val_mae: 47.3960\n",
      "Epoch 460/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.1350 - mae: 1.5344 - val_loss: 46.4279 - val_mae: 47.1081\n",
      "Epoch 461/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.8413 - mae: 1.2181 - val_loss: 46.8529 - val_mae: 47.5345\n",
      "Epoch 462/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.0410 - mae: 1.4339 - val_loss: 47.0646 - val_mae: 47.7465\n",
      "Epoch 463/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.0632 - mae: 1.4499 - val_loss: 46.7489 - val_mae: 47.4305\n",
      "Epoch 464/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.8910 - mae: 1.2629 - val_loss: 46.1770 - val_mae: 46.8580\n",
      "Epoch 465/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.9578 - mae: 1.3344 - val_loss: 45.9610 - val_mae: 46.6455\n",
      "Epoch 466/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.9608 - mae: 1.3279 - val_loss: 46.3772 - val_mae: 47.0595\n",
      "Epoch 467/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.8908 - mae: 1.2656 - val_loss: 46.8043 - val_mae: 47.4855\n",
      "Epoch 468/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.9130 - mae: 1.3062 - val_loss: 45.7710 - val_mae: 46.4527\n",
      "Epoch 469/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 1.0142 - mae: 1.3977 - val_loss: 47.4821 - val_mae: 48.1650\n",
      "Epoch 470/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.9854 - mae: 1.3814 - val_loss: 48.2066 - val_mae: 48.8877\n",
      "Epoch 471/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.2514 - mae: 1.6633 - val_loss: 46.0276 - val_mae: 46.7097\n",
      "Epoch 472/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.0157 - mae: 1.4034 - val_loss: 46.5563 - val_mae: 47.2377\n",
      "Epoch 473/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.9141 - mae: 1.2838 - val_loss: 47.1968 - val_mae: 47.8757\n",
      "Epoch 474/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.0972 - mae: 1.5026 - val_loss: 47.6631 - val_mae: 48.3442\n",
      "Epoch 475/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.0492 - mae: 1.4262 - val_loss: 46.9504 - val_mae: 47.6285\n",
      "Epoch 476/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.8770 - mae: 1.2608 - val_loss: 46.5176 - val_mae: 47.1997\n",
      "Epoch 477/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1.0370 - mae: 1.4192 - val_loss: 46.0755 - val_mae: 46.7572\n",
      "Epoch 478/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.8851 - mae: 1.2707 - val_loss: 46.8358 - val_mae: 47.5148\n",
      "Epoch 479/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.0465 - mae: 1.4314 - val_loss: 46.8475 - val_mae: 47.5288\n",
      "Epoch 480/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 5ms/step - loss: 0.8814 - mae: 1.2554 - val_loss: 47.2046 - val_mae: 47.8828\n",
      "Epoch 481/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.9548 - mae: 1.3334 - val_loss: 46.5362 - val_mae: 47.2174\n",
      "Epoch 482/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.8963 - mae: 1.2748 - val_loss: 46.3528 - val_mae: 47.0360\n",
      "Epoch 483/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.9016 - mae: 1.2790 - val_loss: 47.4336 - val_mae: 48.1182\n",
      "Epoch 484/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.7677 - mae: 1.1325 - val_loss: 46.5451 - val_mae: 47.2291\n",
      "Epoch 485/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.9891 - mae: 1.3683 - val_loss: 47.7053 - val_mae: 48.3873\n",
      "Epoch 486/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.9934 - mae: 1.3891 - val_loss: 46.6170 - val_mae: 47.2992\n",
      "Epoch 487/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.9656 - mae: 1.3388 - val_loss: 47.0303 - val_mae: 47.7110\n",
      "Epoch 488/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.9237 - mae: 1.3097 - val_loss: 46.8198 - val_mae: 47.5007\n",
      "Epoch 489/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.9483 - mae: 1.3334 - val_loss: 47.1258 - val_mae: 47.8084\n",
      "Epoch 490/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.0970 - mae: 1.4931 - val_loss: 47.2585 - val_mae: 47.9431\n",
      "Epoch 491/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.0096 - mae: 1.3920 - val_loss: 46.3057 - val_mae: 46.9867\n",
      "Epoch 492/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.9899 - mae: 1.3727 - val_loss: 46.7877 - val_mae: 47.4735\n",
      "Epoch 493/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.0315 - mae: 1.4275 - val_loss: 47.7367 - val_mae: 48.4125\n",
      "Epoch 494/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.1580 - mae: 1.5583 - val_loss: 46.8701 - val_mae: 47.5526\n",
      "Epoch 495/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.9043 - mae: 1.2803 - val_loss: 47.3352 - val_mae: 48.0163\n",
      "Epoch 496/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.9109 - mae: 1.2927 - val_loss: 46.1421 - val_mae: 46.8201\n",
      "Epoch 497/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.0527 - mae: 1.4533 - val_loss: 46.6982 - val_mae: 47.3763\n",
      "Epoch 498/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.9428 - mae: 1.3121 - val_loss: 46.6576 - val_mae: 47.3402\n",
      "Epoch 499/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.9756 - mae: 1.3514 - val_loss: 46.7304 - val_mae: 47.4119\n",
      "Epoch 500/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1.0179 - mae: 1.4085 - val_loss: 46.7254 - val_mae: 47.4091\n",
      "Epoch 501/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1.1351 - mae: 1.5283 - val_loss: 46.2631 - val_mae: 46.9478\n",
      "Epoch 502/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.0971 - mae: 1.4955 - val_loss: 47.3496 - val_mae: 48.0315\n",
      "Epoch 503/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.9312 - mae: 1.3146 - val_loss: 46.8385 - val_mae: 47.5208\n",
      "Epoch 504/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.0698 - mae: 1.4691 - val_loss: 46.3125 - val_mae: 46.9948\n",
      "Epoch 505/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.9880 - mae: 1.3945 - val_loss: 46.2438 - val_mae: 46.9280\n",
      "Epoch 506/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.9503 - mae: 1.3486 - val_loss: 46.0533 - val_mae: 46.7374\n",
      "Epoch 507/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.9407 - mae: 1.3229 - val_loss: 47.4606 - val_mae: 48.1430\n",
      "Epoch 508/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.8242 - mae: 1.1813 - val_loss: 47.1443 - val_mae: 47.8256\n",
      "Epoch 509/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.8331 - mae: 1.2028 - val_loss: 47.6664 - val_mae: 48.3475\n",
      "Epoch 510/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.9411 - mae: 1.3139 - val_loss: 46.7097 - val_mae: 47.3930\n",
      "Epoch 511/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.7988 - mae: 1.1685 - val_loss: 46.9243 - val_mae: 47.6072\n",
      "Epoch 512/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.1399 - mae: 1.5390 - val_loss: 47.0481 - val_mae: 47.7281\n",
      "Epoch 513/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.8503 - mae: 1.2157 - val_loss: 48.2594 - val_mae: 48.9372\n",
      "Epoch 514/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.0389 - mae: 1.4207 - val_loss: 47.1356 - val_mae: 47.8158\n",
      "Epoch 515/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.0861 - mae: 1.4906 - val_loss: 46.4151 - val_mae: 47.0971\n",
      "Epoch 516/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.9246 - mae: 1.3050 - val_loss: 46.8301 - val_mae: 47.5132\n",
      "Epoch 517/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.9574 - mae: 1.3416 - val_loss: 47.1067 - val_mae: 47.7892\n",
      "Epoch 518/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.8286 - mae: 1.2016 - val_loss: 47.2739 - val_mae: 47.9528\n",
      "Epoch 519/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.9385 - mae: 1.3206 - val_loss: 46.7866 - val_mae: 47.4706\n",
      "Epoch 520/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.8124 - mae: 1.1758 - val_loss: 47.3678 - val_mae: 48.0473\n",
      "Epoch 521/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.0879 - mae: 1.4910 - val_loss: 46.5882 - val_mae: 47.2727\n",
      "Epoch 522/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.8998 - mae: 1.2745 - val_loss: 47.3673 - val_mae: 48.0507\n",
      "Epoch 523/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.8792 - mae: 1.2532 - val_loss: 45.9137 - val_mae: 46.5996\n",
      "Epoch 524/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.7529 - mae: 1.1127 - val_loss: 47.1047 - val_mae: 47.7869\n",
      "Epoch 525/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.8637 - mae: 1.2389 - val_loss: 46.9764 - val_mae: 47.6602\n",
      "Epoch 526/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.9715 - mae: 1.3696 - val_loss: 47.6057 - val_mae: 48.2887\n",
      "Epoch 527/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.8378 - mae: 1.2139 - val_loss: 47.4165 - val_mae: 48.1012\n",
      "Epoch 528/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.8724 - mae: 1.2478 - val_loss: 47.8923 - val_mae: 48.5770\n",
      "Epoch 529/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.8586 - mae: 1.2381 - val_loss: 46.8749 - val_mae: 47.5576\n",
      "Epoch 530/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.7999 - mae: 1.1617 - val_loss: 47.7805 - val_mae: 48.4645\n",
      "Epoch 531/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.0368 - mae: 1.4284 - val_loss: 47.6765 - val_mae: 48.3605\n",
      "Epoch 532/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.9328 - mae: 1.3123 - val_loss: 47.1025 - val_mae: 47.7854\n",
      "Epoch 533/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.7813 - mae: 1.1285 - val_loss: 47.7901 - val_mae: 48.4714\n",
      "Epoch 534/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1.0301 - mae: 1.4206 - val_loss: 47.6324 - val_mae: 48.3166\n",
      "Epoch 535/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.8555 - mae: 1.2373 - val_loss: 46.8012 - val_mae: 47.4850\n",
      "Epoch 536/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.8299 - mae: 1.2026 - val_loss: 47.9665 - val_mae: 48.6497\n",
      "Epoch 537/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.9634 - mae: 1.3474 - val_loss: 46.4012 - val_mae: 47.0849\n",
      "Epoch 538/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.0067 - mae: 1.4043 - val_loss: 47.3673 - val_mae: 48.0497\n",
      "Epoch 539/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1.1529 - mae: 1.5636 - val_loss: 47.3155 - val_mae: 47.9999\n",
      "Epoch 540/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 4ms/step - loss: 1.0403 - mae: 1.4303 - val_loss: 48.2623 - val_mae: 48.9439\n",
      "Epoch 541/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.0306 - mae: 1.4204 - val_loss: 46.6699 - val_mae: 47.3541\n",
      "Epoch 542/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.8694 - mae: 1.2464 - val_loss: 47.0687 - val_mae: 47.7541\n",
      "Epoch 543/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.9480 - mae: 1.3266 - val_loss: 47.5093 - val_mae: 48.1915\n",
      "Epoch 544/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.8803 - mae: 1.2613 - val_loss: 47.1298 - val_mae: 47.8117\n",
      "Epoch 545/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.8804 - mae: 1.2605 - val_loss: 47.4709 - val_mae: 48.1542\n",
      "Epoch 546/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.8106 - mae: 1.1706 - val_loss: 46.7129 - val_mae: 47.3959\n",
      "Epoch 547/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.9668 - mae: 1.3453 - val_loss: 47.0444 - val_mae: 47.7262\n",
      "Epoch 548/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.8698 - mae: 1.2376 - val_loss: 47.1733 - val_mae: 47.8576\n",
      "Epoch 549/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 0.7891 - mae: 1.1562 - val_loss: 46.6509 - val_mae: 47.3337\n",
      "Epoch 550/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 0.9127 - mae: 1.2663 - val_loss: 47.8697 - val_mae: 48.5534\n",
      "Epoch 551/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.9633 - mae: 1.3466 - val_loss: 46.7250 - val_mae: 47.4082\n",
      "Epoch 552/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.8071 - mae: 1.1675 - val_loss: 46.7777 - val_mae: 47.4611\n",
      "Epoch 553/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.7160 - mae: 1.0615 - val_loss: 46.9384 - val_mae: 47.6221\n",
      "Epoch 554/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.7987 - mae: 1.1545 - val_loss: 46.5130 - val_mae: 47.1942\n",
      "Epoch 555/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.7432 - mae: 1.0989 - val_loss: 46.6947 - val_mae: 47.3768\n",
      "Epoch 556/5000\n",
      " 1/46 [..............................] - ETA: 0s - loss: 0.5267 - mae: 0.8898"
     ]
    }
   ],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_mae',\n",
    "    directory='ltc_tune',\n",
    "    project_name='ANN_TUNE'\n",
    ")\n",
    "\n",
    "tuner.search(X_train, Y_train, epochs=5000, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9884fdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters.\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "# Build the model with the best hp.\n",
    "regressor = build_model(best_hp)\n",
    "# Fit with the entire dataset.\n",
    "X_all = np.concatenate((X_train, X_val))\n",
    "Y_all = np.concatenate((Y_train, Y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46c3b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best hyperparameters\n",
    "best_hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a7de56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a85653fe",
   "metadata": {},
   "source": [
    "stop training when loss is not increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48b24b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='loss', patience=1000,verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85174a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(x=X_all, y=Y_all, epochs=5000, use_multiprocessing=True, callbacks=[earlyStopping])\n",
    "# regressor=KerasRegressor(build_fn=sequential_model,epochs=5000,verbose=1, use_multiprocessing=True, callback=[earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf8814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred=regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0f090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for check\n",
    "Y_train_pred=regressor.predict(X_train)\n",
    "r2_score(Y_train, Y_train_pred) #training score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e757ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2=r2_score(Y_test[:-30],y_pred[:-30]) #score/ r^2\n",
    "print(f'r2:{r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ef1c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2_oos\n",
    "def r2_oos(ret, pred):\n",
    "    sum_of_sq_res = np.nansum(np.power((ret-pred), 2))\n",
    "    sum_of_sq_total = np.nansum(np.power(ret, 2))\n",
    "    \n",
    "    return 1-sum_of_sq_res/sum_of_sq_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b87143",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae=mean_absolute_error(Y_test[:-30],y_pred[:-30]) #mae\n",
    "print(f'mae:{mae}')\n",
    "\n",
    "rmse=np.sqrt(mean_squared_error(Y_test[:-30],y_pred[:-30])) #rmse\n",
    "print(f'rmse:{rmse}')\n",
    "\n",
    "mape=mean_absolute_percentage_error(Y_test[:-30],y_pred[:-30]) #mape\n",
    "print(f'mape:{mape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f5da98",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_pred.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8969a6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_oos = r2_oos(Y_test[:-30], y_pred[:-30])\n",
    "print(f'r2_oos:{r2_oos}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb36caf",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5641115",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df = pd.DataFrame(zip(Y_test,y_pred),columns=['Y_test','y_pred'])\n",
    "pre_df.index = Y_test.index\n",
    "pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4bd2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df['pred_returns'] = pre_df['y_pred'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b94aa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9674c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(zip(Y_test,y_pred),columns=['Y_test','y_pred']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732393ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df.to_csv(\"../result/ANN/ltc_NN.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12ff04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!kdeconnect-cli -n TAS-AN00 --ping-msg 'Script complete!'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "efc08374433b8d8e4a9fd8a0a66f7295c7ce37eceb639810a945045512ff181b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
