{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5b918ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-03 12:27:48.258069: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# conda install keras-tuner\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_validate\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from collections import Counter\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import randint\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, accuracy_score, f1_score, classification_report\n",
    "# import pickle\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n",
    "from tensorflow.keras import losses\n",
    "# from tensorflow.keras.constraints import maxnorm\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import *\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ded3682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535a3fac",
   "metadata": {},
   "source": [
    "### Bitcoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1467391e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_192511/2421463472.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  btc['Date'][i]  =  datetime.strptime(btc['Date'][i], '%Y/%m/%d')\n"
     ]
    }
   ],
   "source": [
    "regs = pd.read_csv(\"../Data/train_btc_selected_features.csv\")\n",
    "btc = pd.read_csv(\"../Data/btc_Data.csv\")\n",
    "for i in range(len(btc['Date'])):\n",
    "    btc['Date'][i]  =  datetime.strptime(btc['Date'][i], '%Y/%m/%d')\n",
    "\n",
    "btc = btc.set_index(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d665a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "btcData = btc[regs.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8227b01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_192511/751970969.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  btcData['returns'] = btcData['priceUSD'].pct_change()\n"
     ]
    }
   ],
   "source": [
    "btcData['returns'] = btcData['priceUSD'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0204bfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = btcData.drop(columns=['priceUSD'])\n",
    "Data = Data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a926d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>difficulty14mom</th>\n",
       "      <th>difficulty30mom</th>\n",
       "      <th>difficulty3rsi</th>\n",
       "      <th>difficulty90mom</th>\n",
       "      <th>difficulty90sma</th>\n",
       "      <th>hashrate90std</th>\n",
       "      <th>hashrate90var</th>\n",
       "      <th>median_transaction_fee90rocUSD</th>\n",
       "      <th>median_transaction_feeUSD</th>\n",
       "      <th>mining_profitability</th>\n",
       "      <th>...</th>\n",
       "      <th>price90wmaUSD</th>\n",
       "      <th>sentinusd90emaUSD</th>\n",
       "      <th>sentinusdUSD</th>\n",
       "      <th>top100cap</th>\n",
       "      <th>top100cap14trx</th>\n",
       "      <th>top100cap30rsi</th>\n",
       "      <th>top100cap30trx</th>\n",
       "      <th>top100cap90rsi</th>\n",
       "      <th>top100cap90trx</th>\n",
       "      <th>returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-07-18</th>\n",
       "      <td>1.621380e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.737230e+02</td>\n",
       "      <td>2.055500e+01</td>\n",
       "      <td>8.384389e+08</td>\n",
       "      <td>1.757449e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>401834.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.612000e+03</td>\n",
       "      <td>25.782</td>\n",
       "      <td>0.139</td>\n",
       "      <td>71.191</td>\n",
       "      <td>0.308</td>\n",
       "      <td>79.756</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-19</th>\n",
       "      <td>1.621380e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.737230e+02</td>\n",
       "      <td>2.248600e+01</td>\n",
       "      <td>8.819952e+08</td>\n",
       "      <td>1.944789e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>481473.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.047000e+03</td>\n",
       "      <td>25.685</td>\n",
       "      <td>0.123</td>\n",
       "      <td>68.863</td>\n",
       "      <td>0.300</td>\n",
       "      <td>78.999</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.183196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-20</th>\n",
       "      <td>1.584060e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.735570e+02</td>\n",
       "      <td>2.441400e+01</td>\n",
       "      <td>9.280542e+08</td>\n",
       "      <td>2.153212e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>431831.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.341000e+03</td>\n",
       "      <td>25.602</td>\n",
       "      <td>0.107</td>\n",
       "      <td>66.923</td>\n",
       "      <td>0.292</td>\n",
       "      <td>78.355</td>\n",
       "      <td>0.544</td>\n",
       "      <td>-0.088475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-21</th>\n",
       "      <td>1.580420e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.700790e+02</td>\n",
       "      <td>2.630400e+01</td>\n",
       "      <td>9.761949e+08</td>\n",
       "      <td>2.382391e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>460783.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.122000e+03</td>\n",
       "      <td>25.638</td>\n",
       "      <td>0.092</td>\n",
       "      <td>67.334</td>\n",
       "      <td>0.284</td>\n",
       "      <td>78.431</td>\n",
       "      <td>0.541</td>\n",
       "      <td>-0.020434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-22</th>\n",
       "      <td>1.580420e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.700790e+02</td>\n",
       "      <td>2.819400e+01</td>\n",
       "      <td>1.019474e+09</td>\n",
       "      <td>2.598318e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>334641.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.570000e+03</td>\n",
       "      <td>25.604</td>\n",
       "      <td>0.078</td>\n",
       "      <td>66.526</td>\n",
       "      <td>0.275</td>\n",
       "      <td>78.166</td>\n",
       "      <td>0.537</td>\n",
       "      <td>-0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-24</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.384949e+13</td>\n",
       "      <td>4.612156e+19</td>\n",
       "      <td>5.317997e+38</td>\n",
       "      <td>165.380</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.0577</td>\n",
       "      <td>...</td>\n",
       "      <td>18820.0</td>\n",
       "      <td>28544682257</td>\n",
       "      <td>6.114577e+09</td>\n",
       "      <td>15.236</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>40.118</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>45.140</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.008075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-25</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.394503e+13</td>\n",
       "      <td>4.542600e+19</td>\n",
       "      <td>5.158804e+38</td>\n",
       "      <td>214.464</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.0541</td>\n",
       "      <td>...</td>\n",
       "      <td>18759.0</td>\n",
       "      <td>28076113671</td>\n",
       "      <td>7.224812e+09</td>\n",
       "      <td>15.231</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>39.880</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>45.047</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.006565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-26</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.404058e+13</td>\n",
       "      <td>4.624135e+19</td>\n",
       "      <td>5.345655e+38</td>\n",
       "      <td>255.004</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.0571</td>\n",
       "      <td>...</td>\n",
       "      <td>18701.0</td>\n",
       "      <td>27566714445</td>\n",
       "      <td>4.898449e+09</td>\n",
       "      <td>15.233</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>40.069</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>45.106</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.004001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-27</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.413612e+13</td>\n",
       "      <td>4.576227e+19</td>\n",
       "      <td>5.235464e+38</td>\n",
       "      <td>113.430</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.0618</td>\n",
       "      <td>...</td>\n",
       "      <td>18642.0</td>\n",
       "      <td>27053515674</td>\n",
       "      <td>4.216170e+09</td>\n",
       "      <td>15.259</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>41.833</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>45.656</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.423166e+13</td>\n",
       "      <td>4.552126e+19</td>\n",
       "      <td>5.180463e+38</td>\n",
       "      <td>104.294</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.0592</td>\n",
       "      <td>...</td>\n",
       "      <td>18578.0</td>\n",
       "      <td>26631320968</td>\n",
       "      <td>7.843657e+09</td>\n",
       "      <td>15.219</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>39.965</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>44.957</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.019828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4517 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            difficulty14mom  difficulty30mom  difficulty3rsi  difficulty90mom  \\\n",
       "Date                                                                            \n",
       "2010-07-18     1.621380e+02     1.641620e+02          100.00     1.737230e+02   \n",
       "2010-07-19     1.621380e+02     1.641620e+02          100.00     1.737230e+02   \n",
       "2010-07-20     1.584060e+02     1.641620e+02          100.00     1.735570e+02   \n",
       "2010-07-21     1.580420e+02     1.641620e+02          100.00     1.700790e+02   \n",
       "2010-07-22     1.580420e+02     1.641620e+02          100.00     1.700790e+02   \n",
       "...                     ...              ...             ...              ...   \n",
       "2022-11-24     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-25     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-26     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-27     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-28     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "\n",
       "            difficulty90sma  hashrate90std  hashrate90var  \\\n",
       "Date                                                        \n",
       "2010-07-18     2.055500e+01   8.384389e+08   1.757449e+17   \n",
       "2010-07-19     2.248600e+01   8.819952e+08   1.944789e+17   \n",
       "2010-07-20     2.441400e+01   9.280542e+08   2.153212e+17   \n",
       "2010-07-21     2.630400e+01   9.761949e+08   2.382391e+17   \n",
       "2010-07-22     2.819400e+01   1.019474e+09   2.598318e+17   \n",
       "...                     ...            ...            ...   \n",
       "2022-11-24     3.384949e+13   4.612156e+19   5.317997e+38   \n",
       "2022-11-25     3.394503e+13   4.542600e+19   5.158804e+38   \n",
       "2022-11-26     3.404058e+13   4.624135e+19   5.345655e+38   \n",
       "2022-11-27     3.413612e+13   4.576227e+19   5.235464e+38   \n",
       "2022-11-28     3.423166e+13   4.552126e+19   5.180463e+38   \n",
       "\n",
       "            median_transaction_fee90rocUSD  median_transaction_feeUSD  \\\n",
       "Date                                                                    \n",
       "2010-07-18                           0.000                      0.000   \n",
       "2010-07-19                           0.000                      0.000   \n",
       "2010-07-20                           0.000                      0.000   \n",
       "2010-07-21                           0.000                      0.000   \n",
       "2010-07-22                           0.000                      0.000   \n",
       "...                                    ...                        ...   \n",
       "2022-11-24                         165.380                      0.818   \n",
       "2022-11-25                         214.464                      0.812   \n",
       "2022-11-26                         255.004                      0.809   \n",
       "2022-11-27                         113.430                      0.619   \n",
       "2022-11-28                         104.294                      0.622   \n",
       "\n",
       "            mining_profitability  ...  price90wmaUSD  sentinusd90emaUSD  \\\n",
       "Date                              ...                                     \n",
       "2010-07-18           401834.0000  ...            0.0                  0   \n",
       "2010-07-19           481473.0000  ...            0.0                  0   \n",
       "2010-07-20           431831.0000  ...            0.0                  0   \n",
       "2010-07-21           460783.0000  ...            0.0                  0   \n",
       "2010-07-22           334641.0000  ...            0.0                  0   \n",
       "...                          ...  ...            ...                ...   \n",
       "2022-11-24                0.0577  ...        18820.0        28544682257   \n",
       "2022-11-25                0.0541  ...        18759.0        28076113671   \n",
       "2022-11-26                0.0571  ...        18701.0        27566714445   \n",
       "2022-11-27                0.0618  ...        18642.0        27053515674   \n",
       "2022-11-28                0.0592  ...        18578.0        26631320968   \n",
       "\n",
       "            sentinusdUSD  top100cap  top100cap14trx  top100cap30rsi  \\\n",
       "Date                                                                  \n",
       "2010-07-18  2.612000e+03     25.782           0.139          71.191   \n",
       "2010-07-19  4.047000e+03     25.685           0.123          68.863   \n",
       "2010-07-20  2.341000e+03     25.602           0.107          66.923   \n",
       "2010-07-21  2.122000e+03     25.638           0.092          67.334   \n",
       "2010-07-22  2.570000e+03     25.604           0.078          66.526   \n",
       "...                  ...        ...             ...             ...   \n",
       "2022-11-24  6.114577e+09     15.236          -0.080          40.118   \n",
       "2022-11-25  7.224812e+09     15.231          -0.075          39.880   \n",
       "2022-11-26  4.898449e+09     15.233          -0.070          40.069   \n",
       "2022-11-27  4.216170e+09     15.259          -0.065          41.833   \n",
       "2022-11-28  7.843657e+09     15.219          -0.061          39.965   \n",
       "\n",
       "            top100cap30trx  top100cap90rsi  top100cap90trx   returns  \n",
       "Date                                                                  \n",
       "2010-07-18           0.308          79.756           0.550  0.466667  \n",
       "2010-07-19           0.300          78.999           0.547  0.183196  \n",
       "2010-07-20           0.292          78.355           0.544 -0.088475  \n",
       "2010-07-21           0.284          78.431           0.541 -0.020434  \n",
       "2010-07-22           0.275          78.166           0.537 -0.153846  \n",
       "...                    ...             ...             ...       ...  \n",
       "2022-11-24          -0.042          45.140           0.001  0.008075  \n",
       "2022-11-25          -0.043          45.047           0.001 -0.006565  \n",
       "2022-11-26          -0.043          45.106           0.001  0.004001  \n",
       "2022-11-27          -0.044          45.656           0.000 -0.001147  \n",
       "2022-11-28          -0.044          44.957           0.000 -0.019828  \n",
       "\n",
       "[4517 rows x 32 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5d4f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide X and Y\n",
    "X = Data.iloc[:,0:]\n",
    "#Y = Data['returns']   # 用returns的话就用这一行，然后把下一行comment掉\n",
    "Y = btcData['priceUSD'].shift(-30)[1:] # 反之亦然"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb3275b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>difficulty14mom</th>\n",
       "      <th>difficulty30mom</th>\n",
       "      <th>difficulty3rsi</th>\n",
       "      <th>difficulty90mom</th>\n",
       "      <th>difficulty90sma</th>\n",
       "      <th>hashrate90std</th>\n",
       "      <th>hashrate90var</th>\n",
       "      <th>median_transaction_fee90rocUSD</th>\n",
       "      <th>median_transaction_feeUSD</th>\n",
       "      <th>mining_profitability</th>\n",
       "      <th>...</th>\n",
       "      <th>price90wmaUSD</th>\n",
       "      <th>sentinusd90emaUSD</th>\n",
       "      <th>sentinusdUSD</th>\n",
       "      <th>top100cap</th>\n",
       "      <th>top100cap14trx</th>\n",
       "      <th>top100cap30rsi</th>\n",
       "      <th>top100cap30trx</th>\n",
       "      <th>top100cap90rsi</th>\n",
       "      <th>top100cap90trx</th>\n",
       "      <th>returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-07-18</th>\n",
       "      <td>162.138</td>\n",
       "      <td>164.162</td>\n",
       "      <td>100.0</td>\n",
       "      <td>173.723</td>\n",
       "      <td>20.555</td>\n",
       "      <td>838438881.0</td>\n",
       "      <td>1.757449e+17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>401834.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2612.0</td>\n",
       "      <td>25.782</td>\n",
       "      <td>0.139</td>\n",
       "      <td>71.191</td>\n",
       "      <td>0.308</td>\n",
       "      <td>79.756</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-19</th>\n",
       "      <td>162.138</td>\n",
       "      <td>164.162</td>\n",
       "      <td>100.0</td>\n",
       "      <td>173.723</td>\n",
       "      <td>22.486</td>\n",
       "      <td>881995244.0</td>\n",
       "      <td>1.944789e+17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>481473.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4047.0</td>\n",
       "      <td>25.685</td>\n",
       "      <td>0.123</td>\n",
       "      <td>68.863</td>\n",
       "      <td>0.300</td>\n",
       "      <td>78.999</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.183196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-20</th>\n",
       "      <td>158.406</td>\n",
       "      <td>164.162</td>\n",
       "      <td>100.0</td>\n",
       "      <td>173.557</td>\n",
       "      <td>24.414</td>\n",
       "      <td>928054231.0</td>\n",
       "      <td>2.153212e+17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>431831.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2341.0</td>\n",
       "      <td>25.602</td>\n",
       "      <td>0.107</td>\n",
       "      <td>66.923</td>\n",
       "      <td>0.292</td>\n",
       "      <td>78.355</td>\n",
       "      <td>0.544</td>\n",
       "      <td>-0.088475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            difficulty14mom  difficulty30mom  difficulty3rsi  difficulty90mom  \\\n",
       "Date                                                                            \n",
       "2010-07-18          162.138          164.162           100.0          173.723   \n",
       "2010-07-19          162.138          164.162           100.0          173.723   \n",
       "2010-07-20          158.406          164.162           100.0          173.557   \n",
       "\n",
       "            difficulty90sma  hashrate90std  hashrate90var  \\\n",
       "Date                                                        \n",
       "2010-07-18           20.555    838438881.0   1.757449e+17   \n",
       "2010-07-19           22.486    881995244.0   1.944789e+17   \n",
       "2010-07-20           24.414    928054231.0   2.153212e+17   \n",
       "\n",
       "            median_transaction_fee90rocUSD  median_transaction_feeUSD  \\\n",
       "Date                                                                    \n",
       "2010-07-18                             0.0                        0.0   \n",
       "2010-07-19                             0.0                        0.0   \n",
       "2010-07-20                             0.0                        0.0   \n",
       "\n",
       "            mining_profitability  ...  price90wmaUSD  sentinusd90emaUSD  \\\n",
       "Date                              ...                                     \n",
       "2010-07-18              401834.0  ...            0.0                  0   \n",
       "2010-07-19              481473.0  ...            0.0                  0   \n",
       "2010-07-20              431831.0  ...            0.0                  0   \n",
       "\n",
       "            sentinusdUSD  top100cap  top100cap14trx  top100cap30rsi  \\\n",
       "Date                                                                  \n",
       "2010-07-18        2612.0     25.782           0.139          71.191   \n",
       "2010-07-19        4047.0     25.685           0.123          68.863   \n",
       "2010-07-20        2341.0     25.602           0.107          66.923   \n",
       "\n",
       "            top100cap30trx  top100cap90rsi  top100cap90trx   returns  \n",
       "Date                                                                  \n",
       "2010-07-18           0.308          79.756           0.550  0.466667  \n",
       "2010-07-19           0.300          78.999           0.547  0.183196  \n",
       "2010-07-20           0.292          78.355           0.544 -0.088475  \n",
       "\n",
       "[3 rows x 32 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd589ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into three data sets\n",
    "X_train = X['2016-01-01':'2019-12-31']\n",
    "X_val = X['2020-01-01':'2021-05-31']\n",
    "X_test = X['2021-06-01':'2023-01-01']\n",
    "\n",
    "Y_train = Y['2016-01-01':'2019-12-31']\n",
    "Y_val = Y['2020-01-01':'2021-05-31']\n",
    "Y_test = Y['2021-06-01':'2023-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e5e9fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............ (step 1 of 2) Processing mixmax, total=   0.0s\n",
      "[Pipeline] ............ (step 2 of 2) Processing robust, total=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;mixmax&#x27;, MinMaxScaler()), [&#x27;robust&#x27;, RobustScaler()]],\n",
       "         verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;mixmax&#x27;, MinMaxScaler()), [&#x27;robust&#x27;, RobustScaler()]],\n",
       "         verbose=True)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RobustScaler</label><div class=\"sk-toggleable__content\"><pre>RobustScaler()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('mixmax', MinMaxScaler()), ['robust', RobustScaler()]],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators=[]\n",
    "estimators.append(['mixmax',MinMaxScaler()])\n",
    "estimators.append(['robust',RobustScaler()])\n",
    "scale=Pipeline(estimators,verbose=True)\n",
    "scale.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6fc0859",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=scale.transform(X_train)\n",
    "X_test=scale.transform(X_test)\n",
    "X_val = scale.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8a84d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f6916ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define neural network model\n",
    "shape=X.shape[1]\n",
    "def build_model(hp, initializer='normal', activation='relu', NUM_FEATURES=shape):\n",
    "    # create model\n",
    "    hp_units1 = hp.Int('units1', min_value=32, max_value=512, step=32)\n",
    "    hp_units2 = hp.Int('units2', min_value=32, max_value=512, step=32)\n",
    "    hp_units3 = hp.Int('units3', min_value=32, max_value=512, step=32)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hp_units1, input_shape=(NUM_FEATURES,), kernel_initializer=initializer, activation=activation))\n",
    "    model.add(Dense(hp_units2, activation=activation))\n",
    "    model.add(Dense(hp_units3, activation=activation))\n",
    "    model.add(Dense(1, activation=activation))\n",
    "    # Compile model\n",
    "    adam=keras.optimizers.Adam(lr=lr_schedule(0), amsgrad=True)\n",
    "    #sgd=keras.optimizers.SGD(learning_rate=0.08, momentum=0.9, nesterov=False)\n",
    "    model.compile(loss='logcosh', optimizer=adam, metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "112c8df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-03 12:27:50.039159: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-03 12:27:50.040045: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "/home/spectre/anaconda3/envs/tensorplustorch/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "192               |?                 |units1\n",
      "480               |?                 |units2\n",
      "128               |?                 |units3\n",
      "\n",
      "Learning rate:  0.001\n",
      "Epoch 1/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 4700.4834 - mae: 4701.1777 - val_loss: 19015.2461 - val_mae: 19015.9395\n",
      "Epoch 2/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 2899.4216 - mae: 2900.1147 - val_loss: 6112.8398 - val_mae: 6113.5322\n",
      "Epoch 3/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1660.9309 - mae: 1661.6235 - val_loss: 6193.5210 - val_mae: 6194.2134\n",
      "Epoch 4/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1392.7250 - mae: 1393.4175 - val_loss: 6650.4624 - val_mae: 6651.1548\n",
      "Epoch 5/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 1224.2288 - mae: 1224.9205 - val_loss: 6486.4419 - val_mae: 6487.1348\n",
      "Epoch 6/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1124.7727 - mae: 1125.4651 - val_loss: 6763.2017 - val_mae: 6763.8940\n",
      "Epoch 7/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1070.4647 - mae: 1071.1575 - val_loss: 7187.6636 - val_mae: 7188.3569\n",
      "Epoch 8/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1009.3589 - mae: 1010.0510 - val_loss: 6787.5962 - val_mae: 6788.2886\n",
      "Epoch 9/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 960.9757 - mae: 961.6680 - val_loss: 7533.2188 - val_mae: 7533.9111\n",
      "Epoch 10/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 918.3917 - mae: 919.0840 - val_loss: 7004.4717 - val_mae: 7005.1646\n",
      "Epoch 11/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 846.8982 - mae: 847.5909 - val_loss: 7102.9980 - val_mae: 7103.6909\n",
      "Epoch 12/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 817.3525 - mae: 818.0443 - val_loss: 7292.7310 - val_mae: 7293.4248\n",
      "Epoch 13/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 785.6158 - mae: 786.3090 - val_loss: 7175.8428 - val_mae: 7176.5361\n",
      "Epoch 14/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 774.3965 - mae: 775.0884 - val_loss: 7603.6577 - val_mae: 7604.3516\n",
      "Epoch 15/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 734.5633 - mae: 735.2554 - val_loss: 7378.0947 - val_mae: 7378.7881\n",
      "Epoch 16/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 742.0563 - mae: 742.7492 - val_loss: 7367.6069 - val_mae: 7368.3003\n",
      "Epoch 17/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 709.1876 - mae: 709.8801 - val_loss: 7964.6670 - val_mae: 7965.3599\n",
      "Epoch 18/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 712.8084 - mae: 713.5001 - val_loss: 8315.2432 - val_mae: 8315.9365\n",
      "Epoch 19/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 682.4293 - mae: 683.1212 - val_loss: 7195.4478 - val_mae: 7196.1411\n",
      "Epoch 20/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 626.7233 - mae: 627.4149 - val_loss: 7600.7505 - val_mae: 7601.4429\n",
      "Epoch 21/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 634.6567 - mae: 635.3479 - val_loss: 7042.4819 - val_mae: 7043.1753\n",
      "Epoch 22/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 644.1171 - mae: 644.8085 - val_loss: 6987.6855 - val_mae: 6988.3784\n",
      "Epoch 23/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 622.2442 - mae: 622.9367 - val_loss: 7626.2881 - val_mae: 7626.9824\n",
      "Epoch 24/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 602.5005 - mae: 603.1931 - val_loss: 7794.1777 - val_mae: 7794.8716\n",
      "Epoch 25/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 573.6299 - mae: 574.3218 - val_loss: 7707.6606 - val_mae: 7708.3535\n",
      "Epoch 26/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 589.4586 - mae: 590.1512 - val_loss: 7522.6875 - val_mae: 7523.3799\n",
      "Epoch 27/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 563.2450 - mae: 563.9360 - val_loss: 7588.0664 - val_mae: 7588.7598\n",
      "Epoch 28/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 541.4514 - mae: 542.1423 - val_loss: 7809.5034 - val_mae: 7810.1968\n",
      "Epoch 29/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 524.2791 - mae: 524.9708 - val_loss: 7343.2466 - val_mae: 7343.9395\n",
      "Epoch 30/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 534.3075 - mae: 534.9985 - val_loss: 7651.2588 - val_mae: 7651.9526\n",
      "Epoch 31/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 530.2238 - mae: 530.9138 - val_loss: 7332.7310 - val_mae: 7333.4238\n",
      "Epoch 32/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 529.8586 - mae: 530.5510 - val_loss: 8097.9673 - val_mae: 8098.6611\n",
      "Epoch 33/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 525.7169 - mae: 526.4083 - val_loss: 7593.1880 - val_mae: 7593.8813\n",
      "Epoch 34/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 508.5183 - mae: 509.2095 - val_loss: 7696.7905 - val_mae: 7697.4839\n",
      "Epoch 35/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 502.8318 - mae: 503.5230 - val_loss: 7840.8599 - val_mae: 7841.5537\n",
      "Epoch 36/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 494.7885 - mae: 495.4803 - val_loss: 8219.7666 - val_mae: 8220.4590\n",
      "Epoch 37/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 493.4861 - mae: 494.1779 - val_loss: 7746.6670 - val_mae: 7747.3613\n",
      "Epoch 38/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 478.8991 - mae: 479.5894 - val_loss: 7894.3184 - val_mae: 7895.0127\n",
      "Epoch 39/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 495.1941 - mae: 495.8854 - val_loss: 7359.6260 - val_mae: 7360.3203\n",
      "Epoch 40/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 479.0962 - mae: 479.7866 - val_loss: 8086.9980 - val_mae: 8087.6919\n",
      "Epoch 41/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 472.8170 - mae: 473.5077 - val_loss: 7484.0410 - val_mae: 7484.7339\n",
      "Epoch 42/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 450.6184 - mae: 451.3100 - val_loss: 7933.5918 - val_mae: 7934.2842\n",
      "Epoch 43/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 456.8791 - mae: 457.5706 - val_loss: 7445.1006 - val_mae: 7445.7930\n",
      "Epoch 44/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 454.4332 - mae: 455.1232 - val_loss: 7721.0918 - val_mae: 7721.7847\n",
      "Epoch 45/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 427.7219 - mae: 428.4133 - val_loss: 8858.2783 - val_mae: 8858.9727\n",
      "Epoch 46/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 440.2206 - mae: 440.9111 - val_loss: 7862.0200 - val_mae: 7862.7139\n",
      "Epoch 47/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 434.2657 - mae: 434.9571 - val_loss: 8305.0576 - val_mae: 8305.7490\n",
      "Epoch 48/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 427.5716 - mae: 428.2632 - val_loss: 8423.2822 - val_mae: 8423.9746\n",
      "Epoch 49/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 409.3220 - mae: 410.0133 - val_loss: 8749.4316 - val_mae: 8750.1250\n",
      "Epoch 50/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 416.6836 - mae: 417.3729 - val_loss: 8412.6484 - val_mae: 8413.3418\n",
      "Epoch 51/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 403.3611 - mae: 404.0518 - val_loss: 7621.9009 - val_mae: 7622.5923\n",
      "Epoch 52/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 433.0283 - mae: 433.7189 - val_loss: 8015.5269 - val_mae: 8016.2188\n",
      "Epoch 53/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 389.6527 - mae: 390.3437 - val_loss: 8688.7559 - val_mae: 8689.4502\n",
      "Epoch 54/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 401.9268 - mae: 402.6169 - val_loss: 8197.7451 - val_mae: 8198.4375\n",
      "Epoch 55/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 418.0663 - mae: 418.7578 - val_loss: 8643.7930 - val_mae: 8644.4863\n",
      "Epoch 56/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 7ms/step - loss: 403.3755 - mae: 404.0666 - val_loss: 8578.7080 - val_mae: 8579.4004\n",
      "Epoch 57/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 390.0590 - mae: 390.7504 - val_loss: 8647.2139 - val_mae: 8647.9082\n",
      "Epoch 58/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 376.3340 - mae: 377.0245 - val_loss: 8536.7646 - val_mae: 8537.4580\n",
      "Epoch 59/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 378.9096 - mae: 379.5991 - val_loss: 8439.2275 - val_mae: 8439.9199\n",
      "Epoch 60/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 395.5767 - mae: 396.2689 - val_loss: 8097.2798 - val_mae: 8097.9736\n",
      "Epoch 61/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 364.0860 - mae: 364.7775 - val_loss: 8087.5483 - val_mae: 8088.2407\n",
      "Epoch 62/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 367.7780 - mae: 368.4695 - val_loss: 8952.0391 - val_mae: 8952.7324\n",
      "Epoch 63/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 358.9005 - mae: 359.5925 - val_loss: 8799.5098 - val_mae: 8800.2031\n",
      "Epoch 64/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 365.6368 - mae: 366.3284 - val_loss: 9029.8105 - val_mae: 9030.5039\n",
      "Epoch 65/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 389.1479 - mae: 389.8377 - val_loss: 8633.3125 - val_mae: 8634.0059\n",
      "Epoch 66/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 375.4184 - mae: 376.1100 - val_loss: 8276.8252 - val_mae: 8277.5186\n",
      "Epoch 67/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 372.1309 - mae: 372.8230 - val_loss: 8640.0469 - val_mae: 8640.7383\n",
      "Epoch 68/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 338.8633 - mae: 339.5551 - val_loss: 8823.5234 - val_mae: 8824.2168\n",
      "Epoch 69/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 346.5730 - mae: 347.2633 - val_loss: 9181.1709 - val_mae: 9181.8623\n",
      "Epoch 70/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 353.1400 - mae: 353.8315 - val_loss: 8933.4170 - val_mae: 8934.1094\n",
      "Epoch 71/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 329.9050 - mae: 330.5962 - val_loss: 9350.8574 - val_mae: 9351.5498\n",
      "Epoch 72/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 336.6672 - mae: 337.3587 - val_loss: 8507.4961 - val_mae: 8508.1904\n",
      "Epoch 73/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 338.6579 - mae: 339.3487 - val_loss: 9084.4531 - val_mae: 9085.1465\n",
      "Epoch 74/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 338.7064 - mae: 339.3969 - val_loss: 8584.7217 - val_mae: 8585.4150\n",
      "Epoch 75/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 324.1104 - mae: 324.8019 - val_loss: 9006.4395 - val_mae: 9007.1318\n",
      "Epoch 76/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 328.8516 - mae: 329.5399 - val_loss: 9100.9688 - val_mae: 9101.6631\n",
      "Epoch 77/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 341.2751 - mae: 341.9655 - val_loss: 9349.6084 - val_mae: 9350.2998\n",
      "Epoch 78/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 343.7976 - mae: 344.4888 - val_loss: 9390.0430 - val_mae: 9390.7363\n",
      "Epoch 79/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 331.5985 - mae: 332.2899 - val_loss: 8798.0830 - val_mae: 8798.7764\n",
      "Epoch 80/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 328.6027 - mae: 329.2926 - val_loss: 9320.1006 - val_mae: 9320.7939\n",
      "Epoch 81/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 314.0378 - mae: 314.7283 - val_loss: 9002.8496 - val_mae: 9003.5430\n",
      "Epoch 82/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 346.1462 - mae: 346.8360 - val_loss: 9936.6270 - val_mae: 9937.3193\n",
      "Epoch 83/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 332.4629 - mae: 333.1534 - val_loss: 9070.8018 - val_mae: 9071.4961\n",
      "Epoch 84/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 329.4751 - mae: 330.1667 - val_loss: 9398.0039 - val_mae: 9398.6982\n",
      "Epoch 85/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 362.1216 - mae: 362.8123 - val_loss: 8128.6733 - val_mae: 8129.3677\n",
      "Epoch 86/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 323.2375 - mae: 323.9292 - val_loss: 8694.1797 - val_mae: 8694.8721\n",
      "Epoch 87/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 320.0046 - mae: 320.6961 - val_loss: 8846.2744 - val_mae: 8846.9678\n",
      "Epoch 88/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 307.5485 - mae: 308.2404 - val_loss: 9418.5439 - val_mae: 9419.2363\n",
      "Epoch 89/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 310.8316 - mae: 311.5230 - val_loss: 9033.3965 - val_mae: 9034.0889\n",
      "Epoch 90/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 322.3775 - mae: 323.0691 - val_loss: 9001.1240 - val_mae: 9001.8184\n",
      "Epoch 91/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 315.9011 - mae: 316.5918 - val_loss: 8905.9805 - val_mae: 8906.6738\n",
      "Epoch 92/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 324.1438 - mae: 324.8344 - val_loss: 8800.2822 - val_mae: 8800.9746\n",
      "Epoch 93/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 313.6874 - mae: 314.3777 - val_loss: 9001.9561 - val_mae: 9002.6504\n",
      "Epoch 94/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 307.0628 - mae: 307.7509 - val_loss: 8546.5010 - val_mae: 8547.1943\n",
      "Epoch 95/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 338.4529 - mae: 339.1440 - val_loss: 9492.7832 - val_mae: 9493.4756\n",
      "Epoch 96/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 307.3765 - mae: 308.0660 - val_loss: 9635.6396 - val_mae: 9636.3320\n",
      "Epoch 97/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 321.7248 - mae: 322.4159 - val_loss: 8928.9150 - val_mae: 8929.6084\n",
      "Epoch 98/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 303.6616 - mae: 304.3528 - val_loss: 8267.2969 - val_mae: 8267.9893\n",
      "Epoch 99/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 315.9659 - mae: 316.6574 - val_loss: 9032.0791 - val_mae: 9032.7734\n",
      "Epoch 100/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 292.1640 - mae: 292.8541 - val_loss: 8914.0713 - val_mae: 8914.7637\n",
      "Epoch 101/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 305.6898 - mae: 306.3799 - val_loss: 8884.5459 - val_mae: 8885.2402\n",
      "Epoch 102/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 299.4573 - mae: 300.1481 - val_loss: 8965.8604 - val_mae: 8966.5518\n",
      "Epoch 103/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 300.9252 - mae: 301.6166 - val_loss: 9247.5176 - val_mae: 9248.2119\n",
      "Epoch 104/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 284.0524 - mae: 284.7416 - val_loss: 9908.3848 - val_mae: 9909.0771\n",
      "Epoch 105/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 312.4955 - mae: 313.1873 - val_loss: 9346.8926 - val_mae: 9347.5850\n",
      "Epoch 106/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 305.4543 - mae: 306.1446 - val_loss: 8717.3193 - val_mae: 8718.0127\n",
      "Epoch 107/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 278.5834 - mae: 279.2716 - val_loss: 9214.9785 - val_mae: 9215.6709\n",
      "Epoch 108/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 303.5869 - mae: 304.2782 - val_loss: 8661.2783 - val_mae: 8661.9707\n",
      "Epoch 109/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 317.0164 - mae: 317.7067 - val_loss: 8770.9326 - val_mae: 8771.6260\n",
      "Epoch 110/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 292.9983 - mae: 293.6870 - val_loss: 8700.8115 - val_mae: 8701.5049\n",
      "Epoch 111/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 302.4463 - mae: 303.1370 - val_loss: 9368.4014 - val_mae: 9369.0947\n",
      "Epoch 112/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 287.1923 - mae: 287.8823 - val_loss: 9636.5332 - val_mae: 9637.2246\n",
      "Epoch 113/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 6ms/step - loss: 287.5983 - mae: 288.2884 - val_loss: 9091.7871 - val_mae: 9092.4805\n",
      "Epoch 114/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 288.1454 - mae: 288.8371 - val_loss: 9405.0693 - val_mae: 9405.7617\n",
      "Epoch 115/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 283.5887 - mae: 284.2784 - val_loss: 9087.4883 - val_mae: 9088.1816\n",
      "Epoch 116/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 287.6760 - mae: 288.3662 - val_loss: 8811.7734 - val_mae: 8812.4668\n",
      "Epoch 117/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 297.5975 - mae: 298.2869 - val_loss: 9171.6504 - val_mae: 9172.3438\n",
      "Epoch 118/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 290.3791 - mae: 291.0677 - val_loss: 8596.4580 - val_mae: 8597.1523\n",
      "Epoch 119/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 289.6675 - mae: 290.3565 - val_loss: 9268.2539 - val_mae: 9268.9473\n",
      "Epoch 120/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 274.5096 - mae: 275.1995 - val_loss: 9007.6016 - val_mae: 9008.2949\n",
      "Epoch 121/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 289.4412 - mae: 290.1326 - val_loss: 8682.1953 - val_mae: 8682.8877\n",
      "Epoch 122/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 287.7180 - mae: 288.4086 - val_loss: 8885.6924 - val_mae: 8886.3857\n",
      "Epoch 123/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 274.0042 - mae: 274.6933 - val_loss: 9767.2441 - val_mae: 9767.9385\n",
      "Epoch 124/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 297.7092 - mae: 298.3995 - val_loss: 9394.7227 - val_mae: 9395.4150\n",
      "Epoch 125/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 282.1772 - mae: 282.8656 - val_loss: 9195.2549 - val_mae: 9195.9492\n",
      "Epoch 126/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 270.3669 - mae: 271.0585 - val_loss: 9464.8350 - val_mae: 9465.5283\n",
      "Epoch 127/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 272.1953 - mae: 272.8848 - val_loss: 9089.8672 - val_mae: 9090.5615\n",
      "Epoch 128/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 273.7995 - mae: 274.4888 - val_loss: 9671.4941 - val_mae: 9672.1885\n",
      "Epoch 129/5000\n",
      "38/46 [=======================>......] - ETA: 0s - loss: 279.8325 - mae: 280.5243"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m tuner \u001b[38;5;241m=\u001b[39m kt\u001b[38;5;241m.\u001b[39mRandomSearch(\n\u001b[1;32m      2\u001b[0m     build_model,\n\u001b[1;32m      3\u001b[0m     objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_mae\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m     directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbtc_tune\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m     project_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mANN_TUNE\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorplustorch/lib/python3.10/site-packages/keras_tuner/engine/base_tuner.py:183\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[0;32m--> 183\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# `results` is None indicates user updated oracle in `run_trial()`.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorplustorch/lib/python3.10/site-packages/keras_tuner/engine/tuner.py:295\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(model_checkpoint)\n\u001b[1;32m    294\u001b[0m     copied_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m callbacks\n\u001b[0;32m--> 295\u001b[0m     obj_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_and_fit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcopied_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m     histories\u001b[38;5;241m.\u001b[39mappend(obj_value)\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m histories\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorplustorch/lib/python3.10/site-packages/keras_tuner/engine/tuner.py:222\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m hp \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39mhyperparameters\n\u001b[1;32m    221\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_build(hp)\n\u001b[0;32m--> 222\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhypermodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m tuner_utils\u001b[38;5;241m.\u001b[39mvalidate_trial_results(\n\u001b[1;32m    224\u001b[0m     results, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mobjective, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHyperModel.fit()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    225\u001b[0m )\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorplustorch/lib/python3.10/site-packages/keras_tuner/engine/hypermodel.py:140\u001b[0m, in \u001b[0;36mHyperModel.fit\u001b[0;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;124;03m\"\"\"Train the model.\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m        If return a float, it should be the `objective` value.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorplustorch/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorplustorch/lib/python3.10/site-packages/keras/engine/training.py:1606\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[1;32m   1593\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[1;32m   1594\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1604\u001b[0m         steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution,\n\u001b[1;32m   1605\u001b[0m     )\n\u001b[0;32m-> 1606\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1608\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1611\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1615\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1617\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1618\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1619\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1620\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1621\u001b[0m }\n\u001b[1;32m   1622\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorplustorch/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorplustorch/lib/python3.10/site-packages/keras/engine/training.py:1947\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1943\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1944\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1945\u001b[0m ):\n\u001b[1;32m   1946\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[0;32m-> 1947\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1948\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1949\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    952\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 954\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateful_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    956\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    957\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_mae',\n",
    "    directory='btc_tune',\n",
    "    project_name='ANN_TUNE'\n",
    ")\n",
    "\n",
    "tuner.search(X_train, Y_train, epochs=5000, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9884fdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters.\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "# Build the model with the best hp.\n",
    "regressor = build_model(best_hp)\n",
    "# Fit with the entire dataset.\n",
    "X_all = np.concatenate((X_train, X_val))\n",
    "Y_all = np.concatenate((Y_train, Y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46c3b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best hyperparameters\n",
    "best_hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a7de56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a85653fe",
   "metadata": {},
   "source": [
    "stop training when val_loss is not increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48b24b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='loss', patience=1000,verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85174a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(x=X_all, y=Y_all, epochs=5000, use_multiprocessing=True, callbacks=[earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf8814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred=regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0f090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for check\n",
    "Y_train_pred=regressor.predict(X_train)\n",
    "r2_score(Y_train, Y_train_pred) #training score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e757ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2=r2_score(Y_test[:-30],y_pred[:-30]) #score/ r^2\n",
    "print(f'r2:{r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ef1c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2_oos\n",
    "def r2_oos(ret, pred):\n",
    "    sum_of_sq_res = np.nansum(np.power((ret-pred), 2))\n",
    "    sum_of_sq_total = np.nansum(np.power(ret, 2))\n",
    "    \n",
    "    return 1-sum_of_sq_res/sum_of_sq_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8969a6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_oos = r2_oos(Y_test[:-30], y_pred[:-30])\n",
    "print(f'r2_oos:{r2_oos}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b87143",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae=mean_absolute_error(Y_test[:-30],y_pred[:-30]) #mae\n",
    "print(f'mae:{mae}')\n",
    "\n",
    "rmse=np.sqrt(mean_squared_error(Y_test[:-30],y_pred[:-30])) #rmse\n",
    "print(f'rmse:{rmse}')\n",
    "\n",
    "mape=mean_absolute_percentage_error(Y_test[:-30],y_pred[:-30]) #mape\n",
    "print(f'mape:{mape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb36caf",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5641115",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df = pd.DataFrame(zip(Y_test,y_pred),columns=['Y_test','y_pred'])\n",
    "pre_df.index = Y_test.index\n",
    "pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4bd2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df['pred_returns'] = pre_df['y_pred'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b94aa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9674c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(zip(Y_test,y_pred),columns=['Y_test','y_pred']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732393ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df.to_csv(\"../result/ANN/btc_NN.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12ff04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kdeconnect-cli -n TAS-AN00 --ping-msg 'Script complete!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b42805c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "efc08374433b8d8e4a9fd8a0a66f7295c7ce37eceb639810a945045512ff181b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
