{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5b918ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-03 11:00:30.309107: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# conda install keras-tuner\n",
    "import keras_tuner as kt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from datetime import datetime\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n",
    "from keras import losses\n",
    "from keras.constraints import maxnorm\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_validate\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from collections import Counter\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import randint\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, accuracy_score, f1_score, classification_report\n",
    "# import pickle\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, roc_auc_score\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ded3682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535a3fac",
   "metadata": {},
   "source": [
    "### Bitcoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1467391e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8633/2421463472.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  btc['Date'][i]  =  datetime.strptime(btc['Date'][i], '%Y/%m/%d')\n"
     ]
    }
   ],
   "source": [
    "regs = pd.read_csv(\"../Data/train_btc_selected_features.csv\")\n",
    "btc = pd.read_csv(\"../Data/btc_Data.csv\")\n",
    "for i in range(len(btc['Date'])):\n",
    "    btc['Date'][i]  =  datetime.strptime(btc['Date'][i], '%Y/%m/%d')\n",
    "\n",
    "btc = btc.set_index(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d665a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "btcData = btc[regs.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8227b01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8633/751970969.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  btcData['returns'] = btcData['priceUSD'].pct_change()\n"
     ]
    }
   ],
   "source": [
    "btcData['returns'] = btcData['priceUSD'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0204bfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = btcData.drop(columns=['priceUSD'])\n",
    "Data = Data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a926d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>difficulty14mom</th>\n",
       "      <th>difficulty30mom</th>\n",
       "      <th>difficulty3rsi</th>\n",
       "      <th>difficulty90mom</th>\n",
       "      <th>difficulty90sma</th>\n",
       "      <th>hashrate90std</th>\n",
       "      <th>hashrate90var</th>\n",
       "      <th>median_transaction_fee90rocUSD</th>\n",
       "      <th>median_transaction_feeUSD</th>\n",
       "      <th>mining_profitability</th>\n",
       "      <th>...</th>\n",
       "      <th>price90wmaUSD</th>\n",
       "      <th>sentinusd90emaUSD</th>\n",
       "      <th>sentinusdUSD</th>\n",
       "      <th>top100cap</th>\n",
       "      <th>top100cap14trx</th>\n",
       "      <th>top100cap30rsi</th>\n",
       "      <th>top100cap30trx</th>\n",
       "      <th>top100cap90rsi</th>\n",
       "      <th>top100cap90trx</th>\n",
       "      <th>returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-07-18</th>\n",
       "      <td>1.621380e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.737230e+02</td>\n",
       "      <td>2.055500e+01</td>\n",
       "      <td>8.384389e+08</td>\n",
       "      <td>1.757449e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>401834.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.612000e+03</td>\n",
       "      <td>25.782</td>\n",
       "      <td>0.139</td>\n",
       "      <td>71.191</td>\n",
       "      <td>0.308</td>\n",
       "      <td>79.756</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-19</th>\n",
       "      <td>1.621380e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.737230e+02</td>\n",
       "      <td>2.248600e+01</td>\n",
       "      <td>8.819952e+08</td>\n",
       "      <td>1.944789e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>481473.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.047000e+03</td>\n",
       "      <td>25.685</td>\n",
       "      <td>0.123</td>\n",
       "      <td>68.863</td>\n",
       "      <td>0.300</td>\n",
       "      <td>78.999</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.183196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-20</th>\n",
       "      <td>1.584060e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.735570e+02</td>\n",
       "      <td>2.441400e+01</td>\n",
       "      <td>9.280542e+08</td>\n",
       "      <td>2.153212e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>431831.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.341000e+03</td>\n",
       "      <td>25.602</td>\n",
       "      <td>0.107</td>\n",
       "      <td>66.923</td>\n",
       "      <td>0.292</td>\n",
       "      <td>78.355</td>\n",
       "      <td>0.544</td>\n",
       "      <td>-0.088475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-21</th>\n",
       "      <td>1.580420e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.700790e+02</td>\n",
       "      <td>2.630400e+01</td>\n",
       "      <td>9.761949e+08</td>\n",
       "      <td>2.382391e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>460783.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.122000e+03</td>\n",
       "      <td>25.638</td>\n",
       "      <td>0.092</td>\n",
       "      <td>67.334</td>\n",
       "      <td>0.284</td>\n",
       "      <td>78.431</td>\n",
       "      <td>0.541</td>\n",
       "      <td>-0.020434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-22</th>\n",
       "      <td>1.580420e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.700790e+02</td>\n",
       "      <td>2.819400e+01</td>\n",
       "      <td>1.019474e+09</td>\n",
       "      <td>2.598318e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>334641.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.570000e+03</td>\n",
       "      <td>25.604</td>\n",
       "      <td>0.078</td>\n",
       "      <td>66.526</td>\n",
       "      <td>0.275</td>\n",
       "      <td>78.166</td>\n",
       "      <td>0.537</td>\n",
       "      <td>-0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-24</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.384949e+13</td>\n",
       "      <td>4.612156e+19</td>\n",
       "      <td>5.317997e+38</td>\n",
       "      <td>165.380</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.0577</td>\n",
       "      <td>...</td>\n",
       "      <td>18820.0</td>\n",
       "      <td>28544682257</td>\n",
       "      <td>6.114577e+09</td>\n",
       "      <td>15.236</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>40.118</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>45.140</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.008075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-25</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.394503e+13</td>\n",
       "      <td>4.542600e+19</td>\n",
       "      <td>5.158804e+38</td>\n",
       "      <td>214.464</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.0541</td>\n",
       "      <td>...</td>\n",
       "      <td>18759.0</td>\n",
       "      <td>28076113671</td>\n",
       "      <td>7.224812e+09</td>\n",
       "      <td>15.231</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>39.880</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>45.047</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.006565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-26</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.404058e+13</td>\n",
       "      <td>4.624135e+19</td>\n",
       "      <td>5.345655e+38</td>\n",
       "      <td>255.004</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.0571</td>\n",
       "      <td>...</td>\n",
       "      <td>18701.0</td>\n",
       "      <td>27566714445</td>\n",
       "      <td>4.898449e+09</td>\n",
       "      <td>15.233</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>40.069</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>45.106</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.004001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-27</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.413612e+13</td>\n",
       "      <td>4.576227e+19</td>\n",
       "      <td>5.235464e+38</td>\n",
       "      <td>113.430</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.0618</td>\n",
       "      <td>...</td>\n",
       "      <td>18642.0</td>\n",
       "      <td>27053515674</td>\n",
       "      <td>4.216170e+09</td>\n",
       "      <td>15.259</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>41.833</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>45.656</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.423166e+13</td>\n",
       "      <td>4.552126e+19</td>\n",
       "      <td>5.180463e+38</td>\n",
       "      <td>104.294</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.0592</td>\n",
       "      <td>...</td>\n",
       "      <td>18578.0</td>\n",
       "      <td>26631320968</td>\n",
       "      <td>7.843657e+09</td>\n",
       "      <td>15.219</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>39.965</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>44.957</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.019828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4517 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            difficulty14mom  difficulty30mom  difficulty3rsi  difficulty90mom  \\\n",
       "Date                                                                            \n",
       "2010-07-18     1.621380e+02     1.641620e+02          100.00     1.737230e+02   \n",
       "2010-07-19     1.621380e+02     1.641620e+02          100.00     1.737230e+02   \n",
       "2010-07-20     1.584060e+02     1.641620e+02          100.00     1.735570e+02   \n",
       "2010-07-21     1.580420e+02     1.641620e+02          100.00     1.700790e+02   \n",
       "2010-07-22     1.580420e+02     1.641620e+02          100.00     1.700790e+02   \n",
       "...                     ...              ...             ...              ...   \n",
       "2022-11-24     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-25     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-26     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-27     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-28     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "\n",
       "            difficulty90sma  hashrate90std  hashrate90var  \\\n",
       "Date                                                        \n",
       "2010-07-18     2.055500e+01   8.384389e+08   1.757449e+17   \n",
       "2010-07-19     2.248600e+01   8.819952e+08   1.944789e+17   \n",
       "2010-07-20     2.441400e+01   9.280542e+08   2.153212e+17   \n",
       "2010-07-21     2.630400e+01   9.761949e+08   2.382391e+17   \n",
       "2010-07-22     2.819400e+01   1.019474e+09   2.598318e+17   \n",
       "...                     ...            ...            ...   \n",
       "2022-11-24     3.384949e+13   4.612156e+19   5.317997e+38   \n",
       "2022-11-25     3.394503e+13   4.542600e+19   5.158804e+38   \n",
       "2022-11-26     3.404058e+13   4.624135e+19   5.345655e+38   \n",
       "2022-11-27     3.413612e+13   4.576227e+19   5.235464e+38   \n",
       "2022-11-28     3.423166e+13   4.552126e+19   5.180463e+38   \n",
       "\n",
       "            median_transaction_fee90rocUSD  median_transaction_feeUSD  \\\n",
       "Date                                                                    \n",
       "2010-07-18                           0.000                      0.000   \n",
       "2010-07-19                           0.000                      0.000   \n",
       "2010-07-20                           0.000                      0.000   \n",
       "2010-07-21                           0.000                      0.000   \n",
       "2010-07-22                           0.000                      0.000   \n",
       "...                                    ...                        ...   \n",
       "2022-11-24                         165.380                      0.818   \n",
       "2022-11-25                         214.464                      0.812   \n",
       "2022-11-26                         255.004                      0.809   \n",
       "2022-11-27                         113.430                      0.619   \n",
       "2022-11-28                         104.294                      0.622   \n",
       "\n",
       "            mining_profitability  ...  price90wmaUSD  sentinusd90emaUSD  \\\n",
       "Date                              ...                                     \n",
       "2010-07-18           401834.0000  ...            0.0                  0   \n",
       "2010-07-19           481473.0000  ...            0.0                  0   \n",
       "2010-07-20           431831.0000  ...            0.0                  0   \n",
       "2010-07-21           460783.0000  ...            0.0                  0   \n",
       "2010-07-22           334641.0000  ...            0.0                  0   \n",
       "...                          ...  ...            ...                ...   \n",
       "2022-11-24                0.0577  ...        18820.0        28544682257   \n",
       "2022-11-25                0.0541  ...        18759.0        28076113671   \n",
       "2022-11-26                0.0571  ...        18701.0        27566714445   \n",
       "2022-11-27                0.0618  ...        18642.0        27053515674   \n",
       "2022-11-28                0.0592  ...        18578.0        26631320968   \n",
       "\n",
       "            sentinusdUSD  top100cap  top100cap14trx  top100cap30rsi  \\\n",
       "Date                                                                  \n",
       "2010-07-18  2.612000e+03     25.782           0.139          71.191   \n",
       "2010-07-19  4.047000e+03     25.685           0.123          68.863   \n",
       "2010-07-20  2.341000e+03     25.602           0.107          66.923   \n",
       "2010-07-21  2.122000e+03     25.638           0.092          67.334   \n",
       "2010-07-22  2.570000e+03     25.604           0.078          66.526   \n",
       "...                  ...        ...             ...             ...   \n",
       "2022-11-24  6.114577e+09     15.236          -0.080          40.118   \n",
       "2022-11-25  7.224812e+09     15.231          -0.075          39.880   \n",
       "2022-11-26  4.898449e+09     15.233          -0.070          40.069   \n",
       "2022-11-27  4.216170e+09     15.259          -0.065          41.833   \n",
       "2022-11-28  7.843657e+09     15.219          -0.061          39.965   \n",
       "\n",
       "            top100cap30trx  top100cap90rsi  top100cap90trx   returns  \n",
       "Date                                                                  \n",
       "2010-07-18           0.308          79.756           0.550  0.466667  \n",
       "2010-07-19           0.300          78.999           0.547  0.183196  \n",
       "2010-07-20           0.292          78.355           0.544 -0.088475  \n",
       "2010-07-21           0.284          78.431           0.541 -0.020434  \n",
       "2010-07-22           0.275          78.166           0.537 -0.153846  \n",
       "...                    ...             ...             ...       ...  \n",
       "2022-11-24          -0.042          45.140           0.001  0.008075  \n",
       "2022-11-25          -0.043          45.047           0.001 -0.006565  \n",
       "2022-11-26          -0.043          45.106           0.001  0.004001  \n",
       "2022-11-27          -0.044          45.656           0.000 -0.001147  \n",
       "2022-11-28          -0.044          44.957           0.000 -0.019828  \n",
       "\n",
       "[4517 rows x 32 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5d4f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide X and Y\n",
    "X = Data.iloc[:,0:]\n",
    "#Y = Data['returns']   # 用returns的话就用这一行，然后把下一行comment掉\n",
    "Y = btcData['priceUSD'].shift(-30)[1:] # 反之亦然"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb3275b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>difficulty14mom</th>\n",
       "      <th>difficulty30mom</th>\n",
       "      <th>difficulty3rsi</th>\n",
       "      <th>difficulty90mom</th>\n",
       "      <th>difficulty90sma</th>\n",
       "      <th>hashrate90std</th>\n",
       "      <th>hashrate90var</th>\n",
       "      <th>median_transaction_fee90rocUSD</th>\n",
       "      <th>median_transaction_feeUSD</th>\n",
       "      <th>mining_profitability</th>\n",
       "      <th>...</th>\n",
       "      <th>price90wmaUSD</th>\n",
       "      <th>sentinusd90emaUSD</th>\n",
       "      <th>sentinusdUSD</th>\n",
       "      <th>top100cap</th>\n",
       "      <th>top100cap14trx</th>\n",
       "      <th>top100cap30rsi</th>\n",
       "      <th>top100cap30trx</th>\n",
       "      <th>top100cap90rsi</th>\n",
       "      <th>top100cap90trx</th>\n",
       "      <th>returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-07-18</th>\n",
       "      <td>162.138</td>\n",
       "      <td>164.162</td>\n",
       "      <td>100.0</td>\n",
       "      <td>173.723</td>\n",
       "      <td>20.555</td>\n",
       "      <td>838438881.0</td>\n",
       "      <td>1.757449e+17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>401834.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2612.0</td>\n",
       "      <td>25.782</td>\n",
       "      <td>0.139</td>\n",
       "      <td>71.191</td>\n",
       "      <td>0.308</td>\n",
       "      <td>79.756</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-19</th>\n",
       "      <td>162.138</td>\n",
       "      <td>164.162</td>\n",
       "      <td>100.0</td>\n",
       "      <td>173.723</td>\n",
       "      <td>22.486</td>\n",
       "      <td>881995244.0</td>\n",
       "      <td>1.944789e+17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>481473.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4047.0</td>\n",
       "      <td>25.685</td>\n",
       "      <td>0.123</td>\n",
       "      <td>68.863</td>\n",
       "      <td>0.300</td>\n",
       "      <td>78.999</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.183196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-20</th>\n",
       "      <td>158.406</td>\n",
       "      <td>164.162</td>\n",
       "      <td>100.0</td>\n",
       "      <td>173.557</td>\n",
       "      <td>24.414</td>\n",
       "      <td>928054231.0</td>\n",
       "      <td>2.153212e+17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>431831.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2341.0</td>\n",
       "      <td>25.602</td>\n",
       "      <td>0.107</td>\n",
       "      <td>66.923</td>\n",
       "      <td>0.292</td>\n",
       "      <td>78.355</td>\n",
       "      <td>0.544</td>\n",
       "      <td>-0.088475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            difficulty14mom  difficulty30mom  difficulty3rsi  difficulty90mom  \\\n",
       "Date                                                                            \n",
       "2010-07-18          162.138          164.162           100.0          173.723   \n",
       "2010-07-19          162.138          164.162           100.0          173.723   \n",
       "2010-07-20          158.406          164.162           100.0          173.557   \n",
       "\n",
       "            difficulty90sma  hashrate90std  hashrate90var  \\\n",
       "Date                                                        \n",
       "2010-07-18           20.555    838438881.0   1.757449e+17   \n",
       "2010-07-19           22.486    881995244.0   1.944789e+17   \n",
       "2010-07-20           24.414    928054231.0   2.153212e+17   \n",
       "\n",
       "            median_transaction_fee90rocUSD  median_transaction_feeUSD  \\\n",
       "Date                                                                    \n",
       "2010-07-18                             0.0                        0.0   \n",
       "2010-07-19                             0.0                        0.0   \n",
       "2010-07-20                             0.0                        0.0   \n",
       "\n",
       "            mining_profitability  ...  price90wmaUSD  sentinusd90emaUSD  \\\n",
       "Date                              ...                                     \n",
       "2010-07-18              401834.0  ...            0.0                  0   \n",
       "2010-07-19              481473.0  ...            0.0                  0   \n",
       "2010-07-20              431831.0  ...            0.0                  0   \n",
       "\n",
       "            sentinusdUSD  top100cap  top100cap14trx  top100cap30rsi  \\\n",
       "Date                                                                  \n",
       "2010-07-18        2612.0     25.782           0.139          71.191   \n",
       "2010-07-19        4047.0     25.685           0.123          68.863   \n",
       "2010-07-20        2341.0     25.602           0.107          66.923   \n",
       "\n",
       "            top100cap30trx  top100cap90rsi  top100cap90trx   returns  \n",
       "Date                                                                  \n",
       "2010-07-18           0.308          79.756           0.550  0.466667  \n",
       "2010-07-19           0.300          78.999           0.547  0.183196  \n",
       "2010-07-20           0.292          78.355           0.544 -0.088475  \n",
       "\n",
       "[3 rows x 32 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd589ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into three data sets\n",
    "X_train = X['2016-01-01':'2019-12-31']\n",
    "X_val = X['2020-01-01':'2021-05-31']\n",
    "X_test = X['2021-06-01':'2023-01-01']\n",
    "\n",
    "Y_train = Y['2016-01-01':'2019-12-31']\n",
    "Y_val = Y['2020-01-01':'2021-05-31']\n",
    "Y_test = Y['2021-06-01':'2023-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e5e9fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............ (step 1 of 2) Processing mixmax, total=   0.0s\n",
      "[Pipeline] ............ (step 2 of 2) Processing robust, total=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;mixmax&#x27;, MinMaxScaler()), [&#x27;robust&#x27;, RobustScaler()]],\n",
       "         verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;mixmax&#x27;, MinMaxScaler()), [&#x27;robust&#x27;, RobustScaler()]],\n",
       "         verbose=True)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RobustScaler</label><div class=\"sk-toggleable__content\"><pre>RobustScaler()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('mixmax', MinMaxScaler()), ['robust', RobustScaler()]],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators=[]\n",
    "estimators.append(['mixmax',MinMaxScaler()])\n",
    "estimators.append(['robust',RobustScaler()])\n",
    "scale=Pipeline(estimators,verbose=True)\n",
    "scale.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6fc0859",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=scale.transform(X_train)\n",
    "X_test=scale.transform(X_test)\n",
    "X_val = scale.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8a84d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f6916ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define neural network model\n",
    "shape=X.shape[1]\n",
    "def build_model(hp, initializer='normal', activation='relu', NUM_FEATURES=shape):\n",
    "    # create model\n",
    "    hp_units1 = hp.Int('units1', min_value=32, max_value=512, step=32)\n",
    "    hp_units2 = hp.Int('units2', min_value=32, max_value=512, step=32)\n",
    "    hp_units3 = hp.Int('units3', min_value=32, max_value=512, step=32)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hp_units1, input_shape=(NUM_FEATURES,), kernel_initializer=initializer, activation=activation))\n",
    "    model.add(Dense(hp_units2, activation=activation))\n",
    "    model.add(Dense(hp_units3, activation=activation))\n",
    "    model.add(Dense(1, activation=activation))\n",
    "    # Compile model\n",
    "    adam=keras.optimizers.Adam(lr=lr_schedule(0), amsgrad=True)\n",
    "    #sgd=keras.optimizers.SGD(learning_rate=0.08, momentum=0.9, nesterov=False)\n",
    "    model.compile(loss='logcosh', optimizer=adam, metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "112c8df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project btc_tune/ANN_TUNE/oracle.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-03 11:00:32.861569: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-03 11:00:32.862361: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "INFO:tensorflow:Reloading Tuner from btc_tune/ANN_TUNE/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spectre/anaconda3/envs/tensorplustorch/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_mae',\n",
    "    directory='btc_tune',\n",
    "    project_name='ANN_TUNE'\n",
    ")\n",
    "\n",
    "tuner.search(X_train, Y_train, epochs=5000, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9884fdb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n"
     ]
    }
   ],
   "source": [
    "# Get the best hyperparameters.\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "# Build the model with the best hp.\n",
    "regressor = build_model(best_hp)\n",
    "# Fit with the entire dataset.\n",
    "X_all = np.concatenate((X_train, X_val))\n",
    "Y_all = np.concatenate((Y_train, Y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d46c3b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_tuner.engine.hyperparameters.HyperParameters at 0x7f19a2be12a0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best hyperparameters\n",
    "best_hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a7de56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a85653fe",
   "metadata": {},
   "source": [
    "stop training when val_loss is not increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b48b24b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='loss', patience=1000,verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85174a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "62/62 [==============================] - 1s 4ms/step - loss: 8850.9678 - mae: 8851.6602\n",
      "Epoch 2/5000\n",
      "62/62 [==============================] - 1s 10ms/step - loss: 3733.7986 - mae: 3734.4917\n",
      "Epoch 3/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 2669.6357 - mae: 2670.3276\n",
      "Epoch 4/5000\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 2469.1504 - mae: 2469.8430\n",
      "Epoch 5/5000\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 2395.4402 - mae: 2396.1335\n",
      "Epoch 6/5000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2328.6453 - mae: 2329.3376\n",
      "Epoch 7/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 2251.0745 - mae: 2251.7676\n",
      "Epoch 8/5000\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 2160.7161 - mae: 2161.4089\n",
      "Epoch 9/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 2115.5527 - mae: 2116.2446\n",
      "Epoch 10/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 2098.0334 - mae: 2098.7258\n",
      "Epoch 11/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 2052.3989 - mae: 2053.0923\n",
      "Epoch 12/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 2053.2854 - mae: 2053.9783\n",
      "Epoch 13/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1977.4333 - mae: 1978.1263\n",
      "Epoch 14/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1945.2374 - mae: 1945.9301\n",
      "Epoch 15/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1891.8015 - mae: 1892.4937\n",
      "Epoch 16/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1881.9249 - mae: 1882.6179\n",
      "Epoch 17/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1818.1331 - mae: 1818.8256\n",
      "Epoch 18/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1776.2211 - mae: 1776.9141\n",
      "Epoch 19/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1725.6729 - mae: 1726.3662\n",
      "Epoch 20/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1711.0538 - mae: 1711.7468\n",
      "Epoch 21/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1638.9559 - mae: 1639.6492\n",
      "Epoch 22/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1577.1711 - mae: 1577.8633\n",
      "Epoch 23/5000\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1545.6073 - mae: 1546.3002\n",
      "Epoch 24/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1576.8365 - mae: 1577.5287\n",
      "Epoch 25/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1495.3049 - mae: 1495.9967\n",
      "Epoch 26/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1478.1215 - mae: 1478.8129\n",
      "Epoch 27/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1448.1836 - mae: 1448.8754\n",
      "Epoch 28/5000\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 1412.6060 - mae: 1413.2988\n",
      "Epoch 29/5000\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1433.6826 - mae: 1434.3745\n",
      "Epoch 30/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1368.7928 - mae: 1369.4863\n",
      "Epoch 31/5000\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1358.3536 - mae: 1359.0464\n",
      "Epoch 32/5000\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 1390.1954 - mae: 1390.8873\n",
      "Epoch 33/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1317.4789 - mae: 1318.1710\n",
      "Epoch 34/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1295.2009 - mae: 1295.8936\n",
      "Epoch 35/5000\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 1287.0820 - mae: 1287.7736\n",
      "Epoch 36/5000\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 1254.0890 - mae: 1254.7812\n",
      "Epoch 37/5000\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1213.6931 - mae: 1214.3856\n",
      "Epoch 38/5000\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1250.4304 - mae: 1251.1235\n",
      "Epoch 39/5000\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1184.0636 - mae: 1184.7550\n",
      "Epoch 40/5000\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1169.3346 - mae: 1170.0269\n",
      "Epoch 41/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1157.7665 - mae: 1158.4592\n",
      "Epoch 42/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1137.3539 - mae: 1138.0465\n",
      "Epoch 43/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1116.4346 - mae: 1117.1273\n",
      "Epoch 44/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1104.4674 - mae: 1105.1591\n",
      "Epoch 45/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1093.8557 - mae: 1094.5474\n",
      "Epoch 46/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1072.8657 - mae: 1073.5573\n",
      "Epoch 47/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1065.2612 - mae: 1065.9537\n",
      "Epoch 48/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1080.2733 - mae: 1080.9651\n",
      "Epoch 49/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1036.9476 - mae: 1037.6401\n",
      "Epoch 50/5000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1057.3176 - mae: 1058.0083\n",
      "Epoch 51/5000\n",
      "62/62 [==============================] - 1s 10ms/step - loss: 1028.5881 - mae: 1029.2805\n",
      "Epoch 52/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1022.4281 - mae: 1023.1204\n",
      "Epoch 53/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 984.1918 - mae: 984.8834\n",
      "Epoch 54/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 971.6675 - mae: 972.3592\n",
      "Epoch 55/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 981.3734 - mae: 982.0659\n",
      "Epoch 56/5000\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 969.4067 - mae: 970.0989\n",
      "Epoch 57/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 936.0402 - mae: 936.7321\n",
      "Epoch 58/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1008.6666 - mae: 1009.3584\n",
      "Epoch 59/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 961.7076 - mae: 962.3988\n",
      "Epoch 60/5000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 907.0483 - mae: 907.7391\n",
      "Epoch 61/5000\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 918.5341 - mae: 919.2260\n",
      "Epoch 62/5000\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 906.2856 - mae: 906.9771\n",
      "Epoch 63/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 901.4763 - mae: 902.1693\n",
      "Epoch 64/5000\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 917.1968 - mae: 917.8891\n",
      "Epoch 65/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 900.9760 - mae: 901.6679\n",
      "Epoch 66/5000\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 843.3281 - mae: 844.0201\n",
      "Epoch 67/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 849.4571 - mae: 850.1494\n",
      "Epoch 68/5000\n",
      "62/62 [==============================] - 1s 10ms/step - loss: 836.3547 - mae: 837.0468\n",
      "Epoch 69/5000\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 817.1335 - mae: 817.8248\n",
      "Epoch 70/5000\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 873.3935 - mae: 874.0842\n",
      "Epoch 71/5000\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 831.3185 - mae: 832.0107\n",
      "Epoch 72/5000\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 910.9843 - mae: 911.6760\n",
      "Epoch 73/5000\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 832.4129 - mae: 833.1050\n",
      "Epoch 74/5000\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 823.3358 - mae: 824.0269\n",
      "Epoch 75/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 800.8076 - mae: 801.4987\n",
      "Epoch 76/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 778.8716 - mae: 779.5628\n",
      "Epoch 77/5000\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 783.1834 - mae: 783.8752\n",
      "Epoch 78/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 831.9978 - mae: 832.6907\n",
      "Epoch 79/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 780.2924 - mae: 780.9839\n",
      "Epoch 80/5000\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 762.8815 - mae: 763.5729\n",
      "Epoch 81/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 0s 6ms/step - loss: 771.2823 - mae: 771.9735\n",
      "Epoch 82/5000\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 747.4719 - mae: 748.1644\n",
      "Epoch 83/5000\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 727.6096 - mae: 728.3003\n",
      "Epoch 84/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 716.5968 - mae: 717.2883\n",
      "Epoch 85/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 763.1710 - mae: 763.8621\n",
      "Epoch 86/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 741.6424 - mae: 742.3347\n",
      "Epoch 87/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 717.8958 - mae: 718.5870\n",
      "Epoch 88/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 728.5400 - mae: 729.2322\n",
      "Epoch 89/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 721.3196 - mae: 722.0111\n",
      "Epoch 90/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 741.8718 - mae: 742.5640\n",
      "Epoch 91/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 707.8892 - mae: 708.5811\n",
      "Epoch 92/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 712.6347 - mae: 713.3268\n",
      "Epoch 93/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 725.6860 - mae: 726.3779\n",
      "Epoch 94/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 723.5980 - mae: 724.2905\n",
      "Epoch 95/5000\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 690.2679 - mae: 690.9595\n",
      "Epoch 96/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 708.0756 - mae: 708.7662\n",
      "Epoch 97/5000\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 709.6544 - mae: 710.3459\n",
      "Epoch 98/5000\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 694.2233 - mae: 694.9153\n",
      "Epoch 99/5000\n",
      "62/62 [==============================] - 2s 25ms/step - loss: 681.2772 - mae: 681.9694\n",
      "Epoch 100/5000\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 667.7122 - mae: 668.4040\n",
      "Epoch 101/5000\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 657.2524 - mae: 657.9440\n",
      "Epoch 102/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 655.4868 - mae: 656.1783\n",
      "Epoch 103/5000\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 701.1743 - mae: 701.8651\n",
      "Epoch 104/5000\n",
      "62/62 [==============================] - 1s 10ms/step - loss: 722.5080 - mae: 723.1994\n",
      "Epoch 105/5000\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 660.1676 - mae: 660.8593\n",
      "Epoch 106/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 670.7784 - mae: 671.4697\n",
      "Epoch 107/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 673.9453 - mae: 674.6371\n",
      "Epoch 108/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 664.8217 - mae: 665.5141\n",
      "Epoch 109/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 660.1570 - mae: 660.8484\n",
      "Epoch 110/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 671.9467 - mae: 672.6377\n",
      "Epoch 111/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 656.6748 - mae: 657.3658\n",
      "Epoch 112/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 675.1259 - mae: 675.8175\n",
      "Epoch 113/5000\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 632.3013 - mae: 632.9927\n",
      "Epoch 114/5000\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 609.2971 - mae: 609.9881\n",
      "Epoch 115/5000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 630.4807 - mae: 631.1711\n",
      "Epoch 116/5000\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 643.7035 - mae: 644.3950\n",
      "Epoch 117/5000\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 621.8590 - mae: 622.5515\n",
      "Epoch 118/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 623.4683 - mae: 624.1586\n",
      "Epoch 119/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 628.2365 - mae: 628.9262\n",
      "Epoch 120/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 636.5629 - mae: 637.2546\n",
      "Epoch 121/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 617.7924 - mae: 618.4834\n",
      "Epoch 122/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 627.0483 - mae: 627.7400\n",
      "Epoch 123/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 612.4852 - mae: 613.1763\n",
      "Epoch 124/5000\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 625.7212 - mae: 626.4131\n",
      "Epoch 125/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 620.4625 - mae: 621.1540\n",
      "Epoch 126/5000\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 606.0054 - mae: 606.6970\n",
      "Epoch 127/5000\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 617.7595 - mae: 618.4508\n",
      "Epoch 128/5000\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 603.2113 - mae: 603.9031\n",
      "Epoch 129/5000\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 600.6306 - mae: 601.3220\n",
      "Epoch 130/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 628.3541 - mae: 629.0445\n",
      "Epoch 131/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 589.9608 - mae: 590.6522\n",
      "Epoch 132/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 606.0672 - mae: 606.7580\n",
      "Epoch 133/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 580.8624 - mae: 581.5538\n",
      "Epoch 134/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 605.4400 - mae: 606.1308\n",
      "Epoch 135/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 600.3219 - mae: 601.0118\n",
      "Epoch 136/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 632.3817 - mae: 633.0724\n",
      "Epoch 137/5000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 595.4608 - mae: 596.1528\n",
      "Epoch 138/5000\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 607.1760 - mae: 607.8678\n",
      "Epoch 139/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 601.2663 - mae: 601.9574\n",
      "Epoch 140/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 581.2154 - mae: 581.9072\n",
      "Epoch 141/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 625.2540 - mae: 625.9459\n",
      "Epoch 142/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 571.3974 - mae: 572.0896\n",
      "Epoch 143/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 597.7749 - mae: 598.4664\n",
      "Epoch 144/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 585.9337 - mae: 586.6243\n",
      "Epoch 145/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 566.8791 - mae: 567.5695\n",
      "Epoch 146/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 578.7542 - mae: 579.4460\n",
      "Epoch 147/5000\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 570.0873 - mae: 570.7788\n",
      "Epoch 148/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 592.6507 - mae: 593.3410\n",
      "Epoch 149/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 590.8823 - mae: 591.5728\n",
      "Epoch 150/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 585.0516 - mae: 585.7435\n",
      "Epoch 151/5000\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 574.7549 - mae: 575.4457\n",
      "Epoch 152/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 560.2135 - mae: 560.9052\n",
      "Epoch 153/5000\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 586.1172 - mae: 586.8091\n",
      "Epoch 154/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 552.2298 - mae: 552.9201\n",
      "Epoch 155/5000\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 567.3992 - mae: 568.0908\n",
      "Epoch 156/5000\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 595.8800 - mae: 596.5714\n",
      "Epoch 157/5000\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 594.3365 - mae: 595.0275\n",
      "Epoch 158/5000\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 572.3792 - mae: 573.0713\n",
      "Epoch 159/5000\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 567.3195 - mae: 568.0108\n",
      "Epoch 160/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 547.7000 - mae: 548.3915\n",
      "Epoch 161/5000\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 570.3781 - mae: 571.0695\n",
      "Epoch 162/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 0s 6ms/step - loss: 581.7689 - mae: 582.4600\n",
      "Epoch 163/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 539.9023 - mae: 540.5939\n",
      "Epoch 164/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 542.2283 - mae: 542.9193\n",
      "Epoch 165/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 599.4760 - mae: 600.1666\n",
      "Epoch 166/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 544.1287 - mae: 544.8209\n",
      "Epoch 167/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 562.6606 - mae: 563.3513\n",
      "Epoch 168/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 556.0809 - mae: 556.7723\n",
      "Epoch 169/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 577.3214 - mae: 578.0124\n",
      "Epoch 170/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 533.9788 - mae: 534.6680\n",
      "Epoch 171/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 558.9071 - mae: 559.5986\n",
      "Epoch 172/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 536.6522 - mae: 537.3442\n",
      "Epoch 173/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 535.7407 - mae: 536.4327\n",
      "Epoch 174/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 552.9425 - mae: 553.6338\n",
      "Epoch 175/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 527.1852 - mae: 527.8766\n",
      "Epoch 176/5000\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 542.2775 - mae: 542.9679\n",
      "Epoch 177/5000\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 526.7022 - mae: 527.3936\n",
      "Epoch 178/5000\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 542.2501 - mae: 542.9412\n",
      "Epoch 179/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 535.7236 - mae: 536.4160\n",
      "Epoch 180/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 546.2161 - mae: 546.9069\n",
      "Epoch 181/5000\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 537.1574 - mae: 537.8491\n",
      "Epoch 182/5000\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 525.5671 - mae: 526.2589\n",
      "Epoch 183/5000\n",
      "62/62 [==============================] - 1s 10ms/step - loss: 520.4985 - mae: 521.1890\n",
      "Epoch 184/5000\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 575.2971 - mae: 575.9879\n",
      "Epoch 185/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 547.8609 - mae: 548.5521\n",
      "Epoch 186/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 515.8349 - mae: 516.5266\n",
      "Epoch 187/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 510.2161 - mae: 510.9068\n",
      "Epoch 188/5000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 542.3761 - mae: 543.0679\n",
      "Epoch 189/5000\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 535.3234 - mae: 536.0151\n",
      "Epoch 190/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 516.2333 - mae: 516.9233\n",
      "Epoch 191/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 498.3966 - mae: 499.0879\n",
      "Epoch 192/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 530.1124 - mae: 530.8034\n",
      "Epoch 193/5000\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 510.7278 - mae: 511.4189\n",
      "Epoch 194/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 550.5164 - mae: 551.2074\n",
      "Epoch 195/5000\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 536.0216 - mae: 536.7120\n",
      "Epoch 196/5000\n",
      "62/62 [==============================] - 1s 16ms/step - loss: 524.7708 - mae: 525.4614\n",
      "Epoch 197/5000\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 505.8068 - mae: 506.4988\n",
      "Epoch 198/5000\n",
      "62/62 [==============================] - 1s 10ms/step - loss: 530.4975 - mae: 531.1885\n",
      "Epoch 199/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 499.5876 - mae: 500.2779\n",
      "Epoch 200/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 541.6953 - mae: 542.3857\n",
      "Epoch 201/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 500.6131 - mae: 501.3033\n",
      "Epoch 202/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 513.2874 - mae: 513.9788\n",
      "Epoch 203/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 500.5952 - mae: 501.2865\n",
      "Epoch 204/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 526.1127 - mae: 526.8019\n",
      "Epoch 205/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 511.1840 - mae: 511.8734\n",
      "Epoch 206/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 508.6279 - mae: 509.3185\n",
      "Epoch 207/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 494.2187 - mae: 494.9100\n",
      "Epoch 208/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 501.7296 - mae: 502.4185\n",
      "Epoch 209/5000\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 509.7590 - mae: 510.4497\n",
      "Epoch 210/5000\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 498.1791 - mae: 498.8708\n",
      "Epoch 211/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 504.2097 - mae: 504.8990\n",
      "Epoch 212/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 542.6643 - mae: 543.3554\n",
      "Epoch 213/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 552.5085 - mae: 553.1982\n",
      "Epoch 214/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 503.0009 - mae: 503.6917\n",
      "Epoch 215/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 519.3561 - mae: 520.0478\n",
      "Epoch 216/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 487.3863 - mae: 488.0772\n",
      "Epoch 217/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 482.2437 - mae: 482.9340\n",
      "Epoch 218/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 538.9849 - mae: 539.6770\n",
      "Epoch 219/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 491.7437 - mae: 492.4332\n",
      "Epoch 220/5000\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 489.4722 - mae: 490.1634\n",
      "Epoch 221/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 473.9164 - mae: 474.6068\n",
      "Epoch 222/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 484.8156 - mae: 485.5069\n",
      "Epoch 223/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 488.0002 - mae: 488.6914\n",
      "Epoch 224/5000\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 493.2793 - mae: 493.9702\n",
      "Epoch 225/5000\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 513.8849 - mae: 514.5759\n",
      "Epoch 226/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 510.7372 - mae: 511.4275\n",
      "Epoch 227/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 483.9043 - mae: 484.5960\n",
      "Epoch 228/5000\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 479.7518 - mae: 480.4420\n",
      "Epoch 229/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 527.0209 - mae: 527.7126\n",
      "Epoch 230/5000\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 487.2018 - mae: 487.8929\n",
      "Epoch 231/5000\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 505.1765 - mae: 505.8669\n",
      "Epoch 232/5000\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 466.3394 - mae: 467.0312\n",
      "Epoch 233/5000\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 483.3704 - mae: 484.0623\n",
      "Epoch 234/5000\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 504.9742 - mae: 505.6655\n",
      "Epoch 235/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 489.6130 - mae: 490.3040\n",
      "Epoch 236/5000\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 466.7482 - mae: 467.4380\n",
      "Epoch 237/5000\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 452.3346 - mae: 453.0242\n",
      "Epoch 238/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 466.3247 - mae: 467.0146\n",
      "Epoch 239/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 471.7058 - mae: 472.3976\n",
      "Epoch 240/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 509.4089 - mae: 510.1006\n",
      "Epoch 241/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 469.5437 - mae: 470.2347\n",
      "Epoch 242/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 487.9160 - mae: 488.6057\n",
      "Epoch 243/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 0s 4ms/step - loss: 467.4346 - mae: 468.1248\n",
      "Epoch 244/5000\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 464.9422 - mae: 465.6324\n",
      "Epoch 245/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 479.3543 - mae: 480.0440\n",
      "Epoch 246/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 513.1806 - mae: 513.8706\n",
      "Epoch 247/5000\n",
      "62/62 [==============================] - 1s 10ms/step - loss: 495.9617 - mae: 496.6530\n",
      "Epoch 248/5000\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 478.0029 - mae: 478.6945\n",
      "Epoch 249/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 495.2543 - mae: 495.9457\n",
      "Epoch 250/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 487.4865 - mae: 488.1781\n",
      "Epoch 251/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 457.3856 - mae: 458.0764\n",
      "Epoch 252/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 465.6347 - mae: 466.3253\n",
      "Epoch 253/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 483.7912 - mae: 484.4808\n",
      "Epoch 254/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 473.8387 - mae: 474.5294\n",
      "Epoch 255/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 480.0564 - mae: 480.7482\n",
      "Epoch 256/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 451.2513 - mae: 451.9419\n",
      "Epoch 257/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 492.2399 - mae: 492.9298\n",
      "Epoch 258/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 443.0058 - mae: 443.6967\n",
      "Epoch 259/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 462.7793 - mae: 463.4706\n",
      "Epoch 260/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 434.5385 - mae: 435.2296\n",
      "Epoch 261/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 433.0907 - mae: 433.7798\n",
      "Epoch 262/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 491.6245 - mae: 492.3169\n",
      "Epoch 263/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 458.0853 - mae: 458.7755\n",
      "Epoch 264/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 457.3688 - mae: 458.0601\n",
      "Epoch 265/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 454.6555 - mae: 455.3468\n",
      "Epoch 266/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 478.9677 - mae: 479.6583\n",
      "Epoch 267/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 432.1299 - mae: 432.8210\n",
      "Epoch 268/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 433.1826 - mae: 433.8738\n",
      "Epoch 269/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 467.5030 - mae: 468.1941\n",
      "Epoch 270/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 443.3246 - mae: 444.0159\n",
      "Epoch 271/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 447.6342 - mae: 448.3238\n",
      "Epoch 272/5000\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 456.4961 - mae: 457.1860\n",
      "Epoch 273/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 478.2447 - mae: 478.9360\n",
      "Epoch 274/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 461.0196 - mae: 461.7102\n",
      "Epoch 275/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 443.6621 - mae: 444.3524\n",
      "Epoch 276/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 436.7794 - mae: 437.4708\n",
      "Epoch 277/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 446.9557 - mae: 447.6466\n",
      "Epoch 278/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 454.5994 - mae: 455.2905\n",
      "Epoch 279/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 476.5367 - mae: 477.2266\n",
      "Epoch 280/5000\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 468.5911 - mae: 469.2813\n",
      "Epoch 281/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 445.9162 - mae: 446.6062\n",
      "Epoch 282/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 438.4420 - mae: 439.1321\n",
      "Epoch 283/5000\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 435.4552 - mae: 436.1454\n",
      "Epoch 284/5000\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 442.3063 - mae: 442.9972\n",
      "Epoch 285/5000\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 470.5981 - mae: 471.2887\n",
      "Epoch 286/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 459.9360 - mae: 460.6276\n",
      "Epoch 287/5000\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 436.8814 - mae: 437.5722\n",
      "Epoch 288/5000\n",
      "62/62 [==============================] - 1s 21ms/step - loss: 443.4529 - mae: 444.1451\n",
      "Epoch 289/5000\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 452.0038 - mae: 452.6943\n",
      "Epoch 290/5000\n",
      "62/62 [==============================] - 1s 10ms/step - loss: 463.1172 - mae: 463.8086\n",
      "Epoch 291/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 425.3657 - mae: 426.0564\n",
      "Epoch 292/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 428.5211 - mae: 429.2111\n",
      "Epoch 293/5000\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 438.1458 - mae: 438.8358\n",
      "Epoch 294/5000\n",
      "31/62 [==============>...............] - ETA: 0s - loss: 395.4811 - mae: 396.1721"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mregressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearlyStopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorplustorch/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorplustorch/lib/python3.10/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorplustorch/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "regressor.fit(x=X_all, y=Y_all, epochs=5000, use_multiprocessing=True, callbacks=[earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf8814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred=regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0f090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for check\n",
    "Y_train_pred=regressor.predict(X_train)\n",
    "r2_score(Y_train, Y_train_pred) #training score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e757ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2=r2_score(Y_test[:-30],y_pred[:-30]) #score/ r^2\n",
    "print(f'r2:{r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ef1c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2_oos\n",
    "def r2_oos(ret, pred):\n",
    "    sum_of_sq_res = np.nansum(np.power((ret-pred), 2))\n",
    "    sum_of_sq_total = np.nansum(np.power(ret, 2))\n",
    "    \n",
    "    return 1-sum_of_sq_res/sum_of_sq_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8969a6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_oos = r2_oos(Y_test[:-30], y_pred[:-30])\n",
    "print(f'r2_oos:{r2_oos}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b87143",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae=mean_absolute_error(Y_test[:-30],y_pred[:-30]) #mae\n",
    "print(f'mae:{mae}')\n",
    "\n",
    "rmse=np.sqrt(mean_squared_error(Y_test[:-30],y_pred[:-30])) #rmse\n",
    "print(f'rmse:{rmse}')\n",
    "\n",
    "mape=mean_absolute_percentage_error(Y_test[:-30],y_pred[:-30]) #mape\n",
    "print(f'mape:{mape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb36caf",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5641115",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df = pd.DataFrame(zip(Y_test,y_pred),columns=['Y_test','y_pred'])\n",
    "pre_df.index = Y_test.index\n",
    "pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4bd2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df['pred_returns'] = pre_df['y_pred'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b94aa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9674c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(zip(Y_test,y_pred),columns=['Y_test','y_pred']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732393ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df.to_csv(\"../result/ANN/btc_NN.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12ff04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kdeconnect-cli -n TAS-AN00 --ping-msg 'Script complete!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b42805c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "efc08374433b8d8e4a9fd8a0a66f7295c7ce37eceb639810a945045512ff181b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
