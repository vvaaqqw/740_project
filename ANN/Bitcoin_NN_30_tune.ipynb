{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5b918ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 14:29:07.644331: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# conda install keras-tuner\n",
    "import keras_tuner as kt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from datetime import datetime\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n",
    "from keras import losses\n",
    "from keras.constraints import maxnorm\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_validate\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from collections import Counter\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import randint\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, accuracy_score, f1_score, classification_report\n",
    "# import pickle\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, roc_auc_score\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ded3682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535a3fac",
   "metadata": {},
   "source": [
    "### Bitcoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1467391e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_247881/2421463472.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  btc['Date'][i]  =  datetime.strptime(btc['Date'][i], '%Y/%m/%d')\n"
     ]
    }
   ],
   "source": [
    "regs = pd.read_csv(\"../Data/train_btc_selected_features.csv\")\n",
    "btc = pd.read_csv(\"../Data/btc_Data.csv\")\n",
    "for i in range(len(btc['Date'])):\n",
    "    btc['Date'][i]  =  datetime.strptime(btc['Date'][i], '%Y/%m/%d')\n",
    "\n",
    "btc = btc.set_index(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d665a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "btcData = btc[regs.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8227b01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_247881/751970969.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  btcData['returns'] = btcData['priceUSD'].pct_change()\n"
     ]
    }
   ],
   "source": [
    "btcData['returns'] = btcData['priceUSD'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0204bfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = btcData.drop(columns=['priceUSD'])\n",
    "Data = Data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a926d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>difficulty14mom</th>\n",
       "      <th>difficulty30mom</th>\n",
       "      <th>difficulty3rsi</th>\n",
       "      <th>difficulty90mom</th>\n",
       "      <th>difficulty90sma</th>\n",
       "      <th>hashrate90std</th>\n",
       "      <th>hashrate90var</th>\n",
       "      <th>median_transaction_fee90rocUSD</th>\n",
       "      <th>median_transaction_feeUSD</th>\n",
       "      <th>mining_profitability</th>\n",
       "      <th>...</th>\n",
       "      <th>price90wmaUSD</th>\n",
       "      <th>sentinusd90emaUSD</th>\n",
       "      <th>sentinusdUSD</th>\n",
       "      <th>top100cap</th>\n",
       "      <th>top100cap14trx</th>\n",
       "      <th>top100cap30rsi</th>\n",
       "      <th>top100cap30trx</th>\n",
       "      <th>top100cap90rsi</th>\n",
       "      <th>top100cap90trx</th>\n",
       "      <th>returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-07-18</th>\n",
       "      <td>1.621380e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.737230e+02</td>\n",
       "      <td>2.055500e+01</td>\n",
       "      <td>8.384389e+08</td>\n",
       "      <td>1.757449e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>401834.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.612000e+03</td>\n",
       "      <td>25.782</td>\n",
       "      <td>0.139</td>\n",
       "      <td>71.191</td>\n",
       "      <td>0.308</td>\n",
       "      <td>79.756</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-19</th>\n",
       "      <td>1.621380e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.737230e+02</td>\n",
       "      <td>2.248600e+01</td>\n",
       "      <td>8.819952e+08</td>\n",
       "      <td>1.944789e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>481473.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.047000e+03</td>\n",
       "      <td>25.685</td>\n",
       "      <td>0.123</td>\n",
       "      <td>68.863</td>\n",
       "      <td>0.300</td>\n",
       "      <td>78.999</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.183196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-20</th>\n",
       "      <td>1.584060e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.735570e+02</td>\n",
       "      <td>2.441400e+01</td>\n",
       "      <td>9.280542e+08</td>\n",
       "      <td>2.153212e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>431831.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.341000e+03</td>\n",
       "      <td>25.602</td>\n",
       "      <td>0.107</td>\n",
       "      <td>66.923</td>\n",
       "      <td>0.292</td>\n",
       "      <td>78.355</td>\n",
       "      <td>0.544</td>\n",
       "      <td>-0.088475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-21</th>\n",
       "      <td>1.580420e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.700790e+02</td>\n",
       "      <td>2.630400e+01</td>\n",
       "      <td>9.761949e+08</td>\n",
       "      <td>2.382391e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>460783.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.122000e+03</td>\n",
       "      <td>25.638</td>\n",
       "      <td>0.092</td>\n",
       "      <td>67.334</td>\n",
       "      <td>0.284</td>\n",
       "      <td>78.431</td>\n",
       "      <td>0.541</td>\n",
       "      <td>-0.020434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-22</th>\n",
       "      <td>1.580420e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.700790e+02</td>\n",
       "      <td>2.819400e+01</td>\n",
       "      <td>1.019474e+09</td>\n",
       "      <td>2.598318e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>334641.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.570000e+03</td>\n",
       "      <td>25.604</td>\n",
       "      <td>0.078</td>\n",
       "      <td>66.526</td>\n",
       "      <td>0.275</td>\n",
       "      <td>78.166</td>\n",
       "      <td>0.537</td>\n",
       "      <td>-0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-24</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.384949e+13</td>\n",
       "      <td>4.612156e+19</td>\n",
       "      <td>5.317997e+38</td>\n",
       "      <td>165.380</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.0577</td>\n",
       "      <td>...</td>\n",
       "      <td>18820.0</td>\n",
       "      <td>28544682257</td>\n",
       "      <td>6.114577e+09</td>\n",
       "      <td>15.236</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>40.118</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>45.140</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.008075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-25</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.394503e+13</td>\n",
       "      <td>4.542600e+19</td>\n",
       "      <td>5.158804e+38</td>\n",
       "      <td>214.464</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.0541</td>\n",
       "      <td>...</td>\n",
       "      <td>18759.0</td>\n",
       "      <td>28076113671</td>\n",
       "      <td>7.224812e+09</td>\n",
       "      <td>15.231</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>39.880</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>45.047</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.006565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-26</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.404058e+13</td>\n",
       "      <td>4.624135e+19</td>\n",
       "      <td>5.345655e+38</td>\n",
       "      <td>255.004</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.0571</td>\n",
       "      <td>...</td>\n",
       "      <td>18701.0</td>\n",
       "      <td>27566714445</td>\n",
       "      <td>4.898449e+09</td>\n",
       "      <td>15.233</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>40.069</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>45.106</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.004001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-27</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.413612e+13</td>\n",
       "      <td>4.576227e+19</td>\n",
       "      <td>5.235464e+38</td>\n",
       "      <td>113.430</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.0618</td>\n",
       "      <td>...</td>\n",
       "      <td>18642.0</td>\n",
       "      <td>27053515674</td>\n",
       "      <td>4.216170e+09</td>\n",
       "      <td>15.259</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>41.833</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>45.656</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.423166e+13</td>\n",
       "      <td>4.552126e+19</td>\n",
       "      <td>5.180463e+38</td>\n",
       "      <td>104.294</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.0592</td>\n",
       "      <td>...</td>\n",
       "      <td>18578.0</td>\n",
       "      <td>26631320968</td>\n",
       "      <td>7.843657e+09</td>\n",
       "      <td>15.219</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>39.965</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>44.957</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.019828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4517 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            difficulty14mom  difficulty30mom  difficulty3rsi  difficulty90mom  \\\n",
       "Date                                                                            \n",
       "2010-07-18     1.621380e+02     1.641620e+02          100.00     1.737230e+02   \n",
       "2010-07-19     1.621380e+02     1.641620e+02          100.00     1.737230e+02   \n",
       "2010-07-20     1.584060e+02     1.641620e+02          100.00     1.735570e+02   \n",
       "2010-07-21     1.580420e+02     1.641620e+02          100.00     1.700790e+02   \n",
       "2010-07-22     1.580420e+02     1.641620e+02          100.00     1.700790e+02   \n",
       "...                     ...              ...             ...              ...   \n",
       "2022-11-24     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-25     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-26     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-27     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-28     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "\n",
       "            difficulty90sma  hashrate90std  hashrate90var  \\\n",
       "Date                                                        \n",
       "2010-07-18     2.055500e+01   8.384389e+08   1.757449e+17   \n",
       "2010-07-19     2.248600e+01   8.819952e+08   1.944789e+17   \n",
       "2010-07-20     2.441400e+01   9.280542e+08   2.153212e+17   \n",
       "2010-07-21     2.630400e+01   9.761949e+08   2.382391e+17   \n",
       "2010-07-22     2.819400e+01   1.019474e+09   2.598318e+17   \n",
       "...                     ...            ...            ...   \n",
       "2022-11-24     3.384949e+13   4.612156e+19   5.317997e+38   \n",
       "2022-11-25     3.394503e+13   4.542600e+19   5.158804e+38   \n",
       "2022-11-26     3.404058e+13   4.624135e+19   5.345655e+38   \n",
       "2022-11-27     3.413612e+13   4.576227e+19   5.235464e+38   \n",
       "2022-11-28     3.423166e+13   4.552126e+19   5.180463e+38   \n",
       "\n",
       "            median_transaction_fee90rocUSD  median_transaction_feeUSD  \\\n",
       "Date                                                                    \n",
       "2010-07-18                           0.000                      0.000   \n",
       "2010-07-19                           0.000                      0.000   \n",
       "2010-07-20                           0.000                      0.000   \n",
       "2010-07-21                           0.000                      0.000   \n",
       "2010-07-22                           0.000                      0.000   \n",
       "...                                    ...                        ...   \n",
       "2022-11-24                         165.380                      0.818   \n",
       "2022-11-25                         214.464                      0.812   \n",
       "2022-11-26                         255.004                      0.809   \n",
       "2022-11-27                         113.430                      0.619   \n",
       "2022-11-28                         104.294                      0.622   \n",
       "\n",
       "            mining_profitability  ...  price90wmaUSD  sentinusd90emaUSD  \\\n",
       "Date                              ...                                     \n",
       "2010-07-18           401834.0000  ...            0.0                  0   \n",
       "2010-07-19           481473.0000  ...            0.0                  0   \n",
       "2010-07-20           431831.0000  ...            0.0                  0   \n",
       "2010-07-21           460783.0000  ...            0.0                  0   \n",
       "2010-07-22           334641.0000  ...            0.0                  0   \n",
       "...                          ...  ...            ...                ...   \n",
       "2022-11-24                0.0577  ...        18820.0        28544682257   \n",
       "2022-11-25                0.0541  ...        18759.0        28076113671   \n",
       "2022-11-26                0.0571  ...        18701.0        27566714445   \n",
       "2022-11-27                0.0618  ...        18642.0        27053515674   \n",
       "2022-11-28                0.0592  ...        18578.0        26631320968   \n",
       "\n",
       "            sentinusdUSD  top100cap  top100cap14trx  top100cap30rsi  \\\n",
       "Date                                                                  \n",
       "2010-07-18  2.612000e+03     25.782           0.139          71.191   \n",
       "2010-07-19  4.047000e+03     25.685           0.123          68.863   \n",
       "2010-07-20  2.341000e+03     25.602           0.107          66.923   \n",
       "2010-07-21  2.122000e+03     25.638           0.092          67.334   \n",
       "2010-07-22  2.570000e+03     25.604           0.078          66.526   \n",
       "...                  ...        ...             ...             ...   \n",
       "2022-11-24  6.114577e+09     15.236          -0.080          40.118   \n",
       "2022-11-25  7.224812e+09     15.231          -0.075          39.880   \n",
       "2022-11-26  4.898449e+09     15.233          -0.070          40.069   \n",
       "2022-11-27  4.216170e+09     15.259          -0.065          41.833   \n",
       "2022-11-28  7.843657e+09     15.219          -0.061          39.965   \n",
       "\n",
       "            top100cap30trx  top100cap90rsi  top100cap90trx   returns  \n",
       "Date                                                                  \n",
       "2010-07-18           0.308          79.756           0.550  0.466667  \n",
       "2010-07-19           0.300          78.999           0.547  0.183196  \n",
       "2010-07-20           0.292          78.355           0.544 -0.088475  \n",
       "2010-07-21           0.284          78.431           0.541 -0.020434  \n",
       "2010-07-22           0.275          78.166           0.537 -0.153846  \n",
       "...                    ...             ...             ...       ...  \n",
       "2022-11-24          -0.042          45.140           0.001  0.008075  \n",
       "2022-11-25          -0.043          45.047           0.001 -0.006565  \n",
       "2022-11-26          -0.043          45.106           0.001  0.004001  \n",
       "2022-11-27          -0.044          45.656           0.000 -0.001147  \n",
       "2022-11-28          -0.044          44.957           0.000 -0.019828  \n",
       "\n",
       "[4517 rows x 32 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5d4f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide X and Y\n",
    "X = Data.iloc[:,0:]\n",
    "#Y = Data['returns']   # 用returns的话就用这一行，然后把下一行comment掉\n",
    "Y = btcData['priceUSD'].shift(-30)[1:] # 反之亦然"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb3275b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>difficulty14mom</th>\n",
       "      <th>difficulty30mom</th>\n",
       "      <th>difficulty3rsi</th>\n",
       "      <th>difficulty90mom</th>\n",
       "      <th>difficulty90sma</th>\n",
       "      <th>hashrate90std</th>\n",
       "      <th>hashrate90var</th>\n",
       "      <th>median_transaction_fee90rocUSD</th>\n",
       "      <th>median_transaction_feeUSD</th>\n",
       "      <th>mining_profitability</th>\n",
       "      <th>...</th>\n",
       "      <th>price90wmaUSD</th>\n",
       "      <th>sentinusd90emaUSD</th>\n",
       "      <th>sentinusdUSD</th>\n",
       "      <th>top100cap</th>\n",
       "      <th>top100cap14trx</th>\n",
       "      <th>top100cap30rsi</th>\n",
       "      <th>top100cap30trx</th>\n",
       "      <th>top100cap90rsi</th>\n",
       "      <th>top100cap90trx</th>\n",
       "      <th>returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-07-18</th>\n",
       "      <td>162.138</td>\n",
       "      <td>164.162</td>\n",
       "      <td>100.0</td>\n",
       "      <td>173.723</td>\n",
       "      <td>20.555</td>\n",
       "      <td>838438881.0</td>\n",
       "      <td>1.757449e+17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>401834.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2612.0</td>\n",
       "      <td>25.782</td>\n",
       "      <td>0.139</td>\n",
       "      <td>71.191</td>\n",
       "      <td>0.308</td>\n",
       "      <td>79.756</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-19</th>\n",
       "      <td>162.138</td>\n",
       "      <td>164.162</td>\n",
       "      <td>100.0</td>\n",
       "      <td>173.723</td>\n",
       "      <td>22.486</td>\n",
       "      <td>881995244.0</td>\n",
       "      <td>1.944789e+17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>481473.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4047.0</td>\n",
       "      <td>25.685</td>\n",
       "      <td>0.123</td>\n",
       "      <td>68.863</td>\n",
       "      <td>0.300</td>\n",
       "      <td>78.999</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.183196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-20</th>\n",
       "      <td>158.406</td>\n",
       "      <td>164.162</td>\n",
       "      <td>100.0</td>\n",
       "      <td>173.557</td>\n",
       "      <td>24.414</td>\n",
       "      <td>928054231.0</td>\n",
       "      <td>2.153212e+17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>431831.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2341.0</td>\n",
       "      <td>25.602</td>\n",
       "      <td>0.107</td>\n",
       "      <td>66.923</td>\n",
       "      <td>0.292</td>\n",
       "      <td>78.355</td>\n",
       "      <td>0.544</td>\n",
       "      <td>-0.088475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            difficulty14mom  difficulty30mom  difficulty3rsi  difficulty90mom  \\\n",
       "Date                                                                            \n",
       "2010-07-18          162.138          164.162           100.0          173.723   \n",
       "2010-07-19          162.138          164.162           100.0          173.723   \n",
       "2010-07-20          158.406          164.162           100.0          173.557   \n",
       "\n",
       "            difficulty90sma  hashrate90std  hashrate90var  \\\n",
       "Date                                                        \n",
       "2010-07-18           20.555    838438881.0   1.757449e+17   \n",
       "2010-07-19           22.486    881995244.0   1.944789e+17   \n",
       "2010-07-20           24.414    928054231.0   2.153212e+17   \n",
       "\n",
       "            median_transaction_fee90rocUSD  median_transaction_feeUSD  \\\n",
       "Date                                                                    \n",
       "2010-07-18                             0.0                        0.0   \n",
       "2010-07-19                             0.0                        0.0   \n",
       "2010-07-20                             0.0                        0.0   \n",
       "\n",
       "            mining_profitability  ...  price90wmaUSD  sentinusd90emaUSD  \\\n",
       "Date                              ...                                     \n",
       "2010-07-18              401834.0  ...            0.0                  0   \n",
       "2010-07-19              481473.0  ...            0.0                  0   \n",
       "2010-07-20              431831.0  ...            0.0                  0   \n",
       "\n",
       "            sentinusdUSD  top100cap  top100cap14trx  top100cap30rsi  \\\n",
       "Date                                                                  \n",
       "2010-07-18        2612.0     25.782           0.139          71.191   \n",
       "2010-07-19        4047.0     25.685           0.123          68.863   \n",
       "2010-07-20        2341.0     25.602           0.107          66.923   \n",
       "\n",
       "            top100cap30trx  top100cap90rsi  top100cap90trx   returns  \n",
       "Date                                                                  \n",
       "2010-07-18           0.308          79.756           0.550  0.466667  \n",
       "2010-07-19           0.300          78.999           0.547  0.183196  \n",
       "2010-07-20           0.292          78.355           0.544 -0.088475  \n",
       "\n",
       "[3 rows x 32 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd589ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into three data sets\n",
    "X_train = X['2016-01-01':'2019-12-31']\n",
    "X_val = X['2020-01-01':'2021-05-31']\n",
    "X_test = X['2021-06-01':'2023-01-01']\n",
    "\n",
    "Y_train = Y['2016-01-01':'2019-12-31']\n",
    "Y_val = Y['2020-01-01':'2021-05-31']\n",
    "Y_test = Y['2021-06-01':'2023-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e5e9fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............ (step 1 of 2) Processing mixmax, total=   0.0s\n",
      "[Pipeline] ............ (step 2 of 2) Processing robust, total=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;mixmax&#x27;, MinMaxScaler()), [&#x27;robust&#x27;, RobustScaler()]],\n",
       "         verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;mixmax&#x27;, MinMaxScaler()), [&#x27;robust&#x27;, RobustScaler()]],\n",
       "         verbose=True)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RobustScaler</label><div class=\"sk-toggleable__content\"><pre>RobustScaler()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('mixmax', MinMaxScaler()), ['robust', RobustScaler()]],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators=[]\n",
    "estimators.append(['mixmax',MinMaxScaler()])\n",
    "estimators.append(['robust',RobustScaler()])\n",
    "scale=Pipeline(estimators,verbose=True)\n",
    "scale.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6fc0859",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=scale.transform(X_train)\n",
    "X_test=scale.transform(X_test)\n",
    "X_val = scale.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8a84d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f6916ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define neural network model\n",
    "shape=X.shape[1]\n",
    "def build_model(hp, initializer='normal', activation='relu', NUM_FEATURES=shape):\n",
    "    # create model\n",
    "    hp_units1 = hp.Int('units1', min_value=32, max_value=512, step=32)\n",
    "    hp_units2 = hp.Int('units2', min_value=32, max_value=512, step=32)\n",
    "    hp_units3 = hp.Int('units3', min_value=32, max_value=512, step=32)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hp_units1, input_shape=(NUM_FEATURES,), kernel_initializer=initializer, activation=activation))\n",
    "    model.add(Dense(hp_units2, activation=activation))\n",
    "    model.add(Dense(hp_units3, activation=activation))\n",
    "    model.add(Dense(1, activation=activation))\n",
    "    # Compile model\n",
    "    adam=keras.optimizers.Adam(lr=lr_schedule(0), amsgrad=True)\n",
    "    #sgd=keras.optimizers.SGD(learning_rate=0.08, momentum=0.9, nesterov=False)\n",
    "    model.compile(loss='logcosh', optimizer=adam, metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "112c8df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9 Complete [01h 12m 32s]\n",
      "val_mae: 904.3529052734375\n",
      "\n",
      "Best val_mae So Far: 904.3529052734375\n",
      "Total elapsed time: 03h 08m 45s\n",
      "\n",
      "Search: Running Trial #10\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "416               |256               |units1\n",
      "288               |416               |units2\n",
      "288               |32                |units3\n",
      "\n",
      "Learning rate:  0.001\n",
      "Epoch 1/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 4473.8027 - mae: 4474.4946 - val_loss: 17747.3750 - val_mae: 17748.0703\n",
      "Epoch 2/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 2416.6565 - mae: 2417.3486 - val_loss: 4177.9028 - val_mae: 4178.5967\n",
      "Epoch 3/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 1008.1214 - mae: 1008.8141 - val_loss: 3154.6248 - val_mae: 3155.3174\n",
      "Epoch 4/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 786.7394 - mae: 787.4313 - val_loss: 2907.4182 - val_mae: 2908.1116\n",
      "Epoch 5/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 653.1328 - mae: 653.8253 - val_loss: 2252.8213 - val_mae: 2253.5137\n",
      "Epoch 6/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 563.6744 - mae: 564.3666 - val_loss: 1921.9747 - val_mae: 1922.6677\n",
      "Epoch 7/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 488.5173 - mae: 489.2098 - val_loss: 1767.6482 - val_mae: 1768.3413\n",
      "Epoch 8/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 438.9599 - mae: 439.6520 - val_loss: 1912.6857 - val_mae: 1913.3777\n",
      "Epoch 9/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 401.3846 - mae: 402.0771 - val_loss: 2140.0271 - val_mae: 2140.7202\n",
      "Epoch 10/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 378.6604 - mae: 379.3518 - val_loss: 1628.4347 - val_mae: 1629.1278\n",
      "Epoch 11/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 360.8244 - mae: 361.5160 - val_loss: 1762.9510 - val_mae: 1763.6440\n",
      "Epoch 12/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 331.4811 - mae: 332.1725 - val_loss: 1526.6975 - val_mae: 1527.3896\n",
      "Epoch 13/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 300.8168 - mae: 301.5084 - val_loss: 1571.1534 - val_mae: 1571.8466\n",
      "Epoch 14/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 274.8858 - mae: 275.5782 - val_loss: 1667.8171 - val_mae: 1668.5104\n",
      "Epoch 15/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 282.4628 - mae: 283.1528 - val_loss: 2071.0413 - val_mae: 2071.7349\n",
      "Epoch 16/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 268.1805 - mae: 268.8726 - val_loss: 1644.6364 - val_mae: 1645.3295\n",
      "Epoch 17/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 277.2382 - mae: 277.9303 - val_loss: 1633.0347 - val_mae: 1633.7267\n",
      "Epoch 18/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 286.7938 - mae: 287.4844 - val_loss: 1940.0759 - val_mae: 1940.7690\n",
      "Epoch 19/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 241.6332 - mae: 242.3242 - val_loss: 1532.8169 - val_mae: 1533.5098\n",
      "Epoch 20/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 257.1263 - mae: 257.8166 - val_loss: 1861.6434 - val_mae: 1862.3367\n",
      "Epoch 21/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 250.2951 - mae: 250.9857 - val_loss: 1694.2708 - val_mae: 1694.9625\n",
      "Epoch 22/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 228.5308 - mae: 229.2189 - val_loss: 1743.1360 - val_mae: 1743.8284\n",
      "Epoch 23/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 232.9455 - mae: 233.6363 - val_loss: 1934.3950 - val_mae: 1935.0883\n",
      "Epoch 24/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 217.9616 - mae: 218.6519 - val_loss: 1616.5380 - val_mae: 1617.2311\n",
      "Epoch 25/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 197.7842 - mae: 198.4747 - val_loss: 1798.8158 - val_mae: 1799.5089\n",
      "Epoch 26/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 191.7720 - mae: 192.4630 - val_loss: 1755.6216 - val_mae: 1756.3146\n",
      "Epoch 27/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 204.4488 - mae: 205.1386 - val_loss: 1414.4343 - val_mae: 1415.1255\n",
      "Epoch 28/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 193.9514 - mae: 194.6417 - val_loss: 1609.1327 - val_mae: 1609.8254\n",
      "Epoch 29/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 213.7152 - mae: 214.4043 - val_loss: 1788.6267 - val_mae: 1789.3199\n",
      "Epoch 30/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 214.2164 - mae: 214.9057 - val_loss: 1320.1385 - val_mae: 1320.8317\n",
      "Epoch 31/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 195.7277 - mae: 196.4156 - val_loss: 1338.0709 - val_mae: 1338.7634\n",
      "Epoch 32/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 198.7476 - mae: 199.4371 - val_loss: 1618.3174 - val_mae: 1619.0104\n",
      "Epoch 33/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 184.8904 - mae: 185.5799 - val_loss: 1412.3137 - val_mae: 1413.0063\n",
      "Epoch 34/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 183.8666 - mae: 184.5567 - val_loss: 1488.5818 - val_mae: 1489.2744\n",
      "Epoch 35/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 182.0840 - mae: 182.7732 - val_loss: 1382.8375 - val_mae: 1383.5309\n",
      "Epoch 36/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 194.1180 - mae: 194.8085 - val_loss: 1594.9692 - val_mae: 1595.6622\n",
      "Epoch 37/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 189.7863 - mae: 190.4772 - val_loss: 1731.2455 - val_mae: 1731.9382\n",
      "Epoch 38/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 201.2472 - mae: 201.9385 - val_loss: 1477.9214 - val_mae: 1478.6147\n",
      "Epoch 39/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 210.8247 - mae: 211.5155 - val_loss: 1193.7559 - val_mae: 1194.4486\n",
      "Epoch 40/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 186.4733 - mae: 187.1614 - val_loss: 1254.8042 - val_mae: 1255.4973\n",
      "Epoch 41/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 185.1629 - mae: 185.8520 - val_loss: 1379.8011 - val_mae: 1380.4938\n",
      "Epoch 42/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 209.6635 - mae: 210.3541 - val_loss: 1606.4601 - val_mae: 1607.1532\n",
      "Epoch 43/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 212.1548 - mae: 212.8436 - val_loss: 1301.6465 - val_mae: 1302.3396\n",
      "Epoch 44/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 173.8318 - mae: 174.5204 - val_loss: 1385.9714 - val_mae: 1386.6644\n",
      "Epoch 45/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 168.9437 - mae: 169.6332 - val_loss: 1242.9875 - val_mae: 1243.6808\n",
      "Epoch 46/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 190.3287 - mae: 191.0171 - val_loss: 1190.1129 - val_mae: 1190.8060\n",
      "Epoch 47/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 196.3298 - mae: 197.0195 - val_loss: 1450.4879 - val_mae: 1451.1813\n",
      "Epoch 48/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 169.7151 - mae: 170.4057 - val_loss: 1416.8076 - val_mae: 1417.5007\n",
      "Epoch 49/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 164.6016 - mae: 165.2896 - val_loss: 1276.1785 - val_mae: 1276.8716\n",
      "Epoch 50/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 176.7036 - mae: 177.3914 - val_loss: 1371.7258 - val_mae: 1372.4182\n",
      "Epoch 51/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 186.7711 - mae: 187.4599 - val_loss: 1322.2728 - val_mae: 1322.9659\n",
      "Epoch 52/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 196.4355 - mae: 197.1219 - val_loss: 1437.9081 - val_mae: 1438.6011\n",
      "Epoch 53/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 158.7325 - mae: 159.4228 - val_loss: 1195.2656 - val_mae: 1195.9572\n",
      "Epoch 54/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 181.5209 - mae: 182.2101 - val_loss: 1320.7462 - val_mae: 1321.4395\n",
      "Epoch 55/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 188.8860 - mae: 189.5763 - val_loss: 1539.9965 - val_mae: 1540.6896\n",
      "Epoch 56/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 175.4030 - mae: 176.0914 - val_loss: 1155.2765 - val_mae: 1155.9697\n",
      "Epoch 57/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 188.3174 - mae: 189.0054 - val_loss: 1131.3866 - val_mae: 1132.0787\n",
      "Epoch 58/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 179.0918 - mae: 179.7813 - val_loss: 1100.5980 - val_mae: 1101.2913\n",
      "Epoch 59/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 172.0965 - mae: 172.7845 - val_loss: 1271.5386 - val_mae: 1272.2316\n",
      "Epoch 60/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 162.9178 - mae: 163.6070 - val_loss: 1166.3781 - val_mae: 1167.0706\n",
      "Epoch 61/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 162.2594 - mae: 162.9473 - val_loss: 1263.6124 - val_mae: 1264.3057\n",
      "Epoch 62/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 171.3464 - mae: 172.0362 - val_loss: 1590.0211 - val_mae: 1590.7144\n",
      "Epoch 63/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 171.0933 - mae: 171.7824 - val_loss: 1189.6602 - val_mae: 1190.3517\n",
      "Epoch 64/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 165.6634 - mae: 166.3524 - val_loss: 1160.4424 - val_mae: 1161.1357\n",
      "Epoch 65/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 166.1472 - mae: 166.8356 - val_loss: 1224.8076 - val_mae: 1225.5002\n",
      "Epoch 66/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 189.1190 - mae: 189.8087 - val_loss: 1253.6460 - val_mae: 1254.3392\n",
      "Epoch 67/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 157.5330 - mae: 158.2194 - val_loss: 1283.4071 - val_mae: 1284.1002\n",
      "Epoch 68/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 165.5588 - mae: 166.2489 - val_loss: 1147.0452 - val_mae: 1147.7374\n",
      "Epoch 69/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 163.8831 - mae: 164.5730 - val_loss: 1272.2986 - val_mae: 1272.9916\n",
      "Epoch 70/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 187.8251 - mae: 188.5120 - val_loss: 1025.4750 - val_mae: 1026.1683\n",
      "Epoch 71/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 161.4748 - mae: 162.1639 - val_loss: 1155.7080 - val_mae: 1156.4006\n",
      "Epoch 72/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 194.1741 - mae: 194.8618 - val_loss: 1322.8396 - val_mae: 1323.5316\n",
      "Epoch 73/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 160.7370 - mae: 161.4255 - val_loss: 1353.2139 - val_mae: 1353.9070\n",
      "Epoch 74/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 187.4741 - mae: 188.1620 - val_loss: 1144.8052 - val_mae: 1145.4984\n",
      "Epoch 75/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 173.9104 - mae: 174.6000 - val_loss: 1355.9268 - val_mae: 1356.6199\n",
      "Epoch 76/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 149.7710 - mae: 150.4615 - val_loss: 1410.7661 - val_mae: 1411.4592\n",
      "Epoch 77/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 157.3770 - mae: 158.0666 - val_loss: 1422.6564 - val_mae: 1423.3496\n",
      "Epoch 78/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 234.1379 - mae: 234.8268 - val_loss: 1336.7468 - val_mae: 1337.4401\n",
      "Epoch 79/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 161.5236 - mae: 162.2114 - val_loss: 1322.7119 - val_mae: 1323.4054\n",
      "Epoch 80/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 147.7302 - mae: 148.4182 - val_loss: 1303.6083 - val_mae: 1304.3015\n",
      "Epoch 81/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 156.7485 - mae: 157.4370 - val_loss: 1544.2468 - val_mae: 1544.9401\n",
      "Epoch 82/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 152.4594 - mae: 153.1486 - val_loss: 1147.4167 - val_mae: 1148.1091\n",
      "Epoch 83/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 153.0983 - mae: 153.7869 - val_loss: 1477.3534 - val_mae: 1478.0457\n",
      "Epoch 84/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 157.0576 - mae: 157.7460 - val_loss: 1619.9089 - val_mae: 1620.6021\n",
      "Epoch 85/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 161.0573 - mae: 161.7466 - val_loss: 1190.2495 - val_mae: 1190.9429\n",
      "Epoch 86/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 149.7489 - mae: 150.4371 - val_loss: 1149.1427 - val_mae: 1149.8335\n",
      "Epoch 87/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 149.0835 - mae: 149.7712 - val_loss: 1096.0109 - val_mae: 1096.7041\n",
      "Epoch 88/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 150.7568 - mae: 151.4460 - val_loss: 1245.3757 - val_mae: 1246.0687\n",
      "Epoch 89/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 149.2240 - mae: 149.9123 - val_loss: 1216.2838 - val_mae: 1216.9771\n",
      "Epoch 90/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 145.0824 - mae: 145.7700 - val_loss: 1163.2792 - val_mae: 1163.9723\n",
      "Epoch 91/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 144.7243 - mae: 145.4140 - val_loss: 1305.9075 - val_mae: 1306.6008\n",
      "Epoch 92/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 152.6973 - mae: 153.3826 - val_loss: 1229.5028 - val_mae: 1230.1952\n",
      "Epoch 93/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 164.0937 - mae: 164.7823 - val_loss: 1482.9020 - val_mae: 1483.5950\n",
      "Epoch 94/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 150.7628 - mae: 151.4504 - val_loss: 1343.3004 - val_mae: 1343.9929\n",
      "Epoch 95/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 147.2230 - mae: 147.9120 - val_loss: 1266.2197 - val_mae: 1266.9126\n",
      "Epoch 96/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 153.7462 - mae: 154.4345 - val_loss: 1287.5068 - val_mae: 1288.1987\n",
      "Epoch 97/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 158.8333 - mae: 159.5217 - val_loss: 1178.7794 - val_mae: 1179.4719\n",
      "Epoch 98/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 142.4775 - mae: 143.1650 - val_loss: 1263.7234 - val_mae: 1264.4159\n",
      "Epoch 99/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 146.7063 - mae: 147.3951 - val_loss: 1376.6982 - val_mae: 1377.3911\n",
      "Epoch 100/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 152.4694 - mae: 153.1585 - val_loss: 1424.8019 - val_mae: 1425.4951\n",
      "Epoch 101/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 154.1019 - mae: 154.7885 - val_loss: 1237.0332 - val_mae: 1237.7263\n",
      "Epoch 102/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 142.6793 - mae: 143.3672 - val_loss: 1326.8654 - val_mae: 1327.5583\n",
      "Epoch 103/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 140.6066 - mae: 141.2939 - val_loss: 1150.5023 - val_mae: 1151.1946\n",
      "Epoch 104/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 140.8480 - mae: 141.5326 - val_loss: 1316.6368 - val_mae: 1317.3300\n",
      "Epoch 105/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 163.3636 - mae: 164.0513 - val_loss: 1189.3613 - val_mae: 1190.0546\n",
      "Epoch 106/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 153.2773 - mae: 153.9651 - val_loss: 1318.2507 - val_mae: 1318.9434\n",
      "Epoch 107/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 177.3652 - mae: 178.0553 - val_loss: 1294.9708 - val_mae: 1295.6633\n",
      "Epoch 108/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 152.0063 - mae: 152.6929 - val_loss: 1293.3026 - val_mae: 1293.9955\n",
      "Epoch 109/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 158.6360 - mae: 159.3235 - val_loss: 1187.1816 - val_mae: 1187.8738\n",
      "Epoch 110/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 150.4361 - mae: 151.1251 - val_loss: 1209.3431 - val_mae: 1210.0360\n",
      "Epoch 111/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 142.6343 - mae: 143.3204 - val_loss: 1210.5306 - val_mae: 1211.2238\n",
      "Epoch 112/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 136.3472 - mae: 137.0338 - val_loss: 1320.9069 - val_mae: 1321.5986\n",
      "Epoch 113/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 144.0617 - mae: 144.7479 - val_loss: 1269.6666 - val_mae: 1270.3582\n",
      "Epoch 114/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 154.9315 - mae: 155.6200 - val_loss: 1521.5402 - val_mae: 1522.2330\n",
      "Epoch 115/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 159.9997 - mae: 160.6879 - val_loss: 1237.9084 - val_mae: 1238.6019\n",
      "Epoch 116/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 177.6563 - mae: 178.3470 - val_loss: 1622.2540 - val_mae: 1622.9469\n",
      "Epoch 117/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 191.9810 - mae: 192.6699 - val_loss: 1056.9070 - val_mae: 1057.5994\n",
      "Epoch 118/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 161.1071 - mae: 161.7956 - val_loss: 1249.3218 - val_mae: 1250.0149\n",
      "Epoch 119/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 144.6490 - mae: 145.3354 - val_loss: 1101.7045 - val_mae: 1102.3967\n",
      "Epoch 120/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 150.0967 - mae: 150.7834 - val_loss: 1161.2996 - val_mae: 1161.9928\n",
      "Epoch 121/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 146.8090 - mae: 147.4980 - val_loss: 1299.0865 - val_mae: 1299.7797\n",
      "Epoch 122/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 160.0356 - mae: 160.7246 - val_loss: 1303.0323 - val_mae: 1303.7255\n",
      "Epoch 123/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 143.5140 - mae: 144.2033 - val_loss: 1321.9027 - val_mae: 1322.5958\n",
      "Epoch 124/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 145.9799 - mae: 146.6678 - val_loss: 1068.9565 - val_mae: 1069.6493\n",
      "Epoch 125/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 146.6654 - mae: 147.3534 - val_loss: 1329.1746 - val_mae: 1329.8679\n",
      "Epoch 126/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 146.9274 - mae: 147.6131 - val_loss: 1128.7102 - val_mae: 1129.4028\n",
      "Epoch 127/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 152.3733 - mae: 153.0606 - val_loss: 1278.6212 - val_mae: 1279.3138\n",
      "Epoch 128/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 159.9149 - mae: 160.6027 - val_loss: 1441.1458 - val_mae: 1441.8389\n",
      "Epoch 129/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 166.6094 - mae: 167.2997 - val_loss: 1283.1802 - val_mae: 1283.8733\n",
      "Epoch 130/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 144.0269 - mae: 144.7141 - val_loss: 1150.7664 - val_mae: 1151.4592\n",
      "Epoch 131/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 157.4572 - mae: 158.1457 - val_loss: 1238.3867 - val_mae: 1239.0800\n",
      "Epoch 132/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 139.0438 - mae: 139.7328 - val_loss: 1068.8484 - val_mae: 1069.5398\n",
      "Epoch 133/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 148.2038 - mae: 148.8910 - val_loss: 1446.5471 - val_mae: 1447.2406\n",
      "Epoch 134/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 141.3871 - mae: 142.0748 - val_loss: 1234.4873 - val_mae: 1235.1804\n",
      "Epoch 135/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 147.0973 - mae: 147.7858 - val_loss: 1106.3079 - val_mae: 1107.0010\n",
      "Epoch 136/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 155.8699 - mae: 156.5568 - val_loss: 1309.5332 - val_mae: 1310.2263\n",
      "Epoch 137/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 134.9990 - mae: 135.6850 - val_loss: 1399.0189 - val_mae: 1399.7118\n",
      "Epoch 138/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 145.6843 - mae: 146.3720 - val_loss: 1576.1171 - val_mae: 1576.8102\n",
      "Epoch 139/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 144.3954 - mae: 145.0825 - val_loss: 1313.8761 - val_mae: 1314.5693\n",
      "Epoch 140/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 141.6114 - mae: 142.2981 - val_loss: 1422.0038 - val_mae: 1422.6958\n",
      "Epoch 141/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 154.6267 - mae: 155.3154 - val_loss: 1106.4368 - val_mae: 1107.1301\n",
      "Epoch 142/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 166.1988 - mae: 166.8874 - val_loss: 1302.9680 - val_mae: 1303.6609\n",
      "Epoch 143/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 145.6707 - mae: 146.3572 - val_loss: 1329.0885 - val_mae: 1329.7811\n",
      "Epoch 144/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 136.8468 - mae: 137.5353 - val_loss: 1148.4121 - val_mae: 1149.1045\n",
      "Epoch 145/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 150.6803 - mae: 151.3677 - val_loss: 1696.7058 - val_mae: 1697.3988\n",
      "Epoch 146/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 147.5882 - mae: 148.2764 - val_loss: 1163.6014 - val_mae: 1164.2941\n",
      "Epoch 147/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 158.4684 - mae: 159.1569 - val_loss: 1277.9634 - val_mae: 1278.6564\n",
      "Epoch 148/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 156.7528 - mae: 157.4387 - val_loss: 1303.8749 - val_mae: 1304.5670\n",
      "Epoch 149/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 139.3376 - mae: 140.0248 - val_loss: 1140.0950 - val_mae: 1140.7882\n",
      "Epoch 150/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 140.7203 - mae: 141.4104 - val_loss: 1071.7812 - val_mae: 1072.4744\n",
      "Epoch 151/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 158.3670 - mae: 159.0564 - val_loss: 1410.5668 - val_mae: 1411.2595\n",
      "Epoch 152/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 141.8809 - mae: 142.5686 - val_loss: 1245.3348 - val_mae: 1246.0280\n",
      "Epoch 153/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 142.2226 - mae: 142.9086 - val_loss: 1569.0190 - val_mae: 1569.7115\n",
      "Epoch 154/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 155.7990 - mae: 156.4890 - val_loss: 1178.9867 - val_mae: 1179.6799\n",
      "Epoch 155/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 153.8938 - mae: 154.5809 - val_loss: 1149.8687 - val_mae: 1150.5618\n",
      "Epoch 156/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 134.3625 - mae: 135.0483 - val_loss: 1421.8816 - val_mae: 1422.5747\n",
      "Epoch 157/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 142.2430 - mae: 142.9303 - val_loss: 1241.6049 - val_mae: 1242.2980\n",
      "Epoch 158/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 147.1246 - mae: 147.8095 - val_loss: 1460.6089 - val_mae: 1461.3020\n",
      "Epoch 159/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 152.1066 - mae: 152.7947 - val_loss: 1240.8303 - val_mae: 1241.5226\n",
      "Epoch 160/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 135.2823 - mae: 135.9691 - val_loss: 1350.2328 - val_mae: 1350.9249\n",
      "Epoch 161/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 155.4465 - mae: 156.1360 - val_loss: 1259.2483 - val_mae: 1259.9410\n",
      "Epoch 162/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 138.9510 - mae: 139.6354 - val_loss: 1593.1616 - val_mae: 1593.8550\n",
      "Epoch 163/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 150.1185 - mae: 150.8076 - val_loss: 1146.2605 - val_mae: 1146.9535\n",
      "Epoch 164/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 143.4924 - mae: 144.1834 - val_loss: 1295.9825 - val_mae: 1296.6755\n",
      "Epoch 165/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 137.4906 - mae: 138.1783 - val_loss: 1295.8186 - val_mae: 1296.5110\n",
      "Epoch 166/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 150.1748 - mae: 150.8625 - val_loss: 1103.4976 - val_mae: 1104.1909\n",
      "Epoch 167/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 143.4912 - mae: 144.1782 - val_loss: 1232.6372 - val_mae: 1233.3304\n",
      "Epoch 168/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 140.5900 - mae: 141.2782 - val_loss: 1351.5044 - val_mae: 1352.1975\n",
      "Epoch 169/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 146.1592 - mae: 146.8460 - val_loss: 1346.0411 - val_mae: 1346.7344\n",
      "Epoch 170/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 144.7255 - mae: 145.4140 - val_loss: 1275.9818 - val_mae: 1276.6743\n",
      "Epoch 171/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 137.3786 - mae: 138.0651 - val_loss: 1288.1501 - val_mae: 1288.8433\n",
      "Epoch 172/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 160.5801 - mae: 161.2702 - val_loss: 1139.3888 - val_mae: 1140.0812\n",
      "Epoch 173/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 147.7854 - mae: 148.4736 - val_loss: 1194.9362 - val_mae: 1195.6293\n",
      "Epoch 174/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 137.7077 - mae: 138.3952 - val_loss: 1178.2454 - val_mae: 1178.9387\n",
      "Epoch 175/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 145.1443 - mae: 145.8330 - val_loss: 1113.2645 - val_mae: 1113.9572\n",
      "Epoch 176/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 134.0931 - mae: 134.7823 - val_loss: 1189.8552 - val_mae: 1190.5476\n",
      "Epoch 177/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 138.0077 - mae: 138.6946 - val_loss: 1360.8424 - val_mae: 1361.5349\n",
      "Epoch 178/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 150.7454 - mae: 151.4342 - val_loss: 1335.4227 - val_mae: 1336.1145\n",
      "Epoch 179/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 136.6408 - mae: 137.3281 - val_loss: 1106.8051 - val_mae: 1107.4968\n",
      "Epoch 180/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 138.8098 - mae: 139.4960 - val_loss: 1106.1586 - val_mae: 1106.8518\n",
      "Epoch 181/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 149.1655 - mae: 149.8547 - val_loss: 1688.8176 - val_mae: 1689.5109\n",
      "Epoch 182/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 161.7125 - mae: 162.4005 - val_loss: 1171.2188 - val_mae: 1171.9120\n",
      "Epoch 183/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 154.1461 - mae: 154.8349 - val_loss: 1125.3175 - val_mae: 1126.0107\n",
      "Epoch 184/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 149.8230 - mae: 150.5116 - val_loss: 1516.5259 - val_mae: 1517.2188\n",
      "Epoch 185/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 132.9549 - mae: 133.6427 - val_loss: 1285.8691 - val_mae: 1286.5626\n",
      "Epoch 186/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 128.2885 - mae: 128.9749 - val_loss: 1508.0509 - val_mae: 1508.7434\n",
      "Epoch 187/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 129.0697 - mae: 129.7553 - val_loss: 1218.5172 - val_mae: 1219.2101\n",
      "Epoch 188/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 132.8365 - mae: 133.5215 - val_loss: 1393.8315 - val_mae: 1394.5247\n",
      "Epoch 189/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 148.0375 - mae: 148.7260 - val_loss: 1354.9695 - val_mae: 1355.6626\n",
      "Epoch 190/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 134.8793 - mae: 135.5674 - val_loss: 1139.2407 - val_mae: 1139.9330\n",
      "Epoch 191/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 138.0566 - mae: 138.7432 - val_loss: 1262.2513 - val_mae: 1262.9445\n",
      "Epoch 192/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 132.4148 - mae: 133.1009 - val_loss: 1430.9220 - val_mae: 1431.6151\n",
      "Epoch 193/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 134.3117 - mae: 134.9990 - val_loss: 1258.7811 - val_mae: 1259.4744\n",
      "Epoch 194/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 132.0555 - mae: 132.7427 - val_loss: 1278.7976 - val_mae: 1279.4908\n",
      "Epoch 195/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 139.4193 - mae: 140.1072 - val_loss: 1147.2578 - val_mae: 1147.9510\n",
      "Epoch 196/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 146.4941 - mae: 147.1819 - val_loss: 1260.8792 - val_mae: 1261.5717\n",
      "Epoch 197/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 134.3716 - mae: 135.0606 - val_loss: 1207.7007 - val_mae: 1208.3938\n",
      "Epoch 198/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 128.8249 - mae: 129.5113 - val_loss: 1425.6907 - val_mae: 1426.3837\n",
      "Epoch 199/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 145.8966 - mae: 146.5845 - val_loss: 1381.6274 - val_mae: 1382.3204\n",
      "Epoch 200/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 135.3736 - mae: 136.0605 - val_loss: 1157.4446 - val_mae: 1158.1366\n",
      "Epoch 201/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 134.4466 - mae: 135.1339 - val_loss: 1281.8013 - val_mae: 1282.4939\n",
      "Epoch 202/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 126.9487 - mae: 127.6341 - val_loss: 1260.5631 - val_mae: 1261.2554\n",
      "Epoch 203/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 128.5238 - mae: 129.2117 - val_loss: 1288.0846 - val_mae: 1288.7777\n",
      "Epoch 204/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 129.1800 - mae: 129.8649 - val_loss: 1347.5309 - val_mae: 1348.2233\n",
      "Epoch 205/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 137.9113 - mae: 138.5978 - val_loss: 1469.1949 - val_mae: 1469.8867\n",
      "Epoch 206/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 131.8348 - mae: 132.5224 - val_loss: 1499.3748 - val_mae: 1500.0679\n",
      "Epoch 207/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 139.5179 - mae: 140.2038 - val_loss: 1311.8346 - val_mae: 1312.5278\n",
      "Epoch 208/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 142.2984 - mae: 142.9848 - val_loss: 1379.6326 - val_mae: 1380.3259\n",
      "Epoch 209/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 131.0976 - mae: 131.7853 - val_loss: 1341.9153 - val_mae: 1342.6075\n",
      "Epoch 210/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 129.1398 - mae: 129.8263 - val_loss: 1514.0632 - val_mae: 1514.7562\n",
      "Epoch 211/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 139.6622 - mae: 140.3485 - val_loss: 1165.1681 - val_mae: 1165.8607\n",
      "Epoch 212/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 122.5351 - mae: 123.2204 - val_loss: 1318.6788 - val_mae: 1319.3721\n",
      "Epoch 213/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 129.9411 - mae: 130.6283 - val_loss: 1263.2816 - val_mae: 1263.9747\n",
      "Epoch 214/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 133.2626 - mae: 133.9510 - val_loss: 1222.0459 - val_mae: 1222.7390\n",
      "Epoch 215/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 143.0185 - mae: 143.7065 - val_loss: 1558.3380 - val_mae: 1559.0302\n",
      "Epoch 216/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 132.7558 - mae: 133.4420 - val_loss: 1083.4319 - val_mae: 1084.1251\n",
      "Epoch 217/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 133.3813 - mae: 134.0689 - val_loss: 1168.4106 - val_mae: 1169.1034\n",
      "Epoch 218/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 129.4160 - mae: 130.1030 - val_loss: 1163.3840 - val_mae: 1164.0765\n",
      "Epoch 219/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 142.5755 - mae: 143.2626 - val_loss: 1157.0713 - val_mae: 1157.7640\n",
      "Epoch 220/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 148.9922 - mae: 149.6775 - val_loss: 1240.0734 - val_mae: 1240.7662\n",
      "Epoch 221/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 142.9805 - mae: 143.6680 - val_loss: 1170.0203 - val_mae: 1170.7135\n",
      "Epoch 222/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 125.8816 - mae: 126.5697 - val_loss: 1182.2482 - val_mae: 1182.9415\n",
      "Epoch 223/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 131.9586 - mae: 132.6450 - val_loss: 1307.6183 - val_mae: 1308.3112\n",
      "Epoch 224/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 129.4852 - mae: 130.1718 - val_loss: 1601.1736 - val_mae: 1601.8667\n",
      "Epoch 225/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 134.3886 - mae: 135.0769 - val_loss: 1197.0929 - val_mae: 1197.7843\n",
      "Epoch 226/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 130.4458 - mae: 131.1328 - val_loss: 1153.3142 - val_mae: 1154.0067\n",
      "Epoch 227/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 124.5101 - mae: 125.1959 - val_loss: 1304.9395 - val_mae: 1305.6327\n",
      "Epoch 228/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 127.4003 - mae: 128.0881 - val_loss: 1107.4346 - val_mae: 1108.1270\n",
      "Epoch 229/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 144.4084 - mae: 145.0958 - val_loss: 1053.7064 - val_mae: 1054.3993\n",
      "Epoch 230/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 137.1625 - mae: 137.8492 - val_loss: 1472.7012 - val_mae: 1473.3942\n",
      "Epoch 231/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 133.2456 - mae: 133.9322 - val_loss: 1346.7589 - val_mae: 1347.4523\n",
      "Epoch 232/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 132.9255 - mae: 133.6110 - val_loss: 1346.8943 - val_mae: 1347.5874\n",
      "Epoch 233/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 132.9593 - mae: 133.6453 - val_loss: 1030.6001 - val_mae: 1031.2933\n",
      "Epoch 234/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 138.0923 - mae: 138.7798 - val_loss: 1499.2371 - val_mae: 1499.9303\n",
      "Epoch 235/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 127.2522 - mae: 127.9408 - val_loss: 1180.4308 - val_mae: 1181.1238\n",
      "Epoch 236/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 144.8147 - mae: 145.5014 - val_loss: 1372.1534 - val_mae: 1372.8467\n",
      "Epoch 237/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 130.6729 - mae: 131.3617 - val_loss: 1299.5443 - val_mae: 1300.2367\n",
      "Epoch 238/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 131.7157 - mae: 132.4010 - val_loss: 1159.9927 - val_mae: 1160.6848\n",
      "Epoch 239/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 131.2172 - mae: 131.9033 - val_loss: 1239.7468 - val_mae: 1240.4393\n",
      "Epoch 240/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 132.0663 - mae: 132.7533 - val_loss: 1456.0608 - val_mae: 1456.7538\n",
      "Epoch 241/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 130.3176 - mae: 131.0051 - val_loss: 1449.2998 - val_mae: 1449.9929\n",
      "Epoch 242/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 138.8816 - mae: 139.5680 - val_loss: 1070.8358 - val_mae: 1071.5291\n",
      "Epoch 243/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 134.2207 - mae: 134.9066 - val_loss: 1117.5726 - val_mae: 1118.2657\n",
      "Epoch 244/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 126.2708 - mae: 126.9567 - val_loss: 1370.4246 - val_mae: 1371.1179\n",
      "Epoch 245/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 129.5037 - mae: 130.1920 - val_loss: 1233.8623 - val_mae: 1234.5549\n",
      "Epoch 246/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 130.0941 - mae: 130.7807 - val_loss: 1422.5605 - val_mae: 1423.2534\n",
      "Epoch 247/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 158.1471 - mae: 158.8346 - val_loss: 1433.5282 - val_mae: 1434.2213\n",
      "Epoch 248/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 135.6893 - mae: 136.3778 - val_loss: 1112.4055 - val_mae: 1113.0985\n",
      "Epoch 249/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 126.8095 - mae: 127.4961 - val_loss: 1461.6598 - val_mae: 1462.3531\n",
      "Epoch 250/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 123.0956 - mae: 123.7807 - val_loss: 1456.8535 - val_mae: 1457.5466\n",
      "Epoch 251/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 130.7998 - mae: 131.4849 - val_loss: 1418.3899 - val_mae: 1419.0829\n",
      "Epoch 252/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 127.5653 - mae: 128.2522 - val_loss: 1178.1606 - val_mae: 1178.8536\n",
      "Epoch 253/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 133.2547 - mae: 133.9412 - val_loss: 1309.4608 - val_mae: 1310.1532\n",
      "Epoch 254/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 141.3870 - mae: 142.0761 - val_loss: 1118.8564 - val_mae: 1119.5496\n",
      "Epoch 255/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 152.4966 - mae: 153.1835 - val_loss: 1573.8114 - val_mae: 1574.5037\n",
      "Epoch 256/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 130.4649 - mae: 131.1531 - val_loss: 1227.1707 - val_mae: 1227.8639\n",
      "Epoch 257/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 127.1610 - mae: 127.8462 - val_loss: 1374.4221 - val_mae: 1375.1155\n",
      "Epoch 258/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 123.3978 - mae: 124.0854 - val_loss: 1173.1078 - val_mae: 1173.7996\n",
      "Epoch 259/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 128.0237 - mae: 128.7087 - val_loss: 1172.5247 - val_mae: 1173.2168\n",
      "Epoch 260/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 125.1597 - mae: 125.8468 - val_loss: 1118.6371 - val_mae: 1119.3291\n",
      "Epoch 261/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 123.9721 - mae: 124.6585 - val_loss: 1300.0294 - val_mae: 1300.7220\n",
      "Epoch 262/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 119.0810 - mae: 119.7696 - val_loss: 1518.7035 - val_mae: 1519.3966\n",
      "Epoch 263/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 125.2604 - mae: 125.9452 - val_loss: 1197.3604 - val_mae: 1198.0536\n",
      "Epoch 264/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 147.4170 - mae: 148.1041 - val_loss: 1483.2916 - val_mae: 1483.9847\n",
      "Epoch 265/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 132.6561 - mae: 133.3448 - val_loss: 1338.3867 - val_mae: 1339.0795\n",
      "Epoch 266/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 129.3638 - mae: 130.0512 - val_loss: 1010.3561 - val_mae: 1011.0493\n",
      "Epoch 267/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 141.0183 - mae: 141.7058 - val_loss: 1284.1486 - val_mae: 1284.8417\n",
      "Epoch 268/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 134.9517 - mae: 135.6353 - val_loss: 1087.9410 - val_mae: 1088.6334\n",
      "Epoch 269/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 136.3591 - mae: 137.0462 - val_loss: 1382.9657 - val_mae: 1383.6589\n",
      "Epoch 270/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 129.6633 - mae: 130.3494 - val_loss: 1285.3176 - val_mae: 1286.0100\n",
      "Epoch 271/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 130.0984 - mae: 130.7850 - val_loss: 1330.7639 - val_mae: 1331.4570\n",
      "Epoch 272/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 134.7287 - mae: 135.4159 - val_loss: 1050.7738 - val_mae: 1051.4667\n",
      "Epoch 273/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 137.8979 - mae: 138.5837 - val_loss: 1318.2878 - val_mae: 1318.9808\n",
      "Epoch 274/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 143.0432 - mae: 143.7322 - val_loss: 998.1234 - val_mae: 998.8145\n",
      "Epoch 275/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 148.0690 - mae: 148.7535 - val_loss: 1360.2795 - val_mae: 1360.9723\n",
      "Epoch 276/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 126.8389 - mae: 127.5263 - val_loss: 1628.9653 - val_mae: 1629.6583\n",
      "Epoch 277/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 135.6251 - mae: 136.3105 - val_loss: 1365.9342 - val_mae: 1366.6274\n",
      "Epoch 278/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 136.9690 - mae: 137.6563 - val_loss: 1150.7644 - val_mae: 1151.4572\n",
      "Epoch 279/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 125.8485 - mae: 126.5347 - val_loss: 1445.7657 - val_mae: 1446.4586\n",
      "Epoch 280/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 130.6619 - mae: 131.3480 - val_loss: 1257.1600 - val_mae: 1257.8524\n",
      "Epoch 281/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 121.9762 - mae: 122.6606 - val_loss: 1210.2649 - val_mae: 1210.9572\n",
      "Epoch 282/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 124.2659 - mae: 124.9499 - val_loss: 1183.0029 - val_mae: 1183.6956\n",
      "Epoch 283/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 135.1050 - mae: 135.7937 - val_loss: 1255.3901 - val_mae: 1256.0820\n",
      "Epoch 284/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 124.3899 - mae: 125.0765 - val_loss: 1314.0037 - val_mae: 1314.6967\n",
      "Epoch 285/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 121.1610 - mae: 121.8466 - val_loss: 1146.7512 - val_mae: 1147.4443\n",
      "Epoch 286/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 124.6164 - mae: 125.3027 - val_loss: 1402.9487 - val_mae: 1403.6420\n",
      "Epoch 287/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 138.2742 - mae: 138.9619 - val_loss: 1474.5449 - val_mae: 1475.2382\n",
      "Epoch 288/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 135.1852 - mae: 135.8709 - val_loss: 1089.3872 - val_mae: 1090.0804\n",
      "Epoch 289/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 137.6149 - mae: 138.3034 - val_loss: 1142.1494 - val_mae: 1142.8419\n",
      "Epoch 290/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 126.8562 - mae: 127.5430 - val_loss: 1361.2969 - val_mae: 1361.9895\n",
      "Epoch 291/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 130.7766 - mae: 131.4625 - val_loss: 1148.5309 - val_mae: 1149.2220\n",
      "Epoch 292/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 127.8978 - mae: 128.5833 - val_loss: 1186.5381 - val_mae: 1187.2312\n",
      "Epoch 293/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 132.8430 - mae: 133.5290 - val_loss: 1162.6832 - val_mae: 1163.3765\n",
      "Epoch 294/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 130.4117 - mae: 131.0986 - val_loss: 1104.5479 - val_mae: 1105.2405\n",
      "Epoch 295/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 135.1078 - mae: 135.7961 - val_loss: 1293.3538 - val_mae: 1294.0464\n",
      "Epoch 296/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 140.0748 - mae: 140.7633 - val_loss: 1211.6597 - val_mae: 1212.3530\n",
      "Epoch 297/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 137.0419 - mae: 137.7290 - val_loss: 1203.4921 - val_mae: 1204.1841\n",
      "Epoch 298/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 125.5129 - mae: 126.2021 - val_loss: 1272.4794 - val_mae: 1273.1705\n",
      "Epoch 299/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 124.9716 - mae: 125.6551 - val_loss: 1091.1536 - val_mae: 1091.8464\n",
      "Epoch 300/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 131.2101 - mae: 131.8975 - val_loss: 1227.2124 - val_mae: 1227.9055\n",
      "Epoch 301/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 123.4627 - mae: 124.1479 - val_loss: 1229.7220 - val_mae: 1230.4144\n",
      "Epoch 302/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 121.1425 - mae: 121.8280 - val_loss: 1296.3580 - val_mae: 1297.0511\n",
      "Epoch 303/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 118.7264 - mae: 119.4102 - val_loss: 1350.9546 - val_mae: 1351.6476\n",
      "Epoch 304/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 130.9157 - mae: 131.6005 - val_loss: 1245.8770 - val_mae: 1246.5690\n",
      "Epoch 305/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 128.4707 - mae: 129.1561 - val_loss: 1371.2423 - val_mae: 1371.9347\n",
      "Epoch 306/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 124.5307 - mae: 125.2143 - val_loss: 1440.4347 - val_mae: 1441.1278\n",
      "Epoch 307/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 135.8820 - mae: 136.5700 - val_loss: 1156.6478 - val_mae: 1157.3413\n",
      "Epoch 308/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 130.8439 - mae: 131.5299 - val_loss: 1234.7972 - val_mae: 1235.4902\n",
      "Epoch 309/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 128.3984 - mae: 129.0863 - val_loss: 1176.3344 - val_mae: 1177.0273\n",
      "Epoch 310/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 122.5318 - mae: 123.2193 - val_loss: 1175.6906 - val_mae: 1176.3832\n",
      "Epoch 311/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 137.1015 - mae: 137.7906 - val_loss: 1367.7924 - val_mae: 1368.4846\n",
      "Epoch 312/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 119.2480 - mae: 119.9323 - val_loss: 1269.8041 - val_mae: 1270.4972\n",
      "Epoch 313/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 119.8508 - mae: 120.5383 - val_loss: 1538.7579 - val_mae: 1539.4510\n",
      "Epoch 314/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 126.9002 - mae: 127.5856 - val_loss: 1436.2000 - val_mae: 1436.8932\n",
      "Epoch 315/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 117.5315 - mae: 118.2182 - val_loss: 1333.5319 - val_mae: 1334.2249\n",
      "Epoch 316/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 131.0036 - mae: 131.6881 - val_loss: 1217.7230 - val_mae: 1218.4149\n",
      "Epoch 317/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 127.8954 - mae: 128.5827 - val_loss: 1313.4050 - val_mae: 1314.0978\n",
      "Epoch 318/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 123.1468 - mae: 123.8343 - val_loss: 1125.2104 - val_mae: 1125.9031\n",
      "Epoch 319/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 121.7258 - mae: 122.4105 - val_loss: 1287.6937 - val_mae: 1288.3871\n",
      "Epoch 320/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 124.5847 - mae: 125.2693 - val_loss: 1310.7913 - val_mae: 1311.4833\n",
      "Epoch 321/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 121.5572 - mae: 122.2419 - val_loss: 1304.4978 - val_mae: 1305.1909\n",
      "Epoch 322/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 114.9925 - mae: 115.6766 - val_loss: 1303.3273 - val_mae: 1304.0204\n",
      "Epoch 323/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 117.4029 - mae: 118.0901 - val_loss: 1186.1949 - val_mae: 1186.8881\n",
      "Epoch 324/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 148.9445 - mae: 149.6349 - val_loss: 1391.8459 - val_mae: 1392.5392\n",
      "Epoch 325/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 121.4489 - mae: 122.1345 - val_loss: 1035.8356 - val_mae: 1036.5288\n",
      "Epoch 326/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 127.5281 - mae: 128.2142 - val_loss: 1497.2308 - val_mae: 1497.9230\n",
      "Epoch 327/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 129.7533 - mae: 130.4408 - val_loss: 1436.0795 - val_mae: 1436.7726\n",
      "Epoch 328/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 123.3135 - mae: 123.9998 - val_loss: 1326.8646 - val_mae: 1327.5568\n",
      "Epoch 329/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 121.3695 - mae: 122.0566 - val_loss: 1528.8856 - val_mae: 1529.5787\n",
      "Epoch 330/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 128.5634 - mae: 129.2487 - val_loss: 1177.1165 - val_mae: 1177.8096\n",
      "Epoch 331/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 124.0062 - mae: 124.6925 - val_loss: 1368.9417 - val_mae: 1369.6346\n",
      "Epoch 332/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 129.8826 - mae: 130.5677 - val_loss: 1167.8523 - val_mae: 1168.5453\n",
      "Epoch 333/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 121.9094 - mae: 122.5968 - val_loss: 1407.4923 - val_mae: 1408.1853\n",
      "Epoch 334/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 120.3624 - mae: 121.0485 - val_loss: 1301.5537 - val_mae: 1302.2466\n",
      "Epoch 335/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 124.6818 - mae: 125.3688 - val_loss: 1071.9381 - val_mae: 1072.6310\n",
      "Epoch 336/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 124.4509 - mae: 125.1370 - val_loss: 1278.3654 - val_mae: 1279.0580\n",
      "Epoch 337/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 126.5934 - mae: 127.2809 - val_loss: 1239.9420 - val_mae: 1240.6348\n",
      "Epoch 338/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 126.4127 - mae: 127.0996 - val_loss: 1282.2427 - val_mae: 1282.9347\n",
      "Epoch 339/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 123.1690 - mae: 123.8546 - val_loss: 1356.6511 - val_mae: 1357.3442\n",
      "Epoch 340/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 127.5316 - mae: 128.2168 - val_loss: 1309.4912 - val_mae: 1310.1844\n",
      "Epoch 341/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 123.5006 - mae: 124.1847 - val_loss: 1141.3965 - val_mae: 1142.0898\n",
      "Epoch 342/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 127.9033 - mae: 128.5897 - val_loss: 1413.1737 - val_mae: 1413.8669\n",
      "Epoch 343/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 134.0472 - mae: 134.7343 - val_loss: 1153.8448 - val_mae: 1154.5381\n",
      "Epoch 344/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 152.3221 - mae: 153.0123 - val_loss: 1544.8441 - val_mae: 1545.5374\n",
      "Epoch 345/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 134.3959 - mae: 135.0800 - val_loss: 1334.6421 - val_mae: 1335.3346\n",
      "Epoch 346/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 126.8087 - mae: 127.4951 - val_loss: 1389.3550 - val_mae: 1390.0481\n",
      "Epoch 347/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 127.8871 - mae: 128.5714 - val_loss: 1190.6366 - val_mae: 1191.3297\n",
      "Epoch 348/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 128.5699 - mae: 129.2549 - val_loss: 1395.9979 - val_mae: 1396.6910\n",
      "Epoch 349/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 118.5728 - mae: 119.2584 - val_loss: 1199.6390 - val_mae: 1200.3317\n",
      "Epoch 350/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 116.5821 - mae: 117.2696 - val_loss: 1159.0840 - val_mae: 1159.7771\n",
      "Epoch 351/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 114.4507 - mae: 115.1338 - val_loss: 1498.6088 - val_mae: 1499.3020\n",
      "Epoch 352/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 117.2979 - mae: 117.9837 - val_loss: 1099.8043 - val_mae: 1100.4974\n",
      "Epoch 353/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 119.9799 - mae: 120.6656 - val_loss: 1299.9911 - val_mae: 1300.6842\n",
      "Epoch 354/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 125.0486 - mae: 125.7329 - val_loss: 1014.9545 - val_mae: 1015.6478\n",
      "Epoch 355/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 125.5241 - mae: 126.2094 - val_loss: 1268.5033 - val_mae: 1269.1964\n",
      "Epoch 356/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 125.6533 - mae: 126.3351 - val_loss: 1172.6599 - val_mae: 1173.3531\n",
      "Epoch 357/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 124.6441 - mae: 125.3314 - val_loss: 1343.9143 - val_mae: 1344.6074\n",
      "Epoch 358/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 123.2613 - mae: 123.9473 - val_loss: 1204.8386 - val_mae: 1205.5317\n",
      "Epoch 359/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 117.1787 - mae: 117.8658 - val_loss: 1429.8639 - val_mae: 1430.5565\n",
      "Epoch 360/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 116.4212 - mae: 117.1048 - val_loss: 1231.4482 - val_mae: 1232.1407\n",
      "Epoch 361/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 114.0825 - mae: 114.7681 - val_loss: 1452.4858 - val_mae: 1453.1783\n",
      "Epoch 362/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 132.1790 - mae: 132.8659 - val_loss: 1828.9781 - val_mae: 1829.6713\n",
      "Epoch 363/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 133.2479 - mae: 133.9357 - val_loss: 1087.7434 - val_mae: 1088.4364\n",
      "Epoch 364/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 124.3411 - mae: 125.0278 - val_loss: 1188.6241 - val_mae: 1189.3173\n",
      "Epoch 365/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 117.9420 - mae: 118.6269 - val_loss: 1218.6118 - val_mae: 1219.3049\n",
      "Epoch 366/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 118.2239 - mae: 118.9094 - val_loss: 1086.3239 - val_mae: 1087.0170\n",
      "Epoch 367/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 127.6207 - mae: 128.3080 - val_loss: 1469.2289 - val_mae: 1469.9220\n",
      "Epoch 368/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 115.3337 - mae: 116.0200 - val_loss: 1197.6118 - val_mae: 1198.3052\n",
      "Epoch 369/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 132.6767 - mae: 133.3641 - val_loss: 1414.5457 - val_mae: 1415.2389\n",
      "Epoch 370/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 121.5013 - mae: 122.1876 - val_loss: 1289.6051 - val_mae: 1290.2964\n",
      "Epoch 371/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 123.1514 - mae: 123.8380 - val_loss: 1267.3026 - val_mae: 1267.9958\n",
      "Epoch 372/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 128.0046 - mae: 128.6909 - val_loss: 1115.5520 - val_mae: 1116.2438\n",
      "Epoch 373/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 116.2110 - mae: 116.8987 - val_loss: 1313.1187 - val_mae: 1313.8119\n",
      "Epoch 374/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 114.3373 - mae: 115.0203 - val_loss: 1287.5768 - val_mae: 1288.2700\n",
      "Epoch 375/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 126.9133 - mae: 127.6013 - val_loss: 1157.9963 - val_mae: 1158.6888\n",
      "Epoch 376/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 137.9217 - mae: 138.6075 - val_loss: 1358.1394 - val_mae: 1358.8322\n",
      "Epoch 377/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 132.0886 - mae: 132.7763 - val_loss: 1293.9702 - val_mae: 1294.6635\n",
      "Epoch 378/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 134.8218 - mae: 135.5101 - val_loss: 1280.4069 - val_mae: 1281.1001\n",
      "Epoch 379/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 123.6846 - mae: 124.3691 - val_loss: 1219.4652 - val_mae: 1220.1583\n",
      "Epoch 380/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 133.5626 - mae: 134.2494 - val_loss: 1057.4858 - val_mae: 1058.1781\n",
      "Epoch 381/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 123.5039 - mae: 124.1874 - val_loss: 1348.8883 - val_mae: 1349.5807\n",
      "Epoch 382/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 117.8167 - mae: 118.5022 - val_loss: 1239.9270 - val_mae: 1240.6201\n",
      "Epoch 383/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 128.0211 - mae: 128.7057 - val_loss: 1240.1710 - val_mae: 1240.8643\n",
      "Epoch 384/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 118.9754 - mae: 119.6604 - val_loss: 1321.0732 - val_mae: 1321.7657\n",
      "Epoch 385/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 122.2517 - mae: 122.9389 - val_loss: 1385.5531 - val_mae: 1386.2446\n",
      "Epoch 386/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 117.4060 - mae: 118.0927 - val_loss: 1117.4554 - val_mae: 1118.1477\n",
      "Epoch 387/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 115.9484 - mae: 116.6340 - val_loss: 1451.6045 - val_mae: 1452.2979\n",
      "Epoch 388/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 116.0457 - mae: 116.7305 - val_loss: 1323.9244 - val_mae: 1324.6172\n",
      "Epoch 389/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 121.1229 - mae: 121.8101 - val_loss: 1302.8303 - val_mae: 1303.5226\n",
      "Epoch 390/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 110.7601 - mae: 111.4438 - val_loss: 1129.4161 - val_mae: 1130.1085\n",
      "Epoch 391/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 118.5014 - mae: 119.1857 - val_loss: 1178.3307 - val_mae: 1179.0234\n",
      "Epoch 392/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 127.2683 - mae: 127.9527 - val_loss: 1295.5195 - val_mae: 1296.2122\n",
      "Epoch 393/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 118.5888 - mae: 119.2739 - val_loss: 1440.2266 - val_mae: 1440.9197\n",
      "Epoch 394/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 115.3199 - mae: 116.0038 - val_loss: 1327.4574 - val_mae: 1328.1493\n",
      "Epoch 395/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 118.4229 - mae: 119.1102 - val_loss: 1345.6145 - val_mae: 1346.3068\n",
      "Epoch 396/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 112.7701 - mae: 113.4521 - val_loss: 1201.8755 - val_mae: 1202.5687\n",
      "Epoch 397/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 113.5641 - mae: 114.2492 - val_loss: 1344.7743 - val_mae: 1345.4675\n",
      "Epoch 398/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 123.8939 - mae: 124.5800 - val_loss: 1130.6758 - val_mae: 1131.3688\n",
      "Epoch 399/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 125.7531 - mae: 126.4395 - val_loss: 1161.7911 - val_mae: 1162.4835\n",
      "Epoch 400/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 118.1446 - mae: 118.8268 - val_loss: 1053.4791 - val_mae: 1054.1715\n",
      "Epoch 401/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 114.9590 - mae: 115.6430 - val_loss: 1245.4023 - val_mae: 1246.0953\n",
      "Epoch 402/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 121.2807 - mae: 121.9642 - val_loss: 1396.9717 - val_mae: 1397.6649\n",
      "Epoch 403/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 122.3625 - mae: 123.0465 - val_loss: 1405.0219 - val_mae: 1405.7150\n",
      "Epoch 404/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 124.2986 - mae: 124.9862 - val_loss: 1099.7338 - val_mae: 1100.4270\n",
      "Epoch 405/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 125.9187 - mae: 126.6035 - val_loss: 1302.3550 - val_mae: 1303.0476\n",
      "Epoch 406/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 119.5088 - mae: 120.1928 - val_loss: 1120.1697 - val_mae: 1120.8629\n",
      "Epoch 407/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 120.7569 - mae: 121.4425 - val_loss: 1493.1058 - val_mae: 1493.7986\n",
      "Epoch 408/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 130.5130 - mae: 131.2002 - val_loss: 1256.6761 - val_mae: 1257.3687\n",
      "Epoch 409/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 127.0595 - mae: 127.7479 - val_loss: 1272.3524 - val_mae: 1273.0444\n",
      "Epoch 410/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 141.9448 - mae: 142.6309 - val_loss: 1171.4113 - val_mae: 1172.1041\n",
      "Epoch 411/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 119.0253 - mae: 119.7077 - val_loss: 1243.4050 - val_mae: 1244.0969\n",
      "Epoch 412/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 117.3066 - mae: 117.9920 - val_loss: 1207.9756 - val_mae: 1208.6687\n",
      "Epoch 413/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 119.4998 - mae: 120.1853 - val_loss: 1121.1172 - val_mae: 1121.8101\n",
      "Epoch 414/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 121.4464 - mae: 122.1324 - val_loss: 1297.4333 - val_mae: 1298.1262\n",
      "Epoch 415/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 116.6822 - mae: 117.3673 - val_loss: 1082.1143 - val_mae: 1082.8074\n",
      "Epoch 416/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 120.9067 - mae: 121.5909 - val_loss: 1121.7389 - val_mae: 1122.4316\n",
      "Epoch 417/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 123.3198 - mae: 124.0071 - val_loss: 1437.1721 - val_mae: 1437.8651\n",
      "Epoch 418/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 119.6018 - mae: 120.2878 - val_loss: 1133.5477 - val_mae: 1134.2411\n",
      "Epoch 419/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 124.9903 - mae: 125.6774 - val_loss: 1164.7509 - val_mae: 1165.4441\n",
      "Epoch 420/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 123.3614 - mae: 124.0482 - val_loss: 1181.5806 - val_mae: 1182.2737\n",
      "Epoch 421/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 124.5049 - mae: 125.1902 - val_loss: 1315.1848 - val_mae: 1315.8779\n",
      "Epoch 422/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 117.6805 - mae: 118.3674 - val_loss: 1344.6398 - val_mae: 1345.3329\n",
      "Epoch 423/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 112.3181 - mae: 113.0006 - val_loss: 1139.3450 - val_mae: 1140.0382\n",
      "Epoch 424/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 120.6135 - mae: 121.2956 - val_loss: 1279.8107 - val_mae: 1280.5039\n",
      "Epoch 425/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 113.8735 - mae: 114.5580 - val_loss: 1320.2568 - val_mae: 1320.9498\n",
      "Epoch 426/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 115.2335 - mae: 115.9159 - val_loss: 1307.5924 - val_mae: 1308.2854\n",
      "Epoch 427/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 118.1507 - mae: 118.8382 - val_loss: 1394.4651 - val_mae: 1395.1580\n",
      "Epoch 428/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 123.7073 - mae: 124.3948 - val_loss: 1253.2505 - val_mae: 1253.9429\n",
      "Epoch 429/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 121.7810 - mae: 122.4633 - val_loss: 1262.1165 - val_mae: 1262.8091\n",
      "Epoch 430/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 112.7662 - mae: 113.4469 - val_loss: 1586.4128 - val_mae: 1587.1061\n",
      "Epoch 431/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 129.2748 - mae: 129.9637 - val_loss: 1268.2375 - val_mae: 1268.9308\n",
      "Epoch 432/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 111.5221 - mae: 112.2047 - val_loss: 1201.6250 - val_mae: 1202.3181\n",
      "Epoch 433/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 126.9398 - mae: 127.6255 - val_loss: 1132.7015 - val_mae: 1133.3929\n",
      "Epoch 434/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 128.0105 - mae: 128.6972 - val_loss: 1234.8922 - val_mae: 1235.5845\n",
      "Epoch 435/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 138.8754 - mae: 139.5629 - val_loss: 1039.4485 - val_mae: 1040.1418\n",
      "Epoch 436/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 118.3414 - mae: 119.0284 - val_loss: 1485.3115 - val_mae: 1486.0046\n",
      "Epoch 437/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 130.7106 - mae: 131.3968 - val_loss: 1144.5835 - val_mae: 1145.2769\n",
      "Epoch 438/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 123.6776 - mae: 124.3660 - val_loss: 1243.6294 - val_mae: 1244.3220\n",
      "Epoch 439/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 123.1461 - mae: 123.8295 - val_loss: 1429.3646 - val_mae: 1430.0577\n",
      "Epoch 440/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 120.4280 - mae: 121.1137 - val_loss: 1307.2489 - val_mae: 1307.9421\n",
      "Epoch 441/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 117.4277 - mae: 118.1117 - val_loss: 1428.9352 - val_mae: 1429.6284\n",
      "Epoch 442/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 131.9046 - mae: 132.5914 - val_loss: 1614.1504 - val_mae: 1614.8435\n",
      "Epoch 443/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 142.3388 - mae: 143.0282 - val_loss: 1488.3898 - val_mae: 1489.0830\n",
      "Epoch 444/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 132.1433 - mae: 132.8304 - val_loss: 1334.3850 - val_mae: 1335.0785\n",
      "Epoch 445/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 115.0487 - mae: 115.7338 - val_loss: 1183.6338 - val_mae: 1184.3270\n",
      "Epoch 446/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 126.7354 - mae: 127.4187 - val_loss: 1135.2004 - val_mae: 1135.8927\n",
      "Epoch 447/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 127.1385 - mae: 127.8273 - val_loss: 1165.2642 - val_mae: 1165.9574\n",
      "Epoch 448/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 118.6536 - mae: 119.3377 - val_loss: 1624.0990 - val_mae: 1624.7921\n",
      "Epoch 449/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 120.3351 - mae: 121.0184 - val_loss: 1218.0654 - val_mae: 1218.7585\n",
      "Epoch 450/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 123.6125 - mae: 124.3001 - val_loss: 1193.4799 - val_mae: 1194.1730\n",
      "Epoch 451/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 126.4426 - mae: 127.1275 - val_loss: 1109.5670 - val_mae: 1110.2601\n",
      "Epoch 452/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 112.8966 - mae: 113.5803 - val_loss: 1255.6343 - val_mae: 1256.3275\n",
      "Epoch 453/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 116.2416 - mae: 116.9258 - val_loss: 1215.7665 - val_mae: 1216.4597\n",
      "Epoch 454/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 121.6063 - mae: 122.2911 - val_loss: 1295.5698 - val_mae: 1296.2631\n",
      "Epoch 455/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 112.7651 - mae: 113.4514 - val_loss: 1165.0079 - val_mae: 1165.7013\n",
      "Epoch 456/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 121.9069 - mae: 122.5920 - val_loss: 1223.8994 - val_mae: 1224.5919\n",
      "Epoch 457/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 116.6707 - mae: 117.3591 - val_loss: 1202.8729 - val_mae: 1203.5658\n",
      "Epoch 458/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 115.2864 - mae: 115.9702 - val_loss: 1078.9949 - val_mae: 1079.6881\n",
      "Epoch 459/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 123.8193 - mae: 124.5081 - val_loss: 1335.5813 - val_mae: 1336.2739\n",
      "Epoch 460/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 118.0390 - mae: 118.7257 - val_loss: 1329.1591 - val_mae: 1329.8524\n",
      "Epoch 461/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 110.0607 - mae: 110.7469 - val_loss: 1138.3636 - val_mae: 1139.0563\n",
      "Epoch 462/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 119.4916 - mae: 120.1783 - val_loss: 1315.5767 - val_mae: 1316.2687\n",
      "Epoch 463/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 116.2819 - mae: 116.9664 - val_loss: 1334.6708 - val_mae: 1335.3640\n",
      "Epoch 464/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 112.8797 - mae: 113.5621 - val_loss: 1390.5062 - val_mae: 1391.1995\n",
      "Epoch 465/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 113.1331 - mae: 113.8146 - val_loss: 1219.2430 - val_mae: 1219.9363\n",
      "Epoch 466/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 110.7288 - mae: 111.4151 - val_loss: 1318.1985 - val_mae: 1318.8920\n",
      "Epoch 467/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 112.8762 - mae: 113.5610 - val_loss: 1416.5281 - val_mae: 1417.2212\n",
      "Epoch 468/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 117.1817 - mae: 117.8675 - val_loss: 1313.7894 - val_mae: 1314.4825\n",
      "Epoch 469/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 113.9402 - mae: 114.6255 - val_loss: 1239.3966 - val_mae: 1240.0895\n",
      "Epoch 470/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 116.5270 - mae: 117.2124 - val_loss: 1307.9164 - val_mae: 1308.6086\n",
      "Epoch 471/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 119.3212 - mae: 120.0034 - val_loss: 1148.8296 - val_mae: 1149.5223\n",
      "Epoch 472/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 110.4453 - mae: 111.1296 - val_loss: 1178.5896 - val_mae: 1179.2826\n",
      "Epoch 473/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 123.2451 - mae: 123.9306 - val_loss: 1430.5762 - val_mae: 1431.2693\n",
      "Epoch 474/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 108.9783 - mae: 109.6617 - val_loss: 1033.0037 - val_mae: 1033.6969\n",
      "Epoch 475/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 129.8636 - mae: 130.5472 - val_loss: 1703.8867 - val_mae: 1704.5798\n",
      "Epoch 476/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 120.2647 - mae: 120.9494 - val_loss: 1231.4071 - val_mae: 1232.0994\n",
      "Epoch 477/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 120.7622 - mae: 121.4495 - val_loss: 1372.7816 - val_mae: 1373.4724\n",
      "Epoch 478/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 115.0985 - mae: 115.7799 - val_loss: 1228.1157 - val_mae: 1228.8090\n",
      "Epoch 479/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 115.6220 - mae: 116.3072 - val_loss: 1351.2627 - val_mae: 1351.9558\n",
      "Epoch 480/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 120.2487 - mae: 120.9315 - val_loss: 1480.0284 - val_mae: 1480.7216\n",
      "Epoch 481/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 129.0216 - mae: 129.7069 - val_loss: 1170.5706 - val_mae: 1171.2638\n",
      "Epoch 482/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 129.2499 - mae: 129.9355 - val_loss: 1051.6093 - val_mae: 1052.3026\n",
      "Epoch 483/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 114.4422 - mae: 115.1254 - val_loss: 1165.8828 - val_mae: 1166.5760\n",
      "Epoch 484/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 109.8442 - mae: 110.5312 - val_loss: 1346.1051 - val_mae: 1346.7981\n",
      "Epoch 485/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 115.8275 - mae: 116.5143 - val_loss: 1194.9877 - val_mae: 1195.6807\n",
      "Epoch 486/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 113.7505 - mae: 114.4330 - val_loss: 1046.4121 - val_mae: 1047.1050\n",
      "Epoch 487/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 133.4778 - mae: 134.1645 - val_loss: 1204.4199 - val_mae: 1205.1127\n",
      "Epoch 488/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 122.0153 - mae: 122.7024 - val_loss: 1335.9493 - val_mae: 1336.6425\n",
      "Epoch 489/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 113.9171 - mae: 114.6034 - val_loss: 1414.5647 - val_mae: 1415.2579\n",
      "Epoch 490/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 110.6020 - mae: 111.2851 - val_loss: 1164.3212 - val_mae: 1165.0143\n",
      "Epoch 491/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 123.8129 - mae: 124.5000 - val_loss: 1305.7662 - val_mae: 1306.4595\n",
      "Epoch 492/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 112.7781 - mae: 113.4625 - val_loss: 1196.4563 - val_mae: 1197.1493\n",
      "Epoch 493/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 110.3920 - mae: 111.0760 - val_loss: 1177.3188 - val_mae: 1178.0118\n",
      "Epoch 494/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 113.7270 - mae: 114.4130 - val_loss: 1239.0743 - val_mae: 1239.7675\n",
      "Epoch 495/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 116.1364 - mae: 116.8203 - val_loss: 1294.4120 - val_mae: 1295.1047\n",
      "Epoch 496/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 117.1905 - mae: 117.8765 - val_loss: 1639.9626 - val_mae: 1640.6559\n",
      "Epoch 497/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 126.2838 - mae: 126.9692 - val_loss: 1217.9103 - val_mae: 1218.6036\n",
      "Epoch 498/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 116.4029 - mae: 117.0876 - val_loss: 1610.3776 - val_mae: 1611.0706\n",
      "Epoch 499/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 122.0336 - mae: 122.7169 - val_loss: 1372.5757 - val_mae: 1373.2689\n",
      "Epoch 500/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 113.7227 - mae: 114.4074 - val_loss: 1183.4980 - val_mae: 1184.1909\n",
      "Epoch 501/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 108.8222 - mae: 109.5059 - val_loss: 1413.5684 - val_mae: 1414.2615\n",
      "Epoch 502/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 115.0696 - mae: 115.7562 - val_loss: 1292.8823 - val_mae: 1293.5747\n",
      "Epoch 503/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 111.4136 - mae: 112.0955 - val_loss: 1143.6664 - val_mae: 1144.3596\n",
      "Epoch 504/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 122.1491 - mae: 122.8383 - val_loss: 1258.7018 - val_mae: 1259.3949\n",
      "Epoch 505/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 115.1618 - mae: 115.8484 - val_loss: 1153.7932 - val_mae: 1154.4844\n",
      "Epoch 506/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 114.7864 - mae: 115.4694 - val_loss: 1388.7939 - val_mae: 1389.4871\n",
      "Epoch 507/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 110.6658 - mae: 111.3498 - val_loss: 1301.7610 - val_mae: 1302.4532\n",
      "Epoch 508/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 115.1704 - mae: 115.8568 - val_loss: 1468.2059 - val_mae: 1468.8990\n",
      "Epoch 509/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 113.7920 - mae: 114.4762 - val_loss: 1307.6780 - val_mae: 1308.3711\n",
      "Epoch 510/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 126.3355 - mae: 127.0224 - val_loss: 1201.0242 - val_mae: 1201.7175\n",
      "Epoch 511/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 116.1016 - mae: 116.7875 - val_loss: 1295.6134 - val_mae: 1296.3064\n",
      "Epoch 512/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 114.6856 - mae: 115.3720 - val_loss: 1459.6079 - val_mae: 1460.3014\n",
      "Epoch 513/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 112.7862 - mae: 113.4722 - val_loss: 1130.4207 - val_mae: 1131.1133\n",
      "Epoch 514/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 111.1198 - mae: 111.8039 - val_loss: 1208.7031 - val_mae: 1209.3961\n",
      "Epoch 515/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 110.6149 - mae: 111.3000 - val_loss: 1385.8135 - val_mae: 1386.5066\n",
      "Epoch 516/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 116.9470 - mae: 117.6301 - val_loss: 1349.9485 - val_mae: 1350.6416\n",
      "Epoch 517/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 113.6925 - mae: 114.3763 - val_loss: 1170.1307 - val_mae: 1170.8240\n",
      "Epoch 518/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 111.3618 - mae: 112.0449 - val_loss: 1599.0211 - val_mae: 1599.7145\n",
      "Epoch 519/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 118.9440 - mae: 119.6273 - val_loss: 1166.2988 - val_mae: 1166.9915\n",
      "Epoch 520/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 119.4266 - mae: 120.1133 - val_loss: 1301.9774 - val_mae: 1302.6702\n",
      "Epoch 521/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 109.9344 - mae: 110.6179 - val_loss: 1276.2104 - val_mae: 1276.9025\n",
      "Epoch 522/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 116.2992 - mae: 116.9858 - val_loss: 1408.5312 - val_mae: 1409.2234\n",
      "Epoch 523/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 111.8875 - mae: 112.5747 - val_loss: 1243.2588 - val_mae: 1243.9508\n",
      "Epoch 524/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 111.6248 - mae: 112.3087 - val_loss: 1011.2500 - val_mae: 1011.9409\n",
      "Epoch 525/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 121.6338 - mae: 122.3201 - val_loss: 1442.1664 - val_mae: 1442.8591\n",
      "Epoch 526/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 112.0286 - mae: 112.7133 - val_loss: 1520.3619 - val_mae: 1521.0543\n",
      "Epoch 527/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 115.9800 - mae: 116.6621 - val_loss: 1217.6705 - val_mae: 1218.3636\n",
      "Epoch 528/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 127.5062 - mae: 128.1920 - val_loss: 1241.0164 - val_mae: 1241.7091\n",
      "Epoch 529/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 138.5516 - mae: 139.2380 - val_loss: 1358.5986 - val_mae: 1359.2915\n",
      "Epoch 530/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 126.2935 - mae: 126.9818 - val_loss: 1387.2668 - val_mae: 1387.9602\n",
      "Epoch 531/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 118.3913 - mae: 119.0787 - val_loss: 1152.9808 - val_mae: 1153.6731\n",
      "Epoch 532/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 110.4079 - mae: 111.0912 - val_loss: 1330.7111 - val_mae: 1331.4044\n",
      "Epoch 533/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 116.4077 - mae: 117.0940 - val_loss: 1070.6726 - val_mae: 1071.3657\n",
      "Epoch 534/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 119.6116 - mae: 120.2976 - val_loss: 1183.7687 - val_mae: 1184.4617\n",
      "Epoch 535/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 108.8668 - mae: 109.5524 - val_loss: 1142.0186 - val_mae: 1142.7109\n",
      "Epoch 536/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 117.6296 - mae: 118.3153 - val_loss: 1423.9829 - val_mae: 1424.6750\n",
      "Epoch 537/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 129.4123 - mae: 130.1009 - val_loss: 1463.1306 - val_mae: 1463.8237\n",
      "Epoch 538/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 109.3406 - mae: 110.0241 - val_loss: 1210.6921 - val_mae: 1211.3854\n",
      "Epoch 539/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 106.4580 - mae: 107.1411 - val_loss: 1416.1290 - val_mae: 1416.8220\n",
      "Epoch 540/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 124.9418 - mae: 125.6242 - val_loss: 1158.1384 - val_mae: 1158.8312\n",
      "Epoch 541/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 127.3532 - mae: 128.0395 - val_loss: 1107.8833 - val_mae: 1108.5762\n",
      "Epoch 542/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 118.3109 - mae: 118.9982 - val_loss: 1574.7042 - val_mae: 1575.3972\n",
      "Epoch 543/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 123.0062 - mae: 123.6920 - val_loss: 1246.0618 - val_mae: 1246.7549\n",
      "Epoch 544/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 112.7242 - mae: 113.4072 - val_loss: 1600.1691 - val_mae: 1600.8618\n",
      "Epoch 545/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 132.6462 - mae: 133.3327 - val_loss: 1505.4984 - val_mae: 1506.1917\n",
      "Epoch 546/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 127.6251 - mae: 128.3106 - val_loss: 1212.6318 - val_mae: 1213.3250\n",
      "Epoch 547/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 121.5222 - mae: 122.2070 - val_loss: 1615.9143 - val_mae: 1616.6071\n",
      "Epoch 548/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 108.3817 - mae: 109.0665 - val_loss: 1230.6689 - val_mae: 1231.3622\n",
      "Epoch 549/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 111.0667 - mae: 111.7518 - val_loss: 1440.7572 - val_mae: 1441.4502\n",
      "Epoch 550/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 112.5439 - mae: 113.2270 - val_loss: 1180.9121 - val_mae: 1181.6052\n",
      "Epoch 551/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 106.1257 - mae: 106.8095 - val_loss: 1496.8685 - val_mae: 1497.5614\n",
      "Epoch 552/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 115.3688 - mae: 116.0562 - val_loss: 1142.2117 - val_mae: 1142.9045\n",
      "Epoch 553/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 129.6507 - mae: 130.3363 - val_loss: 1097.3921 - val_mae: 1098.0852\n",
      "Epoch 554/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 128.4740 - mae: 129.1615 - val_loss: 1175.3474 - val_mae: 1176.0403\n",
      "Epoch 555/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 111.0392 - mae: 111.7212 - val_loss: 1106.6888 - val_mae: 1107.3820\n",
      "Epoch 556/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 124.3921 - mae: 125.0764 - val_loss: 1607.1700 - val_mae: 1607.8624\n",
      "Epoch 557/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 118.2050 - mae: 118.8915 - val_loss: 1159.1565 - val_mae: 1159.8496\n",
      "Epoch 558/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 111.1332 - mae: 111.8181 - val_loss: 1271.6884 - val_mae: 1272.3813\n",
      "Epoch 559/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 114.6386 - mae: 115.3237 - val_loss: 1292.7607 - val_mae: 1293.4540\n",
      "Epoch 560/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 110.6390 - mae: 111.3234 - val_loss: 1524.6558 - val_mae: 1525.3490\n",
      "Epoch 561/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 118.1048 - mae: 118.7885 - val_loss: 1472.0670 - val_mae: 1472.7604\n",
      "Epoch 562/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 114.8488 - mae: 115.5335 - val_loss: 1152.5444 - val_mae: 1153.2372\n",
      "Epoch 563/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 115.4565 - mae: 116.1428 - val_loss: 1316.0554 - val_mae: 1316.7482\n",
      "Epoch 564/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 113.9539 - mae: 114.6400 - val_loss: 1298.7764 - val_mae: 1299.4695\n",
      "Epoch 565/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 109.7891 - mae: 110.4724 - val_loss: 1518.3154 - val_mae: 1519.0079\n",
      "Epoch 566/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 114.7346 - mae: 115.4172 - val_loss: 1261.9933 - val_mae: 1262.6863\n",
      "Epoch 567/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 126.1505 - mae: 126.8374 - val_loss: 1370.2496 - val_mae: 1370.9431\n",
      "Epoch 568/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 115.1707 - mae: 115.8561 - val_loss: 1160.3997 - val_mae: 1161.0930\n",
      "Epoch 569/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 114.6315 - mae: 115.3173 - val_loss: 1302.8751 - val_mae: 1303.5675\n",
      "Epoch 570/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 113.9757 - mae: 114.6582 - val_loss: 1274.3164 - val_mae: 1275.0095\n",
      "Epoch 571/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 111.2327 - mae: 111.9156 - val_loss: 1237.7197 - val_mae: 1238.4130\n",
      "Epoch 572/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 106.5085 - mae: 107.1924 - val_loss: 1230.2322 - val_mae: 1230.9252\n",
      "Epoch 573/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 120.6926 - mae: 121.3757 - val_loss: 1154.1941 - val_mae: 1154.8873\n",
      "Epoch 574/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 108.7386 - mae: 109.4197 - val_loss: 1182.9020 - val_mae: 1183.5946\n",
      "Epoch 575/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 111.4744 - mae: 112.1560 - val_loss: 1198.1941 - val_mae: 1198.8875\n",
      "Epoch 576/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 118.3981 - mae: 119.0846 - val_loss: 1031.9547 - val_mae: 1032.6460\n",
      "Epoch 577/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 117.4997 - mae: 118.1829 - val_loss: 1361.7662 - val_mae: 1362.4576\n",
      "Epoch 578/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 116.7791 - mae: 117.4626 - val_loss: 1599.6283 - val_mae: 1600.3214\n",
      "Epoch 579/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 115.1088 - mae: 115.7923 - val_loss: 1439.8065 - val_mae: 1440.4998\n",
      "Epoch 580/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 109.8003 - mae: 110.4845 - val_loss: 1505.2992 - val_mae: 1505.9926\n",
      "Epoch 581/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 112.7537 - mae: 113.4389 - val_loss: 1356.1071 - val_mae: 1356.8003\n",
      "Epoch 582/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 118.1757 - mae: 118.8586 - val_loss: 1533.3851 - val_mae: 1534.0781\n",
      "Epoch 583/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 115.4837 - mae: 116.1669 - val_loss: 1337.8119 - val_mae: 1338.5050\n",
      "Epoch 584/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 107.2572 - mae: 107.9388 - val_loss: 1162.7574 - val_mae: 1163.4501\n",
      "Epoch 585/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 116.0183 - mae: 116.7020 - val_loss: 1489.2209 - val_mae: 1489.9142\n",
      "Epoch 586/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 114.8749 - mae: 115.5597 - val_loss: 1308.4177 - val_mae: 1309.1108\n",
      "Epoch 587/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 105.3290 - mae: 106.0122 - val_loss: 1143.1421 - val_mae: 1143.8352\n",
      "Epoch 588/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 123.6596 - mae: 124.3472 - val_loss: 1357.7926 - val_mae: 1358.4844\n",
      "Epoch 589/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 112.3339 - mae: 113.0182 - val_loss: 1202.2445 - val_mae: 1202.9360\n",
      "Epoch 590/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 110.1778 - mae: 110.8642 - val_loss: 1344.5551 - val_mae: 1345.2480\n",
      "Epoch 591/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 117.3925 - mae: 118.0773 - val_loss: 1531.0715 - val_mae: 1531.7648\n",
      "Epoch 592/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 106.8947 - mae: 107.5777 - val_loss: 1100.9180 - val_mae: 1101.6108\n",
      "Epoch 593/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 111.2376 - mae: 111.9211 - val_loss: 1802.9872 - val_mae: 1803.6804\n",
      "Epoch 594/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 114.3842 - mae: 115.0680 - val_loss: 1233.3584 - val_mae: 1234.0516\n",
      "Epoch 595/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 124.1396 - mae: 124.8260 - val_loss: 1196.9518 - val_mae: 1197.6439\n",
      "Epoch 596/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 113.3737 - mae: 114.0587 - val_loss: 1235.1908 - val_mae: 1235.8839\n",
      "Epoch 597/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 102.7521 - mae: 103.4342 - val_loss: 1230.0184 - val_mae: 1230.7109\n",
      "Epoch 598/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 108.7213 - mae: 109.4068 - val_loss: 1370.4688 - val_mae: 1371.1617\n",
      "Epoch 599/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 120.9542 - mae: 121.6423 - val_loss: 1263.2660 - val_mae: 1263.9592\n",
      "Epoch 600/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 118.5170 - mae: 119.2031 - val_loss: 1182.3665 - val_mae: 1183.0594\n",
      "Epoch 601/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 116.4631 - mae: 117.1464 - val_loss: 1746.6965 - val_mae: 1747.3894\n",
      "Epoch 602/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 109.1686 - mae: 109.8539 - val_loss: 1152.4353 - val_mae: 1153.1285\n",
      "Epoch 603/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 108.7140 - mae: 109.4001 - val_loss: 1118.0980 - val_mae: 1118.7910\n",
      "Epoch 604/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 114.7374 - mae: 115.4225 - val_loss: 1683.8279 - val_mae: 1684.5209\n",
      "Epoch 605/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 119.3275 - mae: 120.0137 - val_loss: 1655.5408 - val_mae: 1656.2328\n",
      "Epoch 606/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 111.6569 - mae: 112.3409 - val_loss: 1247.3112 - val_mae: 1248.0043\n",
      "Epoch 607/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 111.9524 - mae: 112.6367 - val_loss: 1168.8256 - val_mae: 1169.5187\n",
      "Epoch 608/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 123.6411 - mae: 124.3276 - val_loss: 1287.9473 - val_mae: 1288.6404\n",
      "Epoch 609/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 111.4579 - mae: 112.1435 - val_loss: 1197.7289 - val_mae: 1198.4219\n",
      "Epoch 610/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 118.4303 - mae: 119.1158 - val_loss: 1153.9486 - val_mae: 1154.6417\n",
      "Epoch 611/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 105.5158 - mae: 106.1993 - val_loss: 1291.1255 - val_mae: 1291.8185\n",
      "Epoch 612/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 117.8878 - mae: 118.5734 - val_loss: 1061.9475 - val_mae: 1062.6409\n",
      "Epoch 613/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 114.4361 - mae: 115.1224 - val_loss: 1200.0890 - val_mae: 1200.7803\n",
      "Epoch 614/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 107.3827 - mae: 108.0683 - val_loss: 1318.6420 - val_mae: 1319.3351\n",
      "Epoch 615/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 110.2280 - mae: 110.9136 - val_loss: 1405.3519 - val_mae: 1406.0426\n",
      "Epoch 616/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 104.2875 - mae: 104.9723 - val_loss: 1088.9780 - val_mae: 1089.6707\n",
      "Epoch 617/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 107.9515 - mae: 108.6370 - val_loss: 1353.1641 - val_mae: 1353.8557\n",
      "Epoch 618/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 103.6184 - mae: 104.3024 - val_loss: 1613.8745 - val_mae: 1614.5677\n",
      "Epoch 619/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 107.1619 - mae: 107.8486 - val_loss: 1238.2744 - val_mae: 1238.9678\n",
      "Epoch 620/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 104.5123 - mae: 105.1957 - val_loss: 1303.1465 - val_mae: 1303.8395\n",
      "Epoch 621/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 118.4510 - mae: 119.1363 - val_loss: 1125.8646 - val_mae: 1126.5573\n",
      "Epoch 622/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 113.1762 - mae: 113.8593 - val_loss: 1519.8787 - val_mae: 1520.5718\n",
      "Epoch 623/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 109.7700 - mae: 110.4561 - val_loss: 1189.2260 - val_mae: 1189.9188\n",
      "Epoch 624/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 120.1936 - mae: 120.8781 - val_loss: 1176.7930 - val_mae: 1177.4854\n",
      "Epoch 625/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 121.2823 - mae: 121.9650 - val_loss: 1231.0404 - val_mae: 1231.7334\n",
      "Epoch 626/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 102.3496 - mae: 103.0326 - val_loss: 1135.5039 - val_mae: 1136.1969\n",
      "Epoch 627/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 115.1202 - mae: 115.8046 - val_loss: 1469.3710 - val_mae: 1470.0645\n",
      "Epoch 628/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 115.3329 - mae: 116.0162 - val_loss: 1251.0358 - val_mae: 1251.7291\n",
      "Epoch 629/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 109.4583 - mae: 110.1438 - val_loss: 1318.7720 - val_mae: 1319.4653\n",
      "Epoch 630/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 111.6157 - mae: 112.3033 - val_loss: 1449.3450 - val_mae: 1450.0382\n",
      "Epoch 631/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 105.3381 - mae: 106.0240 - val_loss: 1349.4498 - val_mae: 1350.1431\n",
      "Epoch 632/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 104.8166 - mae: 105.4966 - val_loss: 1099.0065 - val_mae: 1099.6998\n",
      "Epoch 633/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 103.5886 - mae: 104.2726 - val_loss: 1114.2261 - val_mae: 1114.9192\n",
      "Epoch 634/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 101.7884 - mae: 102.4709 - val_loss: 1371.9637 - val_mae: 1372.6567\n",
      "Epoch 635/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 103.0675 - mae: 103.7515 - val_loss: 1114.3823 - val_mae: 1115.0747\n",
      "Epoch 636/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 115.3281 - mae: 116.0142 - val_loss: 1493.0872 - val_mae: 1493.7800\n",
      "Epoch 637/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 114.5666 - mae: 115.2494 - val_loss: 1283.1885 - val_mae: 1283.8815\n",
      "Epoch 638/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 108.0092 - mae: 108.6967 - val_loss: 1207.8776 - val_mae: 1208.5707\n",
      "Epoch 639/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 108.7942 - mae: 109.4815 - val_loss: 1378.6213 - val_mae: 1379.3141\n",
      "Epoch 640/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 105.3763 - mae: 106.0587 - val_loss: 1285.5448 - val_mae: 1286.2372\n",
      "Epoch 641/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 108.2098 - mae: 108.8950 - val_loss: 1264.5441 - val_mae: 1265.2362\n",
      "Epoch 642/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 110.0890 - mae: 110.7732 - val_loss: 1562.8899 - val_mae: 1563.5830\n",
      "Epoch 643/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 107.9525 - mae: 108.6345 - val_loss: 1338.9252 - val_mae: 1339.6185\n",
      "Epoch 644/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 111.0251 - mae: 111.7114 - val_loss: 1303.2053 - val_mae: 1303.8986\n",
      "Epoch 645/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 111.9708 - mae: 112.6568 - val_loss: 1244.6959 - val_mae: 1245.3890\n",
      "Epoch 646/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 119.5834 - mae: 120.2709 - val_loss: 1220.1260 - val_mae: 1220.8191\n",
      "Epoch 647/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 103.3105 - mae: 103.9904 - val_loss: 1354.2963 - val_mae: 1354.9884\n",
      "Epoch 648/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 111.7595 - mae: 112.4465 - val_loss: 1352.4803 - val_mae: 1353.1731\n",
      "Epoch 649/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 112.3631 - mae: 113.0470 - val_loss: 1407.8380 - val_mae: 1408.5312\n",
      "Epoch 650/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 125.1407 - mae: 125.8258 - val_loss: 1240.3026 - val_mae: 1240.9958\n",
      "Epoch 651/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 110.6471 - mae: 111.3322 - val_loss: 1277.7460 - val_mae: 1278.4395\n",
      "Epoch 652/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 112.5636 - mae: 113.2474 - val_loss: 1189.8712 - val_mae: 1190.5643\n",
      "Epoch 653/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 104.4652 - mae: 105.1486 - val_loss: 1194.8518 - val_mae: 1195.5448\n",
      "Epoch 654/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 108.3731 - mae: 109.0554 - val_loss: 1127.5183 - val_mae: 1128.2113\n",
      "Epoch 655/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 108.6187 - mae: 109.3045 - val_loss: 1255.8864 - val_mae: 1256.5793\n",
      "Epoch 656/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 106.3786 - mae: 107.0629 - val_loss: 1363.4967 - val_mae: 1364.1895\n",
      "Epoch 657/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 104.8359 - mae: 105.5203 - val_loss: 1261.6813 - val_mae: 1262.3745\n",
      "Epoch 658/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 116.9329 - mae: 117.6138 - val_loss: 1136.4880 - val_mae: 1137.1802\n",
      "Epoch 659/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 103.5660 - mae: 104.2508 - val_loss: 1220.7328 - val_mae: 1221.4252\n",
      "Epoch 660/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 105.8121 - mae: 106.4960 - val_loss: 1243.3479 - val_mae: 1244.0405\n",
      "Epoch 661/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 107.4388 - mae: 108.1212 - val_loss: 1320.7648 - val_mae: 1321.4580\n",
      "Epoch 662/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 117.4825 - mae: 118.1697 - val_loss: 1304.8168 - val_mae: 1305.5096\n",
      "Epoch 663/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 107.7167 - mae: 108.4016 - val_loss: 1257.1390 - val_mae: 1257.8320\n",
      "Epoch 664/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 108.6880 - mae: 109.3720 - val_loss: 1202.1298 - val_mae: 1202.8229\n",
      "Epoch 665/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 108.2672 - mae: 108.9495 - val_loss: 1224.8824 - val_mae: 1225.5757\n",
      "Epoch 666/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 113.8187 - mae: 114.5021 - val_loss: 1452.7811 - val_mae: 1453.4745\n",
      "Epoch 667/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 112.7558 - mae: 113.4401 - val_loss: 1455.6321 - val_mae: 1456.3253\n",
      "Epoch 668/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 101.9406 - mae: 102.6256 - val_loss: 1483.8781 - val_mae: 1484.5708\n",
      "Epoch 669/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 113.6279 - mae: 114.3117 - val_loss: 1421.9742 - val_mae: 1422.6672\n",
      "Epoch 670/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 108.4407 - mae: 109.1241 - val_loss: 1369.9435 - val_mae: 1370.6366\n",
      "Epoch 671/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 112.6486 - mae: 113.3329 - val_loss: 1227.0223 - val_mae: 1227.7151\n",
      "Epoch 672/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 101.3049 - mae: 101.9895 - val_loss: 1224.2230 - val_mae: 1224.9163\n",
      "Epoch 673/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 107.0770 - mae: 107.7604 - val_loss: 1215.0059 - val_mae: 1215.6989\n",
      "Epoch 674/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 107.2467 - mae: 107.9299 - val_loss: 1277.1871 - val_mae: 1277.8802\n",
      "Epoch 675/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 116.3811 - mae: 117.0648 - val_loss: 1563.5397 - val_mae: 1564.2328\n",
      "Epoch 676/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 115.0937 - mae: 115.7792 - val_loss: 1691.1488 - val_mae: 1691.8419\n",
      "Epoch 677/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 114.0335 - mae: 114.7174 - val_loss: 1399.4270 - val_mae: 1400.1201\n",
      "Epoch 678/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 114.4785 - mae: 115.1661 - val_loss: 1263.1099 - val_mae: 1263.8030\n",
      "Epoch 679/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 111.7246 - mae: 112.4109 - val_loss: 1432.4087 - val_mae: 1433.1019\n",
      "Epoch 680/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 115.3637 - mae: 116.0473 - val_loss: 1191.8979 - val_mae: 1192.5912\n",
      "Epoch 681/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 111.7379 - mae: 112.4247 - val_loss: 1437.1140 - val_mae: 1437.8071\n",
      "Epoch 682/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 113.8974 - mae: 114.5838 - val_loss: 1408.3250 - val_mae: 1409.0168\n",
      "Epoch 683/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 109.1546 - mae: 109.8405 - val_loss: 1263.1882 - val_mae: 1263.8810\n",
      "Epoch 684/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 104.0056 - mae: 104.6910 - val_loss: 1354.5002 - val_mae: 1355.1929\n",
      "Epoch 685/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 110.1273 - mae: 110.8122 - val_loss: 1420.0916 - val_mae: 1420.7845\n",
      "Epoch 686/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 113.0686 - mae: 113.7510 - val_loss: 1438.5123 - val_mae: 1439.2052\n",
      "Epoch 687/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 113.7245 - mae: 114.4072 - val_loss: 1292.3275 - val_mae: 1293.0199\n",
      "Epoch 688/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 120.5257 - mae: 121.2074 - val_loss: 1169.7010 - val_mae: 1170.3943\n",
      "Epoch 689/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 116.2712 - mae: 116.9566 - val_loss: 1334.3159 - val_mae: 1335.0090\n",
      "Epoch 690/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 112.6871 - mae: 113.3748 - val_loss: 1112.9536 - val_mae: 1113.6455\n",
      "Epoch 691/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 106.9069 - mae: 107.5901 - val_loss: 1231.2515 - val_mae: 1231.9446\n",
      "Epoch 692/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 109.5783 - mae: 110.2622 - val_loss: 1397.0043 - val_mae: 1397.6976\n",
      "Epoch 693/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 111.2929 - mae: 111.9756 - val_loss: 1202.2213 - val_mae: 1202.9143\n",
      "Epoch 694/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 110.6745 - mae: 111.3571 - val_loss: 1415.4600 - val_mae: 1416.1532\n",
      "Epoch 695/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 109.5438 - mae: 110.2289 - val_loss: 1197.5680 - val_mae: 1198.2612\n",
      "Epoch 696/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 102.3458 - mae: 103.0319 - val_loss: 1162.6957 - val_mae: 1163.3889\n",
      "Epoch 697/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 105.9110 - mae: 106.5926 - val_loss: 1309.0885 - val_mae: 1309.7817\n",
      "Epoch 698/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 107.0776 - mae: 107.7625 - val_loss: 1606.2010 - val_mae: 1606.8943\n",
      "Epoch 699/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 109.8762 - mae: 110.5627 - val_loss: 1515.8436 - val_mae: 1516.5366\n",
      "Epoch 700/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 118.2639 - mae: 118.9491 - val_loss: 1186.3801 - val_mae: 1187.0726\n",
      "Epoch 701/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 106.9604 - mae: 107.6455 - val_loss: 1244.0291 - val_mae: 1244.7217\n",
      "Epoch 702/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 107.5294 - mae: 108.2136 - val_loss: 1222.4950 - val_mae: 1223.1882\n",
      "Epoch 703/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 100.8796 - mae: 101.5629 - val_loss: 1513.8053 - val_mae: 1514.4984\n",
      "Epoch 704/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 102.9120 - mae: 103.5958 - val_loss: 1286.1299 - val_mae: 1286.8230\n",
      "Epoch 705/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 104.3629 - mae: 105.0493 - val_loss: 1110.2474 - val_mae: 1110.9407\n",
      "Epoch 706/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 129.0840 - mae: 129.7695 - val_loss: 1506.3059 - val_mae: 1506.9991\n",
      "Epoch 707/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 115.0431 - mae: 115.7295 - val_loss: 1246.9335 - val_mae: 1247.6259\n",
      "Epoch 708/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 109.8587 - mae: 110.5446 - val_loss: 1229.5419 - val_mae: 1230.2345\n",
      "Epoch 709/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 109.1679 - mae: 109.8528 - val_loss: 1528.6029 - val_mae: 1529.2959\n",
      "Epoch 710/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 98.5014 - mae: 99.1817 - val_loss: 1400.0214 - val_mae: 1400.7146\n",
      "Epoch 711/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 108.7856 - mae: 109.4679 - val_loss: 1324.2179 - val_mae: 1324.9109\n",
      "Epoch 712/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 110.3053 - mae: 110.9916 - val_loss: 1442.2744 - val_mae: 1442.9675\n",
      "Epoch 713/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 113.7434 - mae: 114.4291 - val_loss: 1354.9369 - val_mae: 1355.6300\n",
      "Epoch 714/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 104.0140 - mae: 104.6982 - val_loss: 1092.1976 - val_mae: 1092.8894\n",
      "Epoch 715/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 106.9213 - mae: 107.6058 - val_loss: 1212.5658 - val_mae: 1213.2589\n",
      "Epoch 716/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 107.8320 - mae: 108.5146 - val_loss: 1296.9124 - val_mae: 1297.6052\n",
      "Epoch 717/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 105.5546 - mae: 106.2381 - val_loss: 1345.3101 - val_mae: 1346.0029\n",
      "Epoch 718/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 102.1550 - mae: 102.8411 - val_loss: 1221.5103 - val_mae: 1222.2036\n",
      "Epoch 719/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 102.2445 - mae: 102.9292 - val_loss: 1580.6316 - val_mae: 1581.3243\n",
      "Epoch 720/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 121.4725 - mae: 122.1573 - val_loss: 1383.0060 - val_mae: 1383.6991\n",
      "Epoch 721/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 109.3249 - mae: 110.0073 - val_loss: 1396.9539 - val_mae: 1397.6470\n",
      "Epoch 722/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 104.4142 - mae: 105.0972 - val_loss: 1140.7872 - val_mae: 1141.4802\n",
      "Epoch 723/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 99.0683 - mae: 99.7488 - val_loss: 1172.5872 - val_mae: 1173.2804\n",
      "Epoch 724/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 106.8487 - mae: 107.5341 - val_loss: 1026.0382 - val_mae: 1026.7295\n",
      "Epoch 725/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 112.9545 - mae: 113.6376 - val_loss: 1312.6279 - val_mae: 1313.3210\n",
      "Epoch 726/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 108.1373 - mae: 108.8201 - val_loss: 1055.1833 - val_mae: 1055.8765\n",
      "Epoch 727/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 111.6297 - mae: 112.3149 - val_loss: 1354.6527 - val_mae: 1355.3458\n",
      "Epoch 728/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 113.9404 - mae: 114.6255 - val_loss: 1240.5121 - val_mae: 1241.2056\n",
      "Epoch 729/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 136.2547 - mae: 136.9424 - val_loss: 1284.1704 - val_mae: 1284.8635\n",
      "Epoch 730/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 99.8811 - mae: 100.5662 - val_loss: 1364.1899 - val_mae: 1364.8831\n",
      "Epoch 731/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 115.5703 - mae: 116.2538 - val_loss: 994.8152 - val_mae: 995.5053\n",
      "Epoch 732/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 115.9352 - mae: 116.6193 - val_loss: 1082.2582 - val_mae: 1082.9513\n",
      "Epoch 733/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 104.7454 - mae: 105.4308 - val_loss: 1298.5760 - val_mae: 1299.2692\n",
      "Epoch 734/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 110.4262 - mae: 111.1078 - val_loss: 1190.8843 - val_mae: 1191.5774\n",
      "Epoch 735/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 102.4859 - mae: 103.1676 - val_loss: 1250.4077 - val_mae: 1251.1006\n",
      "Epoch 736/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 105.5451 - mae: 106.2257 - val_loss: 1488.2906 - val_mae: 1488.9834\n",
      "Epoch 737/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 112.5090 - mae: 113.1961 - val_loss: 1230.1188 - val_mae: 1230.8105\n",
      "Epoch 738/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 111.8116 - mae: 112.4945 - val_loss: 1113.5345 - val_mae: 1114.2277\n",
      "Epoch 739/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 108.4695 - mae: 109.1564 - val_loss: 1283.2362 - val_mae: 1283.9294\n",
      "Epoch 740/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 112.2826 - mae: 112.9678 - val_loss: 1261.1761 - val_mae: 1261.8693\n",
      "Epoch 741/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 98.8471 - mae: 99.5313 - val_loss: 1259.6587 - val_mae: 1260.3519\n",
      "Epoch 742/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 103.7923 - mae: 104.4745 - val_loss: 1375.9976 - val_mae: 1376.6896\n",
      "Epoch 743/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 99.8844 - mae: 100.5669 - val_loss: 1246.7671 - val_mae: 1247.4602\n",
      "Epoch 744/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 102.2912 - mae: 102.9770 - val_loss: 1425.8732 - val_mae: 1426.5663\n",
      "Epoch 745/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 105.7532 - mae: 106.4390 - val_loss: 1321.6370 - val_mae: 1322.3303\n",
      "Epoch 746/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 114.1098 - mae: 114.7974 - val_loss: 1265.6743 - val_mae: 1266.3669\n",
      "Epoch 747/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 113.3063 - mae: 113.9936 - val_loss: 1129.6349 - val_mae: 1130.3281\n",
      "Epoch 748/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 110.5754 - mae: 111.2601 - val_loss: 1208.2645 - val_mae: 1208.9576\n",
      "Epoch 749/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 112.7387 - mae: 113.4238 - val_loss: 1068.8447 - val_mae: 1069.5380\n",
      "Epoch 750/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 104.0660 - mae: 104.7510 - val_loss: 1400.4449 - val_mae: 1401.1384\n",
      "Epoch 751/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 115.8734 - mae: 116.5581 - val_loss: 1199.9519 - val_mae: 1200.6450\n",
      "Epoch 752/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 103.5423 - mae: 104.2265 - val_loss: 1277.3115 - val_mae: 1278.0049\n",
      "Epoch 753/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 114.5874 - mae: 115.2733 - val_loss: 1176.2443 - val_mae: 1176.9375\n",
      "Epoch 754/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 116.4905 - mae: 117.1761 - val_loss: 1029.2467 - val_mae: 1029.9381\n",
      "Epoch 755/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 110.2769 - mae: 110.9622 - val_loss: 1275.6719 - val_mae: 1276.3645\n",
      "Epoch 756/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 103.3376 - mae: 104.0218 - val_loss: 1391.0289 - val_mae: 1391.7212\n",
      "Epoch 757/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 109.1199 - mae: 109.8049 - val_loss: 1259.5067 - val_mae: 1260.1998\n",
      "Epoch 758/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 108.2501 - mae: 108.9333 - val_loss: 1110.5811 - val_mae: 1111.2742\n",
      "Epoch 759/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 106.7271 - mae: 107.4110 - val_loss: 1127.5607 - val_mae: 1128.2537\n",
      "Epoch 760/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 112.1474 - mae: 112.8296 - val_loss: 1497.6511 - val_mae: 1498.3441\n",
      "Epoch 761/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 103.8628 - mae: 104.5460 - val_loss: 1357.4551 - val_mae: 1358.1479\n",
      "Epoch 762/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 103.2284 - mae: 103.9137 - val_loss: 1270.3062 - val_mae: 1270.9994\n",
      "Epoch 763/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 98.8368 - mae: 99.5151 - val_loss: 1436.1552 - val_mae: 1436.8483\n",
      "Epoch 764/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 100.6584 - mae: 101.3443 - val_loss: 1192.6973 - val_mae: 1193.3905\n",
      "Epoch 765/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 99.2703 - mae: 99.9539 - val_loss: 1241.5188 - val_mae: 1242.2113\n",
      "Epoch 766/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 104.6867 - mae: 105.3703 - val_loss: 1401.1121 - val_mae: 1401.8046\n",
      "Epoch 767/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 108.8917 - mae: 109.5792 - val_loss: 1598.5752 - val_mae: 1599.2684\n",
      "Epoch 768/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 118.1120 - mae: 118.7979 - val_loss: 1494.4985 - val_mae: 1495.1917\n",
      "Epoch 769/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 106.5360 - mae: 107.2202 - val_loss: 1172.3411 - val_mae: 1173.0342\n",
      "Epoch 770/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 114.2392 - mae: 114.9239 - val_loss: 1279.5781 - val_mae: 1280.2712\n",
      "Epoch 771/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 103.2342 - mae: 103.9194 - val_loss: 1409.2369 - val_mae: 1409.9303\n",
      "Epoch 772/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 106.6464 - mae: 107.3338 - val_loss: 1330.5664 - val_mae: 1331.2594\n",
      "Epoch 773/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 106.0391 - mae: 106.7244 - val_loss: 1271.5836 - val_mae: 1272.2755\n",
      "Epoch 774/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 101.7547 - mae: 102.4377 - val_loss: 1241.3593 - val_mae: 1242.0526\n",
      "Epoch 775/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 108.1178 - mae: 108.8034 - val_loss: 1438.1803 - val_mae: 1438.8730\n",
      "Epoch 776/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 106.0317 - mae: 106.7173 - val_loss: 1365.9264 - val_mae: 1366.6195\n",
      "Epoch 777/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 114.9881 - mae: 115.6733 - val_loss: 1219.7870 - val_mae: 1220.4792\n",
      "Epoch 778/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 106.6360 - mae: 107.3202 - val_loss: 1499.0046 - val_mae: 1499.6954\n",
      "Epoch 779/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 107.6294 - mae: 108.3141 - val_loss: 1308.8405 - val_mae: 1309.5336\n",
      "Epoch 780/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 104.2375 - mae: 104.9237 - val_loss: 1365.1632 - val_mae: 1365.8551\n",
      "Epoch 781/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 111.4895 - mae: 112.1744 - val_loss: 1298.1262 - val_mae: 1298.8193\n",
      "Epoch 782/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 101.2102 - mae: 101.8947 - val_loss: 1178.4572 - val_mae: 1179.1504\n",
      "Epoch 783/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 103.3646 - mae: 104.0483 - val_loss: 1130.9043 - val_mae: 1131.5972\n",
      "Epoch 784/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 110.4387 - mae: 111.1234 - val_loss: 1575.0551 - val_mae: 1575.7477\n",
      "Epoch 785/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 107.8438 - mae: 108.5273 - val_loss: 1105.3242 - val_mae: 1106.0161\n",
      "Epoch 786/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 104.6105 - mae: 105.2930 - val_loss: 1088.6570 - val_mae: 1089.3494\n",
      "Epoch 787/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 103.4633 - mae: 104.1447 - val_loss: 1292.8058 - val_mae: 1293.4988\n",
      "Epoch 788/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 100.2037 - mae: 100.8851 - val_loss: 1189.3019 - val_mae: 1189.9949\n",
      "Epoch 789/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 111.2455 - mae: 111.9330 - val_loss: 1235.3101 - val_mae: 1236.0032\n",
      "Epoch 790/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 104.8536 - mae: 105.5393 - val_loss: 1121.3998 - val_mae: 1122.0930\n",
      "Epoch 791/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 102.8536 - mae: 103.5358 - val_loss: 1301.6312 - val_mae: 1302.3239\n",
      "Epoch 792/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 103.7597 - mae: 104.4432 - val_loss: 1526.2610 - val_mae: 1526.9541\n",
      "Epoch 793/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 101.5035 - mae: 102.1855 - val_loss: 1404.1422 - val_mae: 1404.8352\n",
      "Epoch 794/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 103.6887 - mae: 104.3719 - val_loss: 1209.8784 - val_mae: 1210.5714\n",
      "Epoch 795/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 98.3132 - mae: 98.9986 - val_loss: 1178.1252 - val_mae: 1178.8185\n",
      "Epoch 796/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 94.4346 - mae: 95.1188 - val_loss: 1199.3650 - val_mae: 1200.0580\n",
      "Epoch 797/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 101.0114 - mae: 101.6953 - val_loss: 1136.2950 - val_mae: 1136.9882\n",
      "Epoch 798/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 96.2113 - mae: 96.8938 - val_loss: 1168.8138 - val_mae: 1169.5068\n",
      "Epoch 799/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 108.8173 - mae: 109.5024 - val_loss: 1258.8728 - val_mae: 1259.5653\n",
      "Epoch 800/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 102.6809 - mae: 103.3663 - val_loss: 1252.8912 - val_mae: 1253.5844\n",
      "Epoch 801/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 100.6248 - mae: 101.3059 - val_loss: 1337.3212 - val_mae: 1338.0143\n",
      "Epoch 802/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 110.0074 - mae: 110.6878 - val_loss: 1104.8861 - val_mae: 1105.5790\n",
      "Epoch 803/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 118.2435 - mae: 118.9290 - val_loss: 1396.5422 - val_mae: 1397.2352\n",
      "Epoch 804/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 103.6122 - mae: 104.2967 - val_loss: 1361.5437 - val_mae: 1362.2367\n",
      "Epoch 805/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 99.1663 - mae: 99.8499 - val_loss: 1380.8545 - val_mae: 1381.5477\n",
      "Epoch 806/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 101.6249 - mae: 102.3057 - val_loss: 1301.6459 - val_mae: 1302.3390\n",
      "Epoch 807/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 111.8282 - mae: 112.5103 - val_loss: 1121.8336 - val_mae: 1122.5269\n",
      "Epoch 808/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 103.3593 - mae: 104.0442 - val_loss: 1161.8728 - val_mae: 1162.5660\n",
      "Epoch 809/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 103.3592 - mae: 104.0418 - val_loss: 1802.1937 - val_mae: 1802.8870\n",
      "Epoch 810/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 120.0590 - mae: 120.7458 - val_loss: 1155.3417 - val_mae: 1156.0347\n",
      "Epoch 811/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 112.8534 - mae: 113.5369 - val_loss: 1282.2343 - val_mae: 1282.9261\n",
      "Epoch 812/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 110.4682 - mae: 111.1507 - val_loss: 1396.0767 - val_mae: 1396.7697\n",
      "Epoch 813/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 105.6219 - mae: 106.3062 - val_loss: 1187.6444 - val_mae: 1188.3376\n",
      "Epoch 814/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 100.7867 - mae: 101.4705 - val_loss: 1191.1263 - val_mae: 1191.8196\n",
      "Epoch 815/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 101.2600 - mae: 101.9445 - val_loss: 1225.8412 - val_mae: 1226.5342\n",
      "Epoch 816/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 110.1102 - mae: 110.7931 - val_loss: 1393.0426 - val_mae: 1393.7357\n",
      "Epoch 817/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 101.7851 - mae: 102.4674 - val_loss: 1217.5968 - val_mae: 1218.2900\n",
      "Epoch 818/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 101.3686 - mae: 102.0514 - val_loss: 1555.8079 - val_mae: 1556.5013\n",
      "Epoch 819/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 108.1186 - mae: 108.8030 - val_loss: 1274.1046 - val_mae: 1274.7977\n",
      "Epoch 820/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 101.4731 - mae: 102.1542 - val_loss: 1282.5336 - val_mae: 1283.2268\n",
      "Epoch 821/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 98.1743 - mae: 98.8548 - val_loss: 1326.2678 - val_mae: 1326.9609\n",
      "Epoch 822/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 96.7289 - mae: 97.4124 - val_loss: 1306.8097 - val_mae: 1307.5029\n",
      "Epoch 823/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 97.8430 - mae: 98.5264 - val_loss: 1377.8522 - val_mae: 1378.5450\n",
      "Epoch 824/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 101.9506 - mae: 102.6356 - val_loss: 1619.4034 - val_mae: 1620.0959\n",
      "Epoch 825/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 101.7314 - mae: 102.4118 - val_loss: 1480.6903 - val_mae: 1481.3837\n",
      "Epoch 826/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 105.3105 - mae: 105.9951 - val_loss: 1186.8770 - val_mae: 1187.5699\n",
      "Epoch 827/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 103.3051 - mae: 103.9919 - val_loss: 1660.5647 - val_mae: 1661.2578\n",
      "Epoch 828/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 113.3433 - mae: 114.0315 - val_loss: 1200.8046 - val_mae: 1201.4972\n",
      "Epoch 829/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 109.1843 - mae: 109.8676 - val_loss: 1181.9332 - val_mae: 1182.6257\n",
      "Epoch 830/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 122.4378 - mae: 123.1222 - val_loss: 1087.7456 - val_mae: 1088.4358\n",
      "Epoch 831/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 101.3846 - mae: 102.0694 - val_loss: 1408.0409 - val_mae: 1408.7340\n",
      "Epoch 832/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 104.4085 - mae: 105.0935 - val_loss: 1432.8951 - val_mae: 1433.5878\n",
      "Epoch 833/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 105.2399 - mae: 105.9253 - val_loss: 1278.3188 - val_mae: 1279.0121\n",
      "Epoch 834/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 109.0994 - mae: 109.7837 - val_loss: 1367.9790 - val_mae: 1368.6721\n",
      "Epoch 835/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 100.3763 - mae: 101.0572 - val_loss: 1017.6870 - val_mae: 1018.3796\n",
      "Epoch 836/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 113.0208 - mae: 113.7053 - val_loss: 1199.9950 - val_mae: 1200.6879\n",
      "Epoch 837/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 98.8591 - mae: 99.5424 - val_loss: 1300.4260 - val_mae: 1301.1187\n",
      "Epoch 838/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 106.6415 - mae: 107.3271 - val_loss: 1384.8544 - val_mae: 1385.5474\n",
      "Epoch 839/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 121.1737 - mae: 121.8608 - val_loss: 1142.8376 - val_mae: 1143.5309\n",
      "Epoch 840/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 104.7675 - mae: 105.4506 - val_loss: 1238.6578 - val_mae: 1239.3507\n",
      "Epoch 841/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 111.3808 - mae: 112.0619 - val_loss: 1255.7328 - val_mae: 1256.4260\n",
      "Epoch 842/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 106.2563 - mae: 106.9392 - val_loss: 1187.1639 - val_mae: 1187.8564\n",
      "Epoch 843/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 106.1406 - mae: 106.8261 - val_loss: 1371.4888 - val_mae: 1372.1820\n",
      "Epoch 844/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 100.3709 - mae: 101.0553 - val_loss: 1170.7826 - val_mae: 1171.4747\n",
      "Epoch 845/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 102.2446 - mae: 102.9291 - val_loss: 1383.0514 - val_mae: 1383.7443\n",
      "Epoch 846/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 102.7492 - mae: 103.4367 - val_loss: 1254.2572 - val_mae: 1254.9503\n",
      "Epoch 847/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 101.3031 - mae: 101.9833 - val_loss: 1260.6666 - val_mae: 1261.3597\n",
      "Epoch 848/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 102.8560 - mae: 103.5366 - val_loss: 1126.5083 - val_mae: 1127.2017\n",
      "Epoch 849/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 97.6106 - mae: 98.2938 - val_loss: 1337.1298 - val_mae: 1337.8224\n",
      "Epoch 850/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 100.4843 - mae: 101.1675 - val_loss: 1415.5332 - val_mae: 1416.2263\n",
      "Epoch 851/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 101.1383 - mae: 101.8220 - val_loss: 1388.2454 - val_mae: 1388.9385\n",
      "Epoch 852/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 105.5859 - mae: 106.2727 - val_loss: 1383.1848 - val_mae: 1383.8779\n",
      "Epoch 853/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 126.7480 - mae: 127.4344 - val_loss: 1012.3012 - val_mae: 1012.9933\n",
      "Epoch 854/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 126.0592 - mae: 126.7461 - val_loss: 1390.2827 - val_mae: 1390.9760\n",
      "Epoch 855/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 110.6422 - mae: 111.3290 - val_loss: 1363.8060 - val_mae: 1364.4982\n",
      "Epoch 856/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 109.9938 - mae: 110.6788 - val_loss: 1461.0719 - val_mae: 1461.7643\n",
      "Epoch 857/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 99.3084 - mae: 99.9932 - val_loss: 1133.1450 - val_mae: 1133.8383\n",
      "Epoch 858/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 100.3689 - mae: 101.0515 - val_loss: 1364.0874 - val_mae: 1364.7806\n",
      "Epoch 859/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 102.3399 - mae: 103.0251 - val_loss: 1266.1155 - val_mae: 1266.8087\n",
      "Epoch 860/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 110.5209 - mae: 111.2057 - val_loss: 982.9282 - val_mae: 983.6215\n",
      "Epoch 861/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 103.3072 - mae: 103.9895 - val_loss: 1374.6548 - val_mae: 1375.3480\n",
      "Epoch 862/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 110.5359 - mae: 111.2240 - val_loss: 1242.3065 - val_mae: 1242.9996\n",
      "Epoch 863/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 102.1128 - mae: 102.7937 - val_loss: 1422.9108 - val_mae: 1423.6040\n",
      "Epoch 864/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 98.3890 - mae: 99.0743 - val_loss: 1146.2939 - val_mae: 1146.9872\n",
      "Epoch 865/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 112.8854 - mae: 113.5695 - val_loss: 1197.6083 - val_mae: 1198.3014\n",
      "Epoch 866/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 110.2094 - mae: 110.8929 - val_loss: 1023.2899 - val_mae: 1023.9831\n",
      "Epoch 867/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 108.3237 - mae: 109.0069 - val_loss: 1204.5864 - val_mae: 1205.2795\n",
      "Epoch 868/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 103.8005 - mae: 104.4854 - val_loss: 1066.9736 - val_mae: 1067.6669\n",
      "Epoch 869/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 103.8239 - mae: 104.5093 - val_loss: 1675.7885 - val_mae: 1676.4814\n",
      "Epoch 870/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 104.8540 - mae: 105.5375 - val_loss: 1229.0160 - val_mae: 1229.7091\n",
      "Epoch 871/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 103.6406 - mae: 104.3221 - val_loss: 1289.1260 - val_mae: 1289.8190\n",
      "Epoch 872/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 96.0991 - mae: 96.7803 - val_loss: 1471.0387 - val_mae: 1471.7319\n",
      "Epoch 873/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 115.7535 - mae: 116.4387 - val_loss: 1443.9290 - val_mae: 1444.6219\n",
      "Epoch 874/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 102.6561 - mae: 103.3372 - val_loss: 1327.5760 - val_mae: 1328.2684\n",
      "Epoch 875/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 105.1501 - mae: 105.8337 - val_loss: 1194.4410 - val_mae: 1195.1342\n",
      "Epoch 876/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 103.0147 - mae: 103.7015 - val_loss: 1123.9589 - val_mae: 1124.6503\n",
      "Epoch 877/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 97.7352 - mae: 98.4206 - val_loss: 1273.4487 - val_mae: 1274.1410\n",
      "Epoch 878/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 99.6003 - mae: 100.2823 - val_loss: 1179.1217 - val_mae: 1179.8145\n",
      "Epoch 879/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 101.4440 - mae: 102.1281 - val_loss: 1456.0208 - val_mae: 1456.7139\n",
      "Epoch 880/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 104.4978 - mae: 105.1821 - val_loss: 1306.8835 - val_mae: 1307.5768\n",
      "Epoch 881/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 119.8468 - mae: 120.5322 - val_loss: 1500.9025 - val_mae: 1501.5955\n",
      "Epoch 882/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 108.5773 - mae: 109.2582 - val_loss: 1114.4410 - val_mae: 1115.1332\n",
      "Epoch 883/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 102.1710 - mae: 102.8536 - val_loss: 1360.0875 - val_mae: 1360.7799\n",
      "Epoch 884/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 113.0745 - mae: 113.7578 - val_loss: 1206.2550 - val_mae: 1206.9474\n",
      "Epoch 885/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 117.2005 - mae: 117.8847 - val_loss: 1484.2369 - val_mae: 1484.9303\n",
      "Epoch 886/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 100.2251 - mae: 100.9064 - val_loss: 1164.3173 - val_mae: 1165.0103\n",
      "Epoch 887/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 100.9004 - mae: 101.5857 - val_loss: 1356.3717 - val_mae: 1357.0636\n",
      "Epoch 888/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 100.2758 - mae: 100.9573 - val_loss: 1356.3922 - val_mae: 1357.0853\n",
      "Epoch 889/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 112.1222 - mae: 112.8032 - val_loss: 1228.6034 - val_mae: 1229.2965\n",
      "Epoch 890/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 99.2542 - mae: 99.9381 - val_loss: 1287.7188 - val_mae: 1288.4120\n",
      "Epoch 891/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 97.9249 - mae: 98.6072 - val_loss: 1279.2786 - val_mae: 1279.9718\n",
      "Epoch 892/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 98.7174 - mae: 99.3973 - val_loss: 1360.0311 - val_mae: 1360.7238\n",
      "Epoch 893/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 98.4724 - mae: 99.1564 - val_loss: 1311.2134 - val_mae: 1311.9066\n",
      "Epoch 894/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 105.7148 - mae: 106.3962 - val_loss: 1167.5919 - val_mae: 1168.2847\n",
      "Epoch 895/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 95.6565 - mae: 96.3420 - val_loss: 1291.6455 - val_mae: 1292.3387\n",
      "Epoch 896/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 95.9515 - mae: 96.6333 - val_loss: 1478.7720 - val_mae: 1479.4652\n",
      "Epoch 897/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 100.3349 - mae: 101.0188 - val_loss: 1101.5544 - val_mae: 1102.2474\n",
      "Epoch 898/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 97.9598 - mae: 98.6452 - val_loss: 1359.5630 - val_mae: 1360.2562\n",
      "Epoch 899/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 106.0566 - mae: 106.7419 - val_loss: 1247.4281 - val_mae: 1248.1215\n",
      "Epoch 900/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 99.8199 - mae: 100.5035 - val_loss: 1099.9475 - val_mae: 1100.6407\n",
      "Epoch 901/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 105.3112 - mae: 105.9954 - val_loss: 1329.3292 - val_mae: 1330.0222\n",
      "Epoch 902/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 104.5851 - mae: 105.2700 - val_loss: 1188.4417 - val_mae: 1189.1343\n",
      "Epoch 903/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 107.3573 - mae: 108.0425 - val_loss: 1068.7755 - val_mae: 1069.4679\n",
      "Epoch 904/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 111.7523 - mae: 112.4381 - val_loss: 1045.8549 - val_mae: 1046.5472\n",
      "Epoch 905/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 101.7929 - mae: 102.4776 - val_loss: 1129.4956 - val_mae: 1130.1886\n",
      "Epoch 906/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 98.9682 - mae: 99.6528 - val_loss: 1424.7574 - val_mae: 1425.4507\n",
      "Epoch 907/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 98.6375 - mae: 99.3245 - val_loss: 1425.6523 - val_mae: 1426.3452\n",
      "Epoch 908/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 98.4749 - mae: 99.1545 - val_loss: 1300.4310 - val_mae: 1301.1243\n",
      "Epoch 909/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 102.9223 - mae: 103.6064 - val_loss: 1308.2739 - val_mae: 1308.9670\n",
      "Epoch 910/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 98.0217 - mae: 98.7059 - val_loss: 1158.3489 - val_mae: 1159.0422\n",
      "Epoch 911/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 92.4756 - mae: 93.1607 - val_loss: 1139.0028 - val_mae: 1139.6954\n",
      "Epoch 912/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 111.8279 - mae: 112.5146 - val_loss: 1175.8054 - val_mae: 1176.4971\n",
      "Epoch 913/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 107.0368 - mae: 107.7210 - val_loss: 1348.9937 - val_mae: 1349.6866\n",
      "Epoch 914/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 103.4378 - mae: 104.1183 - val_loss: 1238.7222 - val_mae: 1239.4142\n",
      "Epoch 915/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 103.9018 - mae: 104.5865 - val_loss: 1219.9695 - val_mae: 1220.6627\n",
      "Epoch 916/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 101.6682 - mae: 102.3544 - val_loss: 1120.1548 - val_mae: 1120.8462\n",
      "Epoch 917/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 110.7012 - mae: 111.3887 - val_loss: 1569.2520 - val_mae: 1569.9451\n",
      "Epoch 918/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 102.7928 - mae: 103.4760 - val_loss: 1308.2361 - val_mae: 1308.9292\n",
      "Epoch 919/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 97.8910 - mae: 98.5739 - val_loss: 1311.6224 - val_mae: 1312.3154\n",
      "Epoch 920/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 97.7734 - mae: 98.4558 - val_loss: 1295.5720 - val_mae: 1296.2654\n",
      "Epoch 921/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 97.0126 - mae: 97.6960 - val_loss: 1144.0922 - val_mae: 1144.7852\n",
      "Epoch 922/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 96.6471 - mae: 97.3318 - val_loss: 1233.9974 - val_mae: 1234.6907\n",
      "Epoch 923/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 107.2760 - mae: 107.9599 - val_loss: 1436.2076 - val_mae: 1436.9009\n",
      "Epoch 924/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 105.7542 - mae: 106.4403 - val_loss: 1272.7760 - val_mae: 1273.4690\n",
      "Epoch 925/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 94.8874 - mae: 95.5712 - val_loss: 1188.3202 - val_mae: 1189.0134\n",
      "Epoch 926/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 98.7454 - mae: 99.4316 - val_loss: 1202.5852 - val_mae: 1203.2784\n",
      "Epoch 927/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 115.1509 - mae: 115.8361 - val_loss: 1287.2317 - val_mae: 1287.9246\n",
      "Epoch 928/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 107.4320 - mae: 108.1145 - val_loss: 1421.5406 - val_mae: 1422.2338\n",
      "Epoch 929/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 100.1331 - mae: 100.8158 - val_loss: 1196.4764 - val_mae: 1197.1696\n",
      "Epoch 930/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 112.5937 - mae: 113.2781 - val_loss: 1099.0239 - val_mae: 1099.7172\n",
      "Epoch 931/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 96.0730 - mae: 96.7568 - val_loss: 1302.8398 - val_mae: 1303.5330\n",
      "Epoch 932/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 103.1948 - mae: 103.8803 - val_loss: 1325.6410 - val_mae: 1326.3341\n",
      "Epoch 933/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 102.5186 - mae: 103.2034 - val_loss: 1377.6593 - val_mae: 1378.3527\n",
      "Epoch 934/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 113.2049 - mae: 113.8848 - val_loss: 1251.3579 - val_mae: 1252.0510\n",
      "Epoch 935/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 113.6392 - mae: 114.3248 - val_loss: 1568.5392 - val_mae: 1569.2323\n",
      "Epoch 936/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 110.0578 - mae: 110.7432 - val_loss: 1206.4292 - val_mae: 1207.1223\n",
      "Epoch 937/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 105.7395 - mae: 106.4230 - val_loss: 1659.8265 - val_mae: 1660.5194\n",
      "Epoch 938/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 105.1451 - mae: 105.8268 - val_loss: 1156.3010 - val_mae: 1156.9941\n",
      "Epoch 939/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 97.5896 - mae: 98.2723 - val_loss: 1367.3649 - val_mae: 1368.0579\n",
      "Epoch 940/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 104.8676 - mae: 105.5474 - val_loss: 1522.0170 - val_mae: 1522.7092\n",
      "Epoch 941/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 106.7528 - mae: 107.4387 - val_loss: 1190.7767 - val_mae: 1191.4696\n",
      "Epoch 942/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 107.4544 - mae: 108.1370 - val_loss: 1335.0474 - val_mae: 1335.7407\n",
      "Epoch 943/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 102.9215 - mae: 103.6063 - val_loss: 1216.6729 - val_mae: 1217.3657\n",
      "Epoch 944/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 100.2688 - mae: 100.9524 - val_loss: 1035.8934 - val_mae: 1036.5862\n",
      "Epoch 945/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 102.6721 - mae: 103.3536 - val_loss: 1308.4509 - val_mae: 1309.1442\n",
      "Epoch 946/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 95.1366 - mae: 95.8191 - val_loss: 1519.1119 - val_mae: 1519.8049\n",
      "Epoch 947/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 101.1656 - mae: 101.8474 - val_loss: 1388.7987 - val_mae: 1389.4919\n",
      "Epoch 948/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 97.6394 - mae: 98.3214 - val_loss: 1231.3051 - val_mae: 1231.9983\n",
      "Epoch 949/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 98.0366 - mae: 98.7197 - val_loss: 1402.2023 - val_mae: 1402.8955\n",
      "Epoch 950/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 96.6435 - mae: 97.3251 - val_loss: 1418.5635 - val_mae: 1419.2565\n",
      "Epoch 951/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 106.1945 - mae: 106.8781 - val_loss: 1123.4952 - val_mae: 1124.1884\n",
      "Epoch 952/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 100.2014 - mae: 100.8862 - val_loss: 1294.8915 - val_mae: 1295.5846\n",
      "Epoch 953/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 104.4329 - mae: 105.1163 - val_loss: 1310.3903 - val_mae: 1311.0835\n",
      "Epoch 954/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 107.1906 - mae: 107.8732 - val_loss: 1421.1096 - val_mae: 1421.8027\n",
      "Epoch 955/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 117.1245 - mae: 117.8097 - val_loss: 1147.8104 - val_mae: 1148.5033\n",
      "Epoch 956/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 104.4024 - mae: 105.0871 - val_loss: 1122.9602 - val_mae: 1123.6534\n",
      "Epoch 957/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 99.2839 - mae: 99.9676 - val_loss: 1198.9193 - val_mae: 1199.6119\n",
      "Epoch 958/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 99.8657 - mae: 100.5493 - val_loss: 1460.0281 - val_mae: 1460.7211\n",
      "Epoch 959/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 94.2629 - mae: 94.9464 - val_loss: 1331.4222 - val_mae: 1332.1154\n",
      "Epoch 960/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 110.2532 - mae: 110.9402 - val_loss: 1012.4290 - val_mae: 1013.1221\n",
      "Epoch 961/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 99.1116 - mae: 99.7972 - val_loss: 1272.8457 - val_mae: 1273.5386\n",
      "Epoch 962/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 116.6340 - mae: 117.3205 - val_loss: 1235.8280 - val_mae: 1236.5212\n",
      "Epoch 963/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 103.2417 - mae: 103.9265 - val_loss: 1274.5444 - val_mae: 1275.2377\n",
      "Epoch 964/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 99.5803 - mae: 100.2639 - val_loss: 1127.2611 - val_mae: 1127.9539\n",
      "Epoch 965/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 98.2943 - mae: 98.9771 - val_loss: 1087.9274 - val_mae: 1088.6196\n",
      "Epoch 966/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 103.1236 - mae: 103.8072 - val_loss: 1315.0684 - val_mae: 1315.7615\n",
      "Epoch 967/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 99.4054 - mae: 100.0854 - val_loss: 1239.7140 - val_mae: 1240.4071\n",
      "Epoch 968/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 101.1606 - mae: 101.8468 - val_loss: 1210.1836 - val_mae: 1210.8767\n",
      "Epoch 969/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 104.5738 - mae: 105.2523 - val_loss: 1131.1417 - val_mae: 1131.8345\n",
      "Epoch 970/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 98.9169 - mae: 99.5990 - val_loss: 1443.6415 - val_mae: 1444.3347\n",
      "Epoch 971/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 99.1809 - mae: 99.8681 - val_loss: 1375.6798 - val_mae: 1376.3730\n",
      "Epoch 972/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 105.7202 - mae: 106.4081 - val_loss: 1320.5137 - val_mae: 1321.2068\n",
      "Epoch 973/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 116.3674 - mae: 117.0551 - val_loss: 977.9158 - val_mae: 978.6089\n",
      "Epoch 974/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 111.3714 - mae: 112.0565 - val_loss: 1221.3246 - val_mae: 1222.0153\n",
      "Epoch 975/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 93.9161 - mae: 94.5997 - val_loss: 1148.0959 - val_mae: 1148.7892\n",
      "Epoch 976/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 105.2924 - mae: 105.9725 - val_loss: 1360.7870 - val_mae: 1361.4802\n",
      "Epoch 977/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 97.0829 - mae: 97.7645 - val_loss: 1286.8816 - val_mae: 1287.5748\n",
      "Epoch 978/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 101.2416 - mae: 101.9242 - val_loss: 1163.4736 - val_mae: 1164.1667\n",
      "Epoch 979/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 101.9149 - mae: 102.5982 - val_loss: 1214.8102 - val_mae: 1215.5033\n",
      "Epoch 980/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 99.0369 - mae: 99.7193 - val_loss: 1156.6765 - val_mae: 1157.3689\n",
      "Epoch 981/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 106.2446 - mae: 106.9316 - val_loss: 1241.7988 - val_mae: 1242.4918\n",
      "Epoch 982/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 95.5986 - mae: 96.2803 - val_loss: 1295.8174 - val_mae: 1296.5106\n",
      "Epoch 983/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 98.1993 - mae: 98.8821 - val_loss: 1244.0927 - val_mae: 1244.7858\n",
      "Epoch 984/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 97.0951 - mae: 97.7785 - val_loss: 1061.3662 - val_mae: 1062.0591\n",
      "Epoch 985/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 101.5473 - mae: 102.2278 - val_loss: 1272.3463 - val_mae: 1273.0394\n",
      "Epoch 986/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 97.4620 - mae: 98.1443 - val_loss: 1331.8945 - val_mae: 1332.5878\n",
      "Epoch 987/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 93.5837 - mae: 94.2682 - val_loss: 1245.2012 - val_mae: 1245.8942\n",
      "Epoch 988/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 99.2436 - mae: 99.9258 - val_loss: 1192.5916 - val_mae: 1193.2847\n",
      "Epoch 989/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 94.3884 - mae: 95.0715 - val_loss: 1213.2469 - val_mae: 1213.9397\n",
      "Epoch 990/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 94.4124 - mae: 95.0970 - val_loss: 1029.5565 - val_mae: 1030.2493\n",
      "Epoch 991/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 104.0966 - mae: 104.7790 - val_loss: 1203.9445 - val_mae: 1204.6377\n",
      "Epoch 992/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 96.8089 - mae: 97.4928 - val_loss: 1471.9076 - val_mae: 1472.5997\n",
      "Epoch 993/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 101.6838 - mae: 102.3672 - val_loss: 1586.8407 - val_mae: 1587.5338\n",
      "Epoch 994/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 103.4037 - mae: 104.0885 - val_loss: 1178.8156 - val_mae: 1179.5082\n",
      "Epoch 995/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 99.3666 - mae: 100.0504 - val_loss: 1530.4485 - val_mae: 1531.1417\n",
      "Epoch 996/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 110.1719 - mae: 110.8551 - val_loss: 1368.9938 - val_mae: 1369.6859\n",
      "Epoch 997/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 106.1693 - mae: 106.8534 - val_loss: 1335.1819 - val_mae: 1335.8750\n",
      "Epoch 998/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 98.7318 - mae: 99.4153 - val_loss: 1299.6882 - val_mae: 1300.3811\n",
      "Epoch 999/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 115.4129 - mae: 116.0990 - val_loss: 1057.7386 - val_mae: 1058.4312\n",
      "Epoch 1000/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 110.6290 - mae: 111.3128 - val_loss: 1186.5292 - val_mae: 1187.2211\n",
      "Epoch 1001/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 97.6580 - mae: 98.3423 - val_loss: 1418.7125 - val_mae: 1419.4058\n",
      "Epoch 1002/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 104.4791 - mae: 105.1640 - val_loss: 1346.7881 - val_mae: 1347.4813\n",
      "Epoch 1003/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 96.5883 - mae: 97.2700 - val_loss: 1193.6449 - val_mae: 1194.3381\n",
      "Epoch 1004/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 103.6115 - mae: 104.2939 - val_loss: 1088.6191 - val_mae: 1089.3107\n",
      "Epoch 1005/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 98.7859 - mae: 99.4663 - val_loss: 1221.2092 - val_mae: 1221.9025\n",
      "Epoch 1006/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 97.5148 - mae: 98.1995 - val_loss: 1468.1647 - val_mae: 1468.8579\n",
      "Epoch 1007/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 104.3629 - mae: 105.0503 - val_loss: 1083.0382 - val_mae: 1083.7312\n",
      "Epoch 1008/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 104.6608 - mae: 105.3462 - val_loss: 1103.3447 - val_mae: 1104.0370\n",
      "Epoch 1009/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 99.4888 - mae: 100.1732 - val_loss: 1114.0703 - val_mae: 1114.7634\n",
      "Epoch 1010/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 103.7889 - mae: 104.4735 - val_loss: 1445.2509 - val_mae: 1445.9440\n",
      "Epoch 1011/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 113.1708 - mae: 113.8552 - val_loss: 1020.4637 - val_mae: 1021.1567\n",
      "Epoch 1012/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 98.5906 - mae: 99.2769 - val_loss: 1398.2815 - val_mae: 1398.9746\n",
      "Epoch 1013/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 104.5878 - mae: 105.2733 - val_loss: 1221.3098 - val_mae: 1222.0029\n",
      "Epoch 1014/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 108.5251 - mae: 109.2117 - val_loss: 1152.9961 - val_mae: 1153.6887\n",
      "Epoch 1015/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 98.5320 - mae: 99.2134 - val_loss: 1095.5944 - val_mae: 1096.2872\n",
      "Epoch 1016/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 98.5588 - mae: 99.2439 - val_loss: 1251.8934 - val_mae: 1252.5858\n",
      "Epoch 1017/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 99.8468 - mae: 100.5302 - val_loss: 1057.6304 - val_mae: 1058.3235\n",
      "Epoch 1018/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 101.1121 - mae: 101.7979 - val_loss: 1226.6605 - val_mae: 1227.3538\n",
      "Epoch 1019/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 97.1771 - mae: 97.8595 - val_loss: 1245.3409 - val_mae: 1246.0341\n",
      "Epoch 1020/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 97.1327 - mae: 97.8142 - val_loss: 1251.2865 - val_mae: 1251.9796\n",
      "Epoch 1021/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 93.8607 - mae: 94.5463 - val_loss: 1185.0793 - val_mae: 1185.7723\n",
      "Epoch 1022/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 94.4111 - mae: 95.0937 - val_loss: 1269.7582 - val_mae: 1270.4514\n",
      "Epoch 1023/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 92.0199 - mae: 92.7022 - val_loss: 1170.9536 - val_mae: 1171.6455\n",
      "Epoch 1024/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 92.4344 - mae: 93.1126 - val_loss: 1136.3080 - val_mae: 1136.9994\n",
      "Epoch 1025/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 100.7277 - mae: 101.4085 - val_loss: 1392.9193 - val_mae: 1393.6124\n",
      "Epoch 1026/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 99.2989 - mae: 99.9797 - val_loss: 1226.5822 - val_mae: 1227.2753\n",
      "Epoch 1027/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 94.4585 - mae: 95.1434 - val_loss: 1303.7722 - val_mae: 1304.4655\n",
      "Epoch 1028/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 101.5279 - mae: 102.2101 - val_loss: 1059.7489 - val_mae: 1060.4414\n",
      "Epoch 1029/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 94.0155 - mae: 94.6983 - val_loss: 1437.8455 - val_mae: 1438.5388\n",
      "Epoch 1030/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 116.5829 - mae: 117.2683 - val_loss: 1192.5658 - val_mae: 1193.2579\n",
      "Epoch 1031/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 101.7807 - mae: 102.4652 - val_loss: 1147.8148 - val_mae: 1148.5068\n",
      "Epoch 1032/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 98.4533 - mae: 99.1344 - val_loss: 1448.5192 - val_mae: 1449.2125\n",
      "Epoch 1033/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 98.6814 - mae: 99.3628 - val_loss: 1225.5596 - val_mae: 1226.2528\n",
      "Epoch 1034/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 95.4082 - mae: 96.0918 - val_loss: 1088.5294 - val_mae: 1089.2223\n",
      "Epoch 1035/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 110.3796 - mae: 111.0629 - val_loss: 1168.6909 - val_mae: 1169.3842\n",
      "Epoch 1036/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 99.5840 - mae: 100.2695 - val_loss: 1231.0256 - val_mae: 1231.7188\n",
      "Epoch 1037/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 97.4557 - mae: 98.1408 - val_loss: 1147.6318 - val_mae: 1148.3250\n",
      "Epoch 1038/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 95.5901 - mae: 96.2719 - val_loss: 1454.5483 - val_mae: 1455.2415\n",
      "Epoch 1039/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 96.1897 - mae: 96.8703 - val_loss: 1167.7285 - val_mae: 1168.4215\n",
      "Epoch 1040/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 99.2493 - mae: 99.9324 - val_loss: 1250.0717 - val_mae: 1250.7648\n",
      "Epoch 1041/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 96.3618 - mae: 97.0449 - val_loss: 1187.6611 - val_mae: 1188.3534\n",
      "Epoch 1042/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 104.6681 - mae: 105.3536 - val_loss: 1111.8538 - val_mae: 1112.5468\n",
      "Epoch 1043/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 99.9390 - mae: 100.6236 - val_loss: 1401.5093 - val_mae: 1402.2023\n",
      "Epoch 1044/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 98.2496 - mae: 98.9319 - val_loss: 1189.4642 - val_mae: 1190.1571\n",
      "Epoch 1045/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 105.9189 - mae: 106.6017 - val_loss: 1399.8945 - val_mae: 1400.5861\n",
      "Epoch 1046/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 111.5990 - mae: 112.2837 - val_loss: 1316.0092 - val_mae: 1316.7009\n",
      "Epoch 1047/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 96.2707 - mae: 96.9533 - val_loss: 1169.8942 - val_mae: 1170.5872\n",
      "Epoch 1048/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 102.1900 - mae: 102.8709 - val_loss: 1148.8943 - val_mae: 1149.5876\n",
      "Epoch 1049/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 99.8540 - mae: 100.5410 - val_loss: 1190.0511 - val_mae: 1190.7443\n",
      "Epoch 1050/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 97.8663 - mae: 98.5506 - val_loss: 1083.9792 - val_mae: 1084.6721\n",
      "Epoch 1051/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 103.5667 - mae: 104.2521 - val_loss: 1175.6262 - val_mae: 1176.3196\n",
      "Epoch 1052/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 98.6428 - mae: 99.3263 - val_loss: 1011.7169 - val_mae: 1012.4092\n",
      "Epoch 1053/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 109.7820 - mae: 110.4693 - val_loss: 1324.1071 - val_mae: 1324.7993\n",
      "Epoch 1054/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 95.1175 - mae: 95.8004 - val_loss: 1227.7866 - val_mae: 1228.4797\n",
      "Epoch 1055/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 92.6705 - mae: 93.3517 - val_loss: 1406.3698 - val_mae: 1407.0630\n",
      "Epoch 1056/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 105.9371 - mae: 106.6230 - val_loss: 1026.0245 - val_mae: 1026.7177\n",
      "Epoch 1057/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 111.8926 - mae: 112.5773 - val_loss: 1232.9945 - val_mae: 1233.6877\n",
      "Epoch 1058/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 103.1030 - mae: 103.7843 - val_loss: 1105.0259 - val_mae: 1105.7191\n",
      "Epoch 1059/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 94.8431 - mae: 95.5264 - val_loss: 1149.7555 - val_mae: 1150.4478\n",
      "Epoch 1060/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 91.1598 - mae: 91.8417 - val_loss: 1281.7484 - val_mae: 1282.4415\n",
      "Epoch 1061/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 98.6242 - mae: 99.3073 - val_loss: 1384.7909 - val_mae: 1385.4839\n",
      "Epoch 1062/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 92.9117 - mae: 93.5941 - val_loss: 1417.6143 - val_mae: 1418.3073\n",
      "Epoch 1063/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 100.2741 - mae: 100.9586 - val_loss: 1389.5729 - val_mae: 1390.2662\n",
      "Epoch 1064/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 102.6312 - mae: 103.3134 - val_loss: 1223.3624 - val_mae: 1224.0540\n",
      "Epoch 1065/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 104.2316 - mae: 104.9139 - val_loss: 1395.0245 - val_mae: 1395.7178\n",
      "Epoch 1066/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 93.9454 - mae: 94.6286 - val_loss: 1218.9642 - val_mae: 1219.6571\n",
      "Epoch 1067/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 94.0117 - mae: 94.6904 - val_loss: 1548.9257 - val_mae: 1549.6188\n",
      "Epoch 1068/5000\n",
      "46/46 [==============================] - 2s 32ms/step - loss: 100.2998 - mae: 100.9822 - val_loss: 1374.9288 - val_mae: 1375.6217\n",
      "Epoch 1069/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 93.4136 - mae: 94.0988 - val_loss: 1278.3627 - val_mae: 1279.0559\n",
      "Epoch 1070/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 103.1127 - mae: 103.7944 - val_loss: 1273.0466 - val_mae: 1273.7400\n",
      "Epoch 1071/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 92.6389 - mae: 93.3214 - val_loss: 1368.1429 - val_mae: 1368.8361\n",
      "Epoch 1072/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 98.6436 - mae: 99.3284 - val_loss: 1248.8622 - val_mae: 1249.5554\n",
      "Epoch 1073/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 97.0698 - mae: 97.7512 - val_loss: 1049.2408 - val_mae: 1049.9341\n",
      "Epoch 1074/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 105.4756 - mae: 106.1604 - val_loss: 1222.2075 - val_mae: 1222.9008\n",
      "Epoch 1075/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 96.1718 - mae: 96.8561 - val_loss: 1095.2000 - val_mae: 1095.8932\n",
      "Epoch 1076/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 101.3204 - mae: 102.0068 - val_loss: 1333.9664 - val_mae: 1334.6593\n",
      "Epoch 1077/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 95.9287 - mae: 96.6133 - val_loss: 1235.3353 - val_mae: 1236.0281\n",
      "Epoch 1078/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 110.4855 - mae: 111.1694 - val_loss: 1368.9091 - val_mae: 1369.6018\n",
      "Epoch 1079/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 93.5645 - mae: 94.2465 - val_loss: 1002.3470 - val_mae: 1003.0394\n",
      "Epoch 1080/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 99.6489 - mae: 100.3341 - val_loss: 1055.0530 - val_mae: 1055.7450\n",
      "Epoch 1081/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 94.0850 - mae: 94.7631 - val_loss: 1394.7858 - val_mae: 1395.4790\n",
      "Epoch 1082/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 102.9451 - mae: 103.6263 - val_loss: 1401.9976 - val_mae: 1402.6907\n",
      "Epoch 1083/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 105.5943 - mae: 106.2772 - val_loss: 1352.0330 - val_mae: 1352.7261\n",
      "Epoch 1084/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 96.9633 - mae: 97.6433 - val_loss: 1430.3346 - val_mae: 1431.0278\n",
      "Epoch 1085/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 92.7016 - mae: 93.3843 - val_loss: 1230.0149 - val_mae: 1230.7080\n",
      "Epoch 1086/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 103.0838 - mae: 103.7670 - val_loss: 1075.6182 - val_mae: 1076.3109\n",
      "Epoch 1087/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 100.9824 - mae: 101.6672 - val_loss: 1302.6812 - val_mae: 1303.3743\n",
      "Epoch 1088/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 95.7722 - mae: 96.4558 - val_loss: 1215.8517 - val_mae: 1216.5438\n",
      "Epoch 1089/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 94.6128 - mae: 95.2946 - val_loss: 1294.2773 - val_mae: 1294.9705\n",
      "Epoch 1090/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 98.4294 - mae: 99.1103 - val_loss: 1249.1530 - val_mae: 1249.8461\n",
      "Epoch 1091/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 97.9189 - mae: 98.6042 - val_loss: 1115.2352 - val_mae: 1115.9285\n",
      "Epoch 1092/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 104.2005 - mae: 104.8855 - val_loss: 1267.9176 - val_mae: 1268.6107\n",
      "Epoch 1093/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 95.9023 - mae: 96.5883 - val_loss: 1120.0265 - val_mae: 1120.7188\n",
      "Epoch 1094/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 93.2116 - mae: 93.8967 - val_loss: 1132.0934 - val_mae: 1132.7855\n",
      "Epoch 1095/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 94.9195 - mae: 95.6045 - val_loss: 1126.6799 - val_mae: 1127.3732\n",
      "Epoch 1096/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 96.0051 - mae: 96.6867 - val_loss: 1167.4843 - val_mae: 1168.1775\n",
      "Epoch 1097/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 92.4858 - mae: 93.1668 - val_loss: 1444.4048 - val_mae: 1445.0980\n",
      "Epoch 1098/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 95.1222 - mae: 95.8054 - val_loss: 965.2529 - val_mae: 965.9457\n",
      "Epoch 1099/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 98.4933 - mae: 99.1748 - val_loss: 1020.9554 - val_mae: 1021.6486\n",
      "Epoch 1100/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 100.7705 - mae: 101.4552 - val_loss: 1233.4882 - val_mae: 1234.1815\n",
      "Epoch 1101/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 105.3858 - mae: 106.0676 - val_loss: 1103.8882 - val_mae: 1104.5795\n",
      "Epoch 1102/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 104.2078 - mae: 104.8938 - val_loss: 1148.2738 - val_mae: 1148.9661\n",
      "Epoch 1103/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 101.1127 - mae: 101.7955 - val_loss: 1276.6838 - val_mae: 1277.3771\n",
      "Epoch 1104/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 95.8221 - mae: 96.5067 - val_loss: 1159.4789 - val_mae: 1160.1714\n",
      "Epoch 1105/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 96.9453 - mae: 97.6259 - val_loss: 1131.4325 - val_mae: 1132.1249\n",
      "Epoch 1106/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 97.0287 - mae: 97.7155 - val_loss: 1361.2386 - val_mae: 1361.9318\n",
      "Epoch 1107/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 99.4397 - mae: 100.1250 - val_loss: 1103.1541 - val_mae: 1103.8470\n",
      "Epoch 1108/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 107.5481 - mae: 108.2312 - val_loss: 1092.8683 - val_mae: 1093.5614\n",
      "Epoch 1109/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 97.2117 - mae: 97.8947 - val_loss: 1278.4333 - val_mae: 1279.1267\n",
      "Epoch 1110/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 100.6935 - mae: 101.3739 - val_loss: 1424.0225 - val_mae: 1424.7146\n",
      "Epoch 1111/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 103.3097 - mae: 103.9890 - val_loss: 1314.0709 - val_mae: 1314.7638\n",
      "Epoch 1112/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 91.3575 - mae: 92.0382 - val_loss: 1306.9768 - val_mae: 1307.6699\n",
      "Epoch 1113/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 94.7548 - mae: 95.4360 - val_loss: 1181.8954 - val_mae: 1182.5884\n",
      "Epoch 1114/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 91.0779 - mae: 91.7593 - val_loss: 1088.1002 - val_mae: 1088.7931\n",
      "Epoch 1115/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 98.4300 - mae: 99.1173 - val_loss: 1209.0981 - val_mae: 1209.7914\n",
      "Epoch 1116/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 97.2360 - mae: 97.9189 - val_loss: 1149.0750 - val_mae: 1149.7682\n",
      "Epoch 1117/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 110.0146 - mae: 110.7033 - val_loss: 1309.6205 - val_mae: 1310.3138\n",
      "Epoch 1118/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 96.8294 - mae: 97.5105 - val_loss: 1423.2583 - val_mae: 1423.9515\n",
      "Epoch 1119/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 92.3433 - mae: 93.0291 - val_loss: 1219.9865 - val_mae: 1220.6794\n",
      "Epoch 1120/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 101.1157 - mae: 101.7965 - val_loss: 1290.5922 - val_mae: 1291.2853\n",
      "Epoch 1121/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 100.9231 - mae: 101.6078 - val_loss: 1509.6969 - val_mae: 1510.3904\n",
      "Epoch 1122/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 105.0729 - mae: 105.7583 - val_loss: 1205.2549 - val_mae: 1205.9478\n",
      "Epoch 1123/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 104.5190 - mae: 105.2030 - val_loss: 1309.2717 - val_mae: 1309.9640\n",
      "Epoch 1124/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 94.6637 - mae: 95.3483 - val_loss: 1125.2046 - val_mae: 1125.8977\n",
      "Epoch 1125/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 93.8298 - mae: 94.5136 - val_loss: 1417.5469 - val_mae: 1418.2401\n",
      "Epoch 1126/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 100.6396 - mae: 101.3249 - val_loss: 1046.6115 - val_mae: 1047.3047\n",
      "Epoch 1127/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 95.1558 - mae: 95.8382 - val_loss: 1257.9840 - val_mae: 1258.6761\n",
      "Epoch 1128/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 91.8402 - mae: 92.5229 - val_loss: 1321.1000 - val_mae: 1321.7933\n",
      "Epoch 1129/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 98.7861 - mae: 99.4697 - val_loss: 1072.2572 - val_mae: 1072.9503\n",
      "Epoch 1130/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 99.8499 - mae: 100.5301 - val_loss: 1102.8038 - val_mae: 1103.4971\n",
      "Epoch 1131/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 105.5680 - mae: 106.2509 - val_loss: 1235.3135 - val_mae: 1236.0067\n",
      "Epoch 1132/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 97.1079 - mae: 97.7930 - val_loss: 1098.8734 - val_mae: 1099.5653\n",
      "Epoch 1133/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 99.0589 - mae: 99.7461 - val_loss: 1225.3984 - val_mae: 1226.0918\n",
      "Epoch 1134/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 100.9528 - mae: 101.6349 - val_loss: 1295.3279 - val_mae: 1296.0210\n",
      "Epoch 1135/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 95.2007 - mae: 95.8875 - val_loss: 1361.9587 - val_mae: 1362.6520\n",
      "Epoch 1136/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 94.8071 - mae: 95.4913 - val_loss: 1198.0387 - val_mae: 1198.7305\n",
      "Epoch 1137/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 97.6436 - mae: 98.3233 - val_loss: 1427.6477 - val_mae: 1428.3408\n",
      "Epoch 1138/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 96.4308 - mae: 97.1149 - val_loss: 1335.6482 - val_mae: 1336.3406\n",
      "Epoch 1139/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 107.5475 - mae: 108.2327 - val_loss: 1218.9286 - val_mae: 1219.6216\n",
      "Epoch 1140/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 103.5150 - mae: 104.1989 - val_loss: 1368.4583 - val_mae: 1369.1516\n",
      "Epoch 1141/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 99.5047 - mae: 100.1893 - val_loss: 1289.7247 - val_mae: 1290.4174\n",
      "Epoch 1142/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 103.7928 - mae: 104.4759 - val_loss: 1128.2832 - val_mae: 1128.9764\n",
      "Epoch 1143/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 96.5196 - mae: 97.2066 - val_loss: 1224.3875 - val_mae: 1225.0793\n",
      "Epoch 1144/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 98.1926 - mae: 98.8774 - val_loss: 1203.3289 - val_mae: 1204.0219\n",
      "Epoch 1145/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 101.8528 - mae: 102.5374 - val_loss: 1211.6854 - val_mae: 1212.3787\n",
      "Epoch 1146/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 102.5610 - mae: 103.2457 - val_loss: 1284.8153 - val_mae: 1285.5085\n",
      "Epoch 1147/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 92.5340 - mae: 93.2189 - val_loss: 1305.0288 - val_mae: 1305.7218\n",
      "Epoch 1148/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 95.7309 - mae: 96.4122 - val_loss: 1306.5265 - val_mae: 1307.2197\n",
      "Epoch 1149/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 92.4027 - mae: 93.0844 - val_loss: 964.5048 - val_mae: 965.1967\n",
      "Epoch 1150/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 107.2485 - mae: 107.9297 - val_loss: 1313.6666 - val_mae: 1314.3596\n",
      "Epoch 1151/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 92.2703 - mae: 92.9528 - val_loss: 1342.3181 - val_mae: 1343.0111\n",
      "Epoch 1152/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 114.6208 - mae: 115.3058 - val_loss: 1113.2085 - val_mae: 1113.9017\n",
      "Epoch 1153/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 91.9206 - mae: 92.6044 - val_loss: 1198.3197 - val_mae: 1199.0129\n",
      "Epoch 1154/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 95.3857 - mae: 96.0673 - val_loss: 1122.1364 - val_mae: 1122.8295\n",
      "Epoch 1155/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 106.0895 - mae: 106.7763 - val_loss: 1153.1887 - val_mae: 1153.8816\n",
      "Epoch 1156/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 93.1300 - mae: 93.8129 - val_loss: 1091.3323 - val_mae: 1092.0248\n",
      "Epoch 1157/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 106.4723 - mae: 107.1573 - val_loss: 1437.5665 - val_mae: 1438.2595\n",
      "Epoch 1158/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 98.1678 - mae: 98.8528 - val_loss: 1263.2377 - val_mae: 1263.9308\n",
      "Epoch 1159/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 110.9658 - mae: 111.6522 - val_loss: 1361.7888 - val_mae: 1362.4818\n",
      "Epoch 1160/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 102.2622 - mae: 102.9483 - val_loss: 1485.4146 - val_mae: 1486.1075\n",
      "Epoch 1161/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 99.4618 - mae: 100.1462 - val_loss: 1254.0085 - val_mae: 1254.7019\n",
      "Epoch 1162/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 93.1694 - mae: 93.8503 - val_loss: 1255.7322 - val_mae: 1256.4250\n",
      "Epoch 1163/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 94.5668 - mae: 95.2508 - val_loss: 1281.9941 - val_mae: 1282.6865\n",
      "Epoch 1164/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 95.1472 - mae: 95.8308 - val_loss: 1158.1057 - val_mae: 1158.7988\n",
      "Epoch 1165/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 89.3273 - mae: 90.0136 - val_loss: 1224.0312 - val_mae: 1224.7242\n",
      "Epoch 1166/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 89.8762 - mae: 90.5611 - val_loss: 1248.5085 - val_mae: 1249.2018\n",
      "Epoch 1167/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 96.6627 - mae: 97.3488 - val_loss: 1309.8512 - val_mae: 1310.5444\n",
      "Epoch 1168/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 90.1763 - mae: 90.8571 - val_loss: 1143.0610 - val_mae: 1143.7542\n",
      "Epoch 1169/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 104.9505 - mae: 105.6351 - val_loss: 1167.8209 - val_mae: 1168.5142\n",
      "Epoch 1170/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 102.5260 - mae: 103.2091 - val_loss: 1189.4417 - val_mae: 1190.1348\n",
      "Epoch 1171/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 95.4249 - mae: 96.1063 - val_loss: 1479.9517 - val_mae: 1480.6447\n",
      "Epoch 1172/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 92.3933 - mae: 93.0758 - val_loss: 1229.5905 - val_mae: 1230.2827\n",
      "Epoch 1173/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 108.4746 - mae: 109.1577 - val_loss: 1433.2501 - val_mae: 1433.9420\n",
      "Epoch 1174/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 103.2296 - mae: 103.9138 - val_loss: 1239.9781 - val_mae: 1240.6714\n",
      "Epoch 1175/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 95.2101 - mae: 95.8921 - val_loss: 1179.2925 - val_mae: 1179.9855\n",
      "Epoch 1176/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 90.3941 - mae: 91.0779 - val_loss: 1277.3037 - val_mae: 1277.9968\n",
      "Epoch 1177/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 90.4516 - mae: 91.1341 - val_loss: 1312.8695 - val_mae: 1313.5620\n",
      "Epoch 1178/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 92.0664 - mae: 92.7493 - val_loss: 1208.2487 - val_mae: 1208.9407\n",
      "Epoch 1179/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 91.7437 - mae: 92.4239 - val_loss: 1238.0121 - val_mae: 1238.7053\n",
      "Epoch 1180/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 102.5490 - mae: 103.2331 - val_loss: 1335.2373 - val_mae: 1335.9305\n",
      "Epoch 1181/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 98.9197 - mae: 99.6044 - val_loss: 1175.3759 - val_mae: 1176.0691\n",
      "Epoch 1182/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 90.8506 - mae: 91.5311 - val_loss: 1322.8314 - val_mae: 1323.5248\n",
      "Epoch 1183/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 94.9526 - mae: 95.6364 - val_loss: 1234.7505 - val_mae: 1235.4437\n",
      "Epoch 1184/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 91.2970 - mae: 91.9816 - val_loss: 1195.6521 - val_mae: 1196.3452\n",
      "Epoch 1185/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 89.9931 - mae: 90.6765 - val_loss: 1207.6816 - val_mae: 1208.3746\n",
      "Epoch 1186/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 97.0754 - mae: 97.7588 - val_loss: 1311.4885 - val_mae: 1312.1816\n",
      "Epoch 1187/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 98.1420 - mae: 98.8259 - val_loss: 1442.8512 - val_mae: 1443.5438\n",
      "Epoch 1188/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 104.2173 - mae: 104.8986 - val_loss: 1235.6097 - val_mae: 1236.3031\n",
      "Epoch 1189/5000\n",
      "46/46 [==============================] - 1s 9ms/step - loss: 94.3367 - mae: 95.0210 - val_loss: 1136.7371 - val_mae: 1137.4302\n",
      "Epoch 1190/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 93.8357 - mae: 94.5208 - val_loss: 1203.2383 - val_mae: 1203.9313\n",
      "Epoch 1191/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 91.7178 - mae: 92.4020 - val_loss: 1276.5542 - val_mae: 1277.2473\n",
      "Epoch 1192/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 91.0480 - mae: 91.7279 - val_loss: 1061.0345 - val_mae: 1061.7279\n",
      "Epoch 1193/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 96.3944 - mae: 97.0778 - val_loss: 1284.9497 - val_mae: 1285.6428\n",
      "Epoch 1194/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 106.8492 - mae: 107.5344 - val_loss: 1163.9767 - val_mae: 1164.6696\n",
      "Epoch 1195/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 94.0288 - mae: 94.7150 - val_loss: 1234.2283 - val_mae: 1234.9213\n",
      "Epoch 1196/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 94.1033 - mae: 94.7851 - val_loss: 1158.4945 - val_mae: 1159.1876\n",
      "Epoch 1197/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 116.6916 - mae: 117.3776 - val_loss: 1483.2435 - val_mae: 1483.9366\n",
      "Epoch 1198/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 98.6934 - mae: 99.3788 - val_loss: 1334.0676 - val_mae: 1334.7607\n",
      "Epoch 1199/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 91.3797 - mae: 92.0632 - val_loss: 1355.5593 - val_mae: 1356.2517\n",
      "Epoch 1200/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 89.9170 - mae: 90.6011 - val_loss: 1167.3777 - val_mae: 1168.0708\n",
      "Epoch 1201/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 93.0010 - mae: 93.6826 - val_loss: 1310.4178 - val_mae: 1311.1106\n",
      "Epoch 1202/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 104.2987 - mae: 104.9841 - val_loss: 1295.0746 - val_mae: 1295.7673\n",
      "Epoch 1203/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 109.0236 - mae: 109.7045 - val_loss: 1085.1438 - val_mae: 1085.8370\n",
      "Epoch 1204/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 96.2425 - mae: 96.9256 - val_loss: 1269.4926 - val_mae: 1270.1857\n",
      "Epoch 1205/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 91.6036 - mae: 92.2859 - val_loss: 1300.5327 - val_mae: 1301.2244\n",
      "Epoch 1206/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 105.5049 - mae: 106.1908 - val_loss: 1031.9744 - val_mae: 1032.6671\n",
      "Epoch 1207/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 99.9970 - mae: 100.6783 - val_loss: 1244.8899 - val_mae: 1245.5824\n",
      "Epoch 1208/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 97.5618 - mae: 98.2474 - val_loss: 1440.3243 - val_mae: 1441.0176\n",
      "Epoch 1209/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 102.2754 - mae: 102.9582 - val_loss: 1255.1493 - val_mae: 1255.8427\n",
      "Epoch 1210/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 95.0257 - mae: 95.7088 - val_loss: 1426.3185 - val_mae: 1427.0112\n",
      "Epoch 1211/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 96.1134 - mae: 96.7948 - val_loss: 1376.3024 - val_mae: 1376.9954\n",
      "Epoch 1212/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 99.2269 - mae: 99.9096 - val_loss: 1257.2729 - val_mae: 1257.9661\n",
      "Epoch 1213/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 90.6757 - mae: 91.3570 - val_loss: 1182.4824 - val_mae: 1183.1758\n",
      "Epoch 1214/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 100.6068 - mae: 101.2910 - val_loss: 1355.1411 - val_mae: 1355.8342\n",
      "Epoch 1215/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 104.0329 - mae: 104.7190 - val_loss: 1277.9772 - val_mae: 1278.6702\n",
      "Epoch 1216/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 96.7734 - mae: 97.4575 - val_loss: 1353.9182 - val_mae: 1354.6113\n",
      "Epoch 1217/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 106.7892 - mae: 107.4752 - val_loss: 1457.3413 - val_mae: 1458.0345\n",
      "Epoch 1218/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 103.8361 - mae: 104.5193 - val_loss: 1211.9189 - val_mae: 1212.6118\n",
      "Epoch 1219/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 91.0626 - mae: 91.7463 - val_loss: 1265.4812 - val_mae: 1266.1746\n",
      "Epoch 1220/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 105.4691 - mae: 106.1529 - val_loss: 1153.5824 - val_mae: 1154.2750\n",
      "Epoch 1221/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 95.7716 - mae: 96.4567 - val_loss: 1284.6235 - val_mae: 1285.3167\n",
      "Epoch 1222/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 95.4145 - mae: 96.0996 - val_loss: 1167.8134 - val_mae: 1168.5065\n",
      "Epoch 1223/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 102.7767 - mae: 103.4613 - val_loss: 1493.6816 - val_mae: 1494.3749\n",
      "Epoch 1224/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 95.5559 - mae: 96.2364 - val_loss: 1210.8411 - val_mae: 1211.5342\n",
      "Epoch 1225/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 92.4109 - mae: 93.0953 - val_loss: 1331.1595 - val_mae: 1331.8528\n",
      "Epoch 1226/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 97.9096 - mae: 98.5937 - val_loss: 1372.6108 - val_mae: 1373.3035\n",
      "Epoch 1227/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 102.3041 - mae: 102.9874 - val_loss: 1187.6478 - val_mae: 1188.3403\n",
      "Epoch 1228/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 93.5685 - mae: 94.2504 - val_loss: 1003.8990 - val_mae: 1004.5919\n",
      "Epoch 1229/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 101.3467 - mae: 102.0310 - val_loss: 1334.7550 - val_mae: 1335.4468\n",
      "Epoch 1230/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 92.7318 - mae: 93.4152 - val_loss: 1378.2971 - val_mae: 1378.9904\n",
      "Epoch 1231/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 104.5517 - mae: 105.2354 - val_loss: 1294.2274 - val_mae: 1294.9205\n",
      "Epoch 1232/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 91.6087 - mae: 92.2906 - val_loss: 1310.5713 - val_mae: 1311.2646\n",
      "Epoch 1233/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 93.3314 - mae: 94.0126 - val_loss: 1270.3724 - val_mae: 1271.0653\n",
      "Epoch 1234/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 98.2670 - mae: 98.9534 - val_loss: 1227.2767 - val_mae: 1227.9698\n",
      "Epoch 1235/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 94.5630 - mae: 95.2446 - val_loss: 1290.2919 - val_mae: 1290.9843\n",
      "Epoch 1236/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 93.0738 - mae: 93.7550 - val_loss: 1286.5479 - val_mae: 1287.2404\n",
      "Epoch 1237/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 91.2540 - mae: 91.9390 - val_loss: 1237.4893 - val_mae: 1238.1818\n",
      "Epoch 1238/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 93.8049 - mae: 94.4885 - val_loss: 1211.6226 - val_mae: 1212.3159\n",
      "Epoch 1239/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 92.3743 - mae: 93.0581 - val_loss: 1155.1694 - val_mae: 1155.8618\n",
      "Epoch 1240/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 94.7033 - mae: 95.3888 - val_loss: 1195.8743 - val_mae: 1196.5674\n",
      "Epoch 1241/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 95.1606 - mae: 95.8415 - val_loss: 1179.9532 - val_mae: 1180.6454\n",
      "Epoch 1242/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 97.0834 - mae: 97.7653 - val_loss: 1255.1619 - val_mae: 1255.8551\n",
      "Epoch 1243/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 99.0808 - mae: 99.7634 - val_loss: 1201.3212 - val_mae: 1202.0142\n",
      "Epoch 1244/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 88.6676 - mae: 89.3512 - val_loss: 1360.8680 - val_mae: 1361.5613\n",
      "Epoch 1245/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 91.1692 - mae: 91.8502 - val_loss: 1316.5581 - val_mae: 1317.2515\n",
      "Epoch 1246/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 94.4216 - mae: 95.1041 - val_loss: 1181.2646 - val_mae: 1181.9576\n",
      "Epoch 1247/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 93.7013 - mae: 94.3850 - val_loss: 1600.9192 - val_mae: 1601.6124\n",
      "Epoch 1248/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 108.9515 - mae: 109.6360 - val_loss: 1344.6025 - val_mae: 1345.2957\n",
      "Epoch 1249/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 102.0384 - mae: 102.7199 - val_loss: 1222.5728 - val_mae: 1223.2661\n",
      "Epoch 1250/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 94.3361 - mae: 95.0217 - val_loss: 1214.3696 - val_mae: 1215.0625\n",
      "Epoch 1251/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 97.0753 - mae: 97.7601 - val_loss: 1401.8862 - val_mae: 1402.5793\n",
      "Epoch 1252/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 103.3670 - mae: 104.0502 - val_loss: 1518.2001 - val_mae: 1518.8932\n",
      "Epoch 1253/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 98.7547 - mae: 99.4380 - val_loss: 1275.3879 - val_mae: 1276.0807\n",
      "Epoch 1254/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 96.0540 - mae: 96.7348 - val_loss: 1188.8612 - val_mae: 1189.5533\n",
      "Epoch 1255/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 101.1704 - mae: 101.8547 - val_loss: 1473.5942 - val_mae: 1474.2875\n",
      "Epoch 1256/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 91.5517 - mae: 92.2338 - val_loss: 1367.5597 - val_mae: 1368.2528\n",
      "Epoch 1257/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 91.8607 - mae: 92.5406 - val_loss: 1154.8258 - val_mae: 1155.5188\n",
      "Epoch 1258/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 97.4839 - mae: 98.1658 - val_loss: 1150.3488 - val_mae: 1151.0404\n",
      "Epoch 1259/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 97.1746 - mae: 97.8584 - val_loss: 1233.1837 - val_mae: 1233.8766\n",
      "Epoch 1260/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 92.7461 - mae: 93.4266 - val_loss: 1118.0288 - val_mae: 1118.7218\n",
      "Epoch 1261/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 97.0716 - mae: 97.7533 - val_loss: 1301.2307 - val_mae: 1301.9224\n",
      "Epoch 1262/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 90.9740 - mae: 91.6560 - val_loss: 1148.0033 - val_mae: 1148.6963\n",
      "Epoch 1263/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 94.0711 - mae: 94.7549 - val_loss: 1354.3376 - val_mae: 1355.0305\n",
      "Epoch 1264/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 88.9897 - mae: 89.6726 - val_loss: 1152.0812 - val_mae: 1152.7743\n",
      "Epoch 1265/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 99.6747 - mae: 100.3571 - val_loss: 1434.2251 - val_mae: 1434.9183\n",
      "Epoch 1266/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 100.9833 - mae: 101.6660 - val_loss: 1328.0317 - val_mae: 1328.7249\n",
      "Epoch 1267/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 92.3152 - mae: 92.9971 - val_loss: 1188.3914 - val_mae: 1189.0845\n",
      "Epoch 1268/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 90.9055 - mae: 91.5900 - val_loss: 1197.0145 - val_mae: 1197.7076\n",
      "Epoch 1269/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 97.3139 - mae: 98.0001 - val_loss: 1148.2109 - val_mae: 1148.9039\n",
      "Epoch 1270/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 91.0968 - mae: 91.7781 - val_loss: 1501.1892 - val_mae: 1501.8821\n",
      "Epoch 1271/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 96.5910 - mae: 97.2728 - val_loss: 1219.9360 - val_mae: 1220.6293\n",
      "Epoch 1272/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 108.8851 - mae: 109.5686 - val_loss: 1012.1421 - val_mae: 1012.8352\n",
      "Epoch 1273/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 112.7707 - mae: 113.4590 - val_loss: 1243.3586 - val_mae: 1244.0516\n",
      "Epoch 1274/5000\n",
      "46/46 [==============================] - 2s 42ms/step - loss: 98.5363 - mae: 99.2171 - val_loss: 1266.1471 - val_mae: 1266.8405\n",
      "Epoch 1275/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 88.7871 - mae: 89.4672 - val_loss: 1287.4943 - val_mae: 1288.1874\n",
      "Epoch 1276/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 93.8729 - mae: 94.5537 - val_loss: 1202.3688 - val_mae: 1203.0610\n",
      "Epoch 1277/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 87.8077 - mae: 88.4888 - val_loss: 1446.1161 - val_mae: 1446.8091\n",
      "Epoch 1278/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 100.6532 - mae: 101.3361 - val_loss: 1115.1211 - val_mae: 1115.8145\n",
      "Epoch 1279/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 93.3843 - mae: 94.0682 - val_loss: 1168.4366 - val_mae: 1169.1299\n",
      "Epoch 1280/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 89.6558 - mae: 90.3367 - val_loss: 1217.8948 - val_mae: 1218.5880\n",
      "Epoch 1281/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 88.3888 - mae: 89.0720 - val_loss: 1206.6628 - val_mae: 1207.3561\n",
      "Epoch 1282/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 94.8788 - mae: 95.5631 - val_loss: 1214.7899 - val_mae: 1215.4823\n",
      "Epoch 1283/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 101.7399 - mae: 102.4241 - val_loss: 1281.0642 - val_mae: 1281.7574\n",
      "Epoch 1284/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 94.1493 - mae: 94.8314 - val_loss: 1208.3490 - val_mae: 1209.0424\n",
      "Epoch 1285/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 92.7348 - mae: 93.4181 - val_loss: 1228.1476 - val_mae: 1228.8408\n",
      "Epoch 1286/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 97.8801 - mae: 98.5652 - val_loss: 1097.4968 - val_mae: 1098.1886\n",
      "Epoch 1287/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 100.0405 - mae: 100.7260 - val_loss: 1279.8252 - val_mae: 1280.5184\n",
      "Epoch 1288/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 94.3017 - mae: 94.9874 - val_loss: 1211.3131 - val_mae: 1212.0062\n",
      "Epoch 1289/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 91.7869 - mae: 92.4704 - val_loss: 1405.1086 - val_mae: 1405.8016\n",
      "Epoch 1290/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 103.3232 - mae: 104.0098 - val_loss: 1306.2358 - val_mae: 1306.9286\n",
      "Epoch 1291/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 90.2606 - mae: 90.9410 - val_loss: 1210.1370 - val_mae: 1210.8301\n",
      "Epoch 1292/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 102.9286 - mae: 103.6131 - val_loss: 1433.9023 - val_mae: 1434.5956\n",
      "Epoch 1293/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 92.3692 - mae: 93.0518 - val_loss: 1362.2920 - val_mae: 1362.9845\n",
      "Epoch 1294/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 88.3039 - mae: 88.9872 - val_loss: 1218.9357 - val_mae: 1219.6292\n",
      "Epoch 1295/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 96.3031 - mae: 96.9855 - val_loss: 1160.4471 - val_mae: 1161.1396\n",
      "Epoch 1296/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 89.3413 - mae: 90.0243 - val_loss: 1249.4232 - val_mae: 1250.1155\n",
      "Epoch 1297/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 90.2045 - mae: 90.8884 - val_loss: 1197.7953 - val_mae: 1198.4884\n",
      "Epoch 1298/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 90.1406 - mae: 90.8229 - val_loss: 1115.2395 - val_mae: 1115.9326\n",
      "Epoch 1299/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 96.3806 - mae: 97.0635 - val_loss: 1377.8242 - val_mae: 1378.5166\n",
      "Epoch 1300/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 94.7481 - mae: 95.4316 - val_loss: 1371.3929 - val_mae: 1372.0857\n",
      "Epoch 1301/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 92.2701 - mae: 92.9530 - val_loss: 1182.2181 - val_mae: 1182.9113\n",
      "Epoch 1302/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 96.7908 - mae: 97.4726 - val_loss: 1212.5741 - val_mae: 1213.2673\n",
      "Epoch 1303/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 92.7234 - mae: 93.4034 - val_loss: 1146.9524 - val_mae: 1147.6449\n",
      "Epoch 1304/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 93.3447 - mae: 94.0264 - val_loss: 1197.8092 - val_mae: 1198.5022\n",
      "Epoch 1305/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 90.2470 - mae: 90.9275 - val_loss: 1121.6553 - val_mae: 1122.3484\n",
      "Epoch 1306/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 100.6462 - mae: 101.3304 - val_loss: 1128.1853 - val_mae: 1128.8785\n",
      "Epoch 1307/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 90.1563 - mae: 90.8364 - val_loss: 1484.1323 - val_mae: 1484.8256\n",
      "Epoch 1308/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 101.9430 - mae: 102.6234 - val_loss: 1200.0975 - val_mae: 1200.7906\n",
      "Epoch 1309/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 94.9286 - mae: 95.6136 - val_loss: 1337.0773 - val_mae: 1337.7704\n",
      "Epoch 1310/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 86.8374 - mae: 87.5175 - val_loss: 1360.4746 - val_mae: 1361.1676\n",
      "Epoch 1311/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 92.9803 - mae: 93.6599 - val_loss: 1299.5073 - val_mae: 1300.2004\n",
      "Epoch 1312/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 92.2953 - mae: 92.9760 - val_loss: 1401.3221 - val_mae: 1402.0148\n",
      "Epoch 1313/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 94.4567 - mae: 95.1394 - val_loss: 1197.5505 - val_mae: 1198.2437\n",
      "Epoch 1314/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 93.3038 - mae: 93.9850 - val_loss: 1141.6086 - val_mae: 1142.3020\n",
      "Epoch 1315/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 100.9156 - mae: 101.5977 - val_loss: 1419.7042 - val_mae: 1420.3972\n",
      "Epoch 1316/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 100.2464 - mae: 100.9285 - val_loss: 1448.4388 - val_mae: 1449.1318\n",
      "Epoch 1317/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 99.1534 - mae: 99.8381 - val_loss: 1423.1852 - val_mae: 1423.8784\n",
      "Epoch 1318/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 101.1519 - mae: 101.8351 - val_loss: 1197.3909 - val_mae: 1198.0840\n",
      "Epoch 1319/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 95.4242 - mae: 96.1068 - val_loss: 1290.8064 - val_mae: 1291.4996\n",
      "Epoch 1320/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 91.9278 - mae: 92.6087 - val_loss: 1585.5243 - val_mae: 1586.2175\n",
      "Epoch 1321/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 92.2974 - mae: 92.9772 - val_loss: 1261.8038 - val_mae: 1262.4969\n",
      "Epoch 1322/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 90.1871 - mae: 90.8668 - val_loss: 1162.0476 - val_mae: 1162.7408\n",
      "Epoch 1323/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 88.3480 - mae: 89.0320 - val_loss: 1299.8741 - val_mae: 1300.5673\n",
      "Epoch 1324/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 99.0725 - mae: 99.7569 - val_loss: 1288.5652 - val_mae: 1289.2582\n",
      "Epoch 1325/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 109.9918 - mae: 110.6769 - val_loss: 1598.4946 - val_mae: 1599.1877\n",
      "Epoch 1326/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 99.6982 - mae: 100.3854 - val_loss: 1242.4622 - val_mae: 1243.1554\n",
      "Epoch 1327/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 92.8652 - mae: 93.5483 - val_loss: 1231.4528 - val_mae: 1232.1459\n",
      "Epoch 1328/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 86.5592 - mae: 87.2416 - val_loss: 1193.0132 - val_mae: 1193.7063\n",
      "Epoch 1329/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 92.2235 - mae: 92.9050 - val_loss: 1428.7710 - val_mae: 1429.4641\n",
      "Epoch 1330/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 98.9003 - mae: 99.5833 - val_loss: 1212.5406 - val_mae: 1213.2336\n",
      "Epoch 1331/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 96.1342 - mae: 96.8196 - val_loss: 1366.4702 - val_mae: 1367.1635\n",
      "Epoch 1332/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 102.2955 - mae: 102.9829 - val_loss: 1210.7867 - val_mae: 1211.4791\n",
      "Epoch 1333/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 93.5085 - mae: 94.1875 - val_loss: 1255.4618 - val_mae: 1256.1550\n",
      "Epoch 1334/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 92.6844 - mae: 93.3679 - val_loss: 1397.1025 - val_mae: 1397.7949\n",
      "Epoch 1335/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 91.8156 - mae: 92.4990 - val_loss: 1223.9711 - val_mae: 1224.6643\n",
      "Epoch 1336/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 98.3208 - mae: 99.0052 - val_loss: 1409.5831 - val_mae: 1410.2751\n",
      "Epoch 1337/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 95.9308 - mae: 96.6100 - val_loss: 1279.4358 - val_mae: 1280.1282\n",
      "Epoch 1338/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 113.0320 - mae: 113.7182 - val_loss: 1349.5811 - val_mae: 1350.2740\n",
      "Epoch 1339/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 110.0467 - mae: 110.7301 - val_loss: 1176.8958 - val_mae: 1177.5880\n",
      "Epoch 1340/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 100.0674 - mae: 100.7507 - val_loss: 1423.2220 - val_mae: 1423.9153\n",
      "Epoch 1341/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 96.4129 - mae: 97.0949 - val_loss: 1125.7659 - val_mae: 1126.4584\n",
      "Epoch 1342/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 97.7953 - mae: 98.4788 - val_loss: 1157.4695 - val_mae: 1158.1626\n",
      "Epoch 1343/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 92.2385 - mae: 92.9200 - val_loss: 1144.3705 - val_mae: 1145.0627\n",
      "Epoch 1344/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 87.3746 - mae: 88.0573 - val_loss: 1142.5914 - val_mae: 1143.2838\n",
      "Epoch 1345/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 93.6154 - mae: 94.2940 - val_loss: 1409.3145 - val_mae: 1410.0077\n",
      "Epoch 1346/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 103.9045 - mae: 104.5861 - val_loss: 1230.4421 - val_mae: 1231.1351\n",
      "Epoch 1347/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 87.6626 - mae: 88.3426 - val_loss: 1237.8492 - val_mae: 1238.5411\n",
      "Epoch 1348/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 88.7716 - mae: 89.4532 - val_loss: 1374.5848 - val_mae: 1375.2778\n",
      "Epoch 1349/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 91.9637 - mae: 92.6458 - val_loss: 1445.7594 - val_mae: 1446.4525\n",
      "Epoch 1350/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 103.6193 - mae: 104.3013 - val_loss: 1336.7239 - val_mae: 1337.4161\n",
      "Epoch 1351/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 90.1745 - mae: 90.8542 - val_loss: 1363.8064 - val_mae: 1364.4984\n",
      "Epoch 1352/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 88.2949 - mae: 88.9760 - val_loss: 1260.0823 - val_mae: 1260.7755\n",
      "Epoch 1353/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 98.9561 - mae: 99.6397 - val_loss: 1196.6611 - val_mae: 1197.3542\n",
      "Epoch 1354/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 91.4218 - mae: 92.1049 - val_loss: 1354.4441 - val_mae: 1355.1376\n",
      "Epoch 1355/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 90.2099 - mae: 90.8883 - val_loss: 1209.8732 - val_mae: 1210.5664\n",
      "Epoch 1356/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 85.5458 - mae: 86.2255 - val_loss: 1247.9824 - val_mae: 1248.6757\n",
      "Epoch 1357/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 95.4582 - mae: 96.1401 - val_loss: 1272.3013 - val_mae: 1272.9941\n",
      "Epoch 1358/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 97.9421 - mae: 98.6268 - val_loss: 1487.2008 - val_mae: 1487.8940\n",
      "Epoch 1359/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 109.5959 - mae: 110.2779 - val_loss: 1207.2460 - val_mae: 1207.9388\n",
      "Epoch 1360/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 116.8579 - mae: 117.5435 - val_loss: 1424.0565 - val_mae: 1424.7494\n",
      "Epoch 1361/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 100.7806 - mae: 101.4660 - val_loss: 1271.1852 - val_mae: 1271.8783\n",
      "Epoch 1362/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 92.3088 - mae: 92.9952 - val_loss: 1142.0869 - val_mae: 1142.7789\n",
      "Epoch 1363/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 93.9830 - mae: 94.6680 - val_loss: 1194.4174 - val_mae: 1195.1086\n",
      "Epoch 1364/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 95.4953 - mae: 96.1823 - val_loss: 1181.1501 - val_mae: 1181.8435\n",
      "Epoch 1365/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 88.7554 - mae: 89.4350 - val_loss: 1198.4758 - val_mae: 1199.1688\n",
      "Epoch 1366/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 88.8493 - mae: 89.5329 - val_loss: 1194.3521 - val_mae: 1195.0439\n",
      "Epoch 1367/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 91.2328 - mae: 91.9169 - val_loss: 1288.3955 - val_mae: 1289.0886\n",
      "Epoch 1368/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 101.6948 - mae: 102.3807 - val_loss: 1105.5973 - val_mae: 1106.2905\n",
      "Epoch 1369/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 93.0380 - mae: 93.7236 - val_loss: 1368.4005 - val_mae: 1369.0938\n",
      "Epoch 1370/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 87.9884 - mae: 88.6688 - val_loss: 1297.1575 - val_mae: 1297.8507\n",
      "Epoch 1371/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 92.0173 - mae: 92.6998 - val_loss: 1166.9352 - val_mae: 1167.6284\n",
      "Epoch 1372/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 89.4653 - mae: 90.1488 - val_loss: 1200.5422 - val_mae: 1201.2349\n",
      "Epoch 1373/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 89.2836 - mae: 89.9672 - val_loss: 1272.4692 - val_mae: 1273.1624\n",
      "Epoch 1374/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 99.0225 - mae: 99.7080 - val_loss: 1561.1155 - val_mae: 1561.8087\n",
      "Epoch 1375/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 94.3318 - mae: 95.0142 - val_loss: 1411.7764 - val_mae: 1412.4695\n",
      "Epoch 1376/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 94.3461 - mae: 95.0307 - val_loss: 1285.9595 - val_mae: 1286.6520\n",
      "Epoch 1377/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 105.4173 - mae: 106.1001 - val_loss: 1262.6653 - val_mae: 1263.3583\n",
      "Epoch 1378/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 89.1525 - mae: 89.8333 - val_loss: 1209.0836 - val_mae: 1209.7766\n",
      "Epoch 1379/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 97.7544 - mae: 98.4367 - val_loss: 1413.8044 - val_mae: 1414.4968\n",
      "Epoch 1380/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 89.9947 - mae: 90.6778 - val_loss: 1283.8734 - val_mae: 1284.5662\n",
      "Epoch 1381/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 87.7820 - mae: 88.4664 - val_loss: 1170.4386 - val_mae: 1171.1312\n",
      "Epoch 1382/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 90.0685 - mae: 90.7516 - val_loss: 1395.3293 - val_mae: 1396.0225\n",
      "Epoch 1383/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 90.3738 - mae: 91.0601 - val_loss: 1147.0427 - val_mae: 1147.7357\n",
      "Epoch 1384/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 91.7387 - mae: 92.4185 - val_loss: 1219.5209 - val_mae: 1220.2142\n",
      "Epoch 1385/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 91.1353 - mae: 91.8197 - val_loss: 1188.8054 - val_mae: 1189.4968\n",
      "Epoch 1386/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 104.9081 - mae: 105.5917 - val_loss: 1215.4222 - val_mae: 1216.1154\n",
      "Epoch 1387/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 98.0470 - mae: 98.7308 - val_loss: 1163.4517 - val_mae: 1164.1448\n",
      "Epoch 1388/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 91.4321 - mae: 92.1171 - val_loss: 1192.1063 - val_mae: 1192.7987\n",
      "Epoch 1389/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 90.6041 - mae: 91.2862 - val_loss: 1514.4950 - val_mae: 1515.1881\n",
      "Epoch 1390/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 90.2580 - mae: 90.9383 - val_loss: 1332.0375 - val_mae: 1332.7291\n",
      "Epoch 1391/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 88.4311 - mae: 89.1129 - val_loss: 1543.8868 - val_mae: 1544.5796\n",
      "Epoch 1392/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 90.6735 - mae: 91.3543 - val_loss: 1109.0537 - val_mae: 1109.7467\n",
      "Epoch 1393/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 89.2620 - mae: 89.9452 - val_loss: 1414.1686 - val_mae: 1414.8616\n",
      "Epoch 1394/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 93.3237 - mae: 94.0053 - val_loss: 1275.8417 - val_mae: 1276.5336\n",
      "Epoch 1395/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 88.6651 - mae: 89.3456 - val_loss: 1276.4550 - val_mae: 1277.1476\n",
      "Epoch 1396/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 84.0243 - mae: 84.7056 - val_loss: 1286.1570 - val_mae: 1286.8499\n",
      "Epoch 1397/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 90.0585 - mae: 90.7414 - val_loss: 1184.6218 - val_mae: 1185.3149\n",
      "Epoch 1398/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 87.8026 - mae: 88.4842 - val_loss: 1197.6021 - val_mae: 1198.2948\n",
      "Epoch 1399/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 88.3911 - mae: 89.0732 - val_loss: 1177.2812 - val_mae: 1177.9746\n",
      "Epoch 1400/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 90.5958 - mae: 91.2793 - val_loss: 1200.5555 - val_mae: 1201.2484\n",
      "Epoch 1401/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 94.7722 - mae: 95.4525 - val_loss: 1230.5830 - val_mae: 1231.2759\n",
      "Epoch 1402/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 107.9146 - mae: 108.5974 - val_loss: 1360.2808 - val_mae: 1360.9738\n",
      "Epoch 1403/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 90.9964 - mae: 91.6807 - val_loss: 1185.1664 - val_mae: 1185.8595\n",
      "Epoch 1404/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 91.1854 - mae: 91.8687 - val_loss: 1356.4686 - val_mae: 1357.1617\n",
      "Epoch 1405/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 90.3414 - mae: 91.0257 - val_loss: 1289.2462 - val_mae: 1289.9386\n",
      "Epoch 1406/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 88.1721 - mae: 88.8525 - val_loss: 981.9486 - val_mae: 982.6416\n",
      "Epoch 1407/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 92.9127 - mae: 93.5902 - val_loss: 1317.2505 - val_mae: 1317.9434\n",
      "Epoch 1408/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 91.4793 - mae: 92.1626 - val_loss: 1306.0598 - val_mae: 1306.7523\n",
      "Epoch 1409/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 87.9487 - mae: 88.6280 - val_loss: 1347.9995 - val_mae: 1348.6926\n",
      "Epoch 1410/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 88.1543 - mae: 88.8384 - val_loss: 1295.8252 - val_mae: 1296.5187\n",
      "Epoch 1411/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 93.7178 - mae: 94.4000 - val_loss: 1033.1685 - val_mae: 1033.8615\n",
      "Epoch 1412/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 92.4510 - mae: 93.1331 - val_loss: 1239.7733 - val_mae: 1240.4663\n",
      "Epoch 1413/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 91.1338 - mae: 91.8151 - val_loss: 1072.5105 - val_mae: 1073.2037\n",
      "Epoch 1414/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 104.5716 - mae: 105.2570 - val_loss: 1111.4390 - val_mae: 1112.1320\n",
      "Epoch 1415/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 88.6288 - mae: 89.3107 - val_loss: 1303.2371 - val_mae: 1303.9299\n",
      "Epoch 1416/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 100.3604 - mae: 101.0413 - val_loss: 1094.4493 - val_mae: 1095.1418\n",
      "Epoch 1417/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 100.9107 - mae: 101.5958 - val_loss: 1087.7344 - val_mae: 1088.4276\n",
      "Epoch 1418/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 85.0453 - mae: 85.7270 - val_loss: 1205.3018 - val_mae: 1205.9949\n",
      "Epoch 1419/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 104.1691 - mae: 104.8550 - val_loss: 1235.4957 - val_mae: 1236.1885\n",
      "Epoch 1420/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 89.4709 - mae: 90.1546 - val_loss: 1234.5968 - val_mae: 1235.2900\n",
      "Epoch 1421/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 95.5429 - mae: 96.2266 - val_loss: 1404.5731 - val_mae: 1405.2660\n",
      "Epoch 1422/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 91.1773 - mae: 91.8649 - val_loss: 1390.0646 - val_mae: 1390.7572\n",
      "Epoch 1423/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 95.2853 - mae: 95.9689 - val_loss: 1761.8453 - val_mae: 1762.5378\n",
      "Epoch 1424/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 104.6322 - mae: 105.3144 - val_loss: 1260.8060 - val_mae: 1261.4993\n",
      "Epoch 1425/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 107.3856 - mae: 108.0703 - val_loss: 1374.7795 - val_mae: 1375.4724\n",
      "Epoch 1426/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 90.1903 - mae: 90.8724 - val_loss: 1218.6704 - val_mae: 1219.3636\n",
      "Epoch 1427/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 87.0977 - mae: 87.7816 - val_loss: 1249.1969 - val_mae: 1249.8900\n",
      "Epoch 1428/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 93.4825 - mae: 94.1643 - val_loss: 1204.2642 - val_mae: 1204.9572\n",
      "Epoch 1429/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 93.1108 - mae: 93.7955 - val_loss: 1284.2731 - val_mae: 1284.9659\n",
      "Epoch 1430/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 89.5094 - mae: 90.1917 - val_loss: 1299.6030 - val_mae: 1300.2961\n",
      "Epoch 1431/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 93.5404 - mae: 94.2225 - val_loss: 1414.0320 - val_mae: 1414.7251\n",
      "Epoch 1432/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 94.9957 - mae: 95.6755 - val_loss: 1417.2279 - val_mae: 1417.9213\n",
      "Epoch 1433/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 106.8612 - mae: 107.5459 - val_loss: 1045.8361 - val_mae: 1046.5288\n",
      "Epoch 1434/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 92.3704 - mae: 93.0534 - val_loss: 1103.2438 - val_mae: 1103.9368\n",
      "Epoch 1435/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 91.5066 - mae: 92.1915 - val_loss: 1448.8993 - val_mae: 1449.5902\n",
      "Epoch 1436/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 93.6815 - mae: 94.3668 - val_loss: 1233.5714 - val_mae: 1234.2637\n",
      "Epoch 1437/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 97.2296 - mae: 97.9113 - val_loss: 1073.3971 - val_mae: 1074.0902\n",
      "Epoch 1438/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 94.6307 - mae: 95.3169 - val_loss: 1451.0405 - val_mae: 1451.7338\n",
      "Epoch 1439/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 100.7176 - mae: 101.4015 - val_loss: 1058.5105 - val_mae: 1059.2026\n",
      "Epoch 1440/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 95.6592 - mae: 96.3420 - val_loss: 1264.1461 - val_mae: 1264.8395\n",
      "Epoch 1441/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 87.2038 - mae: 87.8850 - val_loss: 1192.0948 - val_mae: 1192.7878\n",
      "Epoch 1442/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 89.0828 - mae: 89.7657 - val_loss: 1417.6161 - val_mae: 1418.3088\n",
      "Epoch 1443/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 90.7159 - mae: 91.3985 - val_loss: 1274.4854 - val_mae: 1275.1787\n",
      "Epoch 1444/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 84.3654 - mae: 85.0479 - val_loss: 1250.1993 - val_mae: 1250.8912\n",
      "Epoch 1445/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 87.2555 - mae: 87.9386 - val_loss: 1096.6383 - val_mae: 1097.3311\n",
      "Epoch 1446/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 101.6440 - mae: 102.3257 - val_loss: 1356.6753 - val_mae: 1357.3683\n",
      "Epoch 1447/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 92.1294 - mae: 92.8140 - val_loss: 1309.3232 - val_mae: 1310.0151\n",
      "Epoch 1448/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 96.8260 - mae: 97.5100 - val_loss: 1245.4003 - val_mae: 1246.0924\n",
      "Epoch 1449/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 93.7402 - mae: 94.4258 - val_loss: 1222.1733 - val_mae: 1222.8665\n",
      "Epoch 1450/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 87.9771 - mae: 88.6580 - val_loss: 1164.1980 - val_mae: 1164.8898\n",
      "Epoch 1451/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 109.6496 - mae: 110.3324 - val_loss: 1299.3580 - val_mae: 1300.0513\n",
      "Epoch 1452/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 94.9550 - mae: 95.6416 - val_loss: 1237.9259 - val_mae: 1238.6189\n",
      "Epoch 1453/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 86.8775 - mae: 87.5630 - val_loss: 1211.3829 - val_mae: 1212.0763\n",
      "Epoch 1454/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 91.2096 - mae: 91.8949 - val_loss: 1176.8169 - val_mae: 1177.5100\n",
      "Epoch 1455/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 87.3396 - mae: 88.0214 - val_loss: 1130.6780 - val_mae: 1131.3712\n",
      "Epoch 1456/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 89.2298 - mae: 89.9132 - val_loss: 1289.7363 - val_mae: 1290.4293\n",
      "Epoch 1457/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 92.0676 - mae: 92.7509 - val_loss: 1258.3840 - val_mae: 1259.0768\n",
      "Epoch 1458/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 92.9070 - mae: 93.5908 - val_loss: 1374.3884 - val_mae: 1375.0803\n",
      "Epoch 1459/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 93.4162 - mae: 94.0979 - val_loss: 1509.5012 - val_mae: 1510.1943\n",
      "Epoch 1460/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 88.9099 - mae: 89.5943 - val_loss: 1377.5969 - val_mae: 1378.2902\n",
      "Epoch 1461/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 93.9937 - mae: 94.6761 - val_loss: 1322.0405 - val_mae: 1322.7335\n",
      "Epoch 1462/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 95.5554 - mae: 96.2419 - val_loss: 1149.2676 - val_mae: 1149.9606\n",
      "Epoch 1463/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 93.8256 - mae: 94.5090 - val_loss: 1169.9396 - val_mae: 1170.6327\n",
      "Epoch 1464/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 89.3460 - mae: 90.0288 - val_loss: 1392.0861 - val_mae: 1392.7794\n",
      "Epoch 1465/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 92.8488 - mae: 93.5295 - val_loss: 1400.8994 - val_mae: 1401.5927\n",
      "Epoch 1466/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 98.6954 - mae: 99.3776 - val_loss: 1106.0082 - val_mae: 1106.7012\n",
      "Epoch 1467/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 91.0003 - mae: 91.6789 - val_loss: 1332.9337 - val_mae: 1333.6270\n",
      "Epoch 1468/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 92.0289 - mae: 92.7094 - val_loss: 1450.1532 - val_mae: 1450.8455\n",
      "Epoch 1469/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 94.2827 - mae: 94.9646 - val_loss: 1388.8104 - val_mae: 1389.5033\n",
      "Epoch 1470/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 87.8557 - mae: 88.5377 - val_loss: 1102.3573 - val_mae: 1103.0504\n",
      "Epoch 1471/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 82.8782 - mae: 83.5621 - val_loss: 1218.7710 - val_mae: 1219.4642\n",
      "Epoch 1472/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 85.8490 - mae: 86.5298 - val_loss: 1244.0204 - val_mae: 1244.7129\n",
      "Epoch 1473/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 91.1845 - mae: 91.8678 - val_loss: 1183.5397 - val_mae: 1184.2329\n",
      "Epoch 1474/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 93.4916 - mae: 94.1728 - val_loss: 1173.2986 - val_mae: 1173.9916\n",
      "Epoch 1475/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 92.4984 - mae: 93.1801 - val_loss: 1222.2803 - val_mae: 1222.9735\n",
      "Epoch 1476/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 93.8949 - mae: 94.5786 - val_loss: 1324.4652 - val_mae: 1325.1562\n",
      "Epoch 1477/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 85.1969 - mae: 85.8819 - val_loss: 1341.0369 - val_mae: 1341.7291\n",
      "Epoch 1478/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 92.3737 - mae: 93.0550 - val_loss: 1227.8064 - val_mae: 1228.4995\n",
      "Epoch 1479/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 97.1913 - mae: 97.8736 - val_loss: 1112.9845 - val_mae: 1113.6776\n",
      "Epoch 1480/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 96.3252 - mae: 97.0085 - val_loss: 1383.2047 - val_mae: 1383.8969\n",
      "Epoch 1481/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 103.7503 - mae: 104.4339 - val_loss: 1148.4742 - val_mae: 1149.1674\n",
      "Epoch 1482/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 85.0096 - mae: 85.6893 - val_loss: 1188.7968 - val_mae: 1189.4889\n",
      "Epoch 1483/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 91.2477 - mae: 91.9314 - val_loss: 1107.0612 - val_mae: 1107.7544\n",
      "Epoch 1484/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 95.8273 - mae: 96.5127 - val_loss: 1100.1517 - val_mae: 1100.8450\n",
      "Epoch 1485/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 101.0660 - mae: 101.7465 - val_loss: 1099.4929 - val_mae: 1100.1859\n",
      "Epoch 1486/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 93.7251 - mae: 94.4023 - val_loss: 1103.6598 - val_mae: 1104.3529\n",
      "Epoch 1487/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 109.8753 - mae: 110.5614 - val_loss: 1172.8185 - val_mae: 1173.5111\n",
      "Epoch 1488/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 97.0323 - mae: 97.7167 - val_loss: 1200.4674 - val_mae: 1201.1605\n",
      "Epoch 1489/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 88.8523 - mae: 89.5372 - val_loss: 1108.7573 - val_mae: 1109.4497\n",
      "Epoch 1490/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 97.5476 - mae: 98.2279 - val_loss: 1210.3666 - val_mae: 1211.0597\n",
      "Epoch 1491/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 91.7383 - mae: 92.4214 - val_loss: 1223.8999 - val_mae: 1224.5928\n",
      "Epoch 1492/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 103.0280 - mae: 103.7127 - val_loss: 1665.8115 - val_mae: 1666.5037\n",
      "Epoch 1493/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 113.4373 - mae: 114.1227 - val_loss: 1226.4961 - val_mae: 1227.1893\n",
      "Epoch 1494/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 85.3409 - mae: 86.0230 - val_loss: 1335.4000 - val_mae: 1336.0933\n",
      "Epoch 1495/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 90.7486 - mae: 91.4303 - val_loss: 1145.2991 - val_mae: 1145.9922\n",
      "Epoch 1496/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 89.4644 - mae: 90.1500 - val_loss: 1402.5251 - val_mae: 1403.2178\n",
      "Epoch 1497/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 89.7200 - mae: 90.4060 - val_loss: 1213.3553 - val_mae: 1214.0475\n",
      "Epoch 1498/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 85.8855 - mae: 86.5704 - val_loss: 1207.4944 - val_mae: 1208.1868\n",
      "Epoch 1499/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 86.6430 - mae: 87.3252 - val_loss: 1232.6432 - val_mae: 1233.3363\n",
      "Epoch 1500/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 90.1157 - mae: 90.7973 - val_loss: 1333.5406 - val_mae: 1334.2338\n",
      "Epoch 1501/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 95.7143 - mae: 96.3984 - val_loss: 1140.3385 - val_mae: 1141.0315\n",
      "Epoch 1502/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 90.7348 - mae: 91.4176 - val_loss: 1117.8693 - val_mae: 1118.5626\n",
      "Epoch 1503/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 89.7836 - mae: 90.4685 - val_loss: 1316.3154 - val_mae: 1317.0081\n",
      "Epoch 1504/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 87.5245 - mae: 88.2070 - val_loss: 1317.3330 - val_mae: 1318.0261\n",
      "Epoch 1505/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 90.6430 - mae: 91.3255 - val_loss: 1177.6656 - val_mae: 1178.3588\n",
      "Epoch 1506/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 86.5560 - mae: 87.2405 - val_loss: 1187.1345 - val_mae: 1187.8278\n",
      "Epoch 1507/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 95.0525 - mae: 95.7356 - val_loss: 1228.2974 - val_mae: 1228.9906\n",
      "Epoch 1508/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 87.5986 - mae: 88.2817 - val_loss: 1261.8481 - val_mae: 1262.5414\n",
      "Epoch 1509/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 89.7207 - mae: 90.4046 - val_loss: 1379.8687 - val_mae: 1380.5616\n",
      "Epoch 1510/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 93.5996 - mae: 94.2825 - val_loss: 1400.8387 - val_mae: 1401.5320\n",
      "Epoch 1511/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 88.6940 - mae: 89.3764 - val_loss: 1243.0637 - val_mae: 1243.7571\n",
      "Epoch 1512/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 88.1907 - mae: 88.8706 - val_loss: 1348.5702 - val_mae: 1349.2628\n",
      "Epoch 1513/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 93.8236 - mae: 94.5058 - val_loss: 1303.1710 - val_mae: 1303.8641\n",
      "Epoch 1514/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 106.4014 - mae: 107.0857 - val_loss: 1193.3765 - val_mae: 1194.0692\n",
      "Epoch 1515/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 90.2725 - mae: 90.9558 - val_loss: 1192.3285 - val_mae: 1193.0217\n",
      "Epoch 1516/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 91.5699 - mae: 92.2518 - val_loss: 1035.2954 - val_mae: 1035.9886\n",
      "Epoch 1517/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 90.4971 - mae: 91.1789 - val_loss: 1277.9777 - val_mae: 1278.6693\n",
      "Epoch 1518/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 88.5605 - mae: 89.2426 - val_loss: 1269.8475 - val_mae: 1270.5408\n",
      "Epoch 1519/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 90.2126 - mae: 90.8980 - val_loss: 1237.9097 - val_mae: 1238.6029\n",
      "Epoch 1520/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 92.8680 - mae: 93.5458 - val_loss: 1425.3340 - val_mae: 1426.0271\n",
      "Epoch 1521/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 89.6291 - mae: 90.3138 - val_loss: 1332.2826 - val_mae: 1332.9760\n",
      "Epoch 1522/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 89.1074 - mae: 89.7898 - val_loss: 1225.3441 - val_mae: 1226.0372\n",
      "Epoch 1523/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 91.0713 - mae: 91.7561 - val_loss: 1201.0306 - val_mae: 1201.7239\n",
      "Epoch 1524/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 91.7907 - mae: 92.4738 - val_loss: 1173.9558 - val_mae: 1174.6488\n",
      "Epoch 1525/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 87.1690 - mae: 87.8499 - val_loss: 1191.9929 - val_mae: 1192.6860\n",
      "Epoch 1526/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 86.9107 - mae: 87.5934 - val_loss: 1211.9091 - val_mae: 1212.6022\n",
      "Epoch 1527/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 89.6600 - mae: 90.3414 - val_loss: 1217.4944 - val_mae: 1218.1876\n",
      "Epoch 1528/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 86.5334 - mae: 87.2148 - val_loss: 1166.5516 - val_mae: 1167.2446\n",
      "Epoch 1529/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 89.5336 - mae: 90.2154 - val_loss: 1242.9166 - val_mae: 1243.6096\n",
      "Epoch 1530/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 89.4847 - mae: 90.1670 - val_loss: 1481.9365 - val_mae: 1482.6299\n",
      "Epoch 1531/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 91.5845 - mae: 92.2712 - val_loss: 1172.6902 - val_mae: 1173.3829\n",
      "Epoch 1532/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 90.7010 - mae: 91.3840 - val_loss: 1306.1500 - val_mae: 1306.8423\n",
      "Epoch 1533/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 86.1189 - mae: 86.8038 - val_loss: 1194.3624 - val_mae: 1195.0557\n",
      "Epoch 1534/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 85.3345 - mae: 86.0181 - val_loss: 1136.0521 - val_mae: 1136.7439\n",
      "Epoch 1535/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 85.7188 - mae: 86.4052 - val_loss: 1157.7562 - val_mae: 1158.4495\n",
      "Epoch 1536/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 87.3821 - mae: 88.0634 - val_loss: 1305.2128 - val_mae: 1305.9059\n",
      "Epoch 1537/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 84.8365 - mae: 85.5160 - val_loss: 1340.7196 - val_mae: 1341.4126\n",
      "Epoch 1538/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 87.9327 - mae: 88.6149 - val_loss: 1214.1874 - val_mae: 1214.8807\n",
      "Epoch 1539/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 83.3336 - mae: 84.0141 - val_loss: 1307.5975 - val_mae: 1308.2906\n",
      "Epoch 1540/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 84.1239 - mae: 84.8048 - val_loss: 1342.8516 - val_mae: 1343.5443\n",
      "Epoch 1541/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 86.0349 - mae: 86.7155 - val_loss: 1151.8933 - val_mae: 1152.5863\n",
      "Epoch 1542/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 90.5179 - mae: 91.2017 - val_loss: 1239.2462 - val_mae: 1239.9396\n",
      "Epoch 1543/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 80.5557 - mae: 81.2368 - val_loss: 1205.4929 - val_mae: 1206.1857\n",
      "Epoch 1544/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 90.9816 - mae: 91.6634 - val_loss: 1021.0494 - val_mae: 1021.7426\n",
      "Epoch 1545/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 108.5932 - mae: 109.2777 - val_loss: 1199.7899 - val_mae: 1200.4818\n",
      "Epoch 1546/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 92.7686 - mae: 93.4511 - val_loss: 1315.0457 - val_mae: 1315.7390\n",
      "Epoch 1547/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 89.1130 - mae: 89.7926 - val_loss: 1257.1608 - val_mae: 1257.8539\n",
      "Epoch 1548/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 84.7227 - mae: 85.4062 - val_loss: 1389.5737 - val_mae: 1390.2670\n",
      "Epoch 1549/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 87.7183 - mae: 88.3997 - val_loss: 1202.0535 - val_mae: 1202.7465\n",
      "Epoch 1550/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 98.7274 - mae: 99.4095 - val_loss: 1402.9999 - val_mae: 1403.6923\n",
      "Epoch 1551/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 90.9289 - mae: 91.6130 - val_loss: 1233.5663 - val_mae: 1234.2595\n",
      "Epoch 1552/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 87.0316 - mae: 87.7121 - val_loss: 1228.6167 - val_mae: 1229.3099\n",
      "Epoch 1553/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 86.5486 - mae: 87.2287 - val_loss: 1241.8199 - val_mae: 1242.5133\n",
      "Epoch 1554/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 89.6329 - mae: 90.3130 - val_loss: 1186.0153 - val_mae: 1186.7076\n",
      "Epoch 1555/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 91.0088 - mae: 91.6929 - val_loss: 1305.3749 - val_mae: 1306.0681\n",
      "Epoch 1556/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 85.9907 - mae: 86.6751 - val_loss: 1246.4796 - val_mae: 1247.1727\n",
      "Epoch 1557/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 85.3720 - mae: 86.0553 - val_loss: 1244.4087 - val_mae: 1245.1018\n",
      "Epoch 1558/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 87.4582 - mae: 88.1423 - val_loss: 1424.4127 - val_mae: 1425.1051\n",
      "Epoch 1559/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 92.1414 - mae: 92.8209 - val_loss: 1331.7603 - val_mae: 1332.4536\n",
      "Epoch 1560/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 85.3547 - mae: 86.0351 - val_loss: 1301.3231 - val_mae: 1302.0162\n",
      "Epoch 1561/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 84.6254 - mae: 85.3043 - val_loss: 1389.9747 - val_mae: 1390.6681\n",
      "Epoch 1562/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 89.0136 - mae: 89.6959 - val_loss: 1214.3209 - val_mae: 1215.0128\n",
      "Epoch 1563/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 86.9054 - mae: 87.5882 - val_loss: 1253.5275 - val_mae: 1254.2205\n",
      "Epoch 1564/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 88.6318 - mae: 89.3162 - val_loss: 1384.8070 - val_mae: 1385.4991\n",
      "Epoch 1565/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 87.4461 - mae: 88.1299 - val_loss: 1344.0488 - val_mae: 1344.7413\n",
      "Epoch 1566/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 85.8883 - mae: 86.5691 - val_loss: 1198.4365 - val_mae: 1199.1287\n",
      "Epoch 1567/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 85.5070 - mae: 86.1895 - val_loss: 1380.9838 - val_mae: 1381.6764\n",
      "Epoch 1568/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 91.9459 - mae: 92.6294 - val_loss: 1449.5200 - val_mae: 1450.2134\n",
      "Epoch 1569/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 93.4803 - mae: 94.1652 - val_loss: 1317.9963 - val_mae: 1318.6884\n",
      "Epoch 1570/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 94.9454 - mae: 95.6254 - val_loss: 1262.2686 - val_mae: 1262.9614\n",
      "Epoch 1571/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 83.7279 - mae: 84.4091 - val_loss: 1200.0605 - val_mae: 1200.7533\n",
      "Epoch 1572/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 88.6502 - mae: 89.3289 - val_loss: 1501.0217 - val_mae: 1501.7151\n",
      "Epoch 1573/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 88.3327 - mae: 89.0151 - val_loss: 1346.9440 - val_mae: 1347.6373\n",
      "Epoch 1574/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 89.0095 - mae: 89.6944 - val_loss: 1265.9125 - val_mae: 1266.6058\n",
      "Epoch 1575/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 87.9515 - mae: 88.6305 - val_loss: 1297.5165 - val_mae: 1298.2095\n",
      "Epoch 1576/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 93.3658 - mae: 94.0472 - val_loss: 1134.6675 - val_mae: 1135.3607\n",
      "Epoch 1577/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 88.3878 - mae: 89.0673 - val_loss: 1210.4524 - val_mae: 1211.1455\n",
      "Epoch 1578/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 99.9103 - mae: 100.5901 - val_loss: 1361.0664 - val_mae: 1361.7592\n",
      "Epoch 1579/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 86.8615 - mae: 87.5450 - val_loss: 1148.5979 - val_mae: 1149.2902\n",
      "Epoch 1580/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 88.8318 - mae: 89.5147 - val_loss: 1216.5980 - val_mae: 1217.2913\n",
      "Epoch 1581/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 92.8180 - mae: 93.5005 - val_loss: 1076.8271 - val_mae: 1077.5197\n",
      "Epoch 1582/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 87.8884 - mae: 88.5706 - val_loss: 1369.2926 - val_mae: 1369.9854\n",
      "Epoch 1583/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 88.8652 - mae: 89.5451 - val_loss: 1292.7134 - val_mae: 1293.4056\n",
      "Epoch 1584/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 83.0841 - mae: 83.7661 - val_loss: 1136.2711 - val_mae: 1136.9631\n",
      "Epoch 1585/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 83.2431 - mae: 83.9253 - val_loss: 1364.6444 - val_mae: 1365.3376\n",
      "Epoch 1586/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 92.5113 - mae: 93.1923 - val_loss: 1087.5966 - val_mae: 1088.2892\n",
      "Epoch 1587/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 89.5723 - mae: 90.2565 - val_loss: 1213.2546 - val_mae: 1213.9476\n",
      "Epoch 1588/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 85.3438 - mae: 86.0245 - val_loss: 1287.6044 - val_mae: 1288.2974\n",
      "Epoch 1589/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 94.7102 - mae: 95.3946 - val_loss: 1178.0385 - val_mae: 1178.7312\n",
      "Epoch 1590/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 87.1486 - mae: 87.8329 - val_loss: 1134.9495 - val_mae: 1135.6426\n",
      "Epoch 1591/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 88.2306 - mae: 88.9134 - val_loss: 1218.3849 - val_mae: 1219.0782\n",
      "Epoch 1592/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 90.1818 - mae: 90.8662 - val_loss: 1121.9480 - val_mae: 1122.6406\n",
      "Epoch 1593/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 86.5760 - mae: 87.2590 - val_loss: 1249.5376 - val_mae: 1250.2292\n",
      "Epoch 1594/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 80.4091 - mae: 81.0893 - val_loss: 1325.3474 - val_mae: 1326.0409\n",
      "Epoch 1595/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 88.2038 - mae: 88.8857 - val_loss: 1086.9910 - val_mae: 1087.6841\n",
      "Epoch 1596/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 88.2437 - mae: 88.9278 - val_loss: 1282.7134 - val_mae: 1283.4065\n",
      "Epoch 1597/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 86.6710 - mae: 87.3538 - val_loss: 1154.6050 - val_mae: 1155.2979\n",
      "Epoch 1598/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 92.0870 - mae: 92.7697 - val_loss: 1182.3330 - val_mae: 1183.0262\n",
      "Epoch 1599/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 91.7978 - mae: 92.4817 - val_loss: 1087.0015 - val_mae: 1087.6943\n",
      "Epoch 1600/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 86.3204 - mae: 86.9998 - val_loss: 1305.3738 - val_mae: 1306.0670\n",
      "Epoch 1601/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 90.5244 - mae: 91.2088 - val_loss: 1217.4319 - val_mae: 1218.1252\n",
      "Epoch 1602/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 81.4677 - mae: 82.1513 - val_loss: 1303.7328 - val_mae: 1304.4259\n",
      "Epoch 1603/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 86.7010 - mae: 87.3847 - val_loss: 1302.6813 - val_mae: 1303.3745\n",
      "Epoch 1604/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 87.7962 - mae: 88.4809 - val_loss: 1341.6475 - val_mae: 1342.3407\n",
      "Epoch 1605/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 86.5337 - mae: 87.2141 - val_loss: 1286.9203 - val_mae: 1287.6134\n",
      "Epoch 1606/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 87.6113 - mae: 88.2931 - val_loss: 1347.0372 - val_mae: 1347.7292\n",
      "Epoch 1607/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 87.7434 - mae: 88.4278 - val_loss: 1132.4257 - val_mae: 1133.1189\n",
      "Epoch 1608/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 97.2858 - mae: 97.9703 - val_loss: 1293.1188 - val_mae: 1293.8116\n",
      "Epoch 1609/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 86.4937 - mae: 87.1758 - val_loss: 1419.8278 - val_mae: 1420.5208\n",
      "Epoch 1610/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 88.5128 - mae: 89.1959 - val_loss: 1145.4327 - val_mae: 1146.1260\n",
      "Epoch 1611/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 84.2936 - mae: 84.9750 - val_loss: 1182.4572 - val_mae: 1183.1506\n",
      "Epoch 1612/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 85.0618 - mae: 85.7456 - val_loss: 1188.0399 - val_mae: 1188.7330\n",
      "Epoch 1613/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 89.4859 - mae: 90.1713 - val_loss: 1268.1786 - val_mae: 1268.8717\n",
      "Epoch 1614/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 84.6808 - mae: 85.3593 - val_loss: 1311.2794 - val_mae: 1311.9725\n",
      "Epoch 1615/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 90.0840 - mae: 90.7679 - val_loss: 1226.9076 - val_mae: 1227.6000\n",
      "Epoch 1616/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 89.1167 - mae: 89.7999 - val_loss: 1417.7404 - val_mae: 1418.4336\n",
      "Epoch 1617/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 87.0001 - mae: 87.6830 - val_loss: 1185.3976 - val_mae: 1186.0905\n",
      "Epoch 1618/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 88.6853 - mae: 89.3696 - val_loss: 1129.9773 - val_mae: 1130.6702\n",
      "Epoch 1619/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 94.9582 - mae: 95.6446 - val_loss: 1261.7841 - val_mae: 1262.4756\n",
      "Epoch 1620/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 85.2194 - mae: 85.8994 - val_loss: 1179.6333 - val_mae: 1180.3254\n",
      "Epoch 1621/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 102.9374 - mae: 103.6239 - val_loss: 1344.9049 - val_mae: 1345.5973\n",
      "Epoch 1622/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 89.1040 - mae: 89.7850 - val_loss: 1629.3417 - val_mae: 1630.0348\n",
      "Epoch 1623/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 101.9189 - mae: 102.6069 - val_loss: 1281.4580 - val_mae: 1282.1512\n",
      "Epoch 1624/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 92.9366 - mae: 93.6176 - val_loss: 1256.9336 - val_mae: 1257.6262\n",
      "Epoch 1625/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 86.8395 - mae: 87.5255 - val_loss: 1329.1542 - val_mae: 1329.8472\n",
      "Epoch 1626/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 86.2264 - mae: 86.9085 - val_loss: 1223.3616 - val_mae: 1224.0547\n",
      "Epoch 1627/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 86.4894 - mae: 87.1708 - val_loss: 1291.5927 - val_mae: 1292.2852\n",
      "Epoch 1628/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 93.9743 - mae: 94.6546 - val_loss: 1215.8503 - val_mae: 1216.5435\n",
      "Epoch 1629/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 87.0864 - mae: 87.7671 - val_loss: 1411.1232 - val_mae: 1411.8165\n",
      "Epoch 1630/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 89.7790 - mae: 90.4566 - val_loss: 1473.3049 - val_mae: 1473.9979\n",
      "Epoch 1631/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 89.5434 - mae: 90.2271 - val_loss: 1383.1963 - val_mae: 1383.8894\n",
      "Epoch 1632/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 85.0291 - mae: 85.7136 - val_loss: 1288.9493 - val_mae: 1289.6425\n",
      "Epoch 1633/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 84.1100 - mae: 84.7892 - val_loss: 1381.1631 - val_mae: 1381.8553\n",
      "Epoch 1634/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 93.2562 - mae: 93.9334 - val_loss: 1369.3569 - val_mae: 1370.0500\n",
      "Epoch 1635/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 86.0081 - mae: 86.6929 - val_loss: 1165.9231 - val_mae: 1166.6156\n",
      "Epoch 1636/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 85.0059 - mae: 85.6892 - val_loss: 1179.7350 - val_mae: 1180.4282\n",
      "Epoch 1637/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 85.8696 - mae: 86.5485 - val_loss: 1332.8822 - val_mae: 1333.5754\n",
      "Epoch 1638/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 84.5454 - mae: 85.2246 - val_loss: 1302.4935 - val_mae: 1303.1863\n",
      "Epoch 1639/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 87.3940 - mae: 88.0771 - val_loss: 1290.4703 - val_mae: 1291.1626\n",
      "Epoch 1640/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 90.0729 - mae: 90.7549 - val_loss: 1235.7004 - val_mae: 1236.3932\n",
      "Epoch 1641/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 85.0033 - mae: 85.6861 - val_loss: 1188.5768 - val_mae: 1189.2695\n",
      "Epoch 1642/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 83.5061 - mae: 84.1897 - val_loss: 1242.6387 - val_mae: 1243.3318\n",
      "Epoch 1643/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 88.2984 - mae: 88.9792 - val_loss: 1064.8694 - val_mae: 1065.5626\n",
      "Epoch 1644/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 94.2263 - mae: 94.9086 - val_loss: 1239.6360 - val_mae: 1240.3292\n",
      "Epoch 1645/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 83.4253 - mae: 84.1069 - val_loss: 1307.0275 - val_mae: 1307.7208\n",
      "Epoch 1646/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 95.1917 - mae: 95.8738 - val_loss: 1212.1824 - val_mae: 1212.8745\n",
      "Epoch 1647/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 87.8329 - mae: 88.5143 - val_loss: 1234.4769 - val_mae: 1235.1697\n",
      "Epoch 1648/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 82.0204 - mae: 82.7011 - val_loss: 1188.2880 - val_mae: 1188.9807\n",
      "Epoch 1649/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 86.4779 - mae: 87.1614 - val_loss: 1182.4723 - val_mae: 1183.1653\n",
      "Epoch 1650/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 87.0530 - mae: 87.7376 - val_loss: 1278.4625 - val_mae: 1279.1552\n",
      "Epoch 1651/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 82.5610 - mae: 83.2409 - val_loss: 1260.9708 - val_mae: 1261.6642\n",
      "Epoch 1652/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 96.0443 - mae: 96.7264 - val_loss: 1227.5868 - val_mae: 1228.2800\n",
      "Epoch 1653/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 84.3276 - mae: 85.0074 - val_loss: 1457.9877 - val_mae: 1458.6807\n",
      "Epoch 1654/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 93.2095 - mae: 93.8941 - val_loss: 1176.4032 - val_mae: 1177.0962\n",
      "Epoch 1655/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 98.5976 - mae: 99.2804 - val_loss: 1034.7415 - val_mae: 1035.4342\n",
      "Epoch 1656/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 87.5366 - mae: 88.2204 - val_loss: 1213.1873 - val_mae: 1213.8804\n",
      "Epoch 1657/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 85.2787 - mae: 85.9603 - val_loss: 1327.4117 - val_mae: 1328.1050\n",
      "Epoch 1658/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 86.3160 - mae: 86.9978 - val_loss: 1319.1222 - val_mae: 1319.8156\n",
      "Epoch 1659/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 89.6562 - mae: 90.3356 - val_loss: 1281.4832 - val_mae: 1282.1753\n",
      "Epoch 1660/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 99.8944 - mae: 100.5781 - val_loss: 1175.9484 - val_mae: 1176.6412\n",
      "Epoch 1661/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 81.0248 - mae: 81.7053 - val_loss: 1299.5417 - val_mae: 1300.2347\n",
      "Epoch 1662/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 87.6215 - mae: 88.3031 - val_loss: 1226.6995 - val_mae: 1227.3921\n",
      "Epoch 1663/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 101.1577 - mae: 101.8414 - val_loss: 1280.6931 - val_mae: 1281.3862\n",
      "Epoch 1664/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 92.4875 - mae: 93.1723 - val_loss: 1256.6855 - val_mae: 1257.3789\n",
      "Epoch 1665/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 86.1308 - mae: 86.8127 - val_loss: 1047.5437 - val_mae: 1048.2362\n",
      "Epoch 1666/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 94.1729 - mae: 94.8564 - val_loss: 1140.0352 - val_mae: 1140.7283\n",
      "Epoch 1667/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 86.8065 - mae: 87.4899 - val_loss: 1289.6475 - val_mae: 1290.3408\n",
      "Epoch 1668/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 79.8295 - mae: 80.5103 - val_loss: 1298.4614 - val_mae: 1299.1542\n",
      "Epoch 1669/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 92.3710 - mae: 93.0544 - val_loss: 1237.7795 - val_mae: 1238.4713\n",
      "Epoch 1670/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 84.1029 - mae: 84.7834 - val_loss: 1295.9594 - val_mae: 1296.6519\n",
      "Epoch 1671/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 84.7053 - mae: 85.3850 - val_loss: 1356.8741 - val_mae: 1357.5675\n",
      "Epoch 1672/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 89.1301 - mae: 89.8130 - val_loss: 1320.6082 - val_mae: 1321.3013\n",
      "Epoch 1673/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 84.0033 - mae: 84.6848 - val_loss: 1236.9856 - val_mae: 1237.6790\n",
      "Epoch 1674/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 83.4663 - mae: 84.1494 - val_loss: 1207.3740 - val_mae: 1208.0673\n",
      "Epoch 1675/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 81.1772 - mae: 81.8582 - val_loss: 1267.1147 - val_mae: 1267.8077\n",
      "Epoch 1676/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 86.1842 - mae: 86.8621 - val_loss: 1170.3187 - val_mae: 1171.0118\n",
      "Epoch 1677/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 91.6345 - mae: 92.3179 - val_loss: 1267.6145 - val_mae: 1268.3076\n",
      "Epoch 1678/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 83.8062 - mae: 84.4861 - val_loss: 1242.3228 - val_mae: 1243.0157\n",
      "Epoch 1679/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 94.8466 - mae: 95.5310 - val_loss: 1077.2651 - val_mae: 1077.9573\n",
      "Epoch 1680/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 91.8595 - mae: 92.5436 - val_loss: 1180.4871 - val_mae: 1181.1785\n",
      "Epoch 1681/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 87.7549 - mae: 88.4379 - val_loss: 1336.9802 - val_mae: 1337.6726\n",
      "Epoch 1682/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 85.0290 - mae: 85.7103 - val_loss: 1225.4381 - val_mae: 1226.1304\n",
      "Epoch 1683/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 80.5515 - mae: 81.2306 - val_loss: 1249.3306 - val_mae: 1250.0232\n",
      "Epoch 1684/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 83.6689 - mae: 84.3495 - val_loss: 1105.4546 - val_mae: 1106.1476\n",
      "Epoch 1685/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 89.1387 - mae: 89.8215 - val_loss: 1228.1606 - val_mae: 1228.8527\n",
      "Epoch 1686/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 84.5062 - mae: 85.1878 - val_loss: 1116.2722 - val_mae: 1116.9651\n",
      "Epoch 1687/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 88.4239 - mae: 89.1097 - val_loss: 1190.6571 - val_mae: 1191.3501\n",
      "Epoch 1688/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 85.7549 - mae: 86.4339 - val_loss: 1307.0425 - val_mae: 1307.7355\n",
      "Epoch 1689/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 88.1250 - mae: 88.8084 - val_loss: 1180.8372 - val_mae: 1181.5304\n",
      "Epoch 1690/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 91.1844 - mae: 91.8692 - val_loss: 1214.3027 - val_mae: 1214.9958\n",
      "Epoch 1691/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 87.6191 - mae: 88.3008 - val_loss: 1300.2635 - val_mae: 1300.9569\n",
      "Epoch 1692/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 83.9720 - mae: 84.6557 - val_loss: 1235.8657 - val_mae: 1236.5585\n",
      "Epoch 1693/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 86.9306 - mae: 87.6110 - val_loss: 1335.0662 - val_mae: 1335.7592\n",
      "Epoch 1694/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 82.5247 - mae: 83.2065 - val_loss: 1235.3142 - val_mae: 1236.0072\n",
      "Epoch 1695/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 79.9081 - mae: 80.5860 - val_loss: 1317.6025 - val_mae: 1318.2957\n",
      "Epoch 1696/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 82.1014 - mae: 82.7828 - val_loss: 1211.6326 - val_mae: 1212.3258\n",
      "Epoch 1697/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 92.0296 - mae: 92.7116 - val_loss: 1352.3926 - val_mae: 1353.0857\n",
      "Epoch 1698/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 93.3723 - mae: 94.0571 - val_loss: 1466.9971 - val_mae: 1467.6903\n",
      "Epoch 1699/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 95.0169 - mae: 95.6992 - val_loss: 1490.3254 - val_mae: 1491.0187\n",
      "Epoch 1700/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 94.8268 - mae: 95.5079 - val_loss: 1115.0748 - val_mae: 1115.7681\n",
      "Epoch 1701/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 89.5564 - mae: 90.2387 - val_loss: 1075.2950 - val_mae: 1075.9883\n",
      "Epoch 1702/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 89.7885 - mae: 90.4741 - val_loss: 1327.9225 - val_mae: 1328.6146\n",
      "Epoch 1703/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 84.0329 - mae: 84.7174 - val_loss: 1318.5741 - val_mae: 1319.2673\n",
      "Epoch 1704/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 87.4633 - mae: 88.1448 - val_loss: 1387.3732 - val_mae: 1388.0656\n",
      "Epoch 1705/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 84.3747 - mae: 85.0567 - val_loss: 1362.1337 - val_mae: 1362.8268\n",
      "Epoch 1706/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 89.9125 - mae: 90.5946 - val_loss: 1213.5188 - val_mae: 1214.2112\n",
      "Epoch 1707/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 93.9370 - mae: 94.6173 - val_loss: 1156.3037 - val_mae: 1156.9968\n",
      "Epoch 1708/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 86.8148 - mae: 87.4993 - val_loss: 1282.1140 - val_mae: 1282.8070\n",
      "Epoch 1709/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 83.2568 - mae: 83.9425 - val_loss: 1308.4358 - val_mae: 1309.1290\n",
      "Epoch 1710/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 82.1433 - mae: 82.8265 - val_loss: 1164.1023 - val_mae: 1164.7954\n",
      "Epoch 1711/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 81.5704 - mae: 82.2504 - val_loss: 1474.0431 - val_mae: 1474.7361\n",
      "Epoch 1712/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 83.9973 - mae: 84.6796 - val_loss: 1229.8744 - val_mae: 1230.5676\n",
      "Epoch 1713/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 91.8969 - mae: 92.5770 - val_loss: 1391.6532 - val_mae: 1392.3463\n",
      "Epoch 1714/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 94.5803 - mae: 95.2633 - val_loss: 1191.1016 - val_mae: 1191.7946\n",
      "Epoch 1715/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 81.0586 - mae: 81.7387 - val_loss: 1301.1895 - val_mae: 1301.8823\n",
      "Epoch 1716/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 86.8688 - mae: 87.5496 - val_loss: 1156.1025 - val_mae: 1156.7948\n",
      "Epoch 1717/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 84.1334 - mae: 84.8158 - val_loss: 1181.1570 - val_mae: 1181.8501\n",
      "Epoch 1718/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 89.2331 - mae: 89.9150 - val_loss: 1184.0669 - val_mae: 1184.7596\n",
      "Epoch 1719/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 84.6236 - mae: 85.3094 - val_loss: 1269.4138 - val_mae: 1270.1069\n",
      "Epoch 1720/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 87.6433 - mae: 88.3284 - val_loss: 1235.4415 - val_mae: 1236.1346\n",
      "Epoch 1721/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 87.8155 - mae: 88.4990 - val_loss: 1081.6461 - val_mae: 1082.3392\n",
      "Epoch 1722/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 80.2416 - mae: 80.9224 - val_loss: 1315.6227 - val_mae: 1316.3152\n",
      "Epoch 1723/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 82.1716 - mae: 82.8497 - val_loss: 1101.3628 - val_mae: 1102.0560\n",
      "Epoch 1724/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 90.2837 - mae: 90.9667 - val_loss: 1295.8169 - val_mae: 1296.5100\n",
      "Epoch 1725/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 79.5012 - mae: 80.1819 - val_loss: 1381.5177 - val_mae: 1382.2108\n",
      "Epoch 1726/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 86.6804 - mae: 87.3635 - val_loss: 1284.2885 - val_mae: 1284.9816\n",
      "Epoch 1727/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 85.9437 - mae: 86.6239 - val_loss: 1235.2689 - val_mae: 1235.9617\n",
      "Epoch 1728/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 98.9605 - mae: 99.6443 - val_loss: 1221.2111 - val_mae: 1221.9041\n",
      "Epoch 1729/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 90.3493 - mae: 91.0322 - val_loss: 1399.7875 - val_mae: 1400.4808\n",
      "Epoch 1730/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 86.4154 - mae: 87.0970 - val_loss: 1314.6012 - val_mae: 1315.2943\n",
      "Epoch 1731/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 83.1427 - mae: 83.8250 - val_loss: 1147.4154 - val_mae: 1148.1085\n",
      "Epoch 1732/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 82.3710 - mae: 83.0529 - val_loss: 1209.3595 - val_mae: 1210.0529\n",
      "Epoch 1733/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 85.8083 - mae: 86.4894 - val_loss: 1212.9695 - val_mae: 1213.6615\n",
      "Epoch 1734/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 93.1853 - mae: 93.8681 - val_loss: 1494.0138 - val_mae: 1494.7069\n",
      "Epoch 1735/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 87.5529 - mae: 88.2380 - val_loss: 1267.3920 - val_mae: 1268.0851\n",
      "Epoch 1736/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 84.2321 - mae: 84.9147 - val_loss: 1312.1158 - val_mae: 1312.8075\n",
      "Epoch 1737/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 89.2380 - mae: 89.9247 - val_loss: 1417.1742 - val_mae: 1417.8661\n",
      "Epoch 1738/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 90.4852 - mae: 91.1675 - val_loss: 1254.1078 - val_mae: 1254.8011\n",
      "Epoch 1739/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 85.2930 - mae: 85.9743 - val_loss: 1207.8815 - val_mae: 1208.5743\n",
      "Epoch 1740/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 84.9061 - mae: 85.5908 - val_loss: 1186.2844 - val_mae: 1186.9775\n",
      "Epoch 1741/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 82.9976 - mae: 83.6789 - val_loss: 1214.4333 - val_mae: 1215.1266\n",
      "Epoch 1742/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 82.5701 - mae: 83.2517 - val_loss: 1161.3795 - val_mae: 1162.0726\n",
      "Epoch 1743/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 92.7054 - mae: 93.3866 - val_loss: 1124.0471 - val_mae: 1124.7396\n",
      "Epoch 1744/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 93.8184 - mae: 94.5043 - val_loss: 1281.2181 - val_mae: 1281.9114\n",
      "Epoch 1745/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 86.8543 - mae: 87.5348 - val_loss: 1281.7000 - val_mae: 1282.3916\n",
      "Epoch 1746/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 88.8507 - mae: 89.5334 - val_loss: 1295.0588 - val_mae: 1295.7521\n",
      "Epoch 1747/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 81.0386 - mae: 81.7215 - val_loss: 1296.3661 - val_mae: 1297.0591\n",
      "Epoch 1748/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 84.2337 - mae: 84.9152 - val_loss: 1256.8459 - val_mae: 1257.5392\n",
      "Epoch 1749/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 87.9537 - mae: 88.6355 - val_loss: 1371.6595 - val_mae: 1372.3525\n",
      "Epoch 1750/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 92.0693 - mae: 92.7523 - val_loss: 1227.7069 - val_mae: 1228.4000\n",
      "Epoch 1751/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 83.4247 - mae: 84.1062 - val_loss: 1271.0107 - val_mae: 1271.7031\n",
      "Epoch 1752/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 86.8017 - mae: 87.4815 - val_loss: 1297.4381 - val_mae: 1298.1313\n",
      "Epoch 1753/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 89.0134 - mae: 89.6947 - val_loss: 1221.3583 - val_mae: 1222.0516\n",
      "Epoch 1754/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 90.1374 - mae: 90.8178 - val_loss: 1279.5363 - val_mae: 1280.2295\n",
      "Epoch 1755/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 89.8424 - mae: 90.5261 - val_loss: 1236.7747 - val_mae: 1237.4678\n",
      "Epoch 1756/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 91.6427 - mae: 92.3255 - val_loss: 1210.4191 - val_mae: 1211.1121\n",
      "Epoch 1757/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 80.4166 - mae: 81.0961 - val_loss: 1264.6510 - val_mae: 1265.3440\n",
      "Epoch 1758/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 87.2355 - mae: 87.9203 - val_loss: 1296.3638 - val_mae: 1297.0569\n",
      "Epoch 1759/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 80.3258 - mae: 81.0078 - val_loss: 1243.2236 - val_mae: 1243.9159\n",
      "Epoch 1760/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 78.1752 - mae: 78.8573 - val_loss: 1117.5188 - val_mae: 1118.2119\n",
      "Epoch 1761/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 81.1880 - mae: 81.8730 - val_loss: 1151.3093 - val_mae: 1152.0015\n",
      "Epoch 1762/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 86.9618 - mae: 87.6457 - val_loss: 1170.2513 - val_mae: 1170.9445\n",
      "Epoch 1763/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 85.3513 - mae: 86.0350 - val_loss: 1126.9440 - val_mae: 1127.6368\n",
      "Epoch 1764/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 83.7980 - mae: 84.4815 - val_loss: 1197.2072 - val_mae: 1197.9001\n",
      "Epoch 1765/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 87.7641 - mae: 88.4461 - val_loss: 1236.5764 - val_mae: 1237.2695\n",
      "Epoch 1766/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 89.8563 - mae: 90.5416 - val_loss: 1208.7740 - val_mae: 1209.4672\n",
      "Epoch 1767/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 85.7129 - mae: 86.3930 - val_loss: 1079.0341 - val_mae: 1079.7269\n",
      "Epoch 1768/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 91.3009 - mae: 91.9785 - val_loss: 1186.6522 - val_mae: 1187.3455\n",
      "Epoch 1769/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 87.5550 - mae: 88.2396 - val_loss: 1308.5001 - val_mae: 1309.1934\n",
      "Epoch 1770/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 84.5480 - mae: 85.2306 - val_loss: 1121.4399 - val_mae: 1122.1329\n",
      "Epoch 1771/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 92.2197 - mae: 92.9026 - val_loss: 1202.8856 - val_mae: 1203.5769\n",
      "Epoch 1772/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 82.9773 - mae: 83.6617 - val_loss: 1138.5225 - val_mae: 1139.2156\n",
      "Epoch 1773/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 81.9660 - mae: 82.6480 - val_loss: 1197.1492 - val_mae: 1197.8427\n",
      "Epoch 1774/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 85.5346 - mae: 86.2166 - val_loss: 1253.2200 - val_mae: 1253.9132\n",
      "Epoch 1775/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 91.3351 - mae: 92.0166 - val_loss: 1227.6532 - val_mae: 1228.3463\n",
      "Epoch 1776/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 82.7770 - mae: 83.4595 - val_loss: 1249.2411 - val_mae: 1249.9341\n",
      "Epoch 1777/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 80.8722 - mae: 81.5549 - val_loss: 1256.1069 - val_mae: 1256.8002\n",
      "Epoch 1778/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 91.6029 - mae: 92.2849 - val_loss: 1331.9458 - val_mae: 1332.6387\n",
      "Epoch 1779/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 97.1787 - mae: 97.8635 - val_loss: 1260.3170 - val_mae: 1261.0100\n",
      "Epoch 1780/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 85.8834 - mae: 86.5649 - val_loss: 1152.9022 - val_mae: 1153.5952\n",
      "Epoch 1781/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 78.2958 - mae: 78.9761 - val_loss: 1166.3734 - val_mae: 1167.0664\n",
      "Epoch 1782/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 84.1013 - mae: 84.7832 - val_loss: 1180.6769 - val_mae: 1181.3701\n",
      "Epoch 1783/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 86.0389 - mae: 86.7229 - val_loss: 1229.0862 - val_mae: 1229.7793\n",
      "Epoch 1784/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 86.7764 - mae: 87.4588 - val_loss: 1346.3953 - val_mae: 1347.0884\n",
      "Epoch 1785/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 83.8934 - mae: 84.5752 - val_loss: 1232.0543 - val_mae: 1232.7468\n",
      "Epoch 1786/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 93.4535 - mae: 94.1367 - val_loss: 1219.9164 - val_mae: 1220.6090\n",
      "Epoch 1787/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 91.6358 - mae: 92.3168 - val_loss: 1125.7457 - val_mae: 1126.4391\n",
      "Epoch 1788/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 88.8525 - mae: 89.5322 - val_loss: 1550.8625 - val_mae: 1551.5557\n",
      "Epoch 1789/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 91.6037 - mae: 92.2865 - val_loss: 1192.8104 - val_mae: 1193.5037\n",
      "Epoch 1790/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 87.2557 - mae: 87.9418 - val_loss: 1129.7662 - val_mae: 1130.4594\n",
      "Epoch 1791/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 87.0402 - mae: 87.7210 - val_loss: 1131.7146 - val_mae: 1132.4076\n",
      "Epoch 1792/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 98.5613 - mae: 99.2453 - val_loss: 1243.6658 - val_mae: 1244.3588\n",
      "Epoch 1793/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 91.0848 - mae: 91.7690 - val_loss: 1148.5029 - val_mae: 1149.1954\n",
      "Epoch 1794/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 84.3274 - mae: 85.0089 - val_loss: 1154.0129 - val_mae: 1154.7063\n",
      "Epoch 1795/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 81.6934 - mae: 82.3740 - val_loss: 1338.3837 - val_mae: 1339.0768\n",
      "Epoch 1796/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 80.5048 - mae: 81.1882 - val_loss: 1203.8976 - val_mae: 1204.5908\n",
      "Epoch 1797/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 85.3774 - mae: 86.0626 - val_loss: 1518.7356 - val_mae: 1519.4287\n",
      "Epoch 1798/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 94.0255 - mae: 94.7090 - val_loss: 1422.2841 - val_mae: 1422.9763\n",
      "Epoch 1799/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 95.5459 - mae: 96.2289 - val_loss: 1272.3143 - val_mae: 1273.0074\n",
      "Epoch 1800/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 82.5521 - mae: 83.2316 - val_loss: 1222.9012 - val_mae: 1223.5945\n",
      "Epoch 1801/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 82.0051 - mae: 82.6860 - val_loss: 1253.0450 - val_mae: 1253.7383\n",
      "Epoch 1802/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 83.2785 - mae: 83.9576 - val_loss: 1420.0941 - val_mae: 1420.7874\n",
      "Epoch 1803/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 85.2491 - mae: 85.9326 - val_loss: 1168.3010 - val_mae: 1168.9939\n",
      "Epoch 1804/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 87.4806 - mae: 88.1617 - val_loss: 1494.2657 - val_mae: 1494.9586\n",
      "Epoch 1805/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 84.8491 - mae: 85.5324 - val_loss: 1256.6262 - val_mae: 1257.3193\n",
      "Epoch 1806/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 83.4045 - mae: 84.0866 - val_loss: 1232.4966 - val_mae: 1233.1876\n",
      "Epoch 1807/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 80.6978 - mae: 81.3766 - val_loss: 1113.0883 - val_mae: 1113.7812\n",
      "Epoch 1808/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 84.7802 - mae: 85.4601 - val_loss: 1080.1227 - val_mae: 1080.8159\n",
      "Epoch 1809/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 90.6083 - mae: 91.2892 - val_loss: 1304.5195 - val_mae: 1305.2129\n",
      "Epoch 1810/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 88.9726 - mae: 89.6498 - val_loss: 1420.7928 - val_mae: 1421.4856\n",
      "Epoch 1811/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 83.1702 - mae: 83.8568 - val_loss: 1148.3215 - val_mae: 1149.0128\n",
      "Epoch 1812/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 82.4197 - mae: 83.1007 - val_loss: 1131.2047 - val_mae: 1131.8965\n",
      "Epoch 1813/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 99.6864 - mae: 100.3669 - val_loss: 1120.3911 - val_mae: 1121.0842\n",
      "Epoch 1814/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 86.4828 - mae: 87.1681 - val_loss: 1137.3716 - val_mae: 1138.0648\n",
      "Epoch 1815/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 80.6090 - mae: 81.2913 - val_loss: 1212.5043 - val_mae: 1213.1974\n",
      "Epoch 1816/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 87.6035 - mae: 88.2862 - val_loss: 1230.7361 - val_mae: 1231.4291\n",
      "Epoch 1817/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 121.7419 - mae: 122.4289 - val_loss: 1206.1388 - val_mae: 1206.8320\n",
      "Epoch 1818/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 78.0622 - mae: 78.7393 - val_loss: 1255.3251 - val_mae: 1256.0186\n",
      "Epoch 1819/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 78.9165 - mae: 79.5949 - val_loss: 1235.5391 - val_mae: 1236.2307\n",
      "Epoch 1820/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 84.5073 - mae: 85.1899 - val_loss: 1179.0469 - val_mae: 1179.7396\n",
      "Epoch 1821/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 83.9767 - mae: 84.6595 - val_loss: 1216.3099 - val_mae: 1217.0033\n",
      "Epoch 1822/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 80.6163 - mae: 81.2982 - val_loss: 1224.4362 - val_mae: 1225.1295\n",
      "Epoch 1823/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 78.1756 - mae: 78.8550 - val_loss: 1333.3510 - val_mae: 1334.0437\n",
      "Epoch 1824/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 88.5720 - mae: 89.2561 - val_loss: 1313.8821 - val_mae: 1314.5745\n",
      "Epoch 1825/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 87.6503 - mae: 88.3323 - val_loss: 1378.6108 - val_mae: 1379.3042\n",
      "Epoch 1826/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 88.4730 - mae: 89.1553 - val_loss: 1189.5785 - val_mae: 1190.2716\n",
      "Epoch 1827/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 90.1017 - mae: 90.7838 - val_loss: 1125.2448 - val_mae: 1125.9379\n",
      "Epoch 1828/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 78.3007 - mae: 78.9808 - val_loss: 1202.0458 - val_mae: 1202.7372\n",
      "Epoch 1829/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 79.3780 - mae: 80.0585 - val_loss: 1172.2893 - val_mae: 1172.9824\n",
      "Epoch 1830/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 78.1971 - mae: 78.8759 - val_loss: 1322.3462 - val_mae: 1323.0391\n",
      "Epoch 1831/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 77.8453 - mae: 78.5220 - val_loss: 1164.7188 - val_mae: 1165.4108\n",
      "Epoch 1832/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 83.7018 - mae: 84.3831 - val_loss: 1239.7482 - val_mae: 1240.4414\n",
      "Epoch 1833/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 92.0155 - mae: 92.6990 - val_loss: 1217.1342 - val_mae: 1217.8275\n",
      "Epoch 1834/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 87.1730 - mae: 87.8573 - val_loss: 1109.6035 - val_mae: 1110.2968\n",
      "Epoch 1835/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 91.6998 - mae: 92.3836 - val_loss: 1256.0226 - val_mae: 1256.7159\n",
      "Epoch 1836/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 87.9245 - mae: 88.6066 - val_loss: 1192.6017 - val_mae: 1193.2947\n",
      "Epoch 1837/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 81.3103 - mae: 81.9915 - val_loss: 1210.5145 - val_mae: 1211.2075\n",
      "Epoch 1838/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 77.3055 - mae: 77.9856 - val_loss: 1305.3827 - val_mae: 1306.0759\n",
      "Epoch 1839/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 78.3294 - mae: 79.0097 - val_loss: 1206.9727 - val_mae: 1207.6660\n",
      "Epoch 1840/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 88.3613 - mae: 89.0440 - val_loss: 1101.0286 - val_mae: 1101.7218\n",
      "Epoch 1841/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 90.8558 - mae: 91.5368 - val_loss: 1420.8915 - val_mae: 1421.5845\n",
      "Epoch 1842/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 92.8673 - mae: 93.5491 - val_loss: 1228.5892 - val_mae: 1229.2820\n",
      "Epoch 1843/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 84.8396 - mae: 85.5221 - val_loss: 1144.5226 - val_mae: 1145.2147\n",
      "Epoch 1844/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 77.7034 - mae: 78.3831 - val_loss: 1259.7146 - val_mae: 1260.4067\n",
      "Epoch 1845/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 80.9643 - mae: 81.6474 - val_loss: 1318.3135 - val_mae: 1319.0065\n",
      "Epoch 1846/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 85.9696 - mae: 86.6526 - val_loss: 1334.5786 - val_mae: 1335.2717\n",
      "Epoch 1847/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 84.1345 - mae: 84.8160 - val_loss: 1251.1292 - val_mae: 1251.8223\n",
      "Epoch 1848/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 80.8441 - mae: 81.5260 - val_loss: 1090.5714 - val_mae: 1091.2644\n",
      "Epoch 1849/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 82.7557 - mae: 83.4348 - val_loss: 1093.2827 - val_mae: 1093.9752\n",
      "Epoch 1850/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 80.6725 - mae: 81.3552 - val_loss: 1265.8064 - val_mae: 1266.4995\n",
      "Epoch 1851/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 85.4572 - mae: 86.1371 - val_loss: 1225.4547 - val_mae: 1226.1475\n",
      "Epoch 1852/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 84.1410 - mae: 84.8224 - val_loss: 1191.4763 - val_mae: 1192.1696\n",
      "Epoch 1853/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 99.7933 - mae: 100.4772 - val_loss: 1457.1940 - val_mae: 1457.8850\n",
      "Epoch 1854/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 86.8793 - mae: 87.5593 - val_loss: 1465.4033 - val_mae: 1466.0966\n",
      "Epoch 1855/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 88.2851 - mae: 88.9677 - val_loss: 1338.7390 - val_mae: 1339.4323\n",
      "Epoch 1856/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 90.6696 - mae: 91.3488 - val_loss: 1147.6592 - val_mae: 1148.3510\n",
      "Epoch 1857/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 78.9565 - mae: 79.6362 - val_loss: 1174.0630 - val_mae: 1174.7560\n",
      "Epoch 1858/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 86.3010 - mae: 86.9797 - val_loss: 1209.0387 - val_mae: 1209.7318\n",
      "Epoch 1859/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 84.3110 - mae: 84.9932 - val_loss: 1192.8625 - val_mae: 1193.5558\n",
      "Epoch 1860/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 86.7380 - mae: 87.4191 - val_loss: 1313.5222 - val_mae: 1314.2151\n",
      "Epoch 1861/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 86.3150 - mae: 86.9975 - val_loss: 1074.4808 - val_mae: 1075.1740\n",
      "Epoch 1862/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 84.6062 - mae: 85.2876 - val_loss: 1180.3844 - val_mae: 1181.0771\n",
      "Epoch 1863/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 96.4310 - mae: 97.1108 - val_loss: 1240.2701 - val_mae: 1240.9631\n",
      "Epoch 1864/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 82.5149 - mae: 83.1937 - val_loss: 1295.4812 - val_mae: 1296.1746\n",
      "Epoch 1865/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 79.8724 - mae: 80.5512 - val_loss: 1179.6652 - val_mae: 1180.3579\n",
      "Epoch 1866/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 81.0304 - mae: 81.7105 - val_loss: 1341.2948 - val_mae: 1341.9875\n",
      "Epoch 1867/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 81.3781 - mae: 82.0569 - val_loss: 1128.5677 - val_mae: 1129.2601\n",
      "Epoch 1868/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 78.8489 - mae: 79.5291 - val_loss: 1308.3735 - val_mae: 1309.0669\n",
      "Epoch 1869/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 81.5953 - mae: 82.2791 - val_loss: 1205.0563 - val_mae: 1205.7493\n",
      "Epoch 1870/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 80.3044 - mae: 80.9817 - val_loss: 1190.5089 - val_mae: 1191.2020\n",
      "Epoch 1871/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 80.7549 - mae: 81.4307 - val_loss: 1234.3713 - val_mae: 1235.0646\n",
      "Epoch 1872/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 84.4258 - mae: 85.1064 - val_loss: 1338.7729 - val_mae: 1339.4661\n",
      "Epoch 1873/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 81.7968 - mae: 82.4758 - val_loss: 1297.2701 - val_mae: 1297.9634\n",
      "Epoch 1874/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 86.1078 - mae: 86.7874 - val_loss: 1214.0168 - val_mae: 1214.7091\n",
      "Epoch 1875/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 82.4831 - mae: 83.1665 - val_loss: 1319.8282 - val_mae: 1320.5205\n",
      "Epoch 1876/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 79.1595 - mae: 79.8376 - val_loss: 1316.9062 - val_mae: 1317.5994\n",
      "Epoch 1877/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 88.0576 - mae: 88.7375 - val_loss: 1414.8545 - val_mae: 1415.5477\n",
      "Epoch 1878/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 86.6754 - mae: 87.3583 - val_loss: 1259.3569 - val_mae: 1260.0500\n",
      "Epoch 1879/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 85.1370 - mae: 85.8166 - val_loss: 1438.5735 - val_mae: 1439.2654\n",
      "Epoch 1880/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 86.0889 - mae: 86.7676 - val_loss: 1236.6554 - val_mae: 1237.3485\n",
      "Epoch 1881/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 83.8920 - mae: 84.5766 - val_loss: 1244.2830 - val_mae: 1244.9753\n",
      "Epoch 1882/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 85.0905 - mae: 85.7705 - val_loss: 1265.4843 - val_mae: 1266.1774\n",
      "Epoch 1883/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 84.7731 - mae: 85.4548 - val_loss: 1139.0154 - val_mae: 1139.7074\n",
      "Epoch 1884/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 92.6057 - mae: 93.2912 - val_loss: 1188.3386 - val_mae: 1189.0316\n",
      "Epoch 1885/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 81.5518 - mae: 82.2330 - val_loss: 1197.8228 - val_mae: 1198.5162\n",
      "Epoch 1886/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 80.5755 - mae: 81.2578 - val_loss: 1192.8365 - val_mae: 1193.5298\n",
      "Epoch 1887/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 80.7091 - mae: 81.3873 - val_loss: 1384.0712 - val_mae: 1384.7644\n",
      "Epoch 1888/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 82.1453 - mae: 82.8260 - val_loss: 1124.0267 - val_mae: 1124.7198\n",
      "Epoch 1889/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 83.0178 - mae: 83.6992 - val_loss: 1224.8612 - val_mae: 1225.5544\n",
      "Epoch 1890/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 78.9037 - mae: 79.5863 - val_loss: 1217.5956 - val_mae: 1218.2887\n",
      "Epoch 1891/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 84.5367 - mae: 85.2171 - val_loss: 1310.7201 - val_mae: 1311.4133\n",
      "Epoch 1892/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 80.7275 - mae: 81.4088 - val_loss: 1238.0109 - val_mae: 1238.7040\n",
      "Epoch 1893/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 76.9804 - mae: 77.6583 - val_loss: 1417.1907 - val_mae: 1417.8837\n",
      "Epoch 1894/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 93.8887 - mae: 94.5706 - val_loss: 1200.4293 - val_mae: 1201.1224\n",
      "Epoch 1895/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 86.0003 - mae: 86.6823 - val_loss: 1296.6582 - val_mae: 1297.3513\n",
      "Epoch 1896/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 81.9461 - mae: 82.6255 - val_loss: 1147.8225 - val_mae: 1148.5154\n",
      "Epoch 1897/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 76.1599 - mae: 76.8391 - val_loss: 1176.4043 - val_mae: 1177.0974\n",
      "Epoch 1898/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 76.3377 - mae: 77.0177 - val_loss: 1262.1410 - val_mae: 1262.8337\n",
      "Epoch 1899/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 92.9381 - mae: 93.6208 - val_loss: 1200.8540 - val_mae: 1201.5471\n",
      "Epoch 1900/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 93.3076 - mae: 93.9919 - val_loss: 1207.3370 - val_mae: 1208.0288\n",
      "Epoch 1901/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 81.7322 - mae: 82.4162 - val_loss: 1271.2911 - val_mae: 1271.9835\n",
      "Epoch 1902/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 84.5885 - mae: 85.2705 - val_loss: 1239.8566 - val_mae: 1240.5498\n",
      "Epoch 1903/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 84.6141 - mae: 85.2982 - val_loss: 1253.9065 - val_mae: 1254.5991\n",
      "Epoch 1904/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 80.3259 - mae: 81.0078 - val_loss: 1219.5109 - val_mae: 1220.2042\n",
      "Epoch 1905/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 93.9759 - mae: 94.6570 - val_loss: 1113.0669 - val_mae: 1113.7588\n",
      "Epoch 1906/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 80.6354 - mae: 81.3165 - val_loss: 1309.9420 - val_mae: 1310.6351\n",
      "Epoch 1907/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 82.5342 - mae: 83.2160 - val_loss: 1227.0254 - val_mae: 1227.7185\n",
      "Epoch 1908/5000\n",
      "46/46 [==============================] - 2s 40ms/step - loss: 84.7672 - mae: 85.4493 - val_loss: 1145.1515 - val_mae: 1145.8442\n",
      "Epoch 1909/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 82.4247 - mae: 83.1024 - val_loss: 1176.2458 - val_mae: 1176.9388\n",
      "Epoch 1910/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 79.0619 - mae: 79.7406 - val_loss: 1179.8363 - val_mae: 1180.5294\n",
      "Epoch 1911/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 78.0351 - mae: 78.7168 - val_loss: 1176.2399 - val_mae: 1176.9327\n",
      "Epoch 1912/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 82.7548 - mae: 83.4374 - val_loss: 1277.6141 - val_mae: 1278.3073\n",
      "Epoch 1913/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 84.2093 - mae: 84.8930 - val_loss: 1213.5442 - val_mae: 1214.2366\n",
      "Epoch 1914/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 84.6868 - mae: 85.3702 - val_loss: 1292.7206 - val_mae: 1293.4135\n",
      "Epoch 1915/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 79.3959 - mae: 80.0767 - val_loss: 1227.1141 - val_mae: 1227.8074\n",
      "Epoch 1916/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 86.0161 - mae: 86.6979 - val_loss: 1433.9803 - val_mae: 1434.6735\n",
      "Epoch 1917/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 90.2503 - mae: 90.9332 - val_loss: 1030.2983 - val_mae: 1030.9908\n",
      "Epoch 1918/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 94.0345 - mae: 94.7179 - val_loss: 1181.4695 - val_mae: 1182.1626\n",
      "Epoch 1919/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 82.6506 - mae: 83.3297 - val_loss: 1195.8552 - val_mae: 1196.5483\n",
      "Epoch 1920/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 84.5045 - mae: 85.1871 - val_loss: 1292.5944 - val_mae: 1293.2869\n",
      "Epoch 1921/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 83.9629 - mae: 84.6440 - val_loss: 1171.6826 - val_mae: 1172.3759\n",
      "Epoch 1922/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 85.6923 - mae: 86.3758 - val_loss: 1253.4596 - val_mae: 1254.1527\n",
      "Epoch 1923/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 80.2477 - mae: 80.9291 - val_loss: 1270.9783 - val_mae: 1271.6714\n",
      "Epoch 1924/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 79.4288 - mae: 80.1107 - val_loss: 1175.9709 - val_mae: 1176.6643\n",
      "Epoch 1925/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 78.8588 - mae: 79.5387 - val_loss: 1182.3215 - val_mae: 1183.0149\n",
      "Epoch 1926/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 82.4144 - mae: 83.0951 - val_loss: 1200.4321 - val_mae: 1201.1256\n",
      "Epoch 1927/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 78.6307 - mae: 79.3116 - val_loss: 1240.4664 - val_mae: 1241.1589\n",
      "Epoch 1928/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 86.4960 - mae: 87.1805 - val_loss: 1211.3419 - val_mae: 1212.0342\n",
      "Epoch 1929/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 81.5130 - mae: 82.1925 - val_loss: 1181.4813 - val_mae: 1182.1741\n",
      "Epoch 1930/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 78.1071 - mae: 78.7873 - val_loss: 1122.4829 - val_mae: 1123.1760\n",
      "Epoch 1931/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 80.9871 - mae: 81.6701 - val_loss: 1322.8424 - val_mae: 1323.5343\n",
      "Epoch 1932/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 78.9210 - mae: 79.6037 - val_loss: 1231.8313 - val_mae: 1232.5240\n",
      "Epoch 1933/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 85.2810 - mae: 85.9657 - val_loss: 1225.6001 - val_mae: 1226.2933\n",
      "Epoch 1934/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 79.1995 - mae: 79.8837 - val_loss: 1209.2504 - val_mae: 1209.9436\n",
      "Epoch 1935/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 81.8695 - mae: 82.5531 - val_loss: 1292.1550 - val_mae: 1292.8480\n",
      "Epoch 1936/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 84.5450 - mae: 85.2217 - val_loss: 1464.4969 - val_mae: 1465.1901\n",
      "Epoch 1937/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 85.6971 - mae: 86.3767 - val_loss: 1184.7291 - val_mae: 1185.4222\n",
      "Epoch 1938/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 79.7240 - mae: 80.4027 - val_loss: 1186.5599 - val_mae: 1187.2532\n",
      "Epoch 1939/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 82.3339 - mae: 83.0110 - val_loss: 1298.2000 - val_mae: 1298.8932\n",
      "Epoch 1940/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 80.2962 - mae: 80.9780 - val_loss: 1278.7778 - val_mae: 1279.4709\n",
      "Epoch 1941/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 80.5523 - mae: 81.2341 - val_loss: 1262.7993 - val_mae: 1263.4924\n",
      "Epoch 1942/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 81.7656 - mae: 82.4452 - val_loss: 1265.4205 - val_mae: 1266.1129\n",
      "Epoch 1943/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 79.3743 - mae: 80.0528 - val_loss: 1218.7498 - val_mae: 1219.4427\n",
      "Epoch 1944/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 78.2965 - mae: 78.9758 - val_loss: 1191.0480 - val_mae: 1191.7412\n",
      "Epoch 1945/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 83.5436 - mae: 84.2248 - val_loss: 1293.6129 - val_mae: 1294.3060\n",
      "Epoch 1946/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 86.8617 - mae: 87.5441 - val_loss: 1205.9653 - val_mae: 1206.6584\n",
      "Epoch 1947/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 88.9066 - mae: 89.5868 - val_loss: 1386.6279 - val_mae: 1387.3210\n",
      "Epoch 1948/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 82.8953 - mae: 83.5772 - val_loss: 1170.1276 - val_mae: 1170.8208\n",
      "Epoch 1949/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 81.9099 - mae: 82.5906 - val_loss: 1163.3845 - val_mae: 1164.0778\n",
      "Epoch 1950/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 82.5840 - mae: 83.2641 - val_loss: 1185.9282 - val_mae: 1186.6200\n",
      "Epoch 1951/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 81.3113 - mae: 81.9925 - val_loss: 1329.7450 - val_mae: 1330.4381\n",
      "Epoch 1952/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 77.8263 - mae: 78.5076 - val_loss: 1114.6748 - val_mae: 1115.3676\n",
      "Epoch 1953/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 80.5858 - mae: 81.2646 - val_loss: 1283.0543 - val_mae: 1283.7469\n",
      "Epoch 1954/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 77.7167 - mae: 78.3967 - val_loss: 1171.2191 - val_mae: 1171.9115\n",
      "Epoch 1955/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 81.3059 - mae: 81.9854 - val_loss: 1194.8845 - val_mae: 1195.5778\n",
      "Epoch 1956/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 79.9109 - mae: 80.5909 - val_loss: 1433.0439 - val_mae: 1433.7371\n",
      "Epoch 1957/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 81.7025 - mae: 82.3845 - val_loss: 1282.9535 - val_mae: 1283.6466\n",
      "Epoch 1958/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 79.6636 - mae: 80.3443 - val_loss: 1274.1852 - val_mae: 1274.8785\n",
      "Epoch 1959/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 82.9527 - mae: 83.6346 - val_loss: 1144.3939 - val_mae: 1145.0873\n",
      "Epoch 1960/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 82.7229 - mae: 83.4057 - val_loss: 1220.2052 - val_mae: 1220.8976\n",
      "Epoch 1961/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 80.0756 - mae: 80.7623 - val_loss: 1245.2191 - val_mae: 1245.9124\n",
      "Epoch 1962/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 79.0233 - mae: 79.7048 - val_loss: 1198.3511 - val_mae: 1199.0437\n",
      "Epoch 1963/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 87.7153 - mae: 88.3956 - val_loss: 1360.4486 - val_mae: 1361.1417\n",
      "Epoch 1964/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 89.3886 - mae: 90.0719 - val_loss: 1268.7494 - val_mae: 1269.4408\n",
      "Epoch 1965/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 77.4518 - mae: 78.1353 - val_loss: 1278.5316 - val_mae: 1279.2249\n",
      "Epoch 1966/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 84.2794 - mae: 84.9590 - val_loss: 1235.9205 - val_mae: 1236.6121\n",
      "Epoch 1967/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 84.8705 - mae: 85.5513 - val_loss: 1371.3645 - val_mae: 1372.0575\n",
      "Epoch 1968/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 87.5250 - mae: 88.2084 - val_loss: 1254.4684 - val_mae: 1255.1611\n",
      "Epoch 1969/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 79.7507 - mae: 80.4326 - val_loss: 1232.9700 - val_mae: 1233.6630\n",
      "Epoch 1970/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 77.4406 - mae: 78.1228 - val_loss: 1186.0089 - val_mae: 1186.7014\n",
      "Epoch 1971/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 77.6592 - mae: 78.3394 - val_loss: 1300.2876 - val_mae: 1300.9808\n",
      "Epoch 1972/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 85.1026 - mae: 85.7813 - val_loss: 1039.6230 - val_mae: 1040.3162\n",
      "Epoch 1973/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 94.0856 - mae: 94.7695 - val_loss: 1141.4066 - val_mae: 1142.0999\n",
      "Epoch 1974/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 79.2877 - mae: 79.9690 - val_loss: 1220.7463 - val_mae: 1221.4397\n",
      "Epoch 1975/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 84.5090 - mae: 85.1864 - val_loss: 1320.7152 - val_mae: 1321.4071\n",
      "Epoch 1976/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 83.4370 - mae: 84.1195 - val_loss: 1135.5798 - val_mae: 1136.2728\n",
      "Epoch 1977/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 88.5315 - mae: 89.2139 - val_loss: 1090.9309 - val_mae: 1091.6238\n",
      "Epoch 1978/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 82.6314 - mae: 83.3146 - val_loss: 1233.8148 - val_mae: 1234.5078\n",
      "Epoch 1979/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 78.2893 - mae: 78.9702 - val_loss: 1152.2003 - val_mae: 1152.8931\n",
      "Epoch 1980/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 80.7625 - mae: 81.4430 - val_loss: 1229.8545 - val_mae: 1230.5468\n",
      "Epoch 1981/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 76.2025 - mae: 76.8815 - val_loss: 1300.8827 - val_mae: 1301.5747\n",
      "Epoch 1982/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 87.7507 - mae: 88.4334 - val_loss: 1399.6008 - val_mae: 1400.2942\n",
      "Epoch 1983/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 84.8234 - mae: 85.5045 - val_loss: 1145.2609 - val_mae: 1145.9536\n",
      "Epoch 1984/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 80.6289 - mae: 81.3134 - val_loss: 1269.3667 - val_mae: 1270.0597\n",
      "Epoch 1985/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 76.8788 - mae: 77.5595 - val_loss: 1246.3588 - val_mae: 1247.0519\n",
      "Epoch 1986/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 80.2872 - mae: 80.9674 - val_loss: 1329.7616 - val_mae: 1330.4536\n",
      "Epoch 1987/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 80.8832 - mae: 81.5674 - val_loss: 1334.8511 - val_mae: 1335.5431\n",
      "Epoch 1988/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 85.1604 - mae: 85.8403 - val_loss: 1088.8499 - val_mae: 1089.5416\n",
      "Epoch 1989/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 100.4953 - mae: 101.1798 - val_loss: 1310.1526 - val_mae: 1310.8457\n",
      "Epoch 1990/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 80.6896 - mae: 81.3710 - val_loss: 1325.2716 - val_mae: 1325.9647\n",
      "Epoch 1991/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 84.3581 - mae: 85.0408 - val_loss: 1214.8357 - val_mae: 1215.5283\n",
      "Epoch 1992/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 79.9196 - mae: 80.5986 - val_loss: 1179.3440 - val_mae: 1180.0372\n",
      "Epoch 1993/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 85.1714 - mae: 85.8553 - val_loss: 1186.4530 - val_mae: 1187.1455\n",
      "Epoch 1994/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 83.6307 - mae: 84.3134 - val_loss: 1106.1519 - val_mae: 1106.8447\n",
      "Epoch 1995/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 88.8907 - mae: 89.5747 - val_loss: 1134.1066 - val_mae: 1134.7997\n",
      "Epoch 1996/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 78.3968 - mae: 79.0769 - val_loss: 1318.9719 - val_mae: 1319.6650\n",
      "Epoch 1997/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 89.5622 - mae: 90.2459 - val_loss: 1173.4194 - val_mae: 1174.1127\n",
      "Epoch 1998/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 82.8784 - mae: 83.5619 - val_loss: 1035.3466 - val_mae: 1036.0397\n",
      "Epoch 1999/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 94.7498 - mae: 95.4300 - val_loss: 1257.4730 - val_mae: 1258.1659\n",
      "Epoch 2000/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 78.3647 - mae: 79.0453 - val_loss: 1166.9543 - val_mae: 1167.6473\n",
      "Epoch 2001/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 83.7768 - mae: 84.4597 - val_loss: 1318.3445 - val_mae: 1319.0369\n",
      "Epoch 2002/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 89.9291 - mae: 90.6117 - val_loss: 1258.0938 - val_mae: 1258.7869\n",
      "Epoch 2003/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 81.9707 - mae: 82.6524 - val_loss: 1233.2330 - val_mae: 1233.9261\n",
      "Epoch 2004/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 82.4521 - mae: 83.1361 - val_loss: 1217.7904 - val_mae: 1218.4825\n",
      "Epoch 2005/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 83.3343 - mae: 84.0123 - val_loss: 1199.8314 - val_mae: 1200.5247\n",
      "Epoch 2006/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 82.2750 - mae: 82.9564 - val_loss: 1243.8562 - val_mae: 1244.5493\n",
      "Epoch 2007/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 77.1259 - mae: 77.8057 - val_loss: 1419.6958 - val_mae: 1420.3889\n",
      "Epoch 2008/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 77.9748 - mae: 78.6552 - val_loss: 1193.5270 - val_mae: 1194.2201\n",
      "Epoch 2009/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 76.3851 - mae: 77.0675 - val_loss: 1320.6821 - val_mae: 1321.3755\n",
      "Epoch 2010/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 78.1639 - mae: 78.8448 - val_loss: 1276.1608 - val_mae: 1276.8539\n",
      "Epoch 2011/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 84.1559 - mae: 84.8368 - val_loss: 1335.6926 - val_mae: 1336.3860\n",
      "Epoch 2012/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 90.3743 - mae: 91.0562 - val_loss: 1295.0745 - val_mae: 1295.7676\n",
      "Epoch 2013/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 88.4371 - mae: 89.1161 - val_loss: 1299.0453 - val_mae: 1299.7379\n",
      "Epoch 2014/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 81.2701 - mae: 81.9542 - val_loss: 1173.6746 - val_mae: 1174.3671\n",
      "Epoch 2015/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 82.9687 - mae: 83.6499 - val_loss: 1233.2524 - val_mae: 1233.9454\n",
      "Epoch 2016/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 79.4101 - mae: 80.0895 - val_loss: 1136.8669 - val_mae: 1137.5603\n",
      "Epoch 2017/5000\n",
      "46/46 [==============================] - 2s 41ms/step - loss: 87.8113 - mae: 88.4903 - val_loss: 1097.3610 - val_mae: 1098.0542\n",
      "Epoch 2018/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 82.1576 - mae: 82.8401 - val_loss: 1138.1501 - val_mae: 1138.8434\n",
      "Epoch 2019/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 79.3351 - mae: 80.0195 - val_loss: 1085.2268 - val_mae: 1085.9197\n",
      "Epoch 2020/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 77.4035 - mae: 78.0808 - val_loss: 1135.8005 - val_mae: 1136.4927\n",
      "Epoch 2021/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 85.0551 - mae: 85.7378 - val_loss: 1218.7965 - val_mae: 1219.4896\n",
      "Epoch 2022/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 77.6349 - mae: 78.3164 - val_loss: 1233.9742 - val_mae: 1234.6674\n",
      "Epoch 2023/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 82.7755 - mae: 83.4576 - val_loss: 1249.3951 - val_mae: 1250.0883\n",
      "Epoch 2024/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 81.9117 - mae: 82.5965 - val_loss: 1139.3226 - val_mae: 1140.0159\n",
      "Epoch 2025/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 83.2945 - mae: 83.9763 - val_loss: 1332.7451 - val_mae: 1333.4386\n",
      "Epoch 2026/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 85.4146 - mae: 86.0920 - val_loss: 1213.2633 - val_mae: 1213.9554\n",
      "Epoch 2027/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 83.3191 - mae: 83.9981 - val_loss: 1235.2444 - val_mae: 1235.9376\n",
      "Epoch 2028/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 78.2355 - mae: 78.9184 - val_loss: 1156.5331 - val_mae: 1157.2256\n",
      "Epoch 2029/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 84.5335 - mae: 85.2144 - val_loss: 1084.8756 - val_mae: 1085.5687\n",
      "Epoch 2030/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 78.2260 - mae: 78.9079 - val_loss: 1178.6964 - val_mae: 1179.3890\n",
      "Epoch 2031/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 85.4943 - mae: 86.1772 - val_loss: 1136.1599 - val_mae: 1136.8527\n",
      "Epoch 2032/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 81.9012 - mae: 82.5820 - val_loss: 1122.1964 - val_mae: 1122.8887\n",
      "Epoch 2033/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 82.0358 - mae: 82.7183 - val_loss: 1151.3439 - val_mae: 1152.0370\n",
      "Epoch 2034/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 74.6386 - mae: 75.3175 - val_loss: 1186.3278 - val_mae: 1187.0201\n",
      "Epoch 2035/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 79.1988 - mae: 79.8791 - val_loss: 1193.2109 - val_mae: 1193.9042\n",
      "Epoch 2036/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 77.9069 - mae: 78.5902 - val_loss: 1186.5566 - val_mae: 1187.2498\n",
      "Epoch 2037/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 80.0833 - mae: 80.7623 - val_loss: 1223.7218 - val_mae: 1224.4149\n",
      "Epoch 2038/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 76.3978 - mae: 77.0818 - val_loss: 1159.6433 - val_mae: 1160.3363\n",
      "Epoch 2039/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 78.3979 - mae: 79.0777 - val_loss: 1203.0901 - val_mae: 1203.7814\n",
      "Epoch 2040/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 82.3012 - mae: 82.9832 - val_loss: 1393.8223 - val_mae: 1394.5156\n",
      "Epoch 2041/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 94.4618 - mae: 95.1466 - val_loss: 1245.4314 - val_mae: 1246.1246\n",
      "Epoch 2042/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 83.2467 - mae: 83.9265 - val_loss: 1187.7908 - val_mae: 1188.4840\n",
      "Epoch 2043/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 76.2465 - mae: 76.9285 - val_loss: 1425.1700 - val_mae: 1425.8624\n",
      "Epoch 2044/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 92.1000 - mae: 92.7788 - val_loss: 1369.2034 - val_mae: 1369.8956\n",
      "Epoch 2045/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 78.4773 - mae: 79.1581 - val_loss: 1204.3730 - val_mae: 1205.0662\n",
      "Epoch 2046/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 81.1917 - mae: 81.8720 - val_loss: 1014.1718 - val_mae: 1014.8649\n",
      "Epoch 2047/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 83.4383 - mae: 84.1180 - val_loss: 1196.7462 - val_mae: 1197.4384\n",
      "Epoch 2048/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 77.3657 - mae: 78.0464 - val_loss: 1255.7617 - val_mae: 1256.4548\n",
      "Epoch 2049/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 80.7841 - mae: 81.4627 - val_loss: 1205.8438 - val_mae: 1206.5367\n",
      "Epoch 2050/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 86.0533 - mae: 86.7372 - val_loss: 1388.4476 - val_mae: 1389.1405\n",
      "Epoch 2051/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 83.3289 - mae: 84.0098 - val_loss: 1242.0942 - val_mae: 1242.7874\n",
      "Epoch 2052/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 80.4754 - mae: 81.1586 - val_loss: 1252.8097 - val_mae: 1253.5028\n",
      "Epoch 2053/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 76.6876 - mae: 77.3674 - val_loss: 1181.1680 - val_mae: 1181.8610\n",
      "Epoch 2054/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 81.1546 - mae: 81.8377 - val_loss: 1524.4156 - val_mae: 1525.1089\n",
      "Epoch 2055/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 86.2605 - mae: 86.9412 - val_loss: 1188.2402 - val_mae: 1188.9332\n",
      "Epoch 2056/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 77.7038 - mae: 78.3858 - val_loss: 1194.1858 - val_mae: 1194.8787\n",
      "Epoch 2057/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 83.8419 - mae: 84.5240 - val_loss: 1112.8665 - val_mae: 1113.5596\n",
      "Epoch 2058/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 78.3848 - mae: 79.0663 - val_loss: 1124.4237 - val_mae: 1125.1169\n",
      "Epoch 2059/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 80.0080 - mae: 80.6894 - val_loss: 1396.8344 - val_mae: 1397.5269\n",
      "Epoch 2060/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 76.9673 - mae: 77.6464 - val_loss: 1129.1685 - val_mae: 1129.8616\n",
      "Epoch 2061/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 82.1648 - mae: 82.8468 - val_loss: 1148.6448 - val_mae: 1149.3378\n",
      "Epoch 2062/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 82.2716 - mae: 82.9479 - val_loss: 1267.6215 - val_mae: 1268.3143\n",
      "Epoch 2063/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 79.2313 - mae: 79.9113 - val_loss: 1150.2400 - val_mae: 1150.9330\n",
      "Epoch 2064/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 78.4390 - mae: 79.1223 - val_loss: 1250.0453 - val_mae: 1250.7378\n",
      "Epoch 2065/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 82.9496 - mae: 83.6310 - val_loss: 1263.2217 - val_mae: 1263.9149\n",
      "Epoch 2066/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 81.6666 - mae: 82.3492 - val_loss: 1100.4231 - val_mae: 1101.1157\n",
      "Epoch 2067/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 90.5127 - mae: 91.1953 - val_loss: 1123.7734 - val_mae: 1124.4666\n",
      "Epoch 2068/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 78.3719 - mae: 79.0524 - val_loss: 1204.2810 - val_mae: 1204.9740\n",
      "Epoch 2069/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 76.5330 - mae: 77.2120 - val_loss: 1130.7407 - val_mae: 1131.4333\n",
      "Epoch 2070/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 84.0007 - mae: 84.6764 - val_loss: 1387.6506 - val_mae: 1388.3438\n",
      "Epoch 2071/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 85.0168 - mae: 85.6993 - val_loss: 1270.4205 - val_mae: 1271.1138\n",
      "Epoch 2072/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 75.5030 - mae: 76.1841 - val_loss: 1153.1680 - val_mae: 1153.8612\n",
      "Epoch 2073/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 75.3032 - mae: 75.9856 - val_loss: 1265.5344 - val_mae: 1266.2277\n",
      "Epoch 2074/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 73.8339 - mae: 74.5162 - val_loss: 1139.2979 - val_mae: 1139.9908\n",
      "Epoch 2075/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 90.9232 - mae: 91.6081 - val_loss: 1196.2657 - val_mae: 1196.9589\n",
      "Epoch 2076/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 85.0219 - mae: 85.7072 - val_loss: 1178.7637 - val_mae: 1179.4568\n",
      "Epoch 2077/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 81.5341 - mae: 82.2139 - val_loss: 1197.1323 - val_mae: 1197.8242\n",
      "Epoch 2078/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 78.0273 - mae: 78.7098 - val_loss: 1142.8219 - val_mae: 1143.5153\n",
      "Epoch 2079/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 79.2219 - mae: 79.9031 - val_loss: 1280.4645 - val_mae: 1281.1573\n",
      "Epoch 2080/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 76.5394 - mae: 77.2155 - val_loss: 1177.0856 - val_mae: 1177.7789\n",
      "Epoch 2081/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 74.9409 - mae: 75.6212 - val_loss: 1205.8444 - val_mae: 1206.5376\n",
      "Epoch 2082/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 79.4208 - mae: 80.1015 - val_loss: 1396.0472 - val_mae: 1396.7405\n",
      "Epoch 2083/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 87.7286 - mae: 88.4141 - val_loss: 1161.8230 - val_mae: 1162.5164\n",
      "Epoch 2084/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 78.9403 - mae: 79.6215 - val_loss: 1099.2894 - val_mae: 1099.9824\n",
      "Epoch 2085/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 84.6882 - mae: 85.3656 - val_loss: 1414.9906 - val_mae: 1415.6837\n",
      "Epoch 2086/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 96.9558 - mae: 97.6384 - val_loss: 1139.9659 - val_mae: 1140.6591\n",
      "Epoch 2087/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 79.9196 - mae: 80.6024 - val_loss: 1200.0690 - val_mae: 1200.7621\n",
      "Epoch 2088/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 79.9898 - mae: 80.6654 - val_loss: 1338.8075 - val_mae: 1339.5007\n",
      "Epoch 2089/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 80.1257 - mae: 80.8054 - val_loss: 1266.1298 - val_mae: 1266.8228\n",
      "Epoch 2090/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 80.7338 - mae: 81.4118 - val_loss: 1170.2844 - val_mae: 1170.9760\n",
      "Epoch 2091/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 82.1536 - mae: 82.8349 - val_loss: 1161.2316 - val_mae: 1161.9243\n",
      "Epoch 2092/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 82.0927 - mae: 82.7771 - val_loss: 1315.0337 - val_mae: 1315.7268\n",
      "Epoch 2093/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 99.4254 - mae: 100.1086 - val_loss: 1062.9614 - val_mae: 1063.6537\n",
      "Epoch 2094/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 82.6376 - mae: 83.3249 - val_loss: 1336.2317 - val_mae: 1336.9249\n",
      "Epoch 2095/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 80.8584 - mae: 81.5395 - val_loss: 1238.6060 - val_mae: 1239.2992\n",
      "Epoch 2096/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 85.3880 - mae: 86.0674 - val_loss: 1167.8646 - val_mae: 1168.5543\n",
      "Epoch 2097/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 73.7971 - mae: 74.4747 - val_loss: 1317.2300 - val_mae: 1317.9232\n",
      "Epoch 2098/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 79.6486 - mae: 80.3270 - val_loss: 1254.7531 - val_mae: 1255.4460\n",
      "Epoch 2099/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 83.1344 - mae: 83.8173 - val_loss: 1188.5336 - val_mae: 1189.2269\n",
      "Epoch 2100/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 81.1706 - mae: 81.8520 - val_loss: 1205.2690 - val_mae: 1205.9607\n",
      "Epoch 2101/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 82.7054 - mae: 83.3857 - val_loss: 1193.7925 - val_mae: 1194.4855\n",
      "Epoch 2102/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 84.9421 - mae: 85.6213 - val_loss: 1344.3093 - val_mae: 1345.0017\n",
      "Epoch 2103/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 86.9007 - mae: 87.5824 - val_loss: 1224.4862 - val_mae: 1225.1791\n",
      "Epoch 2104/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 80.5527 - mae: 81.2335 - val_loss: 1078.0601 - val_mae: 1078.7532\n",
      "Epoch 2105/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 84.9102 - mae: 85.5941 - val_loss: 1138.7401 - val_mae: 1139.4320\n",
      "Epoch 2106/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 82.9354 - mae: 83.6198 - val_loss: 1317.7620 - val_mae: 1318.4546\n",
      "Epoch 2107/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 76.8250 - mae: 77.5019 - val_loss: 1359.8607 - val_mae: 1360.5540\n",
      "Epoch 2108/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 99.0720 - mae: 99.7562 - val_loss: 1160.1938 - val_mae: 1160.8871\n",
      "Epoch 2109/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 81.1997 - mae: 81.8840 - val_loss: 1235.2836 - val_mae: 1235.9768\n",
      "Epoch 2110/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 76.3800 - mae: 77.0639 - val_loss: 1161.9639 - val_mae: 1162.6569\n",
      "Epoch 2111/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 81.6294 - mae: 82.3088 - val_loss: 1164.2006 - val_mae: 1164.8936\n",
      "Epoch 2112/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 72.4199 - mae: 73.0984 - val_loss: 1225.7012 - val_mae: 1226.3936\n",
      "Epoch 2113/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 79.1345 - mae: 79.8168 - val_loss: 1272.4388 - val_mae: 1273.1316\n",
      "Epoch 2114/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 86.3017 - mae: 86.9845 - val_loss: 1254.1688 - val_mae: 1254.8619\n",
      "Epoch 2115/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 82.0724 - mae: 82.7547 - val_loss: 1168.5231 - val_mae: 1169.2163\n",
      "Epoch 2116/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 75.0497 - mae: 75.7289 - val_loss: 1224.0024 - val_mae: 1224.6954\n",
      "Epoch 2117/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 86.9184 - mae: 87.5969 - val_loss: 1182.2853 - val_mae: 1182.9786\n",
      "Epoch 2118/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 82.3134 - mae: 82.9907 - val_loss: 1108.1757 - val_mae: 1108.8689\n",
      "Epoch 2119/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 93.4839 - mae: 94.1641 - val_loss: 1146.7341 - val_mae: 1147.4268\n",
      "Epoch 2120/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 78.3773 - mae: 79.0577 - val_loss: 1201.0000 - val_mae: 1201.6920\n",
      "Epoch 2121/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 85.6088 - mae: 86.2927 - val_loss: 1355.3982 - val_mae: 1356.0914\n",
      "Epoch 2122/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 95.6072 - mae: 96.2901 - val_loss: 1079.5626 - val_mae: 1080.2540\n",
      "Epoch 2123/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 98.4551 - mae: 99.1417 - val_loss: 1298.5283 - val_mae: 1299.2201\n",
      "Epoch 2124/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 76.1286 - mae: 76.8061 - val_loss: 1285.7501 - val_mae: 1286.4426\n",
      "Epoch 2125/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 76.4101 - mae: 77.0915 - val_loss: 1241.9569 - val_mae: 1242.6501\n",
      "Epoch 2126/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 75.5173 - mae: 76.1964 - val_loss: 1231.8494 - val_mae: 1232.5427\n",
      "Epoch 2127/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 81.4246 - mae: 82.1040 - val_loss: 1234.4453 - val_mae: 1235.1385\n",
      "Epoch 2128/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 84.0498 - mae: 84.7302 - val_loss: 1191.3007 - val_mae: 1191.9938\n",
      "Epoch 2129/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 84.7679 - mae: 85.4494 - val_loss: 1162.8671 - val_mae: 1163.5601\n",
      "Epoch 2130/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 85.4029 - mae: 86.0852 - val_loss: 1083.9467 - val_mae: 1084.6396\n",
      "Epoch 2131/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 76.4414 - mae: 77.1170 - val_loss: 1242.6786 - val_mae: 1243.3718\n",
      "Epoch 2132/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 77.0044 - mae: 77.6884 - val_loss: 1247.1222 - val_mae: 1247.8156\n",
      "Epoch 2133/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 78.6231 - mae: 79.3044 - val_loss: 1202.5238 - val_mae: 1203.2170\n",
      "Epoch 2134/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 76.0975 - mae: 76.7773 - val_loss: 1177.8552 - val_mae: 1178.5479\n",
      "Epoch 2135/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 76.1576 - mae: 76.8404 - val_loss: 1186.6282 - val_mae: 1187.3213\n",
      "Epoch 2136/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 74.1307 - mae: 74.8127 - val_loss: 1228.3557 - val_mae: 1229.0490\n",
      "Epoch 2137/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 80.9543 - mae: 81.6348 - val_loss: 1210.1482 - val_mae: 1210.8401\n",
      "Epoch 2138/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 78.7469 - mae: 79.4302 - val_loss: 1119.5330 - val_mae: 1120.2261\n",
      "Epoch 2139/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 78.8998 - mae: 79.5836 - val_loss: 1122.7930 - val_mae: 1123.4860\n",
      "Epoch 2140/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 79.4865 - mae: 80.1699 - val_loss: 1180.9854 - val_mae: 1181.6786\n",
      "Epoch 2141/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 79.0100 - mae: 79.6918 - val_loss: 1155.6776 - val_mae: 1156.3696\n",
      "Epoch 2142/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 72.8259 - mae: 73.5059 - val_loss: 1130.5582 - val_mae: 1131.2511\n",
      "Epoch 2143/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 77.8100 - mae: 78.4936 - val_loss: 1217.8297 - val_mae: 1218.5228\n",
      "Epoch 2144/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 71.8035 - mae: 72.4857 - val_loss: 1192.7404 - val_mae: 1193.4335\n",
      "Epoch 2145/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 78.2218 - mae: 78.9004 - val_loss: 1315.3889 - val_mae: 1316.0820\n",
      "Epoch 2146/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 80.6777 - mae: 81.3594 - val_loss: 1316.7142 - val_mae: 1317.4071\n",
      "Epoch 2147/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 80.7373 - mae: 81.4184 - val_loss: 1071.6263 - val_mae: 1072.3195\n",
      "Epoch 2148/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 80.9708 - mae: 81.6530 - val_loss: 1183.1648 - val_mae: 1183.8579\n",
      "Epoch 2149/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 77.1845 - mae: 77.8638 - val_loss: 1139.0510 - val_mae: 1139.7441\n",
      "Epoch 2150/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 78.0593 - mae: 78.7421 - val_loss: 1113.7622 - val_mae: 1114.4545\n",
      "Epoch 2151/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 78.6866 - mae: 79.3704 - val_loss: 1243.4496 - val_mae: 1244.1423\n",
      "Epoch 2152/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 73.3221 - mae: 73.9965 - val_loss: 1118.0621 - val_mae: 1118.7542\n",
      "Epoch 2153/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 76.4429 - mae: 77.1245 - val_loss: 1176.9891 - val_mae: 1177.6823\n",
      "Epoch 2154/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 78.4811 - mae: 79.1618 - val_loss: 1209.5098 - val_mae: 1210.2029\n",
      "Epoch 2155/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 78.5412 - mae: 79.2236 - val_loss: 1160.3958 - val_mae: 1161.0887\n",
      "Epoch 2156/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 80.3632 - mae: 81.0448 - val_loss: 1263.9510 - val_mae: 1264.6444\n",
      "Epoch 2157/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 85.9723 - mae: 86.6534 - val_loss: 1167.4834 - val_mae: 1168.1766\n",
      "Epoch 2158/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 77.3185 - mae: 77.9988 - val_loss: 1301.7605 - val_mae: 1302.4536\n",
      "Epoch 2159/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 81.9039 - mae: 82.5821 - val_loss: 1319.1266 - val_mae: 1319.8197\n",
      "Epoch 2160/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 81.3796 - mae: 82.0596 - val_loss: 1223.1218 - val_mae: 1223.8129\n",
      "Epoch 2161/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 78.8135 - mae: 79.4950 - val_loss: 1244.4645 - val_mae: 1245.1576\n",
      "Epoch 2162/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 75.7209 - mae: 76.3991 - val_loss: 1256.1379 - val_mae: 1256.8298\n",
      "Epoch 2163/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 82.4996 - mae: 83.1789 - val_loss: 1350.4214 - val_mae: 1351.1145\n",
      "Epoch 2164/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 81.5026 - mae: 82.1832 - val_loss: 1283.5405 - val_mae: 1284.2335\n",
      "Epoch 2165/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 92.3748 - mae: 93.0575 - val_loss: 1166.1375 - val_mae: 1166.8307\n",
      "Epoch 2166/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 84.3287 - mae: 85.0133 - val_loss: 1255.5588 - val_mae: 1256.2520\n",
      "Epoch 2167/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 97.4017 - mae: 98.0828 - val_loss: 1183.0496 - val_mae: 1183.7415\n",
      "Epoch 2168/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 83.9061 - mae: 84.5886 - val_loss: 1055.0582 - val_mae: 1055.7515\n",
      "Epoch 2169/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 80.6337 - mae: 81.3150 - val_loss: 1186.9666 - val_mae: 1187.6595\n",
      "Epoch 2170/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 78.7988 - mae: 79.4787 - val_loss: 1187.3983 - val_mae: 1188.0898\n",
      "Epoch 2171/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 76.9645 - mae: 77.6455 - val_loss: 1193.7717 - val_mae: 1194.4650\n",
      "Epoch 2172/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 83.1224 - mae: 83.8052 - val_loss: 1143.4294 - val_mae: 1144.1223\n",
      "Epoch 2173/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 81.6068 - mae: 82.2890 - val_loss: 1242.5166 - val_mae: 1243.2092\n",
      "Epoch 2174/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 84.8616 - mae: 85.5420 - val_loss: 1311.3810 - val_mae: 1312.0740\n",
      "Epoch 2175/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 76.5603 - mae: 77.2381 - val_loss: 1122.2285 - val_mae: 1122.9204\n",
      "Epoch 2176/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 81.1903 - mae: 81.8720 - val_loss: 1166.9792 - val_mae: 1167.6721\n",
      "Epoch 2177/5000\n",
      "46/46 [==============================] - 2s 40ms/step - loss: 77.4534 - mae: 78.1363 - val_loss: 1183.3878 - val_mae: 1184.0811\n",
      "Epoch 2178/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 81.2110 - mae: 81.8930 - val_loss: 1140.8828 - val_mae: 1141.5752\n",
      "Epoch 2179/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 77.9036 - mae: 78.5850 - val_loss: 1204.9712 - val_mae: 1205.6643\n",
      "Epoch 2180/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 79.0992 - mae: 79.7799 - val_loss: 1089.2219 - val_mae: 1089.9147\n",
      "Epoch 2181/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 80.2262 - mae: 80.9068 - val_loss: 1224.4998 - val_mae: 1225.1925\n",
      "Epoch 2182/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 77.7733 - mae: 78.4512 - val_loss: 1255.4330 - val_mae: 1256.1262\n",
      "Epoch 2183/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 76.3126 - mae: 76.9885 - val_loss: 1106.3508 - val_mae: 1107.0439\n",
      "Epoch 2184/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 83.8570 - mae: 84.5403 - val_loss: 1204.1359 - val_mae: 1204.8284\n",
      "Epoch 2185/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 83.6816 - mae: 84.3633 - val_loss: 1254.0664 - val_mae: 1254.7589\n",
      "Epoch 2186/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 76.8412 - mae: 77.5218 - val_loss: 1047.5537 - val_mae: 1048.2468\n",
      "Epoch 2187/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 76.4931 - mae: 77.1760 - val_loss: 1248.8971 - val_mae: 1249.5902\n",
      "Epoch 2188/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 77.6374 - mae: 78.3187 - val_loss: 1191.8954 - val_mae: 1192.5887\n",
      "Epoch 2189/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 77.7558 - mae: 78.4373 - val_loss: 1120.0856 - val_mae: 1120.7789\n",
      "Epoch 2190/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 78.7642 - mae: 79.4474 - val_loss: 1284.4142 - val_mae: 1285.1073\n",
      "Epoch 2191/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 87.4063 - mae: 88.0866 - val_loss: 1206.9182 - val_mae: 1207.6113\n",
      "Epoch 2192/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 78.7760 - mae: 79.4588 - val_loss: 1235.9475 - val_mae: 1236.6407\n",
      "Epoch 2193/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 78.6963 - mae: 79.3767 - val_loss: 1285.3947 - val_mae: 1286.0879\n",
      "Epoch 2194/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 83.2103 - mae: 83.8918 - val_loss: 1248.1990 - val_mae: 1248.8921\n",
      "Epoch 2195/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 88.7075 - mae: 89.3903 - val_loss: 1374.3541 - val_mae: 1375.0464\n",
      "Epoch 2196/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 76.0243 - mae: 76.7070 - val_loss: 1164.4734 - val_mae: 1165.1666\n",
      "Epoch 2197/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 73.0870 - mae: 73.7657 - val_loss: 1154.4371 - val_mae: 1155.1298\n",
      "Epoch 2198/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 84.4660 - mae: 85.1490 - val_loss: 1074.6012 - val_mae: 1075.2939\n",
      "Epoch 2199/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 81.4929 - mae: 82.1747 - val_loss: 1244.3610 - val_mae: 1245.0540\n",
      "Epoch 2200/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 75.6266 - mae: 76.3093 - val_loss: 1143.1406 - val_mae: 1143.8336\n",
      "Epoch 2201/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 79.5422 - mae: 80.2232 - val_loss: 1212.6853 - val_mae: 1213.3784\n",
      "Epoch 2202/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 79.4577 - mae: 80.1439 - val_loss: 1201.4935 - val_mae: 1202.1857\n",
      "Epoch 2203/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 83.4248 - mae: 84.1108 - val_loss: 1301.1851 - val_mae: 1301.8784\n",
      "Epoch 2204/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 75.0676 - mae: 75.7492 - val_loss: 1193.6854 - val_mae: 1194.3777\n",
      "Epoch 2205/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 79.7054 - mae: 80.3890 - val_loss: 1121.8657 - val_mae: 1122.5587\n",
      "Epoch 2206/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 78.2150 - mae: 78.8988 - val_loss: 1274.8949 - val_mae: 1275.5883\n",
      "Epoch 2207/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 90.9040 - mae: 91.5863 - val_loss: 1165.2317 - val_mae: 1165.9249\n",
      "Epoch 2208/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 74.5275 - mae: 75.2076 - val_loss: 1224.7015 - val_mae: 1225.3947\n",
      "Epoch 2209/5000\n",
      "46/46 [==============================] - 2s 41ms/step - loss: 73.5448 - mae: 74.2230 - val_loss: 1171.9279 - val_mae: 1172.6208\n",
      "Epoch 2210/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 73.3742 - mae: 74.0525 - val_loss: 1324.7694 - val_mae: 1325.4611\n",
      "Epoch 2211/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 81.7370 - mae: 82.4165 - val_loss: 1185.1981 - val_mae: 1185.8911\n",
      "Epoch 2212/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 76.6490 - mae: 77.3267 - val_loss: 1172.1951 - val_mae: 1172.8872\n",
      "Epoch 2213/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 77.5648 - mae: 78.2468 - val_loss: 1341.9327 - val_mae: 1342.6254\n",
      "Epoch 2214/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 83.9468 - mae: 84.6307 - val_loss: 1321.4894 - val_mae: 1322.1825\n",
      "Epoch 2215/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 77.6803 - mae: 78.3574 - val_loss: 1171.1329 - val_mae: 1171.8263\n",
      "Epoch 2216/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 77.8700 - mae: 78.5512 - val_loss: 1097.8408 - val_mae: 1098.5337\n",
      "Epoch 2217/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 79.7920 - mae: 80.4744 - val_loss: 1215.7638 - val_mae: 1216.4569\n",
      "Epoch 2218/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 78.8416 - mae: 79.5237 - val_loss: 1161.8394 - val_mae: 1162.5312\n",
      "Epoch 2219/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 70.1632 - mae: 70.8424 - val_loss: 1319.7267 - val_mae: 1320.4197\n",
      "Epoch 2220/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 91.4320 - mae: 92.1148 - val_loss: 1149.6556 - val_mae: 1150.3481\n",
      "Epoch 2221/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 79.0195 - mae: 79.7046 - val_loss: 1271.3870 - val_mae: 1272.0803\n",
      "Epoch 2222/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 77.1504 - mae: 77.8340 - val_loss: 1197.2545 - val_mae: 1197.9476\n",
      "Epoch 2223/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 76.8715 - mae: 77.5535 - val_loss: 1217.8622 - val_mae: 1218.5543\n",
      "Epoch 2224/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 75.9874 - mae: 76.6669 - val_loss: 1195.0669 - val_mae: 1195.7600\n",
      "Epoch 2225/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 72.0930 - mae: 72.7748 - val_loss: 1270.2937 - val_mae: 1270.9862\n",
      "Epoch 2226/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 73.6415 - mae: 74.3236 - val_loss: 1209.4014 - val_mae: 1210.0938\n",
      "Epoch 2227/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 77.5856 - mae: 78.2692 - val_loss: 1259.3766 - val_mae: 1260.0697\n",
      "Epoch 2228/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 80.2162 - mae: 80.8991 - val_loss: 1062.8979 - val_mae: 1063.5909\n",
      "Epoch 2229/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 82.3529 - mae: 83.0374 - val_loss: 1242.8979 - val_mae: 1243.5901\n",
      "Epoch 2230/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 79.5129 - mae: 80.1955 - val_loss: 1210.7661 - val_mae: 1211.4594\n",
      "Epoch 2231/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 86.7246 - mae: 87.4083 - val_loss: 1207.4159 - val_mae: 1208.1089\n",
      "Epoch 2232/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 79.9584 - mae: 80.6395 - val_loss: 1137.6864 - val_mae: 1138.3788\n",
      "Epoch 2233/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 73.2260 - mae: 73.9079 - val_loss: 1102.1237 - val_mae: 1102.8168\n",
      "Epoch 2234/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 80.0993 - mae: 80.7805 - val_loss: 1255.7383 - val_mae: 1256.4313\n",
      "Epoch 2235/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 77.1940 - mae: 77.8740 - val_loss: 1198.7778 - val_mae: 1199.4709\n",
      "Epoch 2236/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 72.7019 - mae: 73.3817 - val_loss: 1266.3385 - val_mae: 1267.0314\n",
      "Epoch 2237/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 74.9364 - mae: 75.6153 - val_loss: 1318.3904 - val_mae: 1319.0837\n",
      "Epoch 2238/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 71.5142 - mae: 72.1910 - val_loss: 1275.6908 - val_mae: 1276.3840\n",
      "Epoch 2239/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 73.9585 - mae: 74.6363 - val_loss: 1312.9137 - val_mae: 1313.6067\n",
      "Epoch 2240/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 84.6584 - mae: 85.3405 - val_loss: 1133.6858 - val_mae: 1134.3792\n",
      "Epoch 2241/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 79.1040 - mae: 79.7836 - val_loss: 1239.3597 - val_mae: 1240.0524\n",
      "Epoch 2242/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 85.6350 - mae: 86.3158 - val_loss: 1140.5397 - val_mae: 1141.2318\n",
      "Epoch 2243/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 77.4853 - mae: 78.1653 - val_loss: 1192.4622 - val_mae: 1193.1554\n",
      "Epoch 2244/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 79.7741 - mae: 80.4538 - val_loss: 1147.8112 - val_mae: 1148.5042\n",
      "Epoch 2245/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 84.7493 - mae: 85.4308 - val_loss: 1262.1707 - val_mae: 1262.8638\n",
      "Epoch 2246/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 77.2839 - mae: 77.9669 - val_loss: 1237.8687 - val_mae: 1238.5616\n",
      "Epoch 2247/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 84.5767 - mae: 85.2556 - val_loss: 1271.3523 - val_mae: 1272.0454\n",
      "Epoch 2248/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 84.3675 - mae: 85.0534 - val_loss: 1384.5709 - val_mae: 1385.2635\n",
      "Epoch 2249/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 73.0186 - mae: 73.6994 - val_loss: 1319.1283 - val_mae: 1319.8212\n",
      "Epoch 2250/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 76.1948 - mae: 76.8781 - val_loss: 1103.9161 - val_mae: 1104.6080\n",
      "Epoch 2251/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 75.5900 - mae: 76.2712 - val_loss: 1154.0121 - val_mae: 1154.7042\n",
      "Epoch 2252/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 71.1199 - mae: 71.7991 - val_loss: 1101.2343 - val_mae: 1101.9269\n",
      "Epoch 2253/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 75.7349 - mae: 76.4151 - val_loss: 1089.9790 - val_mae: 1090.6713\n",
      "Epoch 2254/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 77.8556 - mae: 78.5356 - val_loss: 1132.5198 - val_mae: 1133.2129\n",
      "Epoch 2255/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 72.9799 - mae: 73.6606 - val_loss: 1211.4868 - val_mae: 1212.1799\n",
      "Epoch 2256/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 82.9750 - mae: 83.6546 - val_loss: 1174.9889 - val_mae: 1175.6820\n",
      "Epoch 2257/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 86.4266 - mae: 87.1075 - val_loss: 1206.8923 - val_mae: 1207.5854\n",
      "Epoch 2258/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 78.9541 - mae: 79.6372 - val_loss: 1194.2666 - val_mae: 1194.9598\n",
      "Epoch 2259/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 73.8840 - mae: 74.5636 - val_loss: 1177.5365 - val_mae: 1178.2297\n",
      "Epoch 2260/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 72.0781 - mae: 72.7593 - val_loss: 1111.8203 - val_mae: 1112.5128\n",
      "Epoch 2261/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 84.0468 - mae: 84.7276 - val_loss: 1076.3120 - val_mae: 1077.0031\n",
      "Epoch 2262/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 79.6944 - mae: 80.3746 - val_loss: 1187.7723 - val_mae: 1188.4648\n",
      "Epoch 2263/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 75.1823 - mae: 75.8606 - val_loss: 1234.5759 - val_mae: 1235.2686\n",
      "Epoch 2264/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 78.3847 - mae: 79.0659 - val_loss: 1212.5303 - val_mae: 1213.2228\n",
      "Epoch 2265/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 85.2177 - mae: 85.8995 - val_loss: 1211.7310 - val_mae: 1212.4241\n",
      "Epoch 2266/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 82.6643 - mae: 83.3447 - val_loss: 1089.0884 - val_mae: 1089.7816\n",
      "Epoch 2267/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 74.3549 - mae: 75.0334 - val_loss: 1077.6469 - val_mae: 1078.3392\n",
      "Epoch 2268/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 82.3594 - mae: 83.0413 - val_loss: 1174.9453 - val_mae: 1175.6385\n",
      "Epoch 2269/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 83.4715 - mae: 84.1536 - val_loss: 1217.1782 - val_mae: 1217.8711\n",
      "Epoch 2270/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 78.2061 - mae: 78.8911 - val_loss: 1215.3273 - val_mae: 1216.0204\n",
      "Epoch 2271/5000\n",
      "46/46 [==============================] - 2s 32ms/step - loss: 73.2563 - mae: 73.9387 - val_loss: 1208.8409 - val_mae: 1209.5337\n",
      "Epoch 2272/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 85.1303 - mae: 85.8100 - val_loss: 1558.1868 - val_mae: 1558.8799\n",
      "Epoch 2273/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 82.5138 - mae: 83.1928 - val_loss: 1171.9792 - val_mae: 1172.6724\n",
      "Epoch 2274/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 89.5566 - mae: 90.2397 - val_loss: 1121.2144 - val_mae: 1121.9073\n",
      "Epoch 2275/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 74.5633 - mae: 75.2405 - val_loss: 1156.1522 - val_mae: 1156.8453\n",
      "Epoch 2276/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 80.7348 - mae: 81.4177 - val_loss: 1173.1501 - val_mae: 1173.8428\n",
      "Epoch 2277/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 72.6727 - mae: 73.3549 - val_loss: 1297.8291 - val_mae: 1298.5221\n",
      "Epoch 2278/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 81.5323 - mae: 82.2103 - val_loss: 1092.9147 - val_mae: 1093.6077\n",
      "Epoch 2279/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 77.3994 - mae: 78.0811 - val_loss: 1095.5126 - val_mae: 1096.2058\n",
      "Epoch 2280/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 74.2660 - mae: 74.9434 - val_loss: 1239.2462 - val_mae: 1239.9393\n",
      "Epoch 2281/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 76.7928 - mae: 77.4723 - val_loss: 1099.2273 - val_mae: 1099.9204\n",
      "Epoch 2282/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 83.4793 - mae: 84.1616 - val_loss: 1214.9716 - val_mae: 1215.6639\n",
      "Epoch 2283/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 76.2911 - mae: 76.9680 - val_loss: 1056.1658 - val_mae: 1056.8590\n",
      "Epoch 2284/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 75.0895 - mae: 75.7691 - val_loss: 1112.4424 - val_mae: 1113.1350\n",
      "Epoch 2285/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 78.2674 - mae: 78.9503 - val_loss: 1080.7878 - val_mae: 1081.4812\n",
      "Epoch 2286/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 81.7944 - mae: 82.4775 - val_loss: 1149.0902 - val_mae: 1149.7828\n",
      "Epoch 2287/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 78.8533 - mae: 79.5368 - val_loss: 1125.3157 - val_mae: 1126.0085\n",
      "Epoch 2288/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 71.2224 - mae: 71.9032 - val_loss: 1209.1564 - val_mae: 1209.8494\n",
      "Epoch 2289/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 89.9306 - mae: 90.6144 - val_loss: 1241.1069 - val_mae: 1241.8002\n",
      "Epoch 2290/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 78.3692 - mae: 79.0497 - val_loss: 1137.6675 - val_mae: 1138.3604\n",
      "Epoch 2291/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 76.2671 - mae: 76.9461 - val_loss: 1206.8125 - val_mae: 1207.5057\n",
      "Epoch 2292/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 78.1564 - mae: 78.8328 - val_loss: 1170.4799 - val_mae: 1171.1729\n",
      "Epoch 2293/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 76.1655 - mae: 76.8443 - val_loss: 1210.0813 - val_mae: 1210.7739\n",
      "Epoch 2294/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 73.5144 - mae: 74.1929 - val_loss: 1108.5822 - val_mae: 1109.2753\n",
      "Epoch 2295/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 72.3530 - mae: 73.0303 - val_loss: 1154.4169 - val_mae: 1155.1100\n",
      "Epoch 2296/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 72.7750 - mae: 73.4590 - val_loss: 1183.8171 - val_mae: 1184.5098\n",
      "Epoch 2297/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 77.2115 - mae: 77.8911 - val_loss: 1284.8484 - val_mae: 1285.5411\n",
      "Epoch 2298/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 79.6674 - mae: 80.3515 - val_loss: 1189.2070 - val_mae: 1189.8997\n",
      "Epoch 2299/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 75.9075 - mae: 76.5869 - val_loss: 1280.4144 - val_mae: 1281.1067\n",
      "Epoch 2300/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 79.3428 - mae: 80.0246 - val_loss: 1217.9886 - val_mae: 1218.6813\n",
      "Epoch 2301/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 76.3935 - mae: 77.0725 - val_loss: 1115.2191 - val_mae: 1115.9114\n",
      "Epoch 2302/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 73.1679 - mae: 73.8473 - val_loss: 1251.5168 - val_mae: 1252.2098\n",
      "Epoch 2303/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 76.3510 - mae: 77.0295 - val_loss: 1099.1735 - val_mae: 1099.8666\n",
      "Epoch 2304/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 80.5803 - mae: 81.2627 - val_loss: 1200.8427 - val_mae: 1201.5356\n",
      "Epoch 2305/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 70.3282 - mae: 71.0030 - val_loss: 1386.1569 - val_mae: 1386.8500\n",
      "Epoch 2306/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 78.8399 - mae: 79.5197 - val_loss: 1237.1141 - val_mae: 1237.8070\n",
      "Epoch 2307/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 72.2650 - mae: 72.9456 - val_loss: 1253.7941 - val_mae: 1254.4874\n",
      "Epoch 2308/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 81.1543 - mae: 81.8366 - val_loss: 1164.1844 - val_mae: 1164.8767\n",
      "Epoch 2309/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 75.0628 - mae: 75.7414 - val_loss: 1225.6920 - val_mae: 1226.3851\n",
      "Epoch 2310/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 84.7188 - mae: 85.3995 - val_loss: 1206.0234 - val_mae: 1206.7166\n",
      "Epoch 2311/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 76.6258 - mae: 77.3040 - val_loss: 1119.4534 - val_mae: 1120.1464\n",
      "Epoch 2312/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 70.8944 - mae: 71.5712 - val_loss: 1204.8999 - val_mae: 1205.5922\n",
      "Epoch 2313/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 78.2190 - mae: 78.9009 - val_loss: 1114.8308 - val_mae: 1115.5238\n",
      "Epoch 2314/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 77.1574 - mae: 77.8374 - val_loss: 1140.8533 - val_mae: 1141.5464\n",
      "Epoch 2315/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 72.1899 - mae: 72.8705 - val_loss: 1204.6361 - val_mae: 1205.3287\n",
      "Epoch 2316/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 69.7336 - mae: 70.4127 - val_loss: 1142.8655 - val_mae: 1143.5587\n",
      "Epoch 2317/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 76.4439 - mae: 77.1186 - val_loss: 1174.4086 - val_mae: 1175.1018\n",
      "Epoch 2318/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 75.7412 - mae: 76.4190 - val_loss: 1199.7806 - val_mae: 1200.4736\n",
      "Epoch 2319/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 77.4753 - mae: 78.1506 - val_loss: 1231.2549 - val_mae: 1231.9481\n",
      "Epoch 2320/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 80.8387 - mae: 81.5170 - val_loss: 1166.0588 - val_mae: 1166.7516\n",
      "Epoch 2321/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 74.9802 - mae: 75.6617 - val_loss: 1205.2662 - val_mae: 1205.9591\n",
      "Epoch 2322/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 74.0986 - mae: 74.7831 - val_loss: 1102.5916 - val_mae: 1103.2842\n",
      "Epoch 2323/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 75.4319 - mae: 76.1121 - val_loss: 1217.0272 - val_mae: 1217.7202\n",
      "Epoch 2324/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 79.2854 - mae: 79.9671 - val_loss: 1169.2909 - val_mae: 1169.9840\n",
      "Epoch 2325/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 73.4827 - mae: 74.1621 - val_loss: 1160.1178 - val_mae: 1160.8103\n",
      "Epoch 2326/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 74.1346 - mae: 74.8175 - val_loss: 1114.9121 - val_mae: 1115.6053\n",
      "Epoch 2327/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 87.1919 - mae: 87.8776 - val_loss: 1192.7607 - val_mae: 1193.4537\n",
      "Epoch 2328/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 81.0576 - mae: 81.7369 - val_loss: 1260.9121 - val_mae: 1261.6051\n",
      "Epoch 2329/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 78.4882 - mae: 79.1733 - val_loss: 1150.7468 - val_mae: 1151.4399\n",
      "Epoch 2330/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 78.2762 - mae: 78.9585 - val_loss: 1164.1223 - val_mae: 1164.8157\n",
      "Epoch 2331/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 69.6009 - mae: 70.2790 - val_loss: 1217.3739 - val_mae: 1218.0660\n",
      "Epoch 2332/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 80.4255 - mae: 81.1073 - val_loss: 1179.6377 - val_mae: 1180.3309\n",
      "Epoch 2333/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 78.6986 - mae: 79.3796 - val_loss: 1037.3861 - val_mae: 1038.0793\n",
      "Epoch 2334/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 81.8463 - mae: 82.5291 - val_loss: 1176.1652 - val_mae: 1176.8579\n",
      "Epoch 2335/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 83.6442 - mae: 84.3269 - val_loss: 1165.6951 - val_mae: 1166.3883\n",
      "Epoch 2336/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 76.4883 - mae: 77.1706 - val_loss: 1157.8711 - val_mae: 1158.5641\n",
      "Epoch 2337/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 82.9360 - mae: 83.6190 - val_loss: 1176.4445 - val_mae: 1177.1368\n",
      "Epoch 2338/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 75.9153 - mae: 76.5947 - val_loss: 1213.1304 - val_mae: 1213.8232\n",
      "Epoch 2339/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 81.2913 - mae: 81.9752 - val_loss: 1116.8442 - val_mae: 1117.5367\n",
      "Epoch 2340/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 74.4058 - mae: 75.0824 - val_loss: 1109.1647 - val_mae: 1109.8578\n",
      "Epoch 2341/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 81.3490 - mae: 82.0289 - val_loss: 1101.8802 - val_mae: 1102.5735\n",
      "Epoch 2342/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 75.9763 - mae: 76.6587 - val_loss: 1225.1074 - val_mae: 1225.8000\n",
      "Epoch 2343/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 82.2411 - mae: 82.9203 - val_loss: 1272.5769 - val_mae: 1273.2701\n",
      "Epoch 2344/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 95.0517 - mae: 95.7351 - val_loss: 1087.6941 - val_mae: 1088.3866\n",
      "Epoch 2345/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 80.2141 - mae: 80.8929 - val_loss: 1183.2646 - val_mae: 1183.9576\n",
      "Epoch 2346/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 76.5461 - mae: 77.2261 - val_loss: 1119.5098 - val_mae: 1120.2026\n",
      "Epoch 2347/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 75.0353 - mae: 75.7125 - val_loss: 1178.4845 - val_mae: 1179.1768\n",
      "Epoch 2348/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 73.3185 - mae: 73.9951 - val_loss: 1131.1206 - val_mae: 1131.8136\n",
      "Epoch 2349/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 77.5247 - mae: 78.2034 - val_loss: 1194.5421 - val_mae: 1195.2338\n",
      "Epoch 2350/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 72.5343 - mae: 73.2158 - val_loss: 1152.1044 - val_mae: 1152.7974\n",
      "Epoch 2351/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 76.6989 - mae: 77.3781 - val_loss: 1083.7047 - val_mae: 1084.3978\n",
      "Epoch 2352/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 79.9599 - mae: 80.6400 - val_loss: 1184.3312 - val_mae: 1185.0240\n",
      "Epoch 2353/5000\n",
      "46/46 [==============================] - 2s 48ms/step - loss: 75.0413 - mae: 75.7212 - val_loss: 1207.8146 - val_mae: 1208.5078\n",
      "Epoch 2354/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 74.4162 - mae: 75.0942 - val_loss: 1186.7905 - val_mae: 1187.4833\n",
      "Epoch 2355/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 75.0618 - mae: 75.7424 - val_loss: 1165.2058 - val_mae: 1165.8989\n",
      "Epoch 2356/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 74.0444 - mae: 74.7226 - val_loss: 1239.0889 - val_mae: 1239.7820\n",
      "Epoch 2357/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 78.0677 - mae: 78.7487 - val_loss: 1202.3326 - val_mae: 1203.0258\n",
      "Epoch 2358/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 73.7410 - mae: 74.4207 - val_loss: 1177.5623 - val_mae: 1178.2556\n",
      "Epoch 2359/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 80.8262 - mae: 81.5071 - val_loss: 1290.5836 - val_mae: 1291.2769\n",
      "Epoch 2360/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 79.7625 - mae: 80.4456 - val_loss: 1119.0397 - val_mae: 1119.7328\n",
      "Epoch 2361/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 83.1783 - mae: 83.8634 - val_loss: 1229.3621 - val_mae: 1230.0547\n",
      "Epoch 2362/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 87.3924 - mae: 88.0737 - val_loss: 1238.3596 - val_mae: 1239.0529\n",
      "Epoch 2363/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 77.7348 - mae: 78.4154 - val_loss: 1212.5344 - val_mae: 1213.2273\n",
      "Epoch 2364/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 73.7582 - mae: 74.4397 - val_loss: 1082.1986 - val_mae: 1082.8917\n",
      "Epoch 2365/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 73.2547 - mae: 73.9332 - val_loss: 1164.3047 - val_mae: 1164.9978\n",
      "Epoch 2366/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 84.7086 - mae: 85.3913 - val_loss: 1084.9597 - val_mae: 1085.6527\n",
      "Epoch 2367/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 73.9209 - mae: 74.5957 - val_loss: 1250.8508 - val_mae: 1251.5438\n",
      "Epoch 2368/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 76.9452 - mae: 77.6258 - val_loss: 1065.7434 - val_mae: 1066.4366\n",
      "Epoch 2369/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 72.4952 - mae: 73.1761 - val_loss: 1088.0978 - val_mae: 1088.7910\n",
      "Epoch 2370/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 79.2985 - mae: 79.9797 - val_loss: 1234.9049 - val_mae: 1235.5978\n",
      "Epoch 2371/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 82.3540 - mae: 83.0359 - val_loss: 1118.3173 - val_mae: 1119.0088\n",
      "Epoch 2372/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 77.1268 - mae: 77.8054 - val_loss: 1223.8810 - val_mae: 1224.5740\n",
      "Epoch 2373/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 74.0359 - mae: 74.7159 - val_loss: 1120.4719 - val_mae: 1121.1639\n",
      "Epoch 2374/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 74.8168 - mae: 75.4978 - val_loss: 1160.4658 - val_mae: 1161.1591\n",
      "Epoch 2375/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 79.9235 - mae: 80.6065 - val_loss: 1159.6753 - val_mae: 1160.3685\n",
      "Epoch 2376/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 76.9444 - mae: 77.6241 - val_loss: 1171.9331 - val_mae: 1172.6262\n",
      "Epoch 2377/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 80.5566 - mae: 81.2360 - val_loss: 1135.9243 - val_mae: 1136.6160\n",
      "Epoch 2378/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 75.0300 - mae: 75.7113 - val_loss: 1360.7694 - val_mae: 1361.4625\n",
      "Epoch 2379/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 78.3209 - mae: 79.0015 - val_loss: 1098.0051 - val_mae: 1098.6979\n",
      "Epoch 2380/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 76.5205 - mae: 77.1993 - val_loss: 1095.9044 - val_mae: 1096.5972\n",
      "Epoch 2381/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 82.8645 - mae: 83.5439 - val_loss: 1080.6271 - val_mae: 1081.3196\n",
      "Epoch 2382/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 75.7831 - mae: 76.4648 - val_loss: 1181.7515 - val_mae: 1182.4446\n",
      "Epoch 2383/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 76.4544 - mae: 77.1363 - val_loss: 1180.6234 - val_mae: 1181.3167\n",
      "Epoch 2384/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 70.5711 - mae: 71.2494 - val_loss: 1098.8766 - val_mae: 1099.5696\n",
      "Epoch 2385/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 70.1706 - mae: 70.8470 - val_loss: 1136.9965 - val_mae: 1137.6885\n",
      "Epoch 2386/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 73.0801 - mae: 73.7604 - val_loss: 1098.4012 - val_mae: 1099.0939\n",
      "Epoch 2387/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 75.7806 - mae: 76.4585 - val_loss: 1140.2017 - val_mae: 1140.8949\n",
      "Epoch 2388/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 72.2763 - mae: 72.9545 - val_loss: 1097.1420 - val_mae: 1097.8336\n",
      "Epoch 2389/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 85.7951 - mae: 86.4749 - val_loss: 1118.6663 - val_mae: 1119.3588\n",
      "Epoch 2390/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 84.1159 - mae: 84.7951 - val_loss: 1212.4536 - val_mae: 1213.1467\n",
      "Epoch 2391/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 76.4028 - mae: 77.0848 - val_loss: 1052.1694 - val_mae: 1052.8613\n",
      "Epoch 2392/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 76.4110 - mae: 77.0873 - val_loss: 1101.8960 - val_mae: 1102.5892\n",
      "Epoch 2393/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 81.5016 - mae: 82.1835 - val_loss: 1232.0690 - val_mae: 1232.7621\n",
      "Epoch 2394/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 72.0919 - mae: 72.7736 - val_loss: 1125.9274 - val_mae: 1126.6196\n",
      "Epoch 2395/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 77.7941 - mae: 78.4726 - val_loss: 1114.1163 - val_mae: 1114.8094\n",
      "Epoch 2396/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 71.0905 - mae: 71.7682 - val_loss: 1121.5833 - val_mae: 1122.2762\n",
      "Epoch 2397/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 76.8950 - mae: 77.5775 - val_loss: 1078.2529 - val_mae: 1078.9456\n",
      "Epoch 2398/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 76.6291 - mae: 77.3068 - val_loss: 1237.7238 - val_mae: 1238.4154\n",
      "Epoch 2399/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 76.5436 - mae: 77.2268 - val_loss: 1100.7079 - val_mae: 1101.4010\n",
      "Epoch 2400/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 72.5126 - mae: 73.1941 - val_loss: 1129.9219 - val_mae: 1130.6150\n",
      "Epoch 2401/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 83.8964 - mae: 84.5807 - val_loss: 1269.9543 - val_mae: 1270.6472\n",
      "Epoch 2402/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 77.2066 - mae: 77.8871 - val_loss: 1185.9594 - val_mae: 1186.6527\n",
      "Epoch 2403/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 72.7869 - mae: 73.4660 - val_loss: 1167.8503 - val_mae: 1168.5431\n",
      "Epoch 2404/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 80.2164 - mae: 80.8987 - val_loss: 1188.3448 - val_mae: 1189.0378\n",
      "Epoch 2405/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 73.4004 - mae: 74.0764 - val_loss: 1175.9717 - val_mae: 1176.6650\n",
      "Epoch 2406/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 75.2965 - mae: 75.9776 - val_loss: 1160.9512 - val_mae: 1161.6428\n",
      "Epoch 2407/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 78.5558 - mae: 79.2374 - val_loss: 1192.3593 - val_mae: 1193.0511\n",
      "Epoch 2408/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 78.8510 - mae: 79.5311 - val_loss: 1380.1448 - val_mae: 1380.8379\n",
      "Epoch 2409/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 71.5435 - mae: 72.2216 - val_loss: 1053.9044 - val_mae: 1054.5966\n",
      "Epoch 2410/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 71.5725 - mae: 72.2503 - val_loss: 1084.1012 - val_mae: 1084.7943\n",
      "Epoch 2411/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 76.0269 - mae: 76.7063 - val_loss: 1191.8054 - val_mae: 1192.4955\n",
      "Epoch 2412/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 78.1466 - mae: 78.8257 - val_loss: 1127.5161 - val_mae: 1128.2067\n",
      "Epoch 2413/5000\n",
      "46/46 [==============================] - 1s 9ms/step - loss: 76.8407 - mae: 77.5204 - val_loss: 1182.4294 - val_mae: 1183.1227\n",
      "Epoch 2414/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 76.5942 - mae: 77.2746 - val_loss: 1191.4974 - val_mae: 1192.1907\n",
      "Epoch 2415/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 77.1062 - mae: 77.7854 - val_loss: 1258.6603 - val_mae: 1259.3535\n",
      "Epoch 2416/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 83.5026 - mae: 84.1856 - val_loss: 1046.3450 - val_mae: 1047.0372\n",
      "Epoch 2417/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 76.0738 - mae: 76.7543 - val_loss: 1192.1235 - val_mae: 1192.8169\n",
      "Epoch 2418/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 80.0490 - mae: 80.7277 - val_loss: 1114.3866 - val_mae: 1115.0796\n",
      "Epoch 2419/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 68.7209 - mae: 69.3985 - val_loss: 1158.3860 - val_mae: 1159.0791\n",
      "Epoch 2420/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 74.2524 - mae: 74.9324 - val_loss: 1144.0001 - val_mae: 1144.6920\n",
      "Epoch 2421/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 71.0829 - mae: 71.7596 - val_loss: 1067.1078 - val_mae: 1067.8009\n",
      "Epoch 2422/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 83.0715 - mae: 83.7529 - val_loss: 1168.4097 - val_mae: 1169.1029\n",
      "Epoch 2423/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 75.5131 - mae: 76.1938 - val_loss: 1161.6704 - val_mae: 1162.3634\n",
      "Epoch 2424/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 76.4544 - mae: 77.1379 - val_loss: 1171.4755 - val_mae: 1172.1677\n",
      "Epoch 2425/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 70.4105 - mae: 71.0870 - val_loss: 1187.6119 - val_mae: 1188.3052\n",
      "Epoch 2426/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 70.6977 - mae: 71.3770 - val_loss: 1259.3135 - val_mae: 1260.0062\n",
      "Epoch 2427/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 78.1189 - mae: 78.7999 - val_loss: 1130.4746 - val_mae: 1131.1672\n",
      "Epoch 2428/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 75.9123 - mae: 76.5943 - val_loss: 1137.1876 - val_mae: 1137.8809\n",
      "Epoch 2429/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 73.5158 - mae: 74.1938 - val_loss: 1187.5334 - val_mae: 1188.2266\n",
      "Epoch 2430/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 76.7175 - mae: 77.4001 - val_loss: 1113.4528 - val_mae: 1114.1445\n",
      "Epoch 2431/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 72.1156 - mae: 72.7971 - val_loss: 1231.0453 - val_mae: 1231.7384\n",
      "Epoch 2432/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 82.5943 - mae: 83.2747 - val_loss: 1111.4424 - val_mae: 1112.1353\n",
      "Epoch 2433/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 68.9525 - mae: 69.6318 - val_loss: 1266.2458 - val_mae: 1266.9388\n",
      "Epoch 2434/5000\n",
      "46/46 [==============================] - 2s 32ms/step - loss: 68.5662 - mae: 69.2448 - val_loss: 1212.4255 - val_mae: 1213.1180\n",
      "Epoch 2435/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 73.4155 - mae: 74.0964 - val_loss: 1190.0685 - val_mae: 1190.7616\n",
      "Epoch 2436/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 75.5859 - mae: 76.2688 - val_loss: 1114.2195 - val_mae: 1114.9128\n",
      "Epoch 2437/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 72.3700 - mae: 73.0488 - val_loss: 1184.2638 - val_mae: 1184.9556\n",
      "Epoch 2438/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 75.5960 - mae: 76.2747 - val_loss: 1255.9807 - val_mae: 1256.6736\n",
      "Epoch 2439/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 78.0855 - mae: 78.7687 - val_loss: 1057.5071 - val_mae: 1058.2000\n",
      "Epoch 2440/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 69.5433 - mae: 70.2222 - val_loss: 1138.9459 - val_mae: 1139.6388\n",
      "Epoch 2441/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 70.4870 - mae: 71.1672 - val_loss: 1201.4490 - val_mae: 1202.1420\n",
      "Epoch 2442/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 70.9053 - mae: 71.5853 - val_loss: 1148.6289 - val_mae: 1149.3215\n",
      "Epoch 2443/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 69.0492 - mae: 69.7282 - val_loss: 1227.5653 - val_mae: 1228.2583\n",
      "Epoch 2444/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 74.7383 - mae: 75.4152 - val_loss: 1089.3336 - val_mae: 1090.0259\n",
      "Epoch 2445/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 81.5410 - mae: 82.2244 - val_loss: 1175.4050 - val_mae: 1176.0966\n",
      "Epoch 2446/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 80.0019 - mae: 80.6846 - val_loss: 1140.6121 - val_mae: 1141.3047\n",
      "Epoch 2447/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 73.3565 - mae: 74.0351 - val_loss: 1123.9636 - val_mae: 1124.6562\n",
      "Epoch 2448/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 70.7587 - mae: 71.4394 - val_loss: 1128.0094 - val_mae: 1128.7026\n",
      "Epoch 2449/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 75.6577 - mae: 76.3374 - val_loss: 1125.0057 - val_mae: 1125.6990\n",
      "Epoch 2450/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 76.6088 - mae: 77.2924 - val_loss: 1145.2192 - val_mae: 1145.9126\n",
      "Epoch 2451/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 70.9803 - mae: 71.6583 - val_loss: 1126.3521 - val_mae: 1127.0449\n",
      "Epoch 2452/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 75.4936 - mae: 76.1716 - val_loss: 1057.4298 - val_mae: 1058.1229\n",
      "Epoch 2453/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 78.2678 - mae: 78.9479 - val_loss: 1105.4749 - val_mae: 1106.1681\n",
      "Epoch 2454/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 73.2915 - mae: 73.9691 - val_loss: 1133.6956 - val_mae: 1134.3887\n",
      "Epoch 2455/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 71.0424 - mae: 71.7206 - val_loss: 1177.8280 - val_mae: 1178.5200\n",
      "Epoch 2456/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 76.1054 - mae: 76.7853 - val_loss: 1177.9899 - val_mae: 1178.6830\n",
      "Epoch 2457/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 77.4788 - mae: 78.1614 - val_loss: 1243.6133 - val_mae: 1244.3063\n",
      "Epoch 2458/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 77.3666 - mae: 78.0443 - val_loss: 1154.1952 - val_mae: 1154.8884\n",
      "Epoch 2459/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 76.4649 - mae: 77.1443 - val_loss: 1199.9812 - val_mae: 1200.6740\n",
      "Epoch 2460/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 74.4468 - mae: 75.1258 - val_loss: 1173.0632 - val_mae: 1173.7549\n",
      "Epoch 2461/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 75.0273 - mae: 75.7115 - val_loss: 1224.7537 - val_mae: 1225.4467\n",
      "Epoch 2462/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 81.6844 - mae: 82.3693 - val_loss: 1202.3396 - val_mae: 1203.0327\n",
      "Epoch 2463/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 74.9192 - mae: 75.6012 - val_loss: 1131.1019 - val_mae: 1131.7953\n",
      "Epoch 2464/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 78.1937 - mae: 78.8792 - val_loss: 1260.6215 - val_mae: 1261.3147\n",
      "Epoch 2465/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 70.0167 - mae: 70.6969 - val_loss: 1269.7759 - val_mae: 1270.4678\n",
      "Epoch 2466/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 79.1426 - mae: 79.8254 - val_loss: 1222.6234 - val_mae: 1223.3168\n",
      "Epoch 2467/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 73.0516 - mae: 73.7307 - val_loss: 1313.6281 - val_mae: 1314.3213\n",
      "Epoch 2468/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 70.9450 - mae: 71.6215 - val_loss: 1195.8910 - val_mae: 1196.5842\n",
      "Epoch 2469/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 69.2706 - mae: 69.9478 - val_loss: 1132.7620 - val_mae: 1133.4547\n",
      "Epoch 2470/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 76.6009 - mae: 77.2823 - val_loss: 1382.8762 - val_mae: 1383.5693\n",
      "Epoch 2471/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 79.7839 - mae: 80.4656 - val_loss: 1082.0364 - val_mae: 1082.7292\n",
      "Epoch 2472/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 73.3225 - mae: 74.0007 - val_loss: 1128.2772 - val_mae: 1128.9700\n",
      "Epoch 2473/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 71.2024 - mae: 71.8814 - val_loss: 1131.9612 - val_mae: 1132.6544\n",
      "Epoch 2474/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 73.1043 - mae: 73.7833 - val_loss: 1303.6869 - val_mae: 1304.3799\n",
      "Epoch 2475/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 88.7194 - mae: 89.4028 - val_loss: 1078.6565 - val_mae: 1079.3490\n",
      "Epoch 2476/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 82.1358 - mae: 82.8186 - val_loss: 1125.4430 - val_mae: 1126.1362\n",
      "Epoch 2477/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 74.9403 - mae: 75.6209 - val_loss: 1254.7461 - val_mae: 1255.4392\n",
      "Epoch 2478/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 75.2158 - mae: 75.8958 - val_loss: 1087.8481 - val_mae: 1088.5411\n",
      "Epoch 2479/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 84.1671 - mae: 84.8475 - val_loss: 1202.0320 - val_mae: 1202.7252\n",
      "Epoch 2480/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 80.4549 - mae: 81.1403 - val_loss: 1053.8616 - val_mae: 1054.5538\n",
      "Epoch 2481/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 73.5438 - mae: 74.2258 - val_loss: 1190.7578 - val_mae: 1191.4509\n",
      "Epoch 2482/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 72.4608 - mae: 73.1382 - val_loss: 1124.3230 - val_mae: 1125.0140\n",
      "Epoch 2483/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 74.6439 - mae: 75.3247 - val_loss: 1154.3389 - val_mae: 1155.0315\n",
      "Epoch 2484/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 72.7764 - mae: 73.4568 - val_loss: 1110.8379 - val_mae: 1111.5312\n",
      "Epoch 2485/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 73.4847 - mae: 74.1636 - val_loss: 1261.2828 - val_mae: 1261.9758\n",
      "Epoch 2486/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 68.2066 - mae: 68.8846 - val_loss: 1181.2389 - val_mae: 1181.9320\n",
      "Epoch 2487/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 70.6466 - mae: 71.3268 - val_loss: 1099.3077 - val_mae: 1100.0007\n",
      "Epoch 2488/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 73.9788 - mae: 74.6570 - val_loss: 1067.0774 - val_mae: 1067.7703\n",
      "Epoch 2489/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 75.6595 - mae: 76.3436 - val_loss: 1193.7986 - val_mae: 1194.4915\n",
      "Epoch 2490/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 71.2931 - mae: 71.9753 - val_loss: 1143.6815 - val_mae: 1144.3749\n",
      "Epoch 2491/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 74.5150 - mae: 75.1966 - val_loss: 1178.7574 - val_mae: 1179.4506\n",
      "Epoch 2492/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 68.2848 - mae: 68.9622 - val_loss: 1155.6448 - val_mae: 1156.3379\n",
      "Epoch 2493/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 69.6942 - mae: 70.3743 - val_loss: 1167.1455 - val_mae: 1167.8389\n",
      "Epoch 2494/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 74.6110 - mae: 75.2923 - val_loss: 1152.3398 - val_mae: 1153.0328\n",
      "Epoch 2495/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 71.2652 - mae: 71.9466 - val_loss: 1142.1104 - val_mae: 1142.8032\n",
      "Epoch 2496/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 80.3672 - mae: 81.0521 - val_loss: 1050.1115 - val_mae: 1050.8035\n",
      "Epoch 2497/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 78.5808 - mae: 79.2582 - val_loss: 1182.4023 - val_mae: 1183.0952\n",
      "Epoch 2498/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 71.8614 - mae: 72.5417 - val_loss: 1172.4373 - val_mae: 1173.1304\n",
      "Epoch 2499/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 72.6172 - mae: 73.2966 - val_loss: 1206.2877 - val_mae: 1206.9810\n",
      "Epoch 2500/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 78.6906 - mae: 79.3699 - val_loss: 1170.7644 - val_mae: 1171.4574\n",
      "Epoch 2501/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 73.1334 - mae: 73.8134 - val_loss: 1162.5802 - val_mae: 1163.2732\n",
      "Epoch 2502/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 80.3139 - mae: 80.9967 - val_loss: 1118.7994 - val_mae: 1119.4924\n",
      "Epoch 2503/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 78.7654 - mae: 79.4485 - val_loss: 1387.4188 - val_mae: 1388.1121\n",
      "Epoch 2504/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 76.4688 - mae: 77.1479 - val_loss: 1220.3684 - val_mae: 1221.0604\n",
      "Epoch 2505/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 74.6900 - mae: 75.3728 - val_loss: 1121.0377 - val_mae: 1121.7288\n",
      "Epoch 2506/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 76.0364 - mae: 76.7184 - val_loss: 1189.1431 - val_mae: 1189.8356\n",
      "Epoch 2507/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 79.8412 - mae: 80.5238 - val_loss: 1086.1093 - val_mae: 1086.8024\n",
      "Epoch 2508/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 74.6511 - mae: 75.3311 - val_loss: 1058.1144 - val_mae: 1058.8075\n",
      "Epoch 2509/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 79.1372 - mae: 79.8172 - val_loss: 1203.2521 - val_mae: 1203.9448\n",
      "Epoch 2510/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 75.6252 - mae: 76.3018 - val_loss: 1170.9860 - val_mae: 1171.6786\n",
      "Epoch 2511/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 73.4007 - mae: 74.0840 - val_loss: 1182.2166 - val_mae: 1182.9099\n",
      "Epoch 2512/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 74.6357 - mae: 75.3195 - val_loss: 1127.1897 - val_mae: 1127.8828\n",
      "Epoch 2513/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 87.0628 - mae: 87.7480 - val_loss: 1101.0424 - val_mae: 1101.7343\n",
      "Epoch 2514/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 73.8056 - mae: 74.4873 - val_loss: 1144.5073 - val_mae: 1145.2007\n",
      "Epoch 2515/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 68.3883 - mae: 69.0671 - val_loss: 1170.5442 - val_mae: 1171.2366\n",
      "Epoch 2516/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 78.3727 - mae: 79.0552 - val_loss: 1217.0485 - val_mae: 1217.7417\n",
      "Epoch 2517/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 76.9945 - mae: 77.6732 - val_loss: 1185.5005 - val_mae: 1186.1930\n",
      "Epoch 2518/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 76.6492 - mae: 77.3304 - val_loss: 1267.6418 - val_mae: 1268.3346\n",
      "Epoch 2519/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 76.2199 - mae: 76.9013 - val_loss: 1229.3141 - val_mae: 1230.0070\n",
      "Epoch 2520/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 72.5955 - mae: 73.2749 - val_loss: 1129.9446 - val_mae: 1130.6377\n",
      "Epoch 2521/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 73.5200 - mae: 74.1997 - val_loss: 1209.0975 - val_mae: 1209.7902\n",
      "Epoch 2522/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 74.1057 - mae: 74.7875 - val_loss: 1074.4166 - val_mae: 1075.1095\n",
      "Epoch 2523/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 79.2680 - mae: 79.9491 - val_loss: 1159.9189 - val_mae: 1160.6121\n",
      "Epoch 2524/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 84.5715 - mae: 85.2520 - val_loss: 1362.5306 - val_mae: 1363.2239\n",
      "Epoch 2525/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 74.4659 - mae: 75.1433 - val_loss: 1201.4034 - val_mae: 1202.0963\n",
      "Epoch 2526/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 75.2603 - mae: 75.9398 - val_loss: 1216.3488 - val_mae: 1217.0419\n",
      "Epoch 2527/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 74.7126 - mae: 75.3938 - val_loss: 1212.3478 - val_mae: 1213.0410\n",
      "Epoch 2528/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 73.9581 - mae: 74.6398 - val_loss: 1260.6158 - val_mae: 1261.3091\n",
      "Epoch 2529/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 70.8791 - mae: 71.5598 - val_loss: 1170.6641 - val_mae: 1171.3547\n",
      "Epoch 2530/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 76.0912 - mae: 76.7691 - val_loss: 1082.7312 - val_mae: 1083.4242\n",
      "Epoch 2531/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 76.7965 - mae: 77.4778 - val_loss: 1207.6681 - val_mae: 1208.3608\n",
      "Epoch 2532/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 67.2700 - mae: 67.9490 - val_loss: 1201.4264 - val_mae: 1202.1194\n",
      "Epoch 2533/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 74.8913 - mae: 75.5705 - val_loss: 1283.4048 - val_mae: 1284.0974\n",
      "Epoch 2534/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 77.7538 - mae: 78.4373 - val_loss: 1185.6918 - val_mae: 1186.3845\n",
      "Epoch 2535/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 79.8749 - mae: 80.5550 - val_loss: 1214.1184 - val_mae: 1214.8114\n",
      "Epoch 2536/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 72.2876 - mae: 72.9673 - val_loss: 1099.4414 - val_mae: 1100.1346\n",
      "Epoch 2537/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 77.3276 - mae: 78.0084 - val_loss: 1097.0122 - val_mae: 1097.7053\n",
      "Epoch 2538/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 75.0083 - mae: 75.6868 - val_loss: 1197.1965 - val_mae: 1197.8898\n",
      "Epoch 2539/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 73.9024 - mae: 74.5817 - val_loss: 1141.2913 - val_mae: 1141.9838\n",
      "Epoch 2540/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 70.3571 - mae: 71.0387 - val_loss: 1195.5110 - val_mae: 1196.2034\n",
      "Epoch 2541/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 72.8495 - mae: 73.5263 - val_loss: 1098.0905 - val_mae: 1098.7833\n",
      "Epoch 2542/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 79.1211 - mae: 79.8031 - val_loss: 1094.4258 - val_mae: 1095.1185\n",
      "Epoch 2543/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 81.0006 - mae: 81.6823 - val_loss: 1118.4741 - val_mae: 1119.1672\n",
      "Epoch 2544/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 67.7968 - mae: 68.4782 - val_loss: 1058.0597 - val_mae: 1058.7526\n",
      "Epoch 2545/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 72.0795 - mae: 72.7590 - val_loss: 1210.0818 - val_mae: 1210.7743\n",
      "Epoch 2546/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 71.8883 - mae: 72.5652 - val_loss: 1178.5576 - val_mae: 1179.2507\n",
      "Epoch 2547/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 70.2506 - mae: 70.9275 - val_loss: 1106.9441 - val_mae: 1107.6372\n",
      "Epoch 2548/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 73.6971 - mae: 74.3740 - val_loss: 1116.2247 - val_mae: 1116.9175\n",
      "Epoch 2549/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 69.6444 - mae: 70.3243 - val_loss: 1124.5608 - val_mae: 1125.2537\n",
      "Epoch 2550/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 73.7579 - mae: 74.4381 - val_loss: 1068.9293 - val_mae: 1069.6221\n",
      "Epoch 2551/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 88.5904 - mae: 89.2759 - val_loss: 1145.2097 - val_mae: 1145.9031\n",
      "Epoch 2552/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 72.9614 - mae: 73.6460 - val_loss: 1207.8976 - val_mae: 1208.5907\n",
      "Epoch 2553/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 72.4893 - mae: 73.1711 - val_loss: 1233.5974 - val_mae: 1234.2905\n",
      "Epoch 2554/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 81.9288 - mae: 82.6094 - val_loss: 1668.3859 - val_mae: 1669.0782\n",
      "Epoch 2555/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 94.2750 - mae: 94.9605 - val_loss: 1035.6921 - val_mae: 1036.3844\n",
      "Epoch 2556/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 77.1627 - mae: 77.8423 - val_loss: 1165.9194 - val_mae: 1166.6119\n",
      "Epoch 2557/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 74.0195 - mae: 74.6984 - val_loss: 1065.1611 - val_mae: 1065.8539\n",
      "Epoch 2558/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 73.9959 - mae: 74.6710 - val_loss: 1121.5304 - val_mae: 1122.2236\n",
      "Epoch 2559/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 71.4887 - mae: 72.1665 - val_loss: 1143.5535 - val_mae: 1144.2463\n",
      "Epoch 2560/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 77.6084 - mae: 78.2896 - val_loss: 1110.3622 - val_mae: 1111.0544\n",
      "Epoch 2561/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 74.5687 - mae: 75.2503 - val_loss: 1101.1132 - val_mae: 1101.8063\n",
      "Epoch 2562/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 71.5079 - mae: 72.1873 - val_loss: 1074.4194 - val_mae: 1075.1123\n",
      "Epoch 2563/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 75.3459 - mae: 76.0262 - val_loss: 1094.0289 - val_mae: 1094.7218\n",
      "Epoch 2564/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 79.4876 - mae: 80.1686 - val_loss: 1083.2544 - val_mae: 1083.9467\n",
      "Epoch 2565/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 78.9708 - mae: 79.6477 - val_loss: 1086.1355 - val_mae: 1086.8274\n",
      "Epoch 2566/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 74.5060 - mae: 75.1880 - val_loss: 1163.5168 - val_mae: 1164.2100\n",
      "Epoch 2567/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 74.8188 - mae: 75.4999 - val_loss: 1246.5802 - val_mae: 1247.2721\n",
      "Epoch 2568/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 70.3907 - mae: 71.0710 - val_loss: 1246.7369 - val_mae: 1247.4298\n",
      "Epoch 2569/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 76.8914 - mae: 77.5702 - val_loss: 1111.9508 - val_mae: 1112.6438\n",
      "Epoch 2570/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 74.6590 - mae: 75.3414 - val_loss: 1141.2759 - val_mae: 1141.9689\n",
      "Epoch 2571/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 74.7858 - mae: 75.4647 - val_loss: 1173.5337 - val_mae: 1174.2267\n",
      "Epoch 2572/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 77.6176 - mae: 78.2972 - val_loss: 1191.1923 - val_mae: 1191.8853\n",
      "Epoch 2573/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 70.5456 - mae: 71.2261 - val_loss: 1120.3274 - val_mae: 1121.0204\n",
      "Epoch 2574/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 76.0941 - mae: 76.7693 - val_loss: 1206.2341 - val_mae: 1206.9274\n",
      "Epoch 2575/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 80.0126 - mae: 80.6940 - val_loss: 1207.0195 - val_mae: 1207.7128\n",
      "Epoch 2576/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 72.7667 - mae: 73.4485 - val_loss: 1070.7654 - val_mae: 1071.4585\n",
      "Epoch 2577/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 76.2365 - mae: 76.9175 - val_loss: 1137.4032 - val_mae: 1138.0952\n",
      "Epoch 2578/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 73.4157 - mae: 74.0950 - val_loss: 1111.8497 - val_mae: 1112.5420\n",
      "Epoch 2579/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 72.5594 - mae: 73.2385 - val_loss: 1063.6478 - val_mae: 1064.3398\n",
      "Epoch 2580/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 79.2003 - mae: 79.8801 - val_loss: 1106.7842 - val_mae: 1107.4763\n",
      "Epoch 2581/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 72.6334 - mae: 73.3136 - val_loss: 1137.7466 - val_mae: 1138.4393\n",
      "Epoch 2582/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 70.0284 - mae: 70.7069 - val_loss: 1117.2251 - val_mae: 1117.9182\n",
      "Epoch 2583/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 67.7935 - mae: 68.4704 - val_loss: 1170.5812 - val_mae: 1171.2728\n",
      "Epoch 2584/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 74.8711 - mae: 75.5499 - val_loss: 1123.2153 - val_mae: 1123.9070\n",
      "Epoch 2585/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 70.3129 - mae: 70.9927 - val_loss: 1203.7341 - val_mae: 1204.4272\n",
      "Epoch 2586/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 74.8671 - mae: 75.5488 - val_loss: 1123.4467 - val_mae: 1124.1392\n",
      "Epoch 2587/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 74.3417 - mae: 75.0236 - val_loss: 1123.8429 - val_mae: 1124.5361\n",
      "Epoch 2588/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 68.8562 - mae: 69.5339 - val_loss: 1249.7313 - val_mae: 1250.4243\n",
      "Epoch 2589/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 73.9989 - mae: 74.6809 - val_loss: 1176.3918 - val_mae: 1177.0846\n",
      "Epoch 2590/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 82.1351 - mae: 82.8149 - val_loss: 1076.4733 - val_mae: 1077.1664\n",
      "Epoch 2591/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 77.5773 - mae: 78.2557 - val_loss: 1312.8873 - val_mae: 1313.5802\n",
      "Epoch 2592/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 68.9274 - mae: 69.6055 - val_loss: 1234.3079 - val_mae: 1235.0011\n",
      "Epoch 2593/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 70.2613 - mae: 70.9419 - val_loss: 1209.6046 - val_mae: 1210.2977\n",
      "Epoch 2594/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 74.3836 - mae: 75.0634 - val_loss: 1043.9708 - val_mae: 1044.6639\n",
      "Epoch 2595/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 74.7710 - mae: 75.4513 - val_loss: 1113.4661 - val_mae: 1114.1592\n",
      "Epoch 2596/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 66.6351 - mae: 67.3144 - val_loss: 1133.1600 - val_mae: 1133.8533\n",
      "Epoch 2597/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 76.5814 - mae: 77.2595 - val_loss: 1051.6731 - val_mae: 1052.3660\n",
      "Epoch 2598/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 77.3567 - mae: 78.0330 - val_loss: 1109.1985 - val_mae: 1109.8911\n",
      "Epoch 2599/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 70.3556 - mae: 71.0345 - val_loss: 1187.9039 - val_mae: 1188.5967\n",
      "Epoch 2600/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 84.6167 - mae: 85.2996 - val_loss: 1216.1919 - val_mae: 1216.8849\n",
      "Epoch 2601/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 71.4503 - mae: 72.1298 - val_loss: 1190.3917 - val_mae: 1191.0847\n",
      "Epoch 2602/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 66.9148 - mae: 67.5920 - val_loss: 1156.5287 - val_mae: 1157.2208\n",
      "Epoch 2603/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 67.1389 - mae: 67.8167 - val_loss: 1172.2284 - val_mae: 1172.9210\n",
      "Epoch 2604/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 69.6639 - mae: 70.3421 - val_loss: 1136.6268 - val_mae: 1137.3198\n",
      "Epoch 2605/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 80.6977 - mae: 81.3789 - val_loss: 1208.2783 - val_mae: 1208.9711\n",
      "Epoch 2606/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 68.4120 - mae: 69.0920 - val_loss: 1175.4296 - val_mae: 1176.1218\n",
      "Epoch 2607/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 77.0464 - mae: 77.7268 - val_loss: 1309.6279 - val_mae: 1310.3206\n",
      "Epoch 2608/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 79.1359 - mae: 79.8176 - val_loss: 1255.2056 - val_mae: 1255.8987\n",
      "Epoch 2609/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 72.6977 - mae: 73.3771 - val_loss: 1170.0663 - val_mae: 1170.7582\n",
      "Epoch 2610/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 69.4882 - mae: 70.1636 - val_loss: 1178.1946 - val_mae: 1178.8875\n",
      "Epoch 2611/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 76.0062 - mae: 76.6874 - val_loss: 1201.6766 - val_mae: 1202.3698\n",
      "Epoch 2612/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 74.1779 - mae: 74.8566 - val_loss: 1188.2061 - val_mae: 1188.8990\n",
      "Epoch 2613/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 71.5141 - mae: 72.1946 - val_loss: 1176.8287 - val_mae: 1177.5219\n",
      "Epoch 2614/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 73.8485 - mae: 74.5287 - val_loss: 1203.8224 - val_mae: 1204.5156\n",
      "Epoch 2615/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 75.2194 - mae: 75.8989 - val_loss: 1212.7312 - val_mae: 1213.4244\n",
      "Epoch 2616/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 76.3110 - mae: 76.9937 - val_loss: 1132.2084 - val_mae: 1132.9003\n",
      "Epoch 2617/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 80.3665 - mae: 81.0505 - val_loss: 1355.9932 - val_mae: 1356.6865\n",
      "Epoch 2618/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 74.8406 - mae: 75.5227 - val_loss: 1188.6493 - val_mae: 1189.3416\n",
      "Epoch 2619/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 74.6834 - mae: 75.3585 - val_loss: 1166.5306 - val_mae: 1167.2234\n",
      "Epoch 2620/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 74.5908 - mae: 75.2667 - val_loss: 1093.8789 - val_mae: 1094.5708\n",
      "Epoch 2621/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 71.3972 - mae: 72.0789 - val_loss: 1091.5809 - val_mae: 1092.2728\n",
      "Epoch 2622/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 77.9935 - mae: 78.6709 - val_loss: 1123.2905 - val_mae: 1123.9830\n",
      "Epoch 2623/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 72.8421 - mae: 73.5245 - val_loss: 1141.4552 - val_mae: 1142.1472\n",
      "Epoch 2624/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 73.3743 - mae: 74.0518 - val_loss: 1185.6860 - val_mae: 1186.3793\n",
      "Epoch 2625/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 72.6171 - mae: 73.2978 - val_loss: 1081.1166 - val_mae: 1081.8094\n",
      "Epoch 2626/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 73.1196 - mae: 73.7989 - val_loss: 1134.9219 - val_mae: 1135.6135\n",
      "Epoch 2627/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 83.7136 - mae: 84.3954 - val_loss: 1059.1793 - val_mae: 1059.8723\n",
      "Epoch 2628/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 78.7869 - mae: 79.4685 - val_loss: 1065.3064 - val_mae: 1065.9979\n",
      "Epoch 2629/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 76.2469 - mae: 76.9293 - val_loss: 1325.2421 - val_mae: 1325.9340\n",
      "Epoch 2630/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 78.9574 - mae: 79.6405 - val_loss: 1138.5603 - val_mae: 1139.2535\n",
      "Epoch 2631/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 77.1519 - mae: 77.8303 - val_loss: 1109.9146 - val_mae: 1110.6077\n",
      "Epoch 2632/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 81.1400 - mae: 81.8184 - val_loss: 1068.8666 - val_mae: 1069.5583\n",
      "Epoch 2633/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 77.9836 - mae: 78.6603 - val_loss: 1119.8004 - val_mae: 1120.4935\n",
      "Epoch 2634/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 88.3265 - mae: 89.0086 - val_loss: 1139.1746 - val_mae: 1139.8676\n",
      "Epoch 2635/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 74.4950 - mae: 75.1795 - val_loss: 1079.2694 - val_mae: 1079.9624\n",
      "Epoch 2636/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 77.6279 - mae: 78.3064 - val_loss: 1142.2792 - val_mae: 1142.9723\n",
      "Epoch 2637/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 69.6706 - mae: 70.3503 - val_loss: 1100.4050 - val_mae: 1101.0980\n",
      "Epoch 2638/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 70.0140 - mae: 70.6917 - val_loss: 1104.7056 - val_mae: 1105.3979\n",
      "Epoch 2639/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 66.6802 - mae: 67.3575 - val_loss: 1133.9355 - val_mae: 1134.6287\n",
      "Epoch 2640/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 67.5881 - mae: 68.2650 - val_loss: 1112.8190 - val_mae: 1113.5123\n",
      "Epoch 2641/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 73.6882 - mae: 74.3674 - val_loss: 1119.5765 - val_mae: 1120.2692\n",
      "Epoch 2642/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 79.0491 - mae: 79.7290 - val_loss: 1067.7548 - val_mae: 1068.4480\n",
      "Epoch 2643/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 73.7948 - mae: 74.4739 - val_loss: 1116.6664 - val_mae: 1117.3591\n",
      "Epoch 2644/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 85.6567 - mae: 86.3400 - val_loss: 1215.2949 - val_mae: 1215.9882\n",
      "Epoch 2645/5000\n",
      "46/46 [==============================] - 2s 32ms/step - loss: 76.9319 - mae: 77.6120 - val_loss: 1114.3373 - val_mae: 1115.0304\n",
      "Epoch 2646/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 72.0184 - mae: 72.6978 - val_loss: 1055.2684 - val_mae: 1055.9613\n",
      "Epoch 2647/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 76.8373 - mae: 77.5166 - val_loss: 1120.1748 - val_mae: 1120.8680\n",
      "Epoch 2648/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 70.7651 - mae: 71.4477 - val_loss: 1114.8145 - val_mae: 1115.5066\n",
      "Epoch 2649/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 64.9693 - mae: 65.6501 - val_loss: 1098.4064 - val_mae: 1099.0996\n",
      "Epoch 2650/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 68.2070 - mae: 68.8890 - val_loss: 1098.2212 - val_mae: 1098.9143\n",
      "Epoch 2651/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 66.6240 - mae: 67.3035 - val_loss: 1121.3163 - val_mae: 1122.0079\n",
      "Epoch 2652/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 74.7076 - mae: 75.3871 - val_loss: 1200.1355 - val_mae: 1200.8280\n",
      "Epoch 2653/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 67.6202 - mae: 68.3003 - val_loss: 1127.1295 - val_mae: 1127.8221\n",
      "Epoch 2654/5000\n",
      "46/46 [==============================] - 2s 32ms/step - loss: 68.4623 - mae: 69.1397 - val_loss: 1130.5051 - val_mae: 1131.1984\n",
      "Epoch 2655/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 69.9503 - mae: 70.6273 - val_loss: 1117.4474 - val_mae: 1118.1401\n",
      "Epoch 2656/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 71.7555 - mae: 72.4334 - val_loss: 1235.6641 - val_mae: 1236.3572\n",
      "Epoch 2657/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 72.0520 - mae: 72.7316 - val_loss: 1120.1934 - val_mae: 1120.8864\n",
      "Epoch 2658/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 71.5835 - mae: 72.2651 - val_loss: 1186.4869 - val_mae: 1187.1802\n",
      "Epoch 2659/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 87.5234 - mae: 88.2048 - val_loss: 1091.8265 - val_mae: 1092.5189\n",
      "Epoch 2660/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 78.7414 - mae: 79.4240 - val_loss: 1120.8207 - val_mae: 1121.5137\n",
      "Epoch 2661/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 71.7010 - mae: 72.3827 - val_loss: 1159.6024 - val_mae: 1160.2954\n",
      "Epoch 2662/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 72.0727 - mae: 72.7560 - val_loss: 1082.0573 - val_mae: 1082.7504\n",
      "Epoch 2663/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 77.9915 - mae: 78.6724 - val_loss: 1214.0182 - val_mae: 1214.7114\n",
      "Epoch 2664/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 69.1382 - mae: 69.8169 - val_loss: 1112.7565 - val_mae: 1113.4487\n",
      "Epoch 2665/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 70.2883 - mae: 70.9663 - val_loss: 1178.7729 - val_mae: 1179.4662\n",
      "Epoch 2666/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 68.3061 - mae: 68.9831 - val_loss: 1130.6942 - val_mae: 1131.3866\n",
      "Epoch 2667/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 75.9789 - mae: 76.6585 - val_loss: 1106.4674 - val_mae: 1107.1600\n",
      "Epoch 2668/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 71.4657 - mae: 72.1461 - val_loss: 1228.3202 - val_mae: 1229.0134\n",
      "Epoch 2669/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 75.9084 - mae: 76.5904 - val_loss: 1143.5608 - val_mae: 1144.2539\n",
      "Epoch 2670/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 69.1463 - mae: 69.8249 - val_loss: 1137.3026 - val_mae: 1137.9957\n",
      "Epoch 2671/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 75.7799 - mae: 76.4591 - val_loss: 1135.2600 - val_mae: 1135.9531\n",
      "Epoch 2672/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 76.1937 - mae: 76.8747 - val_loss: 1116.2163 - val_mae: 1116.9091\n",
      "Epoch 2673/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 67.6623 - mae: 68.3428 - val_loss: 1081.0530 - val_mae: 1081.7449\n",
      "Epoch 2674/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 82.2738 - mae: 82.9571 - val_loss: 1315.8433 - val_mae: 1316.5354\n",
      "Epoch 2675/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 69.8403 - mae: 70.5185 - val_loss: 1093.3589 - val_mae: 1094.0520\n",
      "Epoch 2676/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 73.2429 - mae: 73.9216 - val_loss: 1180.8806 - val_mae: 1181.5736\n",
      "Epoch 2677/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 76.8427 - mae: 77.5224 - val_loss: 1081.6016 - val_mae: 1082.2941\n",
      "Epoch 2678/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 83.9169 - mae: 84.5967 - val_loss: 1108.9443 - val_mae: 1109.6375\n",
      "Epoch 2679/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 71.5186 - mae: 72.1984 - val_loss: 1118.1492 - val_mae: 1118.8402\n",
      "Epoch 2680/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 67.2164 - mae: 67.8946 - val_loss: 1099.3755 - val_mae: 1100.0686\n",
      "Epoch 2681/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 68.6036 - mae: 69.2795 - val_loss: 1216.3022 - val_mae: 1216.9946\n",
      "Epoch 2682/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 73.4532 - mae: 74.1355 - val_loss: 1129.0311 - val_mae: 1129.7242\n",
      "Epoch 2683/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 85.5834 - mae: 86.2668 - val_loss: 1174.2852 - val_mae: 1174.9785\n",
      "Epoch 2684/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 72.7846 - mae: 73.4678 - val_loss: 1206.9176 - val_mae: 1207.6107\n",
      "Epoch 2685/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 74.6584 - mae: 75.3383 - val_loss: 1163.7668 - val_mae: 1164.4600\n",
      "Epoch 2686/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 90.9889 - mae: 91.6713 - val_loss: 1220.7019 - val_mae: 1221.3950\n",
      "Epoch 2687/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 73.2657 - mae: 73.9429 - val_loss: 1087.3818 - val_mae: 1088.0751\n",
      "Epoch 2688/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 66.4013 - mae: 67.0773 - val_loss: 1086.8492 - val_mae: 1087.5425\n",
      "Epoch 2689/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 76.6659 - mae: 77.3451 - val_loss: 1134.8807 - val_mae: 1135.5725\n",
      "Epoch 2690/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 74.3000 - mae: 74.9840 - val_loss: 1190.0128 - val_mae: 1190.7050\n",
      "Epoch 2691/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 77.6860 - mae: 78.3691 - val_loss: 1081.5129 - val_mae: 1082.2053\n",
      "Epoch 2692/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 81.7423 - mae: 82.4242 - val_loss: 1086.0480 - val_mae: 1086.7388\n",
      "Epoch 2693/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 71.8312 - mae: 72.5125 - val_loss: 1164.4347 - val_mae: 1165.1274\n",
      "Epoch 2694/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 72.2212 - mae: 72.9003 - val_loss: 1237.8660 - val_mae: 1238.5590\n",
      "Epoch 2695/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 73.3275 - mae: 74.0076 - val_loss: 1078.2423 - val_mae: 1078.9347\n",
      "Epoch 2696/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 72.5418 - mae: 73.2223 - val_loss: 1110.0022 - val_mae: 1110.6953\n",
      "Epoch 2697/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 84.7502 - mae: 85.4324 - val_loss: 1069.6517 - val_mae: 1070.3448\n",
      "Epoch 2698/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 72.6022 - mae: 73.2795 - val_loss: 1172.1793 - val_mae: 1172.8722\n",
      "Epoch 2699/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 68.0045 - mae: 68.6829 - val_loss: 1188.1875 - val_mae: 1188.8806\n",
      "Epoch 2700/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 72.3959 - mae: 73.0749 - val_loss: 1216.4738 - val_mae: 1217.1670\n",
      "Epoch 2701/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 73.9129 - mae: 74.5923 - val_loss: 1148.3235 - val_mae: 1149.0154\n",
      "Epoch 2702/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 70.8383 - mae: 71.5163 - val_loss: 1077.1793 - val_mae: 1077.8724\n",
      "Epoch 2703/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 67.9319 - mae: 68.6090 - val_loss: 1093.4026 - val_mae: 1094.0957\n",
      "Epoch 2704/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 68.8532 - mae: 69.5311 - val_loss: 1137.6259 - val_mae: 1138.3182\n",
      "Epoch 2705/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 71.5286 - mae: 72.2123 - val_loss: 1211.0978 - val_mae: 1211.7911\n",
      "Epoch 2706/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 74.9833 - mae: 75.6641 - val_loss: 1169.6692 - val_mae: 1170.3624\n",
      "Epoch 2707/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 68.5771 - mae: 69.2568 - val_loss: 1148.8763 - val_mae: 1149.5691\n",
      "Epoch 2708/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 74.0094 - mae: 74.6867 - val_loss: 1131.9921 - val_mae: 1132.6851\n",
      "Epoch 2709/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 66.3653 - mae: 67.0394 - val_loss: 1128.2144 - val_mae: 1128.9062\n",
      "Epoch 2710/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 74.1889 - mae: 74.8716 - val_loss: 1169.0062 - val_mae: 1169.6993\n",
      "Epoch 2711/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 70.9963 - mae: 71.6759 - val_loss: 1176.6787 - val_mae: 1177.3718\n",
      "Epoch 2712/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 65.8779 - mae: 66.5551 - val_loss: 1252.8707 - val_mae: 1253.5641\n",
      "Epoch 2713/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 67.3747 - mae: 68.0550 - val_loss: 1260.2175 - val_mae: 1260.9106\n",
      "Epoch 2714/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 83.1400 - mae: 83.8214 - val_loss: 1260.4247 - val_mae: 1261.1178\n",
      "Epoch 2715/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 69.7419 - mae: 70.4195 - val_loss: 1129.3754 - val_mae: 1130.0685\n",
      "Epoch 2716/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 67.7437 - mae: 68.4233 - val_loss: 1105.2037 - val_mae: 1105.8966\n",
      "Epoch 2717/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 75.0730 - mae: 75.7561 - val_loss: 1210.4967 - val_mae: 1211.1899\n",
      "Epoch 2718/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 69.7363 - mae: 70.4158 - val_loss: 1145.9441 - val_mae: 1146.6375\n",
      "Epoch 2719/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 65.6180 - mae: 66.2975 - val_loss: 1173.6930 - val_mae: 1174.3861\n",
      "Epoch 2720/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 67.9703 - mae: 68.6478 - val_loss: 1237.6483 - val_mae: 1238.3414\n",
      "Epoch 2721/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 76.6628 - mae: 77.3406 - val_loss: 1139.5052 - val_mae: 1140.1971\n",
      "Epoch 2722/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 69.8478 - mae: 70.5311 - val_loss: 1124.6992 - val_mae: 1125.3925\n",
      "Epoch 2723/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 70.0314 - mae: 70.7146 - val_loss: 1127.3495 - val_mae: 1128.0427\n",
      "Epoch 2724/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 64.7073 - mae: 65.3830 - val_loss: 1162.2935 - val_mae: 1162.9866\n",
      "Epoch 2725/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 86.8850 - mae: 87.5661 - val_loss: 1225.6368 - val_mae: 1226.3301\n",
      "Epoch 2726/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 72.8789 - mae: 73.5581 - val_loss: 1093.7836 - val_mae: 1094.4769\n",
      "Epoch 2727/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 72.0294 - mae: 72.7107 - val_loss: 1243.7908 - val_mae: 1244.4840\n",
      "Epoch 2728/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 67.2346 - mae: 67.9143 - val_loss: 1201.9771 - val_mae: 1202.6702\n",
      "Epoch 2729/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 66.6087 - mae: 67.2848 - val_loss: 1085.4465 - val_mae: 1086.1388\n",
      "Epoch 2730/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 70.2374 - mae: 70.9195 - val_loss: 1099.2122 - val_mae: 1099.9053\n",
      "Epoch 2731/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 72.2932 - mae: 72.9754 - val_loss: 1170.1107 - val_mae: 1170.8040\n",
      "Epoch 2732/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 67.6674 - mae: 68.3476 - val_loss: 1154.7747 - val_mae: 1155.4680\n",
      "Epoch 2733/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 67.3446 - mae: 68.0248 - val_loss: 1116.0658 - val_mae: 1116.7588\n",
      "Epoch 2734/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 67.7290 - mae: 68.4087 - val_loss: 1162.8561 - val_mae: 1163.5488\n",
      "Epoch 2735/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 80.0082 - mae: 80.6894 - val_loss: 1129.2020 - val_mae: 1129.8950\n",
      "Epoch 2736/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 70.8196 - mae: 71.4979 - val_loss: 1054.5150 - val_mae: 1055.2076\n",
      "Epoch 2737/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 77.3035 - mae: 77.9824 - val_loss: 1126.8240 - val_mae: 1127.5172\n",
      "Epoch 2738/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 69.7638 - mae: 70.4464 - val_loss: 1161.9697 - val_mae: 1162.6631\n",
      "Epoch 2739/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 71.1490 - mae: 71.8301 - val_loss: 1207.5071 - val_mae: 1208.1997\n",
      "Epoch 2740/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 80.0292 - mae: 80.7085 - val_loss: 1163.7948 - val_mae: 1164.4873\n",
      "Epoch 2741/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 73.4525 - mae: 74.1337 - val_loss: 1097.2963 - val_mae: 1097.9894\n",
      "Epoch 2742/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 66.0100 - mae: 66.6868 - val_loss: 1147.7372 - val_mae: 1148.4299\n",
      "Epoch 2743/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 71.5085 - mae: 72.1863 - val_loss: 1075.5743 - val_mae: 1076.2676\n",
      "Epoch 2744/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 72.4525 - mae: 73.1320 - val_loss: 1137.7141 - val_mae: 1138.4072\n",
      "Epoch 2745/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 76.5914 - mae: 77.2734 - val_loss: 1130.8477 - val_mae: 1131.5409\n",
      "Epoch 2746/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 70.3855 - mae: 71.0676 - val_loss: 1087.8883 - val_mae: 1088.5815\n",
      "Epoch 2747/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 67.0942 - mae: 67.7746 - val_loss: 1139.1857 - val_mae: 1139.8787\n",
      "Epoch 2748/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 73.6062 - mae: 74.2819 - val_loss: 1172.5532 - val_mae: 1173.2462\n",
      "Epoch 2749/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 68.5517 - mae: 69.2286 - val_loss: 1101.8562 - val_mae: 1102.5492\n",
      "Epoch 2750/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 72.1264 - mae: 72.8088 - val_loss: 1152.0564 - val_mae: 1152.7496\n",
      "Epoch 2751/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 68.2797 - mae: 68.9595 - val_loss: 1046.9219 - val_mae: 1047.6138\n",
      "Epoch 2752/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 74.7008 - mae: 75.3800 - val_loss: 1138.9735 - val_mae: 1139.6656\n",
      "Epoch 2753/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 69.9817 - mae: 70.6631 - val_loss: 1136.6493 - val_mae: 1137.3425\n",
      "Epoch 2754/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 81.8136 - mae: 82.4917 - val_loss: 1111.2792 - val_mae: 1111.9723\n",
      "Epoch 2755/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 72.7895 - mae: 73.4645 - val_loss: 1101.1422 - val_mae: 1101.8342\n",
      "Epoch 2756/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 66.8948 - mae: 67.5741 - val_loss: 1069.4003 - val_mae: 1070.0936\n",
      "Epoch 2757/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 67.5259 - mae: 68.2027 - val_loss: 1187.0891 - val_mae: 1187.7822\n",
      "Epoch 2758/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 74.3207 - mae: 75.0002 - val_loss: 1068.8297 - val_mae: 1069.5220\n",
      "Epoch 2759/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 66.6154 - mae: 67.2942 - val_loss: 1187.3470 - val_mae: 1188.0394\n",
      "Epoch 2760/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 68.9667 - mae: 69.6470 - val_loss: 1155.0017 - val_mae: 1155.6948\n",
      "Epoch 2761/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 71.8043 - mae: 72.4858 - val_loss: 1106.0934 - val_mae: 1106.7855\n",
      "Epoch 2762/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 65.3470 - mae: 66.0246 - val_loss: 1135.2524 - val_mae: 1135.9451\n",
      "Epoch 2763/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 80.9002 - mae: 81.5826 - val_loss: 1084.4510 - val_mae: 1085.1440\n",
      "Epoch 2764/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 67.9146 - mae: 68.5885 - val_loss: 1133.1774 - val_mae: 1133.8695\n",
      "Epoch 2765/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 69.1204 - mae: 69.7983 - val_loss: 1078.0032 - val_mae: 1078.6956\n",
      "Epoch 2766/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 80.0564 - mae: 80.7405 - val_loss: 1133.8218 - val_mae: 1134.5149\n",
      "Epoch 2767/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 70.9499 - mae: 71.6284 - val_loss: 1080.4980 - val_mae: 1081.1913\n",
      "Epoch 2768/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 73.9710 - mae: 74.6504 - val_loss: 1177.7203 - val_mae: 1178.4137\n",
      "Epoch 2769/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 70.0523 - mae: 70.7292 - val_loss: 1167.9521 - val_mae: 1168.6453\n",
      "Epoch 2770/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 63.5128 - mae: 64.1883 - val_loss: 1121.7594 - val_mae: 1122.4525\n",
      "Epoch 2771/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 66.4079 - mae: 67.0870 - val_loss: 1147.4515 - val_mae: 1148.1447\n",
      "Epoch 2772/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 72.0470 - mae: 72.7294 - val_loss: 1144.7461 - val_mae: 1145.4392\n",
      "Epoch 2773/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 65.6212 - mae: 66.2987 - val_loss: 1147.4056 - val_mae: 1148.0988\n",
      "Epoch 2774/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 64.1138 - mae: 64.7938 - val_loss: 1128.7139 - val_mae: 1129.4069\n",
      "Epoch 2775/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 76.9444 - mae: 77.6241 - val_loss: 1153.4615 - val_mae: 1154.1538\n",
      "Epoch 2776/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 71.0472 - mae: 71.7291 - val_loss: 1140.1636 - val_mae: 1140.8567\n",
      "Epoch 2777/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 67.6918 - mae: 68.3713 - val_loss: 1127.8452 - val_mae: 1128.5382\n",
      "Epoch 2778/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 83.3441 - mae: 84.0242 - val_loss: 1092.0615 - val_mae: 1092.7549\n",
      "Epoch 2779/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 71.4066 - mae: 72.0856 - val_loss: 1085.0548 - val_mae: 1085.7480\n",
      "Epoch 2780/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 69.0551 - mae: 69.7332 - val_loss: 1096.6273 - val_mae: 1097.3204\n",
      "Epoch 2781/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 74.4270 - mae: 75.1073 - val_loss: 1090.4629 - val_mae: 1091.1560\n",
      "Epoch 2782/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 65.3852 - mae: 66.0661 - val_loss: 1146.1152 - val_mae: 1146.8082\n",
      "Epoch 2783/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 72.6981 - mae: 73.3791 - val_loss: 1103.4612 - val_mae: 1104.1544\n",
      "Epoch 2784/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 63.6552 - mae: 64.3356 - val_loss: 1166.2144 - val_mae: 1166.9073\n",
      "Epoch 2785/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 74.1887 - mae: 74.8726 - val_loss: 1080.0189 - val_mae: 1080.7120\n",
      "Epoch 2786/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 76.4201 - mae: 77.1009 - val_loss: 1081.1304 - val_mae: 1081.8214\n",
      "Epoch 2787/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 74.8987 - mae: 75.5784 - val_loss: 1125.8320 - val_mae: 1126.5239\n",
      "Epoch 2788/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 64.9100 - mae: 65.5884 - val_loss: 1136.0132 - val_mae: 1136.7057\n",
      "Epoch 2789/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 73.5666 - mae: 74.2448 - val_loss: 1135.9430 - val_mae: 1136.6361\n",
      "Epoch 2790/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 69.0143 - mae: 69.6930 - val_loss: 1103.0693 - val_mae: 1103.7625\n",
      "Epoch 2791/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 69.4921 - mae: 70.1717 - val_loss: 1187.6641 - val_mae: 1188.3569\n",
      "Epoch 2792/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 67.6191 - mae: 68.2993 - val_loss: 1085.1030 - val_mae: 1085.7961\n",
      "Epoch 2793/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 68.5404 - mae: 69.2183 - val_loss: 1166.2356 - val_mae: 1166.9285\n",
      "Epoch 2794/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 63.0576 - mae: 63.7356 - val_loss: 1193.6541 - val_mae: 1194.3469\n",
      "Epoch 2795/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 67.2164 - mae: 67.8947 - val_loss: 1134.1737 - val_mae: 1134.8666\n",
      "Epoch 2796/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 68.9672 - mae: 69.6456 - val_loss: 1154.4844 - val_mae: 1155.1775\n",
      "Epoch 2797/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 63.9236 - mae: 64.5999 - val_loss: 1153.3030 - val_mae: 1153.9962\n",
      "Epoch 2798/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 68.8201 - mae: 69.4977 - val_loss: 1133.0143 - val_mae: 1133.7073\n",
      "Epoch 2799/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 73.9527 - mae: 74.6308 - val_loss: 1314.4907 - val_mae: 1315.1825\n",
      "Epoch 2800/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 77.1380 - mae: 77.8179 - val_loss: 1121.0652 - val_mae: 1121.7572\n",
      "Epoch 2801/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 72.2469 - mae: 72.9270 - val_loss: 1179.7046 - val_mae: 1180.3978\n",
      "Epoch 2802/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 64.5275 - mae: 65.2045 - val_loss: 1072.1871 - val_mae: 1072.8804\n",
      "Epoch 2803/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 69.9289 - mae: 70.6068 - val_loss: 1136.1866 - val_mae: 1136.8799\n",
      "Epoch 2804/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 66.5095 - mae: 67.1852 - val_loss: 1168.6763 - val_mae: 1169.3693\n",
      "Epoch 2805/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 72.5048 - mae: 73.1817 - val_loss: 1071.0112 - val_mae: 1071.7042\n",
      "Epoch 2806/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 70.8477 - mae: 71.5250 - val_loss: 1086.6034 - val_mae: 1087.2963\n",
      "Epoch 2807/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 68.5155 - mae: 69.1933 - val_loss: 1087.5365 - val_mae: 1088.2296\n",
      "Epoch 2808/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 74.5975 - mae: 75.2772 - val_loss: 1175.0491 - val_mae: 1175.7423\n",
      "Epoch 2809/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 69.8862 - mae: 70.5647 - val_loss: 1084.2534 - val_mae: 1084.9463\n",
      "Epoch 2810/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 70.3110 - mae: 70.9895 - val_loss: 1122.5673 - val_mae: 1123.2603\n",
      "Epoch 2811/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 79.7551 - mae: 80.4370 - val_loss: 1150.9733 - val_mae: 1151.6665\n",
      "Epoch 2812/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 70.6417 - mae: 71.3219 - val_loss: 1088.5503 - val_mae: 1089.2434\n",
      "Epoch 2813/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 73.8299 - mae: 74.5104 - val_loss: 1203.9562 - val_mae: 1204.6490\n",
      "Epoch 2814/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 72.8137 - mae: 73.4944 - val_loss: 1113.0143 - val_mae: 1113.7073\n",
      "Epoch 2815/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 66.1822 - mae: 66.8613 - val_loss: 1115.5444 - val_mae: 1116.2377\n",
      "Epoch 2816/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 66.9933 - mae: 67.6718 - val_loss: 1152.5135 - val_mae: 1153.2064\n",
      "Epoch 2817/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 71.4999 - mae: 72.1804 - val_loss: 1231.2040 - val_mae: 1231.8965\n",
      "Epoch 2818/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 65.4865 - mae: 66.1661 - val_loss: 1098.0803 - val_mae: 1098.7731\n",
      "Epoch 2819/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 67.0393 - mae: 67.7205 - val_loss: 1117.4667 - val_mae: 1118.1598\n",
      "Epoch 2820/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 66.0346 - mae: 66.7089 - val_loss: 1079.5708 - val_mae: 1080.2638\n",
      "Epoch 2821/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 66.8817 - mae: 67.5605 - val_loss: 1118.3530 - val_mae: 1119.0460\n",
      "Epoch 2822/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 76.7931 - mae: 77.4731 - val_loss: 1192.7062 - val_mae: 1193.3994\n",
      "Epoch 2823/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 74.1655 - mae: 74.8463 - val_loss: 1168.5048 - val_mae: 1169.1973\n",
      "Epoch 2824/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 64.7507 - mae: 65.4255 - val_loss: 1136.8099 - val_mae: 1137.5032\n",
      "Epoch 2825/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 64.2593 - mae: 64.9364 - val_loss: 1146.1576 - val_mae: 1146.8507\n",
      "Epoch 2826/5000\n",
      "46/46 [==============================] - 2s 41ms/step - loss: 62.9006 - mae: 63.5795 - val_loss: 1130.8434 - val_mae: 1131.5364\n",
      "Epoch 2827/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 68.0824 - mae: 68.7593 - val_loss: 1060.7894 - val_mae: 1061.4812\n",
      "Epoch 2828/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 66.3753 - mae: 67.0554 - val_loss: 1118.3975 - val_mae: 1119.0902\n",
      "Epoch 2829/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 73.1427 - mae: 73.8233 - val_loss: 1125.8547 - val_mae: 1126.5479\n",
      "Epoch 2830/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 67.8150 - mae: 68.4950 - val_loss: 1163.9683 - val_mae: 1164.6614\n",
      "Epoch 2831/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 67.2170 - mae: 67.8954 - val_loss: 1183.5078 - val_mae: 1184.2009\n",
      "Epoch 2832/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 71.1739 - mae: 71.8511 - val_loss: 1175.9565 - val_mae: 1176.6499\n",
      "Epoch 2833/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 69.7508 - mae: 70.4283 - val_loss: 1149.3746 - val_mae: 1150.0668\n",
      "Epoch 2834/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 68.1970 - mae: 68.8745 - val_loss: 1080.3253 - val_mae: 1081.0186\n",
      "Epoch 2835/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 63.4609 - mae: 64.1391 - val_loss: 1218.8452 - val_mae: 1219.5382\n",
      "Epoch 2836/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 68.1335 - mae: 68.8109 - val_loss: 1210.2969 - val_mae: 1210.9902\n",
      "Epoch 2837/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 64.8437 - mae: 65.5249 - val_loss: 1110.3788 - val_mae: 1111.0710\n",
      "Epoch 2838/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 65.7462 - mae: 66.4281 - val_loss: 1209.5989 - val_mae: 1210.2921\n",
      "Epoch 2839/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 63.9220 - mae: 64.5984 - val_loss: 1092.1943 - val_mae: 1092.8871\n",
      "Epoch 2840/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 71.5125 - mae: 72.1909 - val_loss: 1167.1482 - val_mae: 1167.8413\n",
      "Epoch 2841/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 67.6529 - mae: 68.3314 - val_loss: 1193.0562 - val_mae: 1193.7483\n",
      "Epoch 2842/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 64.0020 - mae: 64.6808 - val_loss: 1118.1544 - val_mae: 1118.8477\n",
      "Epoch 2843/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 77.7350 - mae: 78.4159 - val_loss: 1076.5046 - val_mae: 1077.1978\n",
      "Epoch 2844/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 72.5770 - mae: 73.2564 - val_loss: 1195.4508 - val_mae: 1196.1438\n",
      "Epoch 2845/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 79.0597 - mae: 79.7433 - val_loss: 1091.4509 - val_mae: 1092.1440\n",
      "Epoch 2846/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 67.4662 - mae: 68.1423 - val_loss: 1114.4888 - val_mae: 1115.1821\n",
      "Epoch 2847/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 67.3339 - mae: 68.0154 - val_loss: 1137.7252 - val_mae: 1138.4183\n",
      "Epoch 2848/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 74.3829 - mae: 75.0619 - val_loss: 1196.7946 - val_mae: 1197.4878\n",
      "Epoch 2849/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 68.7011 - mae: 69.3785 - val_loss: 1164.1031 - val_mae: 1164.7959\n",
      "Epoch 2850/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 74.7982 - mae: 75.4748 - val_loss: 1122.3883 - val_mae: 1123.0814\n",
      "Epoch 2851/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 78.2781 - mae: 78.9610 - val_loss: 1146.8469 - val_mae: 1147.5402\n",
      "Epoch 2852/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 75.9641 - mae: 76.6465 - val_loss: 1134.4618 - val_mae: 1135.1533\n",
      "Epoch 2853/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 69.8725 - mae: 70.5516 - val_loss: 1170.1305 - val_mae: 1170.8235\n",
      "Epoch 2854/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 68.3599 - mae: 69.0425 - val_loss: 1089.3367 - val_mae: 1090.0292\n",
      "Epoch 2855/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 79.8837 - mae: 80.5600 - val_loss: 1135.8879 - val_mae: 1136.5809\n",
      "Epoch 2856/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 68.2731 - mae: 68.9519 - val_loss: 1103.8110 - val_mae: 1104.5038\n",
      "Epoch 2857/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 73.1169 - mae: 73.7959 - val_loss: 1184.5859 - val_mae: 1185.2793\n",
      "Epoch 2858/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 70.2806 - mae: 70.9577 - val_loss: 1090.9690 - val_mae: 1091.6615\n",
      "Epoch 2859/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 73.7772 - mae: 74.4545 - val_loss: 1173.1592 - val_mae: 1173.8524\n",
      "Epoch 2860/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 79.5362 - mae: 80.2176 - val_loss: 1253.5404 - val_mae: 1254.2321\n",
      "Epoch 2861/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 72.6824 - mae: 73.3615 - val_loss: 1099.3462 - val_mae: 1100.0378\n",
      "Epoch 2862/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 67.2603 - mae: 67.9401 - val_loss: 1083.7230 - val_mae: 1084.4161\n",
      "Epoch 2863/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 70.6880 - mae: 71.3706 - val_loss: 1126.5278 - val_mae: 1127.2207\n",
      "Epoch 2864/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 68.9866 - mae: 69.6650 - val_loss: 1088.6963 - val_mae: 1089.3894\n",
      "Epoch 2865/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 66.4986 - mae: 67.1776 - val_loss: 1175.1287 - val_mae: 1175.8219\n",
      "Epoch 2866/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 72.9655 - mae: 73.6470 - val_loss: 1167.4158 - val_mae: 1168.1088\n",
      "Epoch 2867/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 64.9374 - mae: 65.6151 - val_loss: 1172.8234 - val_mae: 1173.5165\n",
      "Epoch 2868/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 71.6841 - mae: 72.3664 - val_loss: 1155.8964 - val_mae: 1156.5896\n",
      "Epoch 2869/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 68.7202 - mae: 69.4006 - val_loss: 1101.7933 - val_mae: 1102.4846\n",
      "Epoch 2870/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 74.6994 - mae: 75.3779 - val_loss: 1226.4551 - val_mae: 1227.1475\n",
      "Epoch 2871/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 76.2147 - mae: 76.8925 - val_loss: 1064.6975 - val_mae: 1065.3903\n",
      "Epoch 2872/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 74.4633 - mae: 75.1427 - val_loss: 1130.9700 - val_mae: 1131.6620\n",
      "Epoch 2873/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 73.4595 - mae: 74.1371 - val_loss: 1179.9235 - val_mae: 1180.6163\n",
      "Epoch 2874/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 82.5077 - mae: 83.1885 - val_loss: 1157.8384 - val_mae: 1158.5316\n",
      "Epoch 2875/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 66.8806 - mae: 67.5601 - val_loss: 1164.2975 - val_mae: 1164.9908\n",
      "Epoch 2876/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 64.2069 - mae: 64.8825 - val_loss: 1221.6116 - val_mae: 1222.3032\n",
      "Epoch 2877/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 68.8845 - mae: 69.5627 - val_loss: 1110.7439 - val_mae: 1111.4370\n",
      "Epoch 2878/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 69.1694 - mae: 69.8465 - val_loss: 1086.6854 - val_mae: 1087.3787\n",
      "Epoch 2879/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 71.4640 - mae: 72.1442 - val_loss: 1074.3932 - val_mae: 1075.0864\n",
      "Epoch 2880/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 67.9326 - mae: 68.6110 - val_loss: 1092.9639 - val_mae: 1093.6569\n",
      "Epoch 2881/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 70.9183 - mae: 71.5981 - val_loss: 1172.0619 - val_mae: 1172.7551\n",
      "Epoch 2882/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 71.8815 - mae: 72.5631 - val_loss: 1131.5950 - val_mae: 1132.2872\n",
      "Epoch 2883/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 68.8452 - mae: 69.5240 - val_loss: 1134.8522 - val_mae: 1135.5452\n",
      "Epoch 2884/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 76.2952 - mae: 76.9778 - val_loss: 1261.0433 - val_mae: 1261.7361\n",
      "Epoch 2885/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 72.3071 - mae: 72.9897 - val_loss: 1145.0336 - val_mae: 1145.7257\n",
      "Epoch 2886/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 76.1952 - mae: 76.8752 - val_loss: 1074.0192 - val_mae: 1074.7123\n",
      "Epoch 2887/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 67.7530 - mae: 68.4328 - val_loss: 1182.1907 - val_mae: 1182.8839\n",
      "Epoch 2888/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 67.6377 - mae: 68.3170 - val_loss: 1196.3308 - val_mae: 1197.0236\n",
      "Epoch 2889/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 70.1706 - mae: 70.8505 - val_loss: 1168.6561 - val_mae: 1169.3491\n",
      "Epoch 2890/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 66.3089 - mae: 66.9893 - val_loss: 1134.3202 - val_mae: 1135.0129\n",
      "Epoch 2891/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 67.8754 - mae: 68.5561 - val_loss: 1175.7111 - val_mae: 1176.4038\n",
      "Epoch 2892/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 66.7108 - mae: 67.3895 - val_loss: 1150.9912 - val_mae: 1151.6844\n",
      "Epoch 2893/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 68.9881 - mae: 69.6688 - val_loss: 1133.9879 - val_mae: 1134.6802\n",
      "Epoch 2894/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 69.9201 - mae: 70.6004 - val_loss: 1113.0756 - val_mae: 1113.7682\n",
      "Epoch 2895/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 65.4577 - mae: 66.1357 - val_loss: 1113.5869 - val_mae: 1114.2799\n",
      "Epoch 2896/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 65.5499 - mae: 66.2258 - val_loss: 1151.1709 - val_mae: 1151.8629\n",
      "Epoch 2897/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 63.9753 - mae: 64.6553 - val_loss: 1185.3496 - val_mae: 1186.0430\n",
      "Epoch 2898/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 75.4375 - mae: 76.1202 - val_loss: 1193.8096 - val_mae: 1194.5027\n",
      "Epoch 2899/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 67.5201 - mae: 68.1985 - val_loss: 1113.9056 - val_mae: 1114.5988\n",
      "Epoch 2900/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 64.7225 - mae: 65.4004 - val_loss: 1106.6616 - val_mae: 1107.3538\n",
      "Epoch 2901/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 64.5603 - mae: 65.2333 - val_loss: 1105.0511 - val_mae: 1105.7444\n",
      "Epoch 2902/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 83.2086 - mae: 83.8860 - val_loss: 1138.1600 - val_mae: 1138.8531\n",
      "Epoch 2903/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 75.1855 - mae: 75.8620 - val_loss: 1168.3319 - val_mae: 1169.0251\n",
      "Epoch 2904/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 68.3791 - mae: 69.0610 - val_loss: 1191.0535 - val_mae: 1191.7466\n",
      "Epoch 2905/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 66.1736 - mae: 66.8553 - val_loss: 1144.5787 - val_mae: 1145.2716\n",
      "Epoch 2906/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 66.3374 - mae: 67.0162 - val_loss: 1142.0842 - val_mae: 1142.7762\n",
      "Epoch 2907/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 70.5725 - mae: 71.2467 - val_loss: 1089.1171 - val_mae: 1089.8103\n",
      "Epoch 2908/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 67.0965 - mae: 67.7753 - val_loss: 1069.0272 - val_mae: 1069.7192\n",
      "Epoch 2909/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 66.9932 - mae: 67.6715 - val_loss: 1085.2130 - val_mae: 1085.9062\n",
      "Epoch 2910/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 66.5831 - mae: 67.2605 - val_loss: 1106.4471 - val_mae: 1107.1404\n",
      "Epoch 2911/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 69.7804 - mae: 70.4555 - val_loss: 1184.4547 - val_mae: 1185.1478\n",
      "Epoch 2912/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 67.4515 - mae: 68.1327 - val_loss: 1162.0233 - val_mae: 1162.7163\n",
      "Epoch 2913/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 69.0974 - mae: 69.7734 - val_loss: 1161.6389 - val_mae: 1162.3319\n",
      "Epoch 2914/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 74.4090 - mae: 75.0841 - val_loss: 1119.4889 - val_mae: 1120.1819\n",
      "Epoch 2915/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 64.8144 - mae: 65.4897 - val_loss: 1103.6709 - val_mae: 1104.3641\n",
      "Epoch 2916/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 73.7266 - mae: 74.4044 - val_loss: 1139.2645 - val_mae: 1139.9578\n",
      "Epoch 2917/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 62.8416 - mae: 63.5201 - val_loss: 1208.1382 - val_mae: 1208.8291\n",
      "Epoch 2918/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 71.2785 - mae: 71.9566 - val_loss: 1150.8483 - val_mae: 1151.5402\n",
      "Epoch 2919/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 71.4934 - mae: 72.1735 - val_loss: 1162.8944 - val_mae: 1163.5874\n",
      "Epoch 2920/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 66.1888 - mae: 66.8685 - val_loss: 1089.5889 - val_mae: 1090.2815\n",
      "Epoch 2921/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 68.5396 - mae: 69.2149 - val_loss: 1091.3246 - val_mae: 1092.0177\n",
      "Epoch 2922/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 70.7694 - mae: 71.4468 - val_loss: 1156.2676 - val_mae: 1156.9595\n",
      "Epoch 2923/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 63.0104 - mae: 63.6855 - val_loss: 1159.6788 - val_mae: 1160.3721\n",
      "Epoch 2924/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 68.2027 - mae: 68.8800 - val_loss: 1212.0880 - val_mae: 1212.7803\n",
      "Epoch 2925/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 71.7015 - mae: 72.3801 - val_loss: 1109.4485 - val_mae: 1110.1418\n",
      "Epoch 2926/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 64.4558 - mae: 65.1342 - val_loss: 1134.0450 - val_mae: 1134.7377\n",
      "Epoch 2927/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 80.3191 - mae: 81.0019 - val_loss: 1125.4344 - val_mae: 1126.1274\n",
      "Epoch 2928/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 72.8991 - mae: 73.5795 - val_loss: 1163.4342 - val_mae: 1164.1272\n",
      "Epoch 2929/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 78.8244 - mae: 79.5090 - val_loss: 1070.8215 - val_mae: 1071.5148\n",
      "Epoch 2930/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 71.3830 - mae: 72.0649 - val_loss: 1126.3334 - val_mae: 1127.0266\n",
      "Epoch 2931/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 65.2065 - mae: 65.8856 - val_loss: 1117.0637 - val_mae: 1117.7568\n",
      "Epoch 2932/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 69.2581 - mae: 69.9363 - val_loss: 1113.1218 - val_mae: 1113.8147\n",
      "Epoch 2933/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 69.4989 - mae: 70.1771 - val_loss: 1080.4523 - val_mae: 1081.1439\n",
      "Epoch 2934/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 69.8149 - mae: 70.4939 - val_loss: 1117.5640 - val_mae: 1118.2568\n",
      "Epoch 2935/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 67.0930 - mae: 67.7703 - val_loss: 1153.1671 - val_mae: 1153.8601\n",
      "Epoch 2936/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 79.9881 - mae: 80.6673 - val_loss: 1122.5017 - val_mae: 1123.1949\n",
      "Epoch 2937/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 65.2148 - mae: 65.8949 - val_loss: 1109.1150 - val_mae: 1109.8081\n",
      "Epoch 2938/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 66.3119 - mae: 66.9896 - val_loss: 1123.3380 - val_mae: 1124.0312\n",
      "Epoch 2939/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 73.4373 - mae: 74.1154 - val_loss: 1133.7330 - val_mae: 1134.4264\n",
      "Epoch 2940/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 63.8787 - mae: 64.5554 - val_loss: 1101.3279 - val_mae: 1102.0200\n",
      "Epoch 2941/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 65.2758 - mae: 65.9553 - val_loss: 1083.5626 - val_mae: 1084.2533\n",
      "Epoch 2942/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 72.5428 - mae: 73.2262 - val_loss: 1164.1622 - val_mae: 1164.8552\n",
      "Epoch 2943/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 70.5521 - mae: 71.2352 - val_loss: 1168.6748 - val_mae: 1169.3672\n",
      "Epoch 2944/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 75.5085 - mae: 76.1901 - val_loss: 1170.2302 - val_mae: 1170.9230\n",
      "Epoch 2945/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 77.1594 - mae: 77.8396 - val_loss: 1088.9386 - val_mae: 1089.6317\n",
      "Epoch 2946/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 66.5468 - mae: 67.2297 - val_loss: 1112.0483 - val_mae: 1112.7413\n",
      "Epoch 2947/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 67.0364 - mae: 67.7180 - val_loss: 1237.0383 - val_mae: 1237.7312\n",
      "Epoch 2948/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 71.6533 - mae: 72.3321 - val_loss: 1123.4680 - val_mae: 1124.1611\n",
      "Epoch 2949/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 82.4967 - mae: 83.1772 - val_loss: 1127.2323 - val_mae: 1127.9254\n",
      "Epoch 2950/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 67.1505 - mae: 67.8285 - val_loss: 1100.4503 - val_mae: 1101.1427\n",
      "Epoch 2951/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 71.8706 - mae: 72.5520 - val_loss: 1119.5087 - val_mae: 1120.2010\n",
      "Epoch 2952/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 64.5190 - mae: 65.1933 - val_loss: 1099.3408 - val_mae: 1100.0336\n",
      "Epoch 2953/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 64.5223 - mae: 65.2014 - val_loss: 1097.4790 - val_mae: 1098.1721\n",
      "Epoch 2954/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 69.4521 - mae: 70.1323 - val_loss: 1164.4771 - val_mae: 1165.1699\n",
      "Epoch 2955/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 69.0825 - mae: 69.7662 - val_loss: 1121.2172 - val_mae: 1121.9102\n",
      "Epoch 2956/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 70.9422 - mae: 71.6194 - val_loss: 1103.3188 - val_mae: 1104.0122\n",
      "Epoch 2957/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 79.1034 - mae: 79.7840 - val_loss: 1193.7170 - val_mae: 1194.4100\n",
      "Epoch 2958/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 67.6668 - mae: 68.3421 - val_loss: 1140.5648 - val_mae: 1141.2579\n",
      "Epoch 2959/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 64.1739 - mae: 64.8505 - val_loss: 1147.5846 - val_mae: 1148.2778\n",
      "Epoch 2960/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 66.8052 - mae: 67.4835 - val_loss: 1092.7825 - val_mae: 1093.4755\n",
      "Epoch 2961/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 62.8477 - mae: 63.5237 - val_loss: 1090.5437 - val_mae: 1091.2371\n",
      "Epoch 2962/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 62.5015 - mae: 63.1805 - val_loss: 1160.9202 - val_mae: 1161.6133\n",
      "Epoch 2963/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 66.0641 - mae: 66.7447 - val_loss: 1098.2094 - val_mae: 1098.9025\n",
      "Epoch 2964/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 70.9747 - mae: 71.6509 - val_loss: 1180.9950 - val_mae: 1181.6879\n",
      "Epoch 2965/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 70.9462 - mae: 71.6275 - val_loss: 1238.0320 - val_mae: 1238.7246\n",
      "Epoch 2966/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 74.0714 - mae: 74.7503 - val_loss: 1117.7384 - val_mae: 1118.4315\n",
      "Epoch 2967/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 67.7098 - mae: 68.3881 - val_loss: 1163.7754 - val_mae: 1164.4685\n",
      "Epoch 2968/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 81.7729 - mae: 82.4529 - val_loss: 1210.5148 - val_mae: 1211.2079\n",
      "Epoch 2969/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 79.2462 - mae: 79.9276 - val_loss: 1099.2582 - val_mae: 1099.9514\n",
      "Epoch 2970/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 66.1319 - mae: 66.8087 - val_loss: 1164.8660 - val_mae: 1165.5585\n",
      "Epoch 2971/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 65.8652 - mae: 66.5425 - val_loss: 1170.9260 - val_mae: 1171.6185\n",
      "Epoch 2972/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 74.9863 - mae: 75.6643 - val_loss: 1109.7360 - val_mae: 1110.4288\n",
      "Epoch 2973/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 67.3601 - mae: 68.0431 - val_loss: 1253.1927 - val_mae: 1253.8859\n",
      "Epoch 2974/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 64.5853 - mae: 65.2612 - val_loss: 1114.7997 - val_mae: 1115.4930\n",
      "Epoch 2975/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 71.2558 - mae: 71.9350 - val_loss: 1155.6979 - val_mae: 1156.3911\n",
      "Epoch 2976/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 61.5013 - mae: 62.1790 - val_loss: 1164.3711 - val_mae: 1165.0645\n",
      "Epoch 2977/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 64.7135 - mae: 65.3895 - val_loss: 1095.0024 - val_mae: 1095.6953\n",
      "Epoch 2978/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 64.0422 - mae: 64.7199 - val_loss: 1176.2310 - val_mae: 1176.9241\n",
      "Epoch 2979/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 66.9759 - mae: 67.6544 - val_loss: 1075.0366 - val_mae: 1075.7300\n",
      "Epoch 2980/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 71.4403 - mae: 72.1189 - val_loss: 1155.2095 - val_mae: 1155.9026\n",
      "Epoch 2981/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 66.7697 - mae: 67.4458 - val_loss: 1128.4742 - val_mae: 1129.1675\n",
      "Epoch 2982/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 69.3071 - mae: 69.9862 - val_loss: 1109.1952 - val_mae: 1109.8881\n",
      "Epoch 2983/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 70.8590 - mae: 71.5376 - val_loss: 1103.9893 - val_mae: 1104.6819\n",
      "Epoch 2984/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 80.0664 - mae: 80.7497 - val_loss: 1259.5216 - val_mae: 1260.2142\n",
      "Epoch 2985/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 80.0609 - mae: 80.7461 - val_loss: 1104.6473 - val_mae: 1105.3391\n",
      "Epoch 2986/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 63.7445 - mae: 64.4207 - val_loss: 1202.8352 - val_mae: 1203.5283\n",
      "Epoch 2987/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 64.2384 - mae: 64.9183 - val_loss: 1102.4016 - val_mae: 1103.0936\n",
      "Epoch 2988/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 61.1289 - mae: 61.8059 - val_loss: 1093.5414 - val_mae: 1094.2345\n",
      "Epoch 2989/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 62.5959 - mae: 63.2766 - val_loss: 1148.9390 - val_mae: 1149.6300\n",
      "Epoch 2990/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 61.3669 - mae: 62.0424 - val_loss: 1102.2246 - val_mae: 1102.9180\n",
      "Epoch 2991/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 65.7788 - mae: 66.4605 - val_loss: 1101.1263 - val_mae: 1101.8196\n",
      "Epoch 2992/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 64.2123 - mae: 64.8905 - val_loss: 1165.8022 - val_mae: 1166.4948\n",
      "Epoch 2993/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 65.7138 - mae: 66.3903 - val_loss: 1220.7476 - val_mae: 1221.4407\n",
      "Epoch 2994/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 66.3657 - mae: 67.0430 - val_loss: 1102.0222 - val_mae: 1102.7155\n",
      "Epoch 2995/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 66.6869 - mae: 67.3623 - val_loss: 1174.0009 - val_mae: 1174.6932\n",
      "Epoch 2996/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 66.7551 - mae: 67.4351 - val_loss: 1220.3550 - val_mae: 1221.0481\n",
      "Epoch 2997/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 62.3779 - mae: 63.0572 - val_loss: 1125.1876 - val_mae: 1125.8799\n",
      "Epoch 2998/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 82.6992 - mae: 83.3790 - val_loss: 1070.4934 - val_mae: 1071.1862\n",
      "Epoch 2999/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 72.5316 - mae: 73.2107 - val_loss: 1086.1725 - val_mae: 1086.8657\n",
      "Epoch 3000/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 73.5611 - mae: 74.2421 - val_loss: 1220.0951 - val_mae: 1220.7880\n",
      "Epoch 3001/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 70.4238 - mae: 71.1069 - val_loss: 1106.2599 - val_mae: 1106.9530\n",
      "Epoch 3002/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 69.9416 - mae: 70.6202 - val_loss: 1095.4841 - val_mae: 1096.1774\n",
      "Epoch 3003/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 63.4463 - mae: 64.1232 - val_loss: 1089.1227 - val_mae: 1089.8157\n",
      "Epoch 3004/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 69.6762 - mae: 70.3536 - val_loss: 1146.7454 - val_mae: 1147.4380\n",
      "Epoch 3005/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 66.7271 - mae: 67.4070 - val_loss: 1095.7909 - val_mae: 1096.4840\n",
      "Epoch 3006/5000\n",
      "46/46 [==============================] - 1s 9ms/step - loss: 66.1435 - mae: 66.8221 - val_loss: 1182.1792 - val_mae: 1182.8723\n",
      "Epoch 3007/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 67.7487 - mae: 68.4243 - val_loss: 1124.2052 - val_mae: 1124.8982\n",
      "Epoch 3008/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 69.5324 - mae: 70.2120 - val_loss: 1138.5654 - val_mae: 1139.2585\n",
      "Epoch 3009/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 64.8306 - mae: 65.5078 - val_loss: 1119.0284 - val_mae: 1119.7213\n",
      "Epoch 3010/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 65.7211 - mae: 66.4028 - val_loss: 1104.2440 - val_mae: 1104.9371\n",
      "Epoch 3011/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 67.6565 - mae: 68.3365 - val_loss: 1149.4266 - val_mae: 1150.1196\n",
      "Epoch 3012/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 64.2631 - mae: 64.9444 - val_loss: 1076.2925 - val_mae: 1076.9858\n",
      "Epoch 3013/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 74.9602 - mae: 75.6396 - val_loss: 1193.7333 - val_mae: 1194.4264\n",
      "Epoch 3014/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 70.0211 - mae: 70.6993 - val_loss: 1152.4858 - val_mae: 1153.1788\n",
      "Epoch 3015/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 65.4917 - mae: 66.1709 - val_loss: 1115.3826 - val_mae: 1116.0750\n",
      "Epoch 3016/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 69.6876 - mae: 70.3654 - val_loss: 1136.8838 - val_mae: 1137.5770\n",
      "Epoch 3017/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 62.9570 - mae: 63.6376 - val_loss: 1185.3865 - val_mae: 1186.0790\n",
      "Epoch 3018/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 65.8373 - mae: 66.5188 - val_loss: 1110.1514 - val_mae: 1110.8439\n",
      "Epoch 3019/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 62.5369 - mae: 63.2186 - val_loss: 1215.1792 - val_mae: 1215.8723\n",
      "Epoch 3020/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 64.9171 - mae: 65.5958 - val_loss: 1129.5552 - val_mae: 1130.2483\n",
      "Epoch 3021/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 65.7286 - mae: 66.4071 - val_loss: 1124.1993 - val_mae: 1124.8926\n",
      "Epoch 3022/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 70.0595 - mae: 70.7412 - val_loss: 1116.2396 - val_mae: 1116.9313\n",
      "Epoch 3023/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 63.1600 - mae: 63.8385 - val_loss: 1110.7379 - val_mae: 1111.4310\n",
      "Epoch 3024/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 65.4128 - mae: 66.0897 - val_loss: 1149.6808 - val_mae: 1150.3733\n",
      "Epoch 3025/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 63.6323 - mae: 64.3103 - val_loss: 1133.1611 - val_mae: 1133.8542\n",
      "Epoch 3026/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 65.0209 - mae: 65.6962 - val_loss: 1182.3427 - val_mae: 1183.0356\n",
      "Epoch 3027/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 65.9623 - mae: 66.6393 - val_loss: 1197.1865 - val_mae: 1197.8790\n",
      "Epoch 3028/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 61.4017 - mae: 62.0781 - val_loss: 1113.6189 - val_mae: 1114.3119\n",
      "Epoch 3029/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 72.4798 - mae: 73.1600 - val_loss: 1086.6172 - val_mae: 1087.3097\n",
      "Epoch 3030/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 67.6266 - mae: 68.3097 - val_loss: 1173.9584 - val_mae: 1174.6514\n",
      "Epoch 3031/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 71.5153 - mae: 72.1966 - val_loss: 1203.2946 - val_mae: 1203.9868\n",
      "Epoch 3032/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 65.9050 - mae: 66.5825 - val_loss: 1156.5214 - val_mae: 1157.2144\n",
      "Epoch 3033/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 68.1507 - mae: 68.8291 - val_loss: 1106.2711 - val_mae: 1106.9637\n",
      "Epoch 3034/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 73.7815 - mae: 74.4643 - val_loss: 1103.5203 - val_mae: 1104.2134\n",
      "Epoch 3035/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 62.7160 - mae: 63.3910 - val_loss: 1155.8917 - val_mae: 1156.5833\n",
      "Epoch 3036/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 70.1393 - mae: 70.8227 - val_loss: 1085.0709 - val_mae: 1085.7639\n",
      "Epoch 3037/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 74.0097 - mae: 74.6903 - val_loss: 1115.3826 - val_mae: 1116.0757\n",
      "Epoch 3038/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 80.9981 - mae: 81.6801 - val_loss: 1189.1567 - val_mae: 1189.8500\n",
      "Epoch 3039/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 68.8992 - mae: 69.5758 - val_loss: 1082.7651 - val_mae: 1083.4583\n",
      "Epoch 3040/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 67.6994 - mae: 68.3774 - val_loss: 1140.0352 - val_mae: 1140.7284\n",
      "Epoch 3041/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 64.6147 - mae: 65.2929 - val_loss: 1120.1147 - val_mae: 1120.8075\n",
      "Epoch 3042/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 68.5915 - mae: 69.2669 - val_loss: 1232.9263 - val_mae: 1233.6194\n",
      "Epoch 3043/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 81.5229 - mae: 82.2043 - val_loss: 1142.3005 - val_mae: 1142.9932\n",
      "Epoch 3044/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 71.7430 - mae: 72.4245 - val_loss: 1203.6603 - val_mae: 1204.3535\n",
      "Epoch 3045/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 77.4418 - mae: 78.1241 - val_loss: 1176.7502 - val_mae: 1177.4434\n",
      "Epoch 3046/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 69.1337 - mae: 69.8148 - val_loss: 1139.7588 - val_mae: 1140.4519\n",
      "Epoch 3047/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 72.4399 - mae: 73.1185 - val_loss: 1164.4071 - val_mae: 1165.0995\n",
      "Epoch 3048/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 75.1653 - mae: 75.8435 - val_loss: 1115.2289 - val_mae: 1115.9220\n",
      "Epoch 3049/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 65.8476 - mae: 66.5278 - val_loss: 1186.5420 - val_mae: 1187.2351\n",
      "Epoch 3050/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 64.6780 - mae: 65.3578 - val_loss: 1153.0372 - val_mae: 1153.7305\n",
      "Epoch 3051/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 79.4359 - mae: 80.1144 - val_loss: 1103.7523 - val_mae: 1104.4448\n",
      "Epoch 3052/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 62.6990 - mae: 63.3780 - val_loss: 1100.8785 - val_mae: 1101.5714\n",
      "Epoch 3053/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 61.4488 - mae: 62.1254 - val_loss: 1145.5109 - val_mae: 1146.2042\n",
      "Epoch 3054/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 78.5192 - mae: 79.1999 - val_loss: 1110.2242 - val_mae: 1110.9175\n",
      "Epoch 3055/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 69.8219 - mae: 70.4984 - val_loss: 1231.7970 - val_mae: 1232.4904\n",
      "Epoch 3056/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 66.3781 - mae: 67.0494 - val_loss: 1106.6327 - val_mae: 1107.3257\n",
      "Epoch 3057/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 69.9823 - mae: 70.6599 - val_loss: 1191.5500 - val_mae: 1192.2432\n",
      "Epoch 3058/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 66.2995 - mae: 66.9761 - val_loss: 1180.8369 - val_mae: 1181.5286\n",
      "Epoch 3059/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 64.8628 - mae: 65.5400 - val_loss: 1147.3584 - val_mae: 1148.0518\n",
      "Epoch 3060/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 67.4265 - mae: 68.1045 - val_loss: 1207.5266 - val_mae: 1208.2200\n",
      "Epoch 3061/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 70.4250 - mae: 71.1038 - val_loss: 1140.5665 - val_mae: 1141.2595\n",
      "Epoch 3062/5000\n",
      "46/46 [==============================] - 2s 40ms/step - loss: 75.4632 - mae: 76.1423 - val_loss: 1230.3771 - val_mae: 1231.0701\n",
      "Epoch 3063/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 72.5269 - mae: 73.2093 - val_loss: 1168.2832 - val_mae: 1168.9764\n",
      "Epoch 3064/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 62.3149 - mae: 62.9935 - val_loss: 1152.2445 - val_mae: 1152.9374\n",
      "Epoch 3065/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 61.6790 - mae: 62.3531 - val_loss: 1194.9816 - val_mae: 1195.6749\n",
      "Epoch 3066/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 70.0742 - mae: 70.7542 - val_loss: 1105.3927 - val_mae: 1106.0858\n",
      "Epoch 3067/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 70.5377 - mae: 71.2176 - val_loss: 1152.9236 - val_mae: 1153.6162\n",
      "Epoch 3068/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 64.9892 - mae: 65.6647 - val_loss: 1132.6829 - val_mae: 1133.3759\n",
      "Epoch 3069/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 66.4998 - mae: 67.1792 - val_loss: 1101.7585 - val_mae: 1102.4520\n",
      "Epoch 3070/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 68.2262 - mae: 68.9009 - val_loss: 1137.4731 - val_mae: 1138.1664\n",
      "Epoch 3071/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 67.2116 - mae: 67.8922 - val_loss: 1084.4137 - val_mae: 1085.1067\n",
      "Epoch 3072/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 67.0634 - mae: 67.7460 - val_loss: 1186.9313 - val_mae: 1187.6245\n",
      "Epoch 3073/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 62.3445 - mae: 63.0173 - val_loss: 1089.5951 - val_mae: 1090.2882\n",
      "Epoch 3074/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 64.6737 - mae: 65.3514 - val_loss: 1114.8429 - val_mae: 1115.5354\n",
      "Epoch 3075/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 67.9188 - mae: 68.5958 - val_loss: 1102.2482 - val_mae: 1102.9409\n",
      "Epoch 3076/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 67.8556 - mae: 68.5339 - val_loss: 1300.7328 - val_mae: 1301.4257\n",
      "Epoch 3077/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 69.9176 - mae: 70.5918 - val_loss: 1089.9059 - val_mae: 1090.5977\n",
      "Epoch 3078/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 66.8830 - mae: 67.5642 - val_loss: 1138.9075 - val_mae: 1139.6000\n",
      "Epoch 3079/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 70.9880 - mae: 71.6678 - val_loss: 1103.3453 - val_mae: 1104.0377\n",
      "Epoch 3080/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 69.1925 - mae: 69.8710 - val_loss: 1130.2480 - val_mae: 1130.9410\n",
      "Epoch 3081/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 64.3145 - mae: 64.9946 - val_loss: 1289.2349 - val_mae: 1289.9280\n",
      "Epoch 3082/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 70.2154 - mae: 70.8949 - val_loss: 1161.4091 - val_mae: 1162.1023\n",
      "Epoch 3083/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 60.5183 - mae: 61.1958 - val_loss: 1119.6860 - val_mae: 1120.3789\n",
      "Epoch 3084/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 67.4481 - mae: 68.1295 - val_loss: 1124.7908 - val_mae: 1125.4836\n",
      "Epoch 3085/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 63.9232 - mae: 64.6027 - val_loss: 1082.1293 - val_mae: 1082.8223\n",
      "Epoch 3086/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 65.8967 - mae: 66.5715 - val_loss: 1185.9418 - val_mae: 1186.6349\n",
      "Epoch 3087/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 72.6298 - mae: 73.3101 - val_loss: 1171.2615 - val_mae: 1171.9546\n",
      "Epoch 3088/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 66.7385 - mae: 67.4123 - val_loss: 1153.4214 - val_mae: 1154.1145\n",
      "Epoch 3089/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 68.0132 - mae: 68.6923 - val_loss: 1188.4978 - val_mae: 1189.1909\n",
      "Epoch 3090/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 68.9315 - mae: 69.6077 - val_loss: 1101.1101 - val_mae: 1101.8027\n",
      "Epoch 3091/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 91.5499 - mae: 92.2330 - val_loss: 1203.5880 - val_mae: 1204.2812\n",
      "Epoch 3092/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 77.0455 - mae: 77.7242 - val_loss: 1141.6208 - val_mae: 1142.3141\n",
      "Epoch 3093/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 73.6195 - mae: 74.3016 - val_loss: 1161.8188 - val_mae: 1162.5116\n",
      "Epoch 3094/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 60.6439 - mae: 61.3220 - val_loss: 1153.2495 - val_mae: 1153.9417\n",
      "Epoch 3095/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 63.8111 - mae: 64.4893 - val_loss: 1127.9141 - val_mae: 1128.6071\n",
      "Epoch 3096/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 63.8255 - mae: 64.5046 - val_loss: 1137.3585 - val_mae: 1138.0518\n",
      "Epoch 3097/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 60.5321 - mae: 61.2110 - val_loss: 1125.4529 - val_mae: 1126.1458\n",
      "Epoch 3098/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 68.8521 - mae: 69.5316 - val_loss: 1304.3685 - val_mae: 1305.0608\n",
      "Epoch 3099/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 73.6082 - mae: 74.2912 - val_loss: 1127.1420 - val_mae: 1127.8351\n",
      "Epoch 3100/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 68.3973 - mae: 69.0747 - val_loss: 1159.2111 - val_mae: 1159.9042\n",
      "Epoch 3101/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 61.1743 - mae: 61.8534 - val_loss: 1147.2668 - val_mae: 1147.9598\n",
      "Epoch 3102/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 65.3399 - mae: 66.0203 - val_loss: 1155.6779 - val_mae: 1156.3710\n",
      "Epoch 3103/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 71.2917 - mae: 71.9709 - val_loss: 1318.1469 - val_mae: 1318.8400\n",
      "Epoch 3104/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 79.7858 - mae: 80.4682 - val_loss: 1188.5736 - val_mae: 1189.2667\n",
      "Epoch 3105/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 70.9208 - mae: 71.6000 - val_loss: 1122.3986 - val_mae: 1123.0918\n",
      "Epoch 3106/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 68.9219 - mae: 69.6008 - val_loss: 1088.5193 - val_mae: 1089.2125\n",
      "Epoch 3107/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 66.8599 - mae: 67.5395 - val_loss: 1128.3075 - val_mae: 1129.0005\n",
      "Epoch 3108/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 63.7003 - mae: 64.3812 - val_loss: 1080.7581 - val_mae: 1081.4512\n",
      "Epoch 3109/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 63.2846 - mae: 63.9622 - val_loss: 1141.9066 - val_mae: 1142.6001\n",
      "Epoch 3110/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 66.1011 - mae: 66.7830 - val_loss: 1173.3229 - val_mae: 1174.0150\n",
      "Epoch 3111/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 64.0115 - mae: 64.6885 - val_loss: 1127.3198 - val_mae: 1128.0129\n",
      "Epoch 3112/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 65.7069 - mae: 66.3868 - val_loss: 1121.4044 - val_mae: 1122.0968\n",
      "Epoch 3113/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 60.8970 - mae: 61.5736 - val_loss: 1094.9237 - val_mae: 1095.6169\n",
      "Epoch 3114/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 67.8613 - mae: 68.5389 - val_loss: 1158.7186 - val_mae: 1159.4115\n",
      "Epoch 3115/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 69.6133 - mae: 70.2911 - val_loss: 1229.9279 - val_mae: 1230.6211\n",
      "Epoch 3116/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 62.2032 - mae: 62.8801 - val_loss: 1134.6783 - val_mae: 1135.3713\n",
      "Epoch 3117/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 62.4885 - mae: 63.1660 - val_loss: 1136.0165 - val_mae: 1136.7095\n",
      "Epoch 3118/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 60.3075 - mae: 60.9842 - val_loss: 1102.0345 - val_mae: 1102.7277\n",
      "Epoch 3119/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 66.6376 - mae: 67.3130 - val_loss: 1077.9344 - val_mae: 1078.6277\n",
      "Epoch 3120/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 64.8110 - mae: 65.4947 - val_loss: 1083.6843 - val_mae: 1084.3771\n",
      "Epoch 3121/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 63.5041 - mae: 64.1820 - val_loss: 1109.2651 - val_mae: 1109.9576\n",
      "Epoch 3122/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 62.4841 - mae: 63.1656 - val_loss: 1090.1085 - val_mae: 1090.8010\n",
      "Epoch 3123/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 60.6654 - mae: 61.3406 - val_loss: 1163.0317 - val_mae: 1163.7250\n",
      "Epoch 3124/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 63.4856 - mae: 64.1626 - val_loss: 1099.2136 - val_mae: 1099.9067\n",
      "Epoch 3125/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 63.1972 - mae: 63.8748 - val_loss: 1157.0225 - val_mae: 1157.7157\n",
      "Epoch 3126/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 66.0035 - mae: 66.6799 - val_loss: 1151.9613 - val_mae: 1152.6544\n",
      "Epoch 3127/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 65.0333 - mae: 65.7141 - val_loss: 1090.2688 - val_mae: 1090.9619\n",
      "Epoch 3128/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 70.7431 - mae: 71.4242 - val_loss: 1125.6312 - val_mae: 1126.3242\n",
      "Epoch 3129/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 60.8181 - mae: 61.4985 - val_loss: 1134.0791 - val_mae: 1134.7722\n",
      "Epoch 3130/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 67.4292 - mae: 68.1097 - val_loss: 1170.0824 - val_mae: 1170.7751\n",
      "Epoch 3131/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 63.7359 - mae: 64.4135 - val_loss: 1109.8529 - val_mae: 1110.5452\n",
      "Epoch 3132/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 70.1652 - mae: 70.8479 - val_loss: 1133.1553 - val_mae: 1133.8474\n",
      "Epoch 3133/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 63.2420 - mae: 63.9179 - val_loss: 1131.5848 - val_mae: 1132.2773\n",
      "Epoch 3134/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 68.9313 - mae: 69.6096 - val_loss: 1183.0480 - val_mae: 1183.7412\n",
      "Epoch 3135/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 71.5760 - mae: 72.2560 - val_loss: 1104.1525 - val_mae: 1104.8445\n",
      "Epoch 3136/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 71.0270 - mae: 71.7055 - val_loss: 1134.3315 - val_mae: 1135.0247\n",
      "Epoch 3137/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 65.1592 - mae: 65.8400 - val_loss: 1115.1294 - val_mae: 1115.8228\n",
      "Epoch 3138/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 69.1020 - mae: 69.7818 - val_loss: 1154.8999 - val_mae: 1155.5927\n",
      "Epoch 3139/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 63.6536 - mae: 64.3331 - val_loss: 1207.0706 - val_mae: 1207.7635\n",
      "Epoch 3140/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 69.0862 - mae: 69.7614 - val_loss: 1154.8842 - val_mae: 1155.5768\n",
      "Epoch 3141/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 66.8335 - mae: 67.5117 - val_loss: 1104.0353 - val_mae: 1104.7281\n",
      "Epoch 3142/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 67.1493 - mae: 67.8308 - val_loss: 1145.1497 - val_mae: 1145.8422\n",
      "Epoch 3143/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 66.4366 - mae: 67.1128 - val_loss: 1111.2700 - val_mae: 1111.9615\n",
      "Epoch 3144/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 70.4385 - mae: 71.1240 - val_loss: 1199.1146 - val_mae: 1199.8077\n",
      "Epoch 3145/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 69.8881 - mae: 70.5649 - val_loss: 1085.9873 - val_mae: 1086.6801\n",
      "Epoch 3146/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 65.0340 - mae: 65.7137 - val_loss: 1111.3568 - val_mae: 1112.0500\n",
      "Epoch 3147/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 66.0775 - mae: 66.7505 - val_loss: 1155.6299 - val_mae: 1156.3229\n",
      "Epoch 3148/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 64.5792 - mae: 65.2570 - val_loss: 1139.6254 - val_mae: 1140.3185\n",
      "Epoch 3149/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 68.1756 - mae: 68.8506 - val_loss: 1125.3937 - val_mae: 1126.0858\n",
      "Epoch 3150/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 68.7841 - mae: 69.4612 - val_loss: 1099.9720 - val_mae: 1100.6653\n",
      "Epoch 3151/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 70.3311 - mae: 71.0088 - val_loss: 1170.9910 - val_mae: 1171.6841\n",
      "Epoch 3152/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 77.4197 - mae: 78.1000 - val_loss: 1180.6440 - val_mae: 1181.3370\n",
      "Epoch 3153/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 71.7878 - mae: 72.4715 - val_loss: 1178.8800 - val_mae: 1179.5720\n",
      "Epoch 3154/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 66.9929 - mae: 67.6684 - val_loss: 1253.4308 - val_mae: 1254.1238\n",
      "Epoch 3155/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 66.1946 - mae: 66.8738 - val_loss: 1238.9512 - val_mae: 1239.6439\n",
      "Epoch 3156/5000\n",
      "46/46 [==============================] - 2s 41ms/step - loss: 67.5906 - mae: 68.2711 - val_loss: 1162.9806 - val_mae: 1163.6716\n",
      "Epoch 3157/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 62.8404 - mae: 63.5234 - val_loss: 1115.9878 - val_mae: 1116.6808\n",
      "Epoch 3158/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 64.4586 - mae: 65.1341 - val_loss: 1197.5371 - val_mae: 1198.2294\n",
      "Epoch 3159/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 65.8417 - mae: 66.5188 - val_loss: 1215.8400 - val_mae: 1216.5331\n",
      "Epoch 3160/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 68.9064 - mae: 69.5875 - val_loss: 1104.5746 - val_mae: 1105.2677\n",
      "Epoch 3161/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 66.0718 - mae: 66.7521 - val_loss: 1265.8339 - val_mae: 1266.5272\n",
      "Epoch 3162/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 77.8626 - mae: 78.5445 - val_loss: 1139.9691 - val_mae: 1140.6617\n",
      "Epoch 3163/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 63.2955 - mae: 63.9702 - val_loss: 1113.8835 - val_mae: 1114.5768\n",
      "Epoch 3164/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 64.1281 - mae: 64.8060 - val_loss: 1114.3137 - val_mae: 1115.0070\n",
      "Epoch 3165/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 65.1934 - mae: 65.8695 - val_loss: 1155.2185 - val_mae: 1155.9115\n",
      "Epoch 3166/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 62.9932 - mae: 63.6654 - val_loss: 1081.1902 - val_mae: 1081.8820\n",
      "Epoch 3167/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 64.2088 - mae: 64.8862 - val_loss: 1123.6134 - val_mae: 1124.3065\n",
      "Epoch 3168/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 67.5947 - mae: 68.2738 - val_loss: 1197.4548 - val_mae: 1198.1479\n",
      "Epoch 3169/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 76.3522 - mae: 77.0289 - val_loss: 1163.7604 - val_mae: 1164.4537\n",
      "Epoch 3170/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 63.0772 - mae: 63.7547 - val_loss: 1118.3864 - val_mae: 1119.0796\n",
      "Epoch 3171/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 65.7915 - mae: 66.4712 - val_loss: 1134.1305 - val_mae: 1134.8237\n",
      "Epoch 3172/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 63.7595 - mae: 64.4389 - val_loss: 1113.2842 - val_mae: 1113.9761\n",
      "Epoch 3173/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 66.4502 - mae: 67.1302 - val_loss: 1103.6497 - val_mae: 1104.3429\n",
      "Epoch 3174/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 66.3984 - mae: 67.0788 - val_loss: 1116.0765 - val_mae: 1116.7694\n",
      "Epoch 3175/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 67.9572 - mae: 68.6354 - val_loss: 1136.1677 - val_mae: 1136.8610\n",
      "Epoch 3176/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 64.8819 - mae: 65.5574 - val_loss: 1107.2157 - val_mae: 1107.9087\n",
      "Epoch 3177/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 60.9123 - mae: 61.5890 - val_loss: 1121.4497 - val_mae: 1122.1429\n",
      "Epoch 3178/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 60.4534 - mae: 61.1324 - val_loss: 1142.3793 - val_mae: 1143.0720\n",
      "Epoch 3179/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 61.4983 - mae: 62.1755 - val_loss: 1128.2417 - val_mae: 1128.9336\n",
      "Epoch 3180/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 66.0628 - mae: 66.7404 - val_loss: 1099.0867 - val_mae: 1099.7793\n",
      "Epoch 3181/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 72.5905 - mae: 73.2694 - val_loss: 1132.5438 - val_mae: 1133.2368\n",
      "Epoch 3182/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 62.3355 - mae: 63.0115 - val_loss: 1130.8925 - val_mae: 1131.5853\n",
      "Epoch 3183/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 60.4300 - mae: 61.1067 - val_loss: 1172.1948 - val_mae: 1172.8881\n",
      "Epoch 3184/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 67.4073 - mae: 68.0892 - val_loss: 1232.3945 - val_mae: 1233.0874\n",
      "Epoch 3185/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 65.4502 - mae: 66.1305 - val_loss: 1140.1249 - val_mae: 1140.8180\n",
      "Epoch 3186/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 62.0275 - mae: 62.7068 - val_loss: 1280.4044 - val_mae: 1281.0975\n",
      "Epoch 3187/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 68.0242 - mae: 68.7055 - val_loss: 1120.1743 - val_mae: 1120.8673\n",
      "Epoch 3188/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 66.1449 - mae: 66.8238 - val_loss: 1230.2727 - val_mae: 1230.9652\n",
      "Epoch 3189/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 68.1172 - mae: 68.7972 - val_loss: 1092.3079 - val_mae: 1092.9999\n",
      "Epoch 3190/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 69.7246 - mae: 70.4045 - val_loss: 1114.2542 - val_mae: 1114.9469\n",
      "Epoch 3191/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 80.3601 - mae: 81.0391 - val_loss: 1095.6542 - val_mae: 1096.3462\n",
      "Epoch 3192/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 72.2514 - mae: 72.9292 - val_loss: 1183.5522 - val_mae: 1184.2454\n",
      "Epoch 3193/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 64.3750 - mae: 65.0524 - val_loss: 1155.3333 - val_mae: 1156.0265\n",
      "Epoch 3194/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 72.1333 - mae: 72.8126 - val_loss: 1169.3201 - val_mae: 1170.0134\n",
      "Epoch 3195/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 71.4767 - mae: 72.1567 - val_loss: 1129.6765 - val_mae: 1130.3691\n",
      "Epoch 3196/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 75.4242 - mae: 76.0991 - val_loss: 1143.5933 - val_mae: 1144.2864\n",
      "Epoch 3197/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 69.8827 - mae: 70.5671 - val_loss: 1110.6206 - val_mae: 1111.3130\n",
      "Epoch 3198/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 62.9381 - mae: 63.6153 - val_loss: 1107.3274 - val_mae: 1108.0205\n",
      "Epoch 3199/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 68.7140 - mae: 69.3930 - val_loss: 1183.5702 - val_mae: 1184.2635\n",
      "Epoch 3200/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 67.4892 - mae: 68.1686 - val_loss: 1126.3385 - val_mae: 1127.0315\n",
      "Epoch 3201/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 66.3710 - mae: 67.0532 - val_loss: 1091.2517 - val_mae: 1091.9447\n",
      "Epoch 3202/5000\n",
      "46/46 [==============================] - 2s 31ms/step - loss: 74.2239 - mae: 74.9046 - val_loss: 1110.7540 - val_mae: 1111.4457\n",
      "Epoch 3203/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 63.4236 - mae: 64.0985 - val_loss: 1210.3009 - val_mae: 1210.9940\n",
      "Epoch 3204/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 64.9545 - mae: 65.6341 - val_loss: 1118.1204 - val_mae: 1118.8137\n",
      "Epoch 3205/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 68.3423 - mae: 69.0225 - val_loss: 1143.0614 - val_mae: 1143.7545\n",
      "Epoch 3206/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 63.4286 - mae: 64.1084 - val_loss: 1186.1414 - val_mae: 1186.8341\n",
      "Epoch 3207/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 65.4169 - mae: 66.0918 - val_loss: 1163.6038 - val_mae: 1164.2970\n",
      "Epoch 3208/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 60.2933 - mae: 60.9702 - val_loss: 1129.2645 - val_mae: 1129.9575\n",
      "Epoch 3209/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 64.8881 - mae: 65.5641 - val_loss: 1136.6781 - val_mae: 1137.3700\n",
      "Epoch 3210/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 60.9824 - mae: 61.6596 - val_loss: 1177.4623 - val_mae: 1178.1554\n",
      "Epoch 3211/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 62.9980 - mae: 63.6748 - val_loss: 1170.4534 - val_mae: 1171.1464\n",
      "Epoch 3212/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 71.9217 - mae: 72.6017 - val_loss: 1176.7852 - val_mae: 1177.4778\n",
      "Epoch 3213/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 65.6340 - mae: 66.3121 - val_loss: 1110.5895 - val_mae: 1111.2817\n",
      "Epoch 3214/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 57.2043 - mae: 57.8789 - val_loss: 1133.5392 - val_mae: 1134.2323\n",
      "Epoch 3215/5000\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 63.1484 - mae: 63.8272 - val_loss: 1170.9778 - val_mae: 1171.6709\n",
      "Epoch 3216/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 64.4005 - mae: 65.0807 - val_loss: 1165.4865 - val_mae: 1166.1797\n",
      "Epoch 3217/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 61.7604 - mae: 62.4374 - val_loss: 1124.9502 - val_mae: 1125.6433\n",
      "Epoch 3218/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 65.8050 - mae: 66.4872 - val_loss: 1132.6581 - val_mae: 1133.3514\n",
      "Epoch 3219/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 72.6849 - mae: 73.3633 - val_loss: 1269.5724 - val_mae: 1270.2655\n",
      "Epoch 3220/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 73.4140 - mae: 74.0918 - val_loss: 1219.8439 - val_mae: 1220.5371\n",
      "Epoch 3221/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 66.3494 - mae: 67.0315 - val_loss: 1224.1852 - val_mae: 1224.8774\n",
      "Epoch 3222/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 59.4511 - mae: 60.1242 - val_loss: 1120.5472 - val_mae: 1121.2384\n",
      "Epoch 3223/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 70.5635 - mae: 71.2411 - val_loss: 1159.9866 - val_mae: 1160.6794\n",
      "Epoch 3224/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 64.2196 - mae: 64.8942 - val_loss: 1098.8765 - val_mae: 1099.5696\n",
      "Epoch 3225/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 65.6035 - mae: 66.2855 - val_loss: 1121.9246 - val_mae: 1122.6178\n",
      "Epoch 3226/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 65.7099 - mae: 66.3869 - val_loss: 1134.8881 - val_mae: 1135.5812\n",
      "Epoch 3227/5000\n",
      "46/46 [==============================] - 2s 41ms/step - loss: 77.1554 - mae: 77.8363 - val_loss: 1092.7878 - val_mae: 1093.4811\n",
      "Epoch 3228/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 72.2351 - mae: 72.9144 - val_loss: 1122.8722 - val_mae: 1123.5637\n",
      "Epoch 3229/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 63.6120 - mae: 64.2895 - val_loss: 1159.3951 - val_mae: 1160.0879\n",
      "Epoch 3230/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 65.1603 - mae: 65.8393 - val_loss: 1114.7533 - val_mae: 1115.4452\n",
      "Epoch 3231/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 64.3665 - mae: 65.0424 - val_loss: 1128.7280 - val_mae: 1129.4211\n",
      "Epoch 3232/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 58.9111 - mae: 59.5871 - val_loss: 1120.5459 - val_mae: 1121.2391\n",
      "Epoch 3233/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 68.2450 - mae: 68.9189 - val_loss: 1104.0244 - val_mae: 1104.7177\n",
      "Epoch 3234/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 62.8553 - mae: 63.5356 - val_loss: 1125.7946 - val_mae: 1126.4878\n",
      "Epoch 3235/5000\n",
      "46/46 [==============================] - 2s 42ms/step - loss: 64.1685 - mae: 64.8485 - val_loss: 1107.6113 - val_mae: 1108.3046\n",
      "Epoch 3236/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 75.1320 - mae: 75.8119 - val_loss: 1154.4326 - val_mae: 1155.1259\n",
      "Epoch 3237/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 61.4999 - mae: 62.1741 - val_loss: 1135.9175 - val_mae: 1136.6105\n",
      "Epoch 3238/5000\n",
      "46/46 [==============================] - 2s 32ms/step - loss: 65.9286 - mae: 66.6045 - val_loss: 1114.3186 - val_mae: 1115.0101\n",
      "Epoch 3239/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 58.4762 - mae: 59.1508 - val_loss: 1124.6364 - val_mae: 1125.3281\n",
      "Epoch 3240/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 60.6720 - mae: 61.3466 - val_loss: 1134.5432 - val_mae: 1135.2358\n",
      "Epoch 3241/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 67.6618 - mae: 68.3430 - val_loss: 1096.4872 - val_mae: 1097.1802\n",
      "Epoch 3242/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 62.2252 - mae: 62.9014 - val_loss: 1129.1298 - val_mae: 1129.8220\n",
      "Epoch 3243/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 60.7530 - mae: 61.4304 - val_loss: 1108.4529 - val_mae: 1109.1460\n",
      "Epoch 3244/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 64.6871 - mae: 65.3635 - val_loss: 1094.6891 - val_mae: 1095.3820\n",
      "Epoch 3245/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 61.9636 - mae: 62.6409 - val_loss: 1154.5254 - val_mae: 1155.2185\n",
      "Epoch 3246/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 68.2641 - mae: 68.9396 - val_loss: 1096.5680 - val_mae: 1097.2610\n",
      "Epoch 3247/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 64.8996 - mae: 65.5808 - val_loss: 1112.0619 - val_mae: 1112.7551\n",
      "Epoch 3248/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 61.2961 - mae: 61.9762 - val_loss: 1111.8953 - val_mae: 1112.5886\n",
      "Epoch 3249/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 61.9532 - mae: 62.6327 - val_loss: 1114.3568 - val_mae: 1115.0487\n",
      "Epoch 3250/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 68.5302 - mae: 69.2097 - val_loss: 1116.4536 - val_mae: 1117.1467\n",
      "Epoch 3251/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 64.5785 - mae: 65.2570 - val_loss: 1163.9194 - val_mae: 1164.6125\n",
      "Epoch 3252/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 64.9609 - mae: 65.6398 - val_loss: 1114.3651 - val_mae: 1115.0581\n",
      "Epoch 3253/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 67.3287 - mae: 68.0082 - val_loss: 1388.3472 - val_mae: 1389.0400\n",
      "Epoch 3254/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 69.1838 - mae: 69.8628 - val_loss: 1130.3956 - val_mae: 1131.0884\n",
      "Epoch 3255/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 60.7125 - mae: 61.3888 - val_loss: 1111.4932 - val_mae: 1112.1860\n",
      "Epoch 3256/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 62.5954 - mae: 63.2750 - val_loss: 1101.4381 - val_mae: 1102.1309\n",
      "Epoch 3257/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 67.3659 - mae: 68.0400 - val_loss: 1161.0221 - val_mae: 1161.7148\n",
      "Epoch 3258/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 64.8222 - mae: 65.5020 - val_loss: 1124.8307 - val_mae: 1125.5237\n",
      "Epoch 3259/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 58.8874 - mae: 59.5637 - val_loss: 1142.8035 - val_mae: 1143.4967\n",
      "Epoch 3260/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 61.1226 - mae: 61.7951 - val_loss: 1173.7235 - val_mae: 1174.4166\n",
      "Epoch 3261/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 65.2417 - mae: 65.9143 - val_loss: 1112.5187 - val_mae: 1113.2118\n",
      "Epoch 3262/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 68.6948 - mae: 69.3723 - val_loss: 1124.4918 - val_mae: 1125.1849\n",
      "Epoch 3263/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 62.3330 - mae: 63.0119 - val_loss: 1089.4922 - val_mae: 1090.1840\n",
      "Epoch 3264/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 67.7968 - mae: 68.4768 - val_loss: 1129.3071 - val_mae: 1130.0005\n",
      "Epoch 3265/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 66.9121 - mae: 67.5885 - val_loss: 1186.5024 - val_mae: 1187.1954\n",
      "Epoch 3266/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 62.7406 - mae: 63.4179 - val_loss: 1202.7307 - val_mae: 1203.4235\n",
      "Epoch 3267/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 57.9305 - mae: 58.6072 - val_loss: 1193.5284 - val_mae: 1194.2214\n",
      "Epoch 3268/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 64.2402 - mae: 64.9182 - val_loss: 1169.5962 - val_mae: 1170.2894\n",
      "Epoch 3269/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 66.4876 - mae: 67.1649 - val_loss: 1119.7368 - val_mae: 1120.4292\n",
      "Epoch 3270/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 67.8681 - mae: 68.5497 - val_loss: 1127.5165 - val_mae: 1128.2091\n",
      "Epoch 3271/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 68.7979 - mae: 69.4767 - val_loss: 1160.9858 - val_mae: 1161.6791\n",
      "Epoch 3272/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 62.7857 - mae: 63.4617 - val_loss: 1108.7230 - val_mae: 1109.4159\n",
      "Epoch 3273/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 64.5125 - mae: 65.1929 - val_loss: 1121.5939 - val_mae: 1122.2853\n",
      "Epoch 3274/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 65.1027 - mae: 65.7845 - val_loss: 1133.4688 - val_mae: 1134.1620\n",
      "Epoch 3275/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 65.9972 - mae: 66.6775 - val_loss: 1146.8546 - val_mae: 1147.5475\n",
      "Epoch 3276/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 60.6275 - mae: 61.3048 - val_loss: 1084.8177 - val_mae: 1085.5109\n",
      "Epoch 3277/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 58.5923 - mae: 59.2710 - val_loss: 1126.4305 - val_mae: 1127.1224\n",
      "Epoch 3278/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 73.9406 - mae: 74.6246 - val_loss: 1145.6212 - val_mae: 1146.3135\n",
      "Epoch 3279/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 61.5996 - mae: 62.2777 - val_loss: 1091.9900 - val_mae: 1092.6831\n",
      "Epoch 3280/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 62.9944 - mae: 63.6703 - val_loss: 1122.3364 - val_mae: 1123.0297\n",
      "Epoch 3281/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 64.7077 - mae: 65.3843 - val_loss: 1204.8691 - val_mae: 1205.5624\n",
      "Epoch 3282/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 66.4961 - mae: 67.1747 - val_loss: 1137.5103 - val_mae: 1138.2028\n",
      "Epoch 3283/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 68.0020 - mae: 68.6781 - val_loss: 1129.0646 - val_mae: 1129.7565\n",
      "Epoch 3284/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 72.4385 - mae: 73.1197 - val_loss: 1141.9888 - val_mae: 1142.6813\n",
      "Epoch 3285/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 64.1981 - mae: 64.8768 - val_loss: 1110.5573 - val_mae: 1111.2505\n",
      "Epoch 3286/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 64.1827 - mae: 64.8587 - val_loss: 1141.9215 - val_mae: 1142.6147\n",
      "Epoch 3287/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 63.1632 - mae: 63.8376 - val_loss: 1142.2308 - val_mae: 1142.9238\n",
      "Epoch 3288/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 67.1240 - mae: 67.8047 - val_loss: 1128.3871 - val_mae: 1129.0797\n",
      "Epoch 3289/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 65.3143 - mae: 65.9903 - val_loss: 1165.4847 - val_mae: 1166.1771\n",
      "Epoch 3290/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 62.2625 - mae: 62.9407 - val_loss: 1087.9905 - val_mae: 1088.6835\n",
      "Epoch 3291/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 60.2709 - mae: 60.9478 - val_loss: 1101.4578 - val_mae: 1102.1511\n",
      "Epoch 3292/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 62.8627 - mae: 63.5416 - val_loss: 1098.5425 - val_mae: 1099.2341\n",
      "Epoch 3293/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 80.3835 - mae: 81.0630 - val_loss: 1121.6003 - val_mae: 1122.2936\n",
      "Epoch 3294/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 65.0894 - mae: 65.7656 - val_loss: 1101.0574 - val_mae: 1101.7505\n",
      "Epoch 3295/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 60.1250 - mae: 60.8017 - val_loss: 1147.0117 - val_mae: 1147.7051\n",
      "Epoch 3296/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 61.0598 - mae: 61.7384 - val_loss: 1135.0394 - val_mae: 1135.7325\n",
      "Epoch 3297/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 66.1164 - mae: 66.7959 - val_loss: 1126.6781 - val_mae: 1127.3713\n",
      "Epoch 3298/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 72.4030 - mae: 73.0812 - val_loss: 1137.4927 - val_mae: 1138.1853\n",
      "Epoch 3299/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 64.5819 - mae: 65.2621 - val_loss: 1096.6001 - val_mae: 1097.2932\n",
      "Epoch 3300/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 70.5234 - mae: 71.2000 - val_loss: 1115.3654 - val_mae: 1116.0573\n",
      "Epoch 3301/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 74.9251 - mae: 75.6048 - val_loss: 1093.0142 - val_mae: 1093.7074\n",
      "Epoch 3302/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 66.9365 - mae: 67.6155 - val_loss: 1123.5925 - val_mae: 1124.2856\n",
      "Epoch 3303/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 62.1248 - mae: 62.8041 - val_loss: 1101.8273 - val_mae: 1102.5203\n",
      "Epoch 3304/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 62.3182 - mae: 62.9955 - val_loss: 1158.1003 - val_mae: 1158.7931\n",
      "Epoch 3305/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 70.8829 - mae: 71.5640 - val_loss: 1113.9476 - val_mae: 1114.6406\n",
      "Epoch 3306/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 67.5702 - mae: 68.2490 - val_loss: 1134.5618 - val_mae: 1135.2549\n",
      "Epoch 3307/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 67.9038 - mae: 68.5841 - val_loss: 1092.7356 - val_mae: 1093.4287\n",
      "Epoch 3308/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 69.4283 - mae: 70.1045 - val_loss: 1125.4055 - val_mae: 1126.0978\n",
      "Epoch 3309/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 76.6637 - mae: 77.3449 - val_loss: 1131.4313 - val_mae: 1132.1243\n",
      "Epoch 3310/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 61.7427 - mae: 62.4210 - val_loss: 1144.5903 - val_mae: 1145.2834\n",
      "Epoch 3311/5000\n",
      "46/46 [==============================] - 2s 46ms/step - loss: 68.3988 - mae: 69.0806 - val_loss: 1121.2812 - val_mae: 1121.9738\n",
      "Epoch 3312/5000\n",
      "46/46 [==============================] - 2s 42ms/step - loss: 67.9070 - mae: 68.5864 - val_loss: 1102.3654 - val_mae: 1103.0585\n",
      "Epoch 3313/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 71.2911 - mae: 71.9707 - val_loss: 1160.1239 - val_mae: 1160.8170\n",
      "Epoch 3314/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 66.4774 - mae: 67.1577 - val_loss: 1116.0284 - val_mae: 1116.7213\n",
      "Epoch 3315/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 60.0745 - mae: 60.7522 - val_loss: 1127.7074 - val_mae: 1128.4005\n",
      "Epoch 3316/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 60.4225 - mae: 61.0970 - val_loss: 1112.4733 - val_mae: 1113.1664\n",
      "Epoch 3317/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 60.3239 - mae: 61.0042 - val_loss: 1117.0503 - val_mae: 1117.7430\n",
      "Epoch 3318/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 67.7438 - mae: 68.4253 - val_loss: 1180.8411 - val_mae: 1181.5341\n",
      "Epoch 3319/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 61.1470 - mae: 61.8265 - val_loss: 1104.1239 - val_mae: 1104.8171\n",
      "Epoch 3320/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 66.3169 - mae: 66.9936 - val_loss: 1149.8568 - val_mae: 1150.5488\n",
      "Epoch 3321/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 65.1471 - mae: 65.8246 - val_loss: 1128.1816 - val_mae: 1128.8748\n",
      "Epoch 3322/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 62.3509 - mae: 63.0269 - val_loss: 1100.2323 - val_mae: 1100.9254\n",
      "Epoch 3323/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 60.2201 - mae: 60.8968 - val_loss: 1182.8054 - val_mae: 1183.4985\n",
      "Epoch 3324/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 60.7116 - mae: 61.3899 - val_loss: 1199.8656 - val_mae: 1200.5587\n",
      "Epoch 3325/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 64.4879 - mae: 65.1652 - val_loss: 1160.4893 - val_mae: 1161.1816\n",
      "Epoch 3326/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 64.2607 - mae: 64.9415 - val_loss: 1154.1072 - val_mae: 1154.7999\n",
      "Epoch 3327/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 66.5810 - mae: 67.2610 - val_loss: 1137.7598 - val_mae: 1138.4528\n",
      "Epoch 3328/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 63.8907 - mae: 64.5652 - val_loss: 1094.3492 - val_mae: 1095.0425\n",
      "Epoch 3329/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 63.8147 - mae: 64.4947 - val_loss: 1193.1288 - val_mae: 1193.8217\n",
      "Epoch 3330/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 64.0446 - mae: 64.7215 - val_loss: 1168.7469 - val_mae: 1169.4401\n",
      "Epoch 3331/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 66.9160 - mae: 67.5949 - val_loss: 1138.3152 - val_mae: 1139.0079\n",
      "Epoch 3332/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 62.8265 - mae: 63.5037 - val_loss: 1164.4794 - val_mae: 1165.1724\n",
      "Epoch 3333/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 68.4356 - mae: 69.1142 - val_loss: 1129.8423 - val_mae: 1130.5353\n",
      "Epoch 3334/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 61.1922 - mae: 61.8710 - val_loss: 1102.4031 - val_mae: 1103.0961\n",
      "Epoch 3335/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 68.6877 - mae: 69.3649 - val_loss: 1100.7992 - val_mae: 1101.4923\n",
      "Epoch 3336/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 61.7323 - mae: 62.4102 - val_loss: 1182.4036 - val_mae: 1183.0964\n",
      "Epoch 3337/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 66.4570 - mae: 67.1372 - val_loss: 1082.4733 - val_mae: 1083.1664\n",
      "Epoch 3338/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 71.7148 - mae: 72.3900 - val_loss: 1135.3137 - val_mae: 1136.0063\n",
      "Epoch 3339/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 62.3045 - mae: 62.9824 - val_loss: 1153.8319 - val_mae: 1154.5251\n",
      "Epoch 3340/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 64.3627 - mae: 65.0402 - val_loss: 1126.9056 - val_mae: 1127.5988\n",
      "Epoch 3341/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 59.3155 - mae: 59.9931 - val_loss: 1101.1438 - val_mae: 1101.8368\n",
      "Epoch 3342/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 62.0145 - mae: 62.6940 - val_loss: 1139.9940 - val_mae: 1140.6865\n",
      "Epoch 3343/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 65.6309 - mae: 66.3103 - val_loss: 1155.9807 - val_mae: 1156.6737\n",
      "Epoch 3344/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 71.8360 - mae: 72.5138 - val_loss: 1116.2528 - val_mae: 1116.9454\n",
      "Epoch 3345/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 68.5948 - mae: 69.2743 - val_loss: 1205.8057 - val_mae: 1206.4988\n",
      "Epoch 3346/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 68.8834 - mae: 69.5592 - val_loss: 1117.0608 - val_mae: 1117.7537\n",
      "Epoch 3347/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 62.1025 - mae: 62.7840 - val_loss: 1126.0575 - val_mae: 1126.7506\n",
      "Epoch 3348/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 56.5249 - mae: 57.2032 - val_loss: 1124.9401 - val_mae: 1125.6331\n",
      "Epoch 3349/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 68.8764 - mae: 69.5524 - val_loss: 1128.2135 - val_mae: 1128.9064\n",
      "Epoch 3350/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 65.2540 - mae: 65.9327 - val_loss: 1123.7999 - val_mae: 1124.4924\n",
      "Epoch 3351/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 63.3681 - mae: 64.0436 - val_loss: 1111.2040 - val_mae: 1111.8972\n",
      "Epoch 3352/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 62.7164 - mae: 63.3933 - val_loss: 1127.1490 - val_mae: 1127.8408\n",
      "Epoch 3353/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 60.6907 - mae: 61.3673 - val_loss: 1120.8326 - val_mae: 1121.5259\n",
      "Epoch 3354/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 65.0479 - mae: 65.7270 - val_loss: 1159.7928 - val_mae: 1160.4858\n",
      "Epoch 3355/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 55.5825 - mae: 56.2562 - val_loss: 1097.1863 - val_mae: 1097.8783\n",
      "Epoch 3356/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 58.6688 - mae: 59.3433 - val_loss: 1124.8196 - val_mae: 1125.5127\n",
      "Epoch 3357/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 60.7317 - mae: 61.4078 - val_loss: 1114.5859 - val_mae: 1115.2783\n",
      "Epoch 3358/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 65.6375 - mae: 66.3161 - val_loss: 1106.1079 - val_mae: 1106.8010\n",
      "Epoch 3359/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 66.8087 - mae: 67.4874 - val_loss: 1114.9075 - val_mae: 1115.6001\n",
      "Epoch 3360/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 65.5705 - mae: 66.2471 - val_loss: 1120.7562 - val_mae: 1121.4493\n",
      "Epoch 3361/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 62.3177 - mae: 62.9925 - val_loss: 1169.7448 - val_mae: 1170.4368\n",
      "Epoch 3362/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 61.4405 - mae: 62.1155 - val_loss: 1122.0884 - val_mae: 1122.7808\n",
      "Epoch 3363/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 70.2727 - mae: 70.9518 - val_loss: 1168.6576 - val_mae: 1169.3508\n",
      "Epoch 3364/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 71.8065 - mae: 72.4838 - val_loss: 1120.8113 - val_mae: 1121.5046\n",
      "Epoch 3365/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 64.9651 - mae: 65.6434 - val_loss: 1117.9629 - val_mae: 1118.6560\n",
      "Epoch 3366/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 68.6254 - mae: 69.3062 - val_loss: 1277.0007 - val_mae: 1277.6936\n",
      "Epoch 3367/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 77.4533 - mae: 78.1329 - val_loss: 1176.9133 - val_mae: 1177.6063\n",
      "Epoch 3368/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 73.3825 - mae: 74.0603 - val_loss: 1147.4739 - val_mae: 1148.1667\n",
      "Epoch 3369/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 59.8026 - mae: 60.4792 - val_loss: 1185.3351 - val_mae: 1186.0281\n",
      "Epoch 3370/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 60.9940 - mae: 61.6702 - val_loss: 1164.7841 - val_mae: 1165.4773\n",
      "Epoch 3371/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 70.8233 - mae: 71.5016 - val_loss: 1113.0042 - val_mae: 1113.6974\n",
      "Epoch 3372/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 75.2378 - mae: 75.9120 - val_loss: 1108.9091 - val_mae: 1109.6022\n",
      "Epoch 3373/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 65.3566 - mae: 66.0371 - val_loss: 1108.6479 - val_mae: 1109.3412\n",
      "Epoch 3374/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 65.2955 - mae: 65.9745 - val_loss: 1147.3308 - val_mae: 1148.0234\n",
      "Epoch 3375/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 63.4551 - mae: 64.1318 - val_loss: 1178.2109 - val_mae: 1178.9043\n",
      "Epoch 3376/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 67.9796 - mae: 68.6548 - val_loss: 1224.4607 - val_mae: 1225.1539\n",
      "Epoch 3377/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 67.6055 - mae: 68.2829 - val_loss: 1183.7474 - val_mae: 1184.4407\n",
      "Epoch 3378/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 65.2470 - mae: 65.9240 - val_loss: 1112.8707 - val_mae: 1113.5640\n",
      "Epoch 3379/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 77.6737 - mae: 78.3563 - val_loss: 1209.9893 - val_mae: 1210.6823\n",
      "Epoch 3380/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 77.7165 - mae: 78.4019 - val_loss: 1147.1682 - val_mae: 1147.8610\n",
      "Epoch 3381/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 72.6399 - mae: 73.3209 - val_loss: 1133.9973 - val_mae: 1134.6895\n",
      "Epoch 3382/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 59.0558 - mae: 59.7326 - val_loss: 1127.3635 - val_mae: 1128.0566\n",
      "Epoch 3383/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 70.5095 - mae: 71.1872 - val_loss: 1127.4122 - val_mae: 1128.1045\n",
      "Epoch 3384/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 66.1723 - mae: 66.8498 - val_loss: 1131.6244 - val_mae: 1132.3175\n",
      "Epoch 3385/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 62.3371 - mae: 63.0151 - val_loss: 1153.8588 - val_mae: 1154.5520\n",
      "Epoch 3386/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 66.7844 - mae: 67.4629 - val_loss: 1126.4705 - val_mae: 1127.1636\n",
      "Epoch 3387/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 67.1415 - mae: 67.8196 - val_loss: 1102.2655 - val_mae: 1102.9585\n",
      "Epoch 3388/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 70.8317 - mae: 71.5099 - val_loss: 1157.6462 - val_mae: 1158.3395\n",
      "Epoch 3389/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 64.6679 - mae: 65.3482 - val_loss: 1095.6180 - val_mae: 1096.3110\n",
      "Epoch 3390/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 63.2958 - mae: 63.9733 - val_loss: 1159.5404 - val_mae: 1160.2336\n",
      "Epoch 3391/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 62.6303 - mae: 63.3071 - val_loss: 1130.6904 - val_mae: 1131.3834\n",
      "Epoch 3392/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 58.4866 - mae: 59.1588 - val_loss: 1116.8873 - val_mae: 1117.5798\n",
      "Epoch 3393/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 60.5410 - mae: 61.2138 - val_loss: 1104.2451 - val_mae: 1104.9384\n",
      "Epoch 3394/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 59.0641 - mae: 59.7406 - val_loss: 1112.4261 - val_mae: 1113.1183\n",
      "Epoch 3395/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 68.5823 - mae: 69.2631 - val_loss: 1119.2371 - val_mae: 1119.9303\n",
      "Epoch 3396/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 61.0576 - mae: 61.7321 - val_loss: 1108.7284 - val_mae: 1109.4215\n",
      "Epoch 3397/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 61.3920 - mae: 62.0671 - val_loss: 1122.9982 - val_mae: 1123.6913\n",
      "Epoch 3398/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 64.1505 - mae: 64.8272 - val_loss: 1110.2108 - val_mae: 1110.9038\n",
      "Epoch 3399/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 76.2709 - mae: 76.9501 - val_loss: 1084.2380 - val_mae: 1084.9312\n",
      "Epoch 3400/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 66.4791 - mae: 67.1537 - val_loss: 1137.9913 - val_mae: 1138.6844\n",
      "Epoch 3401/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 63.4606 - mae: 64.1379 - val_loss: 1171.5441 - val_mae: 1172.2373\n",
      "Epoch 3402/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 63.2166 - mae: 63.8898 - val_loss: 1189.0101 - val_mae: 1189.7034\n",
      "Epoch 3403/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 64.9697 - mae: 65.6447 - val_loss: 1121.2509 - val_mae: 1121.9438\n",
      "Epoch 3404/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 74.2499 - mae: 74.9308 - val_loss: 1104.2791 - val_mae: 1104.9720\n",
      "Epoch 3405/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 60.9154 - mae: 61.5951 - val_loss: 1108.4224 - val_mae: 1109.1154\n",
      "Epoch 3406/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 63.0798 - mae: 63.7611 - val_loss: 1106.9913 - val_mae: 1107.6844\n",
      "Epoch 3407/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 68.3205 - mae: 69.0033 - val_loss: 1132.3884 - val_mae: 1133.0803\n",
      "Epoch 3408/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 65.7617 - mae: 66.4394 - val_loss: 1117.8616 - val_mae: 1118.5537\n",
      "Epoch 3409/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 64.7224 - mae: 65.4004 - val_loss: 1104.6362 - val_mae: 1105.3286\n",
      "Epoch 3410/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 68.5326 - mae: 69.2100 - val_loss: 1125.0914 - val_mae: 1125.7830\n",
      "Epoch 3411/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 66.9205 - mae: 67.6048 - val_loss: 1101.4490 - val_mae: 1102.1415\n",
      "Epoch 3412/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 64.0281 - mae: 64.7047 - val_loss: 1147.2102 - val_mae: 1147.9033\n",
      "Epoch 3413/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 60.4873 - mae: 61.1656 - val_loss: 1117.5681 - val_mae: 1118.2614\n",
      "Epoch 3414/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 59.2195 - mae: 59.8950 - val_loss: 1145.0990 - val_mae: 1145.7921\n",
      "Epoch 3415/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 63.2129 - mae: 63.8890 - val_loss: 1144.5599 - val_mae: 1145.2531\n",
      "Epoch 3416/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 59.7404 - mae: 60.4158 - val_loss: 1107.6255 - val_mae: 1108.3185\n",
      "Epoch 3417/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 60.8513 - mae: 61.5285 - val_loss: 1105.5026 - val_mae: 1106.1958\n",
      "Epoch 3418/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 59.7122 - mae: 60.3881 - val_loss: 1144.9969 - val_mae: 1145.6884\n",
      "Epoch 3419/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 66.8616 - mae: 67.5396 - val_loss: 1144.5165 - val_mae: 1145.2095\n",
      "Epoch 3420/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 60.7282 - mae: 61.4025 - val_loss: 1135.3704 - val_mae: 1136.0635\n",
      "Epoch 3421/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 68.1147 - mae: 68.7912 - val_loss: 1174.7968 - val_mae: 1175.4899\n",
      "Epoch 3422/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 66.7254 - mae: 67.4008 - val_loss: 1081.1044 - val_mae: 1081.7974\n",
      "Epoch 3423/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 63.7097 - mae: 64.3908 - val_loss: 1141.9985 - val_mae: 1142.6917\n",
      "Epoch 3424/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 64.1566 - mae: 64.8370 - val_loss: 1162.4551 - val_mae: 1163.1472\n",
      "Epoch 3425/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 57.9219 - mae: 58.5925 - val_loss: 1094.0591 - val_mae: 1094.7518\n",
      "Epoch 3426/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 71.2347 - mae: 71.9126 - val_loss: 1111.1056 - val_mae: 1111.7987\n",
      "Epoch 3427/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 63.3732 - mae: 64.0517 - val_loss: 1184.9897 - val_mae: 1185.6827\n",
      "Epoch 3428/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 59.9696 - mae: 60.6467 - val_loss: 1127.3694 - val_mae: 1128.0612\n",
      "Epoch 3429/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 56.7523 - mae: 57.4293 - val_loss: 1135.0781 - val_mae: 1135.7711\n",
      "Epoch 3430/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 59.4247 - mae: 60.1021 - val_loss: 1138.5565 - val_mae: 1139.2498\n",
      "Epoch 3431/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 65.4687 - mae: 66.1490 - val_loss: 1119.8608 - val_mae: 1120.5537\n",
      "Epoch 3432/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 66.6630 - mae: 67.3447 - val_loss: 1161.0265 - val_mae: 1161.7183\n",
      "Epoch 3433/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 67.0988 - mae: 67.7761 - val_loss: 1133.8990 - val_mae: 1134.5924\n",
      "Epoch 3434/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 74.7438 - mae: 75.4262 - val_loss: 1157.5486 - val_mae: 1158.2417\n",
      "Epoch 3435/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 66.5806 - mae: 67.2640 - val_loss: 1114.2201 - val_mae: 1114.9133\n",
      "Epoch 3436/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 61.2032 - mae: 61.8826 - val_loss: 1132.4480 - val_mae: 1133.1411\n",
      "Epoch 3437/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 66.9929 - mae: 67.6707 - val_loss: 1193.2158 - val_mae: 1193.9087\n",
      "Epoch 3438/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 64.2490 - mae: 64.9283 - val_loss: 1152.5640 - val_mae: 1153.2572\n",
      "Epoch 3439/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 65.9885 - mae: 66.6623 - val_loss: 1216.2817 - val_mae: 1216.9750\n",
      "Epoch 3440/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 65.1602 - mae: 65.8376 - val_loss: 1128.6959 - val_mae: 1129.3890\n",
      "Epoch 3441/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 64.0539 - mae: 64.7326 - val_loss: 1155.4662 - val_mae: 1156.1593\n",
      "Epoch 3442/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 66.8512 - mae: 67.5280 - val_loss: 1090.7874 - val_mae: 1091.4805\n",
      "Epoch 3443/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 67.0159 - mae: 67.6932 - val_loss: 1100.6929 - val_mae: 1101.3860\n",
      "Epoch 3444/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 59.8615 - mae: 60.5344 - val_loss: 1165.2249 - val_mae: 1165.9175\n",
      "Epoch 3445/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 65.6939 - mae: 66.3709 - val_loss: 1100.9105 - val_mae: 1101.6034\n",
      "Epoch 3446/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 67.5371 - mae: 68.2168 - val_loss: 1111.8119 - val_mae: 1112.5049\n",
      "Epoch 3447/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 62.0298 - mae: 62.7077 - val_loss: 1174.3628 - val_mae: 1175.0559\n",
      "Epoch 3448/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 59.0905 - mae: 59.7682 - val_loss: 1110.2316 - val_mae: 1110.9247\n",
      "Epoch 3449/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 63.9404 - mae: 64.6141 - val_loss: 1177.6952 - val_mae: 1178.3883\n",
      "Epoch 3450/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 60.5089 - mae: 61.1885 - val_loss: 1130.5021 - val_mae: 1131.1954\n",
      "Epoch 3451/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 67.1860 - mae: 67.8629 - val_loss: 1107.1478 - val_mae: 1107.8408\n",
      "Epoch 3452/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 56.8517 - mae: 57.5270 - val_loss: 1125.0679 - val_mae: 1125.7607\n",
      "Epoch 3453/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 60.4541 - mae: 61.1321 - val_loss: 1143.1193 - val_mae: 1143.8126\n",
      "Epoch 3454/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 59.1593 - mae: 59.8325 - val_loss: 1124.0225 - val_mae: 1124.7156\n",
      "Epoch 3455/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 55.1614 - mae: 55.8337 - val_loss: 1111.7443 - val_mae: 1112.4364\n",
      "Epoch 3456/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 61.0018 - mae: 61.6814 - val_loss: 1113.8135 - val_mae: 1114.5066\n",
      "Epoch 3457/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 61.6591 - mae: 62.3380 - val_loss: 1102.5344 - val_mae: 1103.2278\n",
      "Epoch 3458/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 55.5508 - mae: 56.2268 - val_loss: 1112.8607 - val_mae: 1113.5540\n",
      "Epoch 3459/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 67.6773 - mae: 68.3568 - val_loss: 1202.5388 - val_mae: 1203.2318\n",
      "Epoch 3460/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 62.3028 - mae: 62.9791 - val_loss: 1124.4095 - val_mae: 1125.1029\n",
      "Epoch 3461/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 62.6234 - mae: 63.3024 - val_loss: 1201.1907 - val_mae: 1201.8837\n",
      "Epoch 3462/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 69.4623 - mae: 70.1382 - val_loss: 1164.8160 - val_mae: 1165.5093\n",
      "Epoch 3463/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 60.3382 - mae: 61.0165 - val_loss: 1133.2937 - val_mae: 1133.9868\n",
      "Epoch 3464/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 60.1131 - mae: 60.7913 - val_loss: 1129.3878 - val_mae: 1130.0812\n",
      "Epoch 3465/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 63.0399 - mae: 63.7167 - val_loss: 1160.1964 - val_mae: 1160.8882\n",
      "Epoch 3466/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 69.3326 - mae: 70.0119 - val_loss: 1105.4200 - val_mae: 1106.1134\n",
      "Epoch 3467/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 76.4162 - mae: 77.0983 - val_loss: 1120.4310 - val_mae: 1121.1243\n",
      "Epoch 3468/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 63.3929 - mae: 64.0718 - val_loss: 1107.0444 - val_mae: 1107.7374\n",
      "Epoch 3469/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 66.0812 - mae: 66.7605 - val_loss: 1119.5896 - val_mae: 1120.2823\n",
      "Epoch 3470/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 58.8603 - mae: 59.5372 - val_loss: 1118.3170 - val_mae: 1119.0101\n",
      "Epoch 3471/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 59.8554 - mae: 60.5349 - val_loss: 1116.1189 - val_mae: 1116.8123\n",
      "Epoch 3472/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 67.0891 - mae: 67.7697 - val_loss: 1137.2074 - val_mae: 1137.9005\n",
      "Epoch 3473/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 73.5504 - mae: 74.2254 - val_loss: 1141.2670 - val_mae: 1141.9601\n",
      "Epoch 3474/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 64.2470 - mae: 64.9250 - val_loss: 1170.9724 - val_mae: 1171.6655\n",
      "Epoch 3475/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 63.6728 - mae: 64.3531 - val_loss: 1187.3309 - val_mae: 1188.0242\n",
      "Epoch 3476/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 58.7981 - mae: 59.4743 - val_loss: 1114.8561 - val_mae: 1115.5483\n",
      "Epoch 3477/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 59.1120 - mae: 59.7878 - val_loss: 1149.3727 - val_mae: 1150.0653\n",
      "Epoch 3478/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 68.5343 - mae: 69.2153 - val_loss: 1119.3860 - val_mae: 1120.0792\n",
      "Epoch 3479/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 62.6114 - mae: 63.2881 - val_loss: 1149.2915 - val_mae: 1149.9845\n",
      "Epoch 3480/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 60.3955 - mae: 61.0781 - val_loss: 1127.6173 - val_mae: 1128.3103\n",
      "Epoch 3481/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 71.3125 - mae: 71.9903 - val_loss: 1111.1942 - val_mae: 1111.8873\n",
      "Epoch 3482/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 73.7739 - mae: 74.4571 - val_loss: 1109.6940 - val_mae: 1110.3866\n",
      "Epoch 3483/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 64.7148 - mae: 65.3982 - val_loss: 1112.8104 - val_mae: 1113.5035\n",
      "Epoch 3484/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 62.2580 - mae: 62.9369 - val_loss: 1125.1581 - val_mae: 1125.8510\n",
      "Epoch 3485/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 59.6687 - mae: 60.3486 - val_loss: 1092.8173 - val_mae: 1093.5098\n",
      "Epoch 3486/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 59.3781 - mae: 60.0522 - val_loss: 1090.1842 - val_mae: 1090.8768\n",
      "Epoch 3487/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 60.5654 - mae: 61.2441 - val_loss: 1116.0590 - val_mae: 1116.7524\n",
      "Epoch 3488/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 62.9501 - mae: 63.6270 - val_loss: 1131.4886 - val_mae: 1132.1816\n",
      "Epoch 3489/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 66.7119 - mae: 67.3883 - val_loss: 1138.5735 - val_mae: 1139.2668\n",
      "Epoch 3490/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 64.4020 - mae: 65.0770 - val_loss: 1097.6738 - val_mae: 1098.3672\n",
      "Epoch 3491/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 64.8034 - mae: 65.4824 - val_loss: 1151.3478 - val_mae: 1152.0408\n",
      "Epoch 3492/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 58.8199 - mae: 59.4955 - val_loss: 1139.2869 - val_mae: 1139.9802\n",
      "Epoch 3493/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 62.2056 - mae: 62.8792 - val_loss: 1102.0054 - val_mae: 1102.6985\n",
      "Epoch 3494/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 69.0229 - mae: 69.7032 - val_loss: 1111.1359 - val_mae: 1111.8290\n",
      "Epoch 3495/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 59.0049 - mae: 59.6802 - val_loss: 1151.8799 - val_mae: 1152.5729\n",
      "Epoch 3496/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 63.7103 - mae: 64.3905 - val_loss: 1118.9351 - val_mae: 1119.6282\n",
      "Epoch 3497/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 64.8458 - mae: 65.5234 - val_loss: 1147.9403 - val_mae: 1148.6333\n",
      "Epoch 3498/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 62.0324 - mae: 62.7031 - val_loss: 1154.3719 - val_mae: 1155.0643\n",
      "Epoch 3499/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 59.8055 - mae: 60.4798 - val_loss: 1130.2336 - val_mae: 1130.9257\n",
      "Epoch 3500/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 64.2087 - mae: 64.8835 - val_loss: 1220.9379 - val_mae: 1221.6307\n",
      "Epoch 3501/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 73.2809 - mae: 73.9607 - val_loss: 1120.2065 - val_mae: 1120.8998\n",
      "Epoch 3502/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 61.4047 - mae: 62.0777 - val_loss: 1106.2430 - val_mae: 1106.9360\n",
      "Epoch 3503/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 62.0138 - mae: 62.6906 - val_loss: 1137.7623 - val_mae: 1138.4556\n",
      "Epoch 3504/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 59.3942 - mae: 60.0700 - val_loss: 1148.7244 - val_mae: 1149.4166\n",
      "Epoch 3505/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 60.7125 - mae: 61.3867 - val_loss: 1119.6112 - val_mae: 1120.3024\n",
      "Epoch 3506/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 65.4066 - mae: 66.0868 - val_loss: 1149.2703 - val_mae: 1149.9634\n",
      "Epoch 3507/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 58.4115 - mae: 59.0944 - val_loss: 1155.6017 - val_mae: 1156.2941\n",
      "Epoch 3508/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 69.5841 - mae: 70.2622 - val_loss: 1115.3510 - val_mae: 1116.0441\n",
      "Epoch 3509/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 59.4304 - mae: 60.1056 - val_loss: 1192.7373 - val_mae: 1193.4304\n",
      "Epoch 3510/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 59.0769 - mae: 59.7492 - val_loss: 1130.5598 - val_mae: 1131.2528\n",
      "Epoch 3511/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 61.4373 - mae: 62.1143 - val_loss: 1120.9907 - val_mae: 1121.6838\n",
      "Epoch 3512/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 59.3483 - mae: 60.0256 - val_loss: 1107.3770 - val_mae: 1108.0701\n",
      "Epoch 3513/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 60.7536 - mae: 61.4290 - val_loss: 1119.5780 - val_mae: 1120.2710\n",
      "Epoch 3514/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 70.6038 - mae: 71.2835 - val_loss: 1201.9465 - val_mae: 1202.6398\n",
      "Epoch 3515/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 60.2942 - mae: 60.9671 - val_loss: 1157.0649 - val_mae: 1157.7556\n",
      "Epoch 3516/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 73.6413 - mae: 74.3205 - val_loss: 1151.1107 - val_mae: 1151.8040\n",
      "Epoch 3517/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 71.1043 - mae: 71.7820 - val_loss: 1221.8269 - val_mae: 1222.5188\n",
      "Epoch 3518/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 66.3306 - mae: 67.0122 - val_loss: 1115.3887 - val_mae: 1116.0818\n",
      "Epoch 3519/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 74.2684 - mae: 74.9485 - val_loss: 1207.2247 - val_mae: 1207.9180\n",
      "Epoch 3520/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 60.9766 - mae: 61.6531 - val_loss: 1181.9014 - val_mae: 1182.5944\n",
      "Epoch 3521/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 61.6365 - mae: 62.3135 - val_loss: 1144.8507 - val_mae: 1145.5415\n",
      "Epoch 3522/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 78.6145 - mae: 79.2909 - val_loss: 1130.2462 - val_mae: 1130.9395\n",
      "Epoch 3523/5000\n",
      "46/46 [==============================] - 2s 40ms/step - loss: 64.1152 - mae: 64.7936 - val_loss: 1114.8533 - val_mae: 1115.5464\n",
      "Epoch 3524/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 56.7881 - mae: 57.4640 - val_loss: 1106.6907 - val_mae: 1107.3838\n",
      "Epoch 3525/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 58.3382 - mae: 59.0149 - val_loss: 1124.9844 - val_mae: 1125.6774\n",
      "Epoch 3526/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 62.3738 - mae: 63.0554 - val_loss: 1113.1047 - val_mae: 1113.7977\n",
      "Epoch 3527/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 62.4884 - mae: 63.1661 - val_loss: 1126.6921 - val_mae: 1127.3853\n",
      "Epoch 3528/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 60.9676 - mae: 61.6464 - val_loss: 1101.7986 - val_mae: 1102.4917\n",
      "Epoch 3529/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 62.2818 - mae: 62.9614 - val_loss: 1209.3439 - val_mae: 1210.0366\n",
      "Epoch 3530/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 66.8098 - mae: 67.4896 - val_loss: 1137.0237 - val_mae: 1137.7170\n",
      "Epoch 3531/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 63.1794 - mae: 63.8556 - val_loss: 1136.1577 - val_mae: 1136.8510\n",
      "Epoch 3532/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 58.6756 - mae: 59.3457 - val_loss: 1182.7473 - val_mae: 1183.4404\n",
      "Epoch 3533/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 67.3169 - mae: 67.9941 - val_loss: 1131.6152 - val_mae: 1132.3077\n",
      "Epoch 3534/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 63.5777 - mae: 64.2550 - val_loss: 1171.5116 - val_mae: 1172.2046\n",
      "Epoch 3535/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 61.2552 - mae: 61.9301 - val_loss: 1132.7433 - val_mae: 1133.4365\n",
      "Epoch 3536/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 59.4291 - mae: 60.1085 - val_loss: 1156.6403 - val_mae: 1157.3335\n",
      "Epoch 3537/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 69.4637 - mae: 70.1459 - val_loss: 1151.5455 - val_mae: 1152.2384\n",
      "Epoch 3538/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 60.4877 - mae: 61.1645 - val_loss: 1127.1805 - val_mae: 1127.8737\n",
      "Epoch 3539/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 64.6463 - mae: 65.3210 - val_loss: 1136.7209 - val_mae: 1137.4142\n",
      "Epoch 3540/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 60.5903 - mae: 61.2683 - val_loss: 1127.3057 - val_mae: 1127.9988\n",
      "Epoch 3541/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 64.1801 - mae: 64.8584 - val_loss: 1217.3054 - val_mae: 1217.9980\n",
      "Epoch 3542/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 65.9040 - mae: 66.5803 - val_loss: 1116.2928 - val_mae: 1116.9850\n",
      "Epoch 3543/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 64.4030 - mae: 65.0752 - val_loss: 1127.0122 - val_mae: 1127.7052\n",
      "Epoch 3544/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 58.3920 - mae: 59.0695 - val_loss: 1133.0175 - val_mae: 1133.7106\n",
      "Epoch 3545/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 57.1707 - mae: 57.8466 - val_loss: 1138.2732 - val_mae: 1138.9662\n",
      "Epoch 3546/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 60.5213 - mae: 61.2041 - val_loss: 1137.3704 - val_mae: 1138.0631\n",
      "Epoch 3547/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 57.5117 - mae: 58.1876 - val_loss: 1130.1925 - val_mae: 1130.8855\n",
      "Epoch 3548/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 62.6295 - mae: 63.3066 - val_loss: 1148.8580 - val_mae: 1149.5513\n",
      "Epoch 3549/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 62.8223 - mae: 63.4990 - val_loss: 1143.0719 - val_mae: 1143.7651\n",
      "Epoch 3550/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 67.2885 - mae: 67.9666 - val_loss: 1108.4086 - val_mae: 1109.1006\n",
      "Epoch 3551/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 62.4206 - mae: 63.0987 - val_loss: 1225.6218 - val_mae: 1226.3142\n",
      "Epoch 3552/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 68.5880 - mae: 69.2648 - val_loss: 1143.5751 - val_mae: 1144.2681\n",
      "Epoch 3553/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 62.5656 - mae: 63.2436 - val_loss: 1125.3551 - val_mae: 1126.0481\n",
      "Epoch 3554/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 59.7861 - mae: 60.4656 - val_loss: 1166.2810 - val_mae: 1166.9741\n",
      "Epoch 3555/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 60.9050 - mae: 61.5834 - val_loss: 1135.2069 - val_mae: 1135.9001\n",
      "Epoch 3556/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 66.9637 - mae: 67.6403 - val_loss: 1142.6764 - val_mae: 1143.3691\n",
      "Epoch 3557/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 68.0055 - mae: 68.6850 - val_loss: 1153.9139 - val_mae: 1154.6060\n",
      "Epoch 3558/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 55.6597 - mae: 56.3358 - val_loss: 1152.2659 - val_mae: 1152.9591\n",
      "Epoch 3559/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 58.0525 - mae: 58.7298 - val_loss: 1128.2152 - val_mae: 1128.9082\n",
      "Epoch 3560/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 61.8566 - mae: 62.5299 - val_loss: 1138.0651 - val_mae: 1138.7583\n",
      "Epoch 3561/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 60.2975 - mae: 60.9725 - val_loss: 1133.8156 - val_mae: 1134.5085\n",
      "Epoch 3562/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 65.1790 - mae: 65.8582 - val_loss: 1131.4955 - val_mae: 1132.1887\n",
      "Epoch 3563/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 62.4787 - mae: 63.1569 - val_loss: 1111.2734 - val_mae: 1111.9651\n",
      "Epoch 3564/5000\n",
      "46/46 [==============================] - 2s 41ms/step - loss: 67.7734 - mae: 68.4471 - val_loss: 1115.2341 - val_mae: 1115.9257\n",
      "Epoch 3565/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 60.1645 - mae: 60.8472 - val_loss: 1129.5000 - val_mae: 1130.1932\n",
      "Epoch 3566/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 62.9405 - mae: 63.6136 - val_loss: 1145.2609 - val_mae: 1145.9541\n",
      "Epoch 3567/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 58.2272 - mae: 58.9068 - val_loss: 1100.2441 - val_mae: 1100.9362\n",
      "Epoch 3568/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 67.4900 - mae: 68.1664 - val_loss: 1114.3610 - val_mae: 1115.0532\n",
      "Epoch 3569/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 64.3157 - mae: 64.9909 - val_loss: 1162.5050 - val_mae: 1163.1980\n",
      "Epoch 3570/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 62.5614 - mae: 63.2378 - val_loss: 1144.9404 - val_mae: 1145.6335\n",
      "Epoch 3571/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 60.5773 - mae: 61.2511 - val_loss: 1109.0198 - val_mae: 1109.7122\n",
      "Epoch 3572/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 70.3078 - mae: 70.9851 - val_loss: 1171.1836 - val_mae: 1171.8768\n",
      "Epoch 3573/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 60.1033 - mae: 60.7821 - val_loss: 1120.6881 - val_mae: 1121.3812\n",
      "Epoch 3574/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 63.5300 - mae: 64.2060 - val_loss: 1133.6057 - val_mae: 1134.2987\n",
      "Epoch 3575/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 62.6873 - mae: 63.3621 - val_loss: 1128.5293 - val_mae: 1129.2214\n",
      "Epoch 3576/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 54.2944 - mae: 54.9695 - val_loss: 1141.0885 - val_mae: 1141.7815\n",
      "Epoch 3577/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 67.1527 - mae: 67.8362 - val_loss: 1141.9834 - val_mae: 1142.6765\n",
      "Epoch 3578/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 63.3455 - mae: 64.0245 - val_loss: 1111.5417 - val_mae: 1112.2350\n",
      "Epoch 3579/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 58.3439 - mae: 59.0189 - val_loss: 1125.8395 - val_mae: 1126.5325\n",
      "Epoch 3580/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 64.7520 - mae: 65.4265 - val_loss: 1126.9194 - val_mae: 1127.6127\n",
      "Epoch 3581/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 71.7645 - mae: 72.4422 - val_loss: 1154.0967 - val_mae: 1154.7896\n",
      "Epoch 3582/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 68.4760 - mae: 69.1538 - val_loss: 1158.9961 - val_mae: 1159.6892\n",
      "Epoch 3583/5000\n",
      "46/46 [==============================] - 2s 40ms/step - loss: 62.9766 - mae: 63.6507 - val_loss: 1126.5846 - val_mae: 1127.2778\n",
      "Epoch 3584/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 62.0982 - mae: 62.7745 - val_loss: 1105.9298 - val_mae: 1106.6216\n",
      "Epoch 3585/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 61.0963 - mae: 61.7745 - val_loss: 1178.9437 - val_mae: 1179.6371\n",
      "Epoch 3586/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 57.4702 - mae: 58.1478 - val_loss: 1158.1876 - val_mae: 1158.8807\n",
      "Epoch 3587/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 63.9344 - mae: 64.6141 - val_loss: 1243.5842 - val_mae: 1244.2773\n",
      "Epoch 3588/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 63.0736 - mae: 63.7515 - val_loss: 1116.4105 - val_mae: 1117.1036\n",
      "Epoch 3589/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 60.8549 - mae: 61.5357 - val_loss: 1175.1960 - val_mae: 1175.8892\n",
      "Epoch 3590/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 58.4524 - mae: 59.1288 - val_loss: 1151.6987 - val_mae: 1152.3918\n",
      "Epoch 3591/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 59.0459 - mae: 59.7193 - val_loss: 1157.7229 - val_mae: 1158.4161\n",
      "Epoch 3592/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 59.6639 - mae: 60.3423 - val_loss: 1135.8849 - val_mae: 1136.5781\n",
      "Epoch 3593/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 59.6450 - mae: 60.3191 - val_loss: 1138.6434 - val_mae: 1139.3364\n",
      "Epoch 3594/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 60.5972 - mae: 61.2719 - val_loss: 1209.0696 - val_mae: 1209.7629\n",
      "Epoch 3595/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 71.9479 - mae: 72.6283 - val_loss: 1138.2130 - val_mae: 1138.9062\n",
      "Epoch 3596/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 65.8105 - mae: 66.4878 - val_loss: 1194.2672 - val_mae: 1194.9604\n",
      "Epoch 3597/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 64.0800 - mae: 64.7589 - val_loss: 1117.3188 - val_mae: 1118.0120\n",
      "Epoch 3598/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 58.0499 - mae: 58.7313 - val_loss: 1125.7084 - val_mae: 1126.4003\n",
      "Epoch 3599/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 72.6417 - mae: 73.3247 - val_loss: 1161.2065 - val_mae: 1161.8998\n",
      "Epoch 3600/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 60.5631 - mae: 61.2399 - val_loss: 1156.7943 - val_mae: 1157.4874\n",
      "Epoch 3601/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 63.3626 - mae: 64.0438 - val_loss: 1136.5566 - val_mae: 1137.2493\n",
      "Epoch 3602/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 60.1177 - mae: 60.7953 - val_loss: 1142.4603 - val_mae: 1143.1533\n",
      "Epoch 3603/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 72.2169 - mae: 72.8953 - val_loss: 1115.2761 - val_mae: 1115.9691\n",
      "Epoch 3604/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 65.8373 - mae: 66.5156 - val_loss: 1213.1401 - val_mae: 1213.8333\n",
      "Epoch 3605/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 68.8702 - mae: 69.5494 - val_loss: 1125.5338 - val_mae: 1126.2271\n",
      "Epoch 3606/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 60.5253 - mae: 61.2064 - val_loss: 1128.8241 - val_mae: 1129.5162\n",
      "Epoch 3607/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 59.2589 - mae: 59.9364 - val_loss: 1135.2158 - val_mae: 1135.9088\n",
      "Epoch 3608/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 57.9446 - mae: 58.6216 - val_loss: 1136.2855 - val_mae: 1136.9788\n",
      "Epoch 3609/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 62.2562 - mae: 62.9345 - val_loss: 1142.3173 - val_mae: 1143.0103\n",
      "Epoch 3610/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 61.0129 - mae: 61.6870 - val_loss: 1143.3879 - val_mae: 1144.0812\n",
      "Epoch 3611/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 60.2929 - mae: 60.9731 - val_loss: 1135.1265 - val_mae: 1135.8193\n",
      "Epoch 3612/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 57.8551 - mae: 58.5307 - val_loss: 1137.0265 - val_mae: 1137.7196\n",
      "Epoch 3613/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 61.1716 - mae: 61.8511 - val_loss: 1153.1938 - val_mae: 1153.8866\n",
      "Epoch 3614/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 61.3687 - mae: 62.0439 - val_loss: 1152.2173 - val_mae: 1152.9103\n",
      "Epoch 3615/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 65.4071 - mae: 66.0892 - val_loss: 1194.8840 - val_mae: 1195.5759\n",
      "Epoch 3616/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 61.0103 - mae: 61.6864 - val_loss: 1123.5992 - val_mae: 1124.2916\n",
      "Epoch 3617/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 60.7947 - mae: 61.4725 - val_loss: 1161.1771 - val_mae: 1161.8702\n",
      "Epoch 3618/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 61.9099 - mae: 62.5878 - val_loss: 1144.8135 - val_mae: 1145.5066\n",
      "Epoch 3619/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 62.3057 - mae: 62.9858 - val_loss: 1179.4282 - val_mae: 1180.1212\n",
      "Epoch 3620/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 67.3553 - mae: 68.0357 - val_loss: 1160.3804 - val_mae: 1161.0736\n",
      "Epoch 3621/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 72.3237 - mae: 73.0031 - val_loss: 1161.6937 - val_mae: 1162.3860\n",
      "Epoch 3622/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 64.5311 - mae: 65.2073 - val_loss: 1109.4379 - val_mae: 1110.1311\n",
      "Epoch 3623/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 65.6881 - mae: 66.3626 - val_loss: 1126.2072 - val_mae: 1126.9000\n",
      "Epoch 3624/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 62.8609 - mae: 63.5413 - val_loss: 1115.2090 - val_mae: 1115.9023\n",
      "Epoch 3625/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 62.7346 - mae: 63.4134 - val_loss: 1149.5859 - val_mae: 1150.2793\n",
      "Epoch 3626/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 57.4411 - mae: 58.1141 - val_loss: 1158.7056 - val_mae: 1159.3984\n",
      "Epoch 3627/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 76.0588 - mae: 76.7398 - val_loss: 1197.7218 - val_mae: 1198.4150\n",
      "Epoch 3628/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 61.6053 - mae: 62.2812 - val_loss: 1129.8075 - val_mae: 1130.5001\n",
      "Epoch 3629/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 63.0266 - mae: 63.7033 - val_loss: 1155.8336 - val_mae: 1156.5267\n",
      "Epoch 3630/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 59.0831 - mae: 59.7630 - val_loss: 1125.7345 - val_mae: 1126.4274\n",
      "Epoch 3631/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 57.6992 - mae: 58.3739 - val_loss: 1137.2990 - val_mae: 1137.9911\n",
      "Epoch 3632/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 57.0607 - mae: 57.7348 - val_loss: 1121.7953 - val_mae: 1122.4886\n",
      "Epoch 3633/5000\n",
      "46/46 [==============================] - 2s 42ms/step - loss: 66.1476 - mae: 66.8257 - val_loss: 1168.5841 - val_mae: 1169.2771\n",
      "Epoch 3634/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 58.7242 - mae: 59.4016 - val_loss: 1121.3923 - val_mae: 1122.0853\n",
      "Epoch 3635/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 61.3906 - mae: 62.0689 - val_loss: 1288.3431 - val_mae: 1289.0361\n",
      "Epoch 3636/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 67.7795 - mae: 68.4552 - val_loss: 1122.8038 - val_mae: 1123.4956\n",
      "Epoch 3637/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 63.1903 - mae: 63.8650 - val_loss: 1124.0439 - val_mae: 1124.7358\n",
      "Epoch 3638/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 67.6301 - mae: 68.3126 - val_loss: 1129.2751 - val_mae: 1129.9685\n",
      "Epoch 3639/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 62.5464 - mae: 63.2240 - val_loss: 1122.6165 - val_mae: 1123.3096\n",
      "Epoch 3640/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 58.4747 - mae: 59.1515 - val_loss: 1130.9220 - val_mae: 1131.6151\n",
      "Epoch 3641/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 64.9826 - mae: 65.6597 - val_loss: 1130.3702 - val_mae: 1131.0637\n",
      "Epoch 3642/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 62.5056 - mae: 63.1827 - val_loss: 1143.0078 - val_mae: 1143.7008\n",
      "Epoch 3643/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 68.7066 - mae: 69.3838 - val_loss: 1127.1571 - val_mae: 1127.8502\n",
      "Epoch 3644/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 63.8498 - mae: 64.5316 - val_loss: 1126.9872 - val_mae: 1127.6804\n",
      "Epoch 3645/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 61.7575 - mae: 62.4320 - val_loss: 1126.0671 - val_mae: 1126.7603\n",
      "Epoch 3646/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 71.3948 - mae: 72.0749 - val_loss: 1114.3605 - val_mae: 1115.0526\n",
      "Epoch 3647/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 60.7191 - mae: 61.3961 - val_loss: 1174.6212 - val_mae: 1175.3145\n",
      "Epoch 3648/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 66.4978 - mae: 67.1763 - val_loss: 1166.4900 - val_mae: 1167.1832\n",
      "Epoch 3649/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 64.0540 - mae: 64.7337 - val_loss: 1141.0739 - val_mae: 1141.7661\n",
      "Epoch 3650/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 63.3450 - mae: 64.0207 - val_loss: 1127.7324 - val_mae: 1128.4253\n",
      "Epoch 3651/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 56.8454 - mae: 57.5232 - val_loss: 1132.6915 - val_mae: 1133.3846\n",
      "Epoch 3652/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 56.7237 - mae: 57.3975 - val_loss: 1141.8314 - val_mae: 1142.5247\n",
      "Epoch 3653/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 60.8073 - mae: 61.4833 - val_loss: 1166.3324 - val_mae: 1167.0255\n",
      "Epoch 3654/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 64.0684 - mae: 64.7466 - val_loss: 1098.7499 - val_mae: 1099.4434\n",
      "Epoch 3655/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 60.3214 - mae: 60.9962 - val_loss: 1119.8895 - val_mae: 1120.5822\n",
      "Epoch 3656/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 57.1235 - mae: 57.7980 - val_loss: 1113.3612 - val_mae: 1114.0537\n",
      "Epoch 3657/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 65.6175 - mae: 66.2928 - val_loss: 1127.3350 - val_mae: 1128.0281\n",
      "Epoch 3658/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 59.7866 - mae: 60.4673 - val_loss: 1131.3322 - val_mae: 1132.0254\n",
      "Epoch 3659/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 59.3587 - mae: 60.0360 - val_loss: 1116.7330 - val_mae: 1117.4259\n",
      "Epoch 3660/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 63.4411 - mae: 64.1199 - val_loss: 1127.9382 - val_mae: 1128.6315\n",
      "Epoch 3661/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 60.5663 - mae: 61.2453 - val_loss: 1087.8267 - val_mae: 1088.5194\n",
      "Epoch 3662/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 64.0414 - mae: 64.7182 - val_loss: 1107.2312 - val_mae: 1107.9238\n",
      "Epoch 3663/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 57.8902 - mae: 58.5664 - val_loss: 1113.5355 - val_mae: 1114.2288\n",
      "Epoch 3664/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 57.3738 - mae: 58.0531 - val_loss: 1115.3810 - val_mae: 1116.0740\n",
      "Epoch 3665/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 61.9982 - mae: 62.6751 - val_loss: 1143.3074 - val_mae: 1144.0007\n",
      "Epoch 3666/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 55.2167 - mae: 55.8913 - val_loss: 1126.5631 - val_mae: 1127.2560\n",
      "Epoch 3667/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 62.1308 - mae: 62.8103 - val_loss: 1122.6117 - val_mae: 1123.3041\n",
      "Epoch 3668/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 58.3665 - mae: 59.0409 - val_loss: 1167.0017 - val_mae: 1167.6948\n",
      "Epoch 3669/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 58.4753 - mae: 59.1497 - val_loss: 1142.1616 - val_mae: 1142.8547\n",
      "Epoch 3670/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 61.5065 - mae: 62.1837 - val_loss: 1128.4247 - val_mae: 1129.1180\n",
      "Epoch 3671/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 66.1099 - mae: 66.7900 - val_loss: 1117.0846 - val_mae: 1117.7778\n",
      "Epoch 3672/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 58.3559 - mae: 59.0334 - val_loss: 1207.0369 - val_mae: 1207.7295\n",
      "Epoch 3673/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 66.0753 - mae: 66.7548 - val_loss: 1189.6243 - val_mae: 1190.3175\n",
      "Epoch 3674/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 62.7002 - mae: 63.3765 - val_loss: 1109.1555 - val_mae: 1109.8474\n",
      "Epoch 3675/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 59.3812 - mae: 60.0570 - val_loss: 1111.9017 - val_mae: 1112.5947\n",
      "Epoch 3676/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 64.9952 - mae: 65.6709 - val_loss: 1133.0972 - val_mae: 1133.7904\n",
      "Epoch 3677/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 61.6658 - mae: 62.3431 - val_loss: 1131.6488 - val_mae: 1132.3420\n",
      "Epoch 3678/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 61.8751 - mae: 62.5469 - val_loss: 1154.4221 - val_mae: 1155.1154\n",
      "Epoch 3679/5000\n",
      "46/46 [==============================] - 2s 45ms/step - loss: 65.0662 - mae: 65.7469 - val_loss: 1122.4146 - val_mae: 1123.1075\n",
      "Epoch 3680/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 58.6423 - mae: 59.3211 - val_loss: 1126.1366 - val_mae: 1126.8290\n",
      "Epoch 3681/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 62.7341 - mae: 63.4100 - val_loss: 1137.9685 - val_mae: 1138.6614\n",
      "Epoch 3682/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 62.0543 - mae: 62.7295 - val_loss: 1122.1663 - val_mae: 1122.8594\n",
      "Epoch 3683/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 65.8346 - mae: 66.5164 - val_loss: 1139.8599 - val_mae: 1140.5531\n",
      "Epoch 3684/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 58.0960 - mae: 58.7750 - val_loss: 1141.4187 - val_mae: 1142.1117\n",
      "Epoch 3685/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 66.0268 - mae: 66.7079 - val_loss: 1184.4739 - val_mae: 1185.1670\n",
      "Epoch 3686/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 60.5714 - mae: 61.2501 - val_loss: 1105.0342 - val_mae: 1105.7267\n",
      "Epoch 3687/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 61.8479 - mae: 62.5275 - val_loss: 1131.5641 - val_mae: 1132.2567\n",
      "Epoch 3688/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 61.3945 - mae: 62.0712 - val_loss: 1137.1115 - val_mae: 1137.8047\n",
      "Epoch 3689/5000\n",
      "46/46 [==============================] - 2s 32ms/step - loss: 67.4572 - mae: 68.1342 - val_loss: 1195.6226 - val_mae: 1196.3158\n",
      "Epoch 3690/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 68.0106 - mae: 68.6934 - val_loss: 1128.1113 - val_mae: 1128.8044\n",
      "Epoch 3691/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 65.4875 - mae: 66.1664 - val_loss: 1140.7520 - val_mae: 1141.4448\n",
      "Epoch 3692/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 64.9556 - mae: 65.6361 - val_loss: 1188.4127 - val_mae: 1189.1058\n",
      "Epoch 3693/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 61.2024 - mae: 61.8809 - val_loss: 1156.8257 - val_mae: 1157.5188\n",
      "Epoch 3694/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 62.0307 - mae: 62.7050 - val_loss: 1131.8954 - val_mae: 1132.5886\n",
      "Epoch 3695/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 54.2725 - mae: 54.9456 - val_loss: 1124.6412 - val_mae: 1125.3344\n",
      "Epoch 3696/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 61.8704 - mae: 62.5476 - val_loss: 1119.2383 - val_mae: 1119.9313\n",
      "Epoch 3697/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 66.5295 - mae: 67.2088 - val_loss: 1139.0635 - val_mae: 1139.7562\n",
      "Epoch 3698/5000\n",
      "46/46 [==============================] - 2s 40ms/step - loss: 60.5714 - mae: 61.2479 - val_loss: 1180.1123 - val_mae: 1180.8054\n",
      "Epoch 3699/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 64.5579 - mae: 65.2342 - val_loss: 1141.3120 - val_mae: 1142.0051\n",
      "Epoch 3700/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 55.5519 - mae: 56.2266 - val_loss: 1162.6512 - val_mae: 1163.3444\n",
      "Epoch 3701/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 67.9508 - mae: 68.6293 - val_loss: 1168.4872 - val_mae: 1169.1804\n",
      "Epoch 3702/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 62.3336 - mae: 63.0099 - val_loss: 1129.2773 - val_mae: 1129.9706\n",
      "Epoch 3703/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 63.5280 - mae: 64.2006 - val_loss: 1125.7920 - val_mae: 1126.4850\n",
      "Epoch 3704/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 60.3822 - mae: 61.0593 - val_loss: 1128.3577 - val_mae: 1129.0508\n",
      "Epoch 3705/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 58.7750 - mae: 59.4535 - val_loss: 1137.6443 - val_mae: 1138.3375\n",
      "Epoch 3706/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 57.7397 - mae: 58.4178 - val_loss: 1137.4696 - val_mae: 1138.1625\n",
      "Epoch 3707/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 58.2337 - mae: 58.9100 - val_loss: 1132.3500 - val_mae: 1133.0428\n",
      "Epoch 3708/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 55.9368 - mae: 56.6099 - val_loss: 1166.3629 - val_mae: 1167.0562\n",
      "Epoch 3709/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 56.4250 - mae: 57.0991 - val_loss: 1130.5966 - val_mae: 1131.2897\n",
      "Epoch 3710/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 63.5104 - mae: 64.1883 - val_loss: 1118.0920 - val_mae: 1118.7845\n",
      "Epoch 3711/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 64.0976 - mae: 64.7784 - val_loss: 1210.3699 - val_mae: 1211.0630\n",
      "Epoch 3712/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 56.1422 - mae: 56.8187 - val_loss: 1123.6516 - val_mae: 1124.3441\n",
      "Epoch 3713/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 60.8900 - mae: 61.5695 - val_loss: 1216.6625 - val_mae: 1217.3555\n",
      "Epoch 3714/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 72.4858 - mae: 73.1612 - val_loss: 1147.9917 - val_mae: 1148.6849\n",
      "Epoch 3715/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 63.2108 - mae: 63.8874 - val_loss: 1124.2496 - val_mae: 1124.9418\n",
      "Epoch 3716/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 62.5035 - mae: 63.1769 - val_loss: 1172.9814 - val_mae: 1173.6748\n",
      "Epoch 3717/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 59.6996 - mae: 60.3779 - val_loss: 1165.6406 - val_mae: 1166.3331\n",
      "Epoch 3718/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 57.7994 - mae: 58.4751 - val_loss: 1130.8839 - val_mae: 1131.5771\n",
      "Epoch 3719/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 59.3640 - mae: 60.0423 - val_loss: 1142.3590 - val_mae: 1143.0524\n",
      "Epoch 3720/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 55.7556 - mae: 56.4338 - val_loss: 1124.7305 - val_mae: 1125.4236\n",
      "Epoch 3721/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 52.9731 - mae: 53.6459 - val_loss: 1140.3599 - val_mae: 1141.0530\n",
      "Epoch 3722/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 57.7655 - mae: 58.4413 - val_loss: 1123.9042 - val_mae: 1124.5970\n",
      "Epoch 3723/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 61.9287 - mae: 62.6014 - val_loss: 1136.5046 - val_mae: 1137.1975\n",
      "Epoch 3724/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 64.6128 - mae: 65.2932 - val_loss: 1141.7834 - val_mae: 1142.4766\n",
      "Epoch 3725/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 62.5571 - mae: 63.2324 - val_loss: 1131.4160 - val_mae: 1132.1090\n",
      "Epoch 3726/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 56.6829 - mae: 57.3598 - val_loss: 1110.9165 - val_mae: 1111.6096\n",
      "Epoch 3727/5000\n",
      "46/46 [==============================] - 2s 42ms/step - loss: 55.3230 - mae: 55.9976 - val_loss: 1132.9476 - val_mae: 1133.6409\n",
      "Epoch 3728/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 68.3632 - mae: 69.0431 - val_loss: 1143.5879 - val_mae: 1144.2811\n",
      "Epoch 3729/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 70.6981 - mae: 71.3763 - val_loss: 1115.8739 - val_mae: 1116.5669\n",
      "Epoch 3730/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 69.4862 - mae: 70.1684 - val_loss: 1110.0648 - val_mae: 1110.7572\n",
      "Epoch 3731/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 56.9539 - mae: 57.6341 - val_loss: 1129.1698 - val_mae: 1129.8625\n",
      "Epoch 3732/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 57.2497 - mae: 57.9280 - val_loss: 1129.4388 - val_mae: 1130.1307\n",
      "Epoch 3733/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 60.5083 - mae: 61.1801 - val_loss: 1159.9996 - val_mae: 1160.6930\n",
      "Epoch 3734/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 55.6258 - mae: 56.3003 - val_loss: 1131.6726 - val_mae: 1132.3654\n",
      "Epoch 3735/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 65.2290 - mae: 65.9100 - val_loss: 1096.7030 - val_mae: 1097.3959\n",
      "Epoch 3736/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 64.0674 - mae: 64.7437 - val_loss: 1177.9248 - val_mae: 1178.6163\n",
      "Epoch 3737/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 60.0086 - mae: 60.6818 - val_loss: 1126.7252 - val_mae: 1127.4186\n",
      "Epoch 3738/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 71.7380 - mae: 72.4193 - val_loss: 1160.6224 - val_mae: 1161.3156\n",
      "Epoch 3739/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 59.0681 - mae: 59.7418 - val_loss: 1131.6528 - val_mae: 1132.3447\n",
      "Epoch 3740/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 57.2653 - mae: 57.9397 - val_loss: 1138.2651 - val_mae: 1138.9583\n",
      "Epoch 3741/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 64.0310 - mae: 64.7072 - val_loss: 1146.8279 - val_mae: 1147.5209\n",
      "Epoch 3742/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 61.5472 - mae: 62.2203 - val_loss: 1125.9934 - val_mae: 1126.6864\n",
      "Epoch 3743/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 56.0819 - mae: 56.7594 - val_loss: 1153.5901 - val_mae: 1154.2831\n",
      "Epoch 3744/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 66.6102 - mae: 67.2893 - val_loss: 1154.1302 - val_mae: 1154.8235\n",
      "Epoch 3745/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 60.8519 - mae: 61.5294 - val_loss: 1149.6012 - val_mae: 1150.2939\n",
      "Epoch 3746/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 59.9639 - mae: 60.6360 - val_loss: 1136.7563 - val_mae: 1137.4496\n",
      "Epoch 3747/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 64.0430 - mae: 64.7210 - val_loss: 1110.1808 - val_mae: 1110.8738\n",
      "Epoch 3748/5000\n",
      "46/46 [==============================] - 2s 43ms/step - loss: 61.1711 - mae: 61.8467 - val_loss: 1120.0526 - val_mae: 1120.7456\n",
      "Epoch 3749/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 72.8673 - mae: 73.5419 - val_loss: 1165.7578 - val_mae: 1166.4510\n",
      "Epoch 3750/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 60.6819 - mae: 61.3614 - val_loss: 1130.0988 - val_mae: 1130.7914\n",
      "Epoch 3751/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 58.1276 - mae: 58.8091 - val_loss: 1121.2814 - val_mae: 1121.9746\n",
      "Epoch 3752/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 74.5351 - mae: 75.2130 - val_loss: 1117.8137 - val_mae: 1118.5068\n",
      "Epoch 3753/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 62.5805 - mae: 63.2599 - val_loss: 1113.2601 - val_mae: 1113.9532\n",
      "Epoch 3754/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 64.4036 - mae: 65.0813 - val_loss: 1097.4393 - val_mae: 1098.1322\n",
      "Epoch 3755/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 62.0898 - mae: 62.7699 - val_loss: 1127.0404 - val_mae: 1127.7334\n",
      "Epoch 3756/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 71.0511 - mae: 71.7316 - val_loss: 1143.5812 - val_mae: 1144.2740\n",
      "Epoch 3757/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 62.0178 - mae: 62.6945 - val_loss: 1104.5045 - val_mae: 1105.1978\n",
      "Epoch 3758/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 60.4575 - mae: 61.1342 - val_loss: 1135.9288 - val_mae: 1136.6219\n",
      "Epoch 3759/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 56.4895 - mae: 57.1657 - val_loss: 1127.7528 - val_mae: 1128.4460\n",
      "Epoch 3760/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 64.1684 - mae: 64.8441 - val_loss: 1101.3384 - val_mae: 1102.0311\n",
      "Epoch 3761/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 80.7747 - mae: 81.4529 - val_loss: 1123.6193 - val_mae: 1124.3125\n",
      "Epoch 3762/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 69.4663 - mae: 70.1452 - val_loss: 1125.1356 - val_mae: 1125.8280\n",
      "Epoch 3763/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 58.7932 - mae: 59.4653 - val_loss: 1134.5148 - val_mae: 1135.2078\n",
      "Epoch 3764/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 54.0725 - mae: 54.7469 - val_loss: 1124.7908 - val_mae: 1125.4836\n",
      "Epoch 3765/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 60.6770 - mae: 61.3541 - val_loss: 1171.3833 - val_mae: 1172.0765\n",
      "Epoch 3766/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 68.9192 - mae: 69.5994 - val_loss: 1173.7294 - val_mae: 1174.4226\n",
      "Epoch 3767/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 58.3300 - mae: 59.0061 - val_loss: 1109.8140 - val_mae: 1110.5070\n",
      "Epoch 3768/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 55.1121 - mae: 55.7870 - val_loss: 1113.3817 - val_mae: 1114.0750\n",
      "Epoch 3769/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 64.9809 - mae: 65.6563 - val_loss: 1125.7059 - val_mae: 1126.3990\n",
      "Epoch 3770/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 58.0313 - mae: 58.7100 - val_loss: 1137.8583 - val_mae: 1138.5516\n",
      "Epoch 3771/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 55.5717 - mae: 56.2477 - val_loss: 1127.9562 - val_mae: 1128.6493\n",
      "Epoch 3772/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 60.3378 - mae: 61.0149 - val_loss: 1175.9261 - val_mae: 1176.6187\n",
      "Epoch 3773/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 68.2253 - mae: 68.9040 - val_loss: 1178.2438 - val_mae: 1178.9370\n",
      "Epoch 3774/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 61.0347 - mae: 61.7101 - val_loss: 1138.2272 - val_mae: 1138.9203\n",
      "Epoch 3775/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 68.6531 - mae: 69.3322 - val_loss: 1180.7649 - val_mae: 1181.4579\n",
      "Epoch 3776/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 57.4638 - mae: 58.1349 - val_loss: 1122.4143 - val_mae: 1123.1073\n",
      "Epoch 3777/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 60.3441 - mae: 61.0207 - val_loss: 1135.4556 - val_mae: 1136.1487\n",
      "Epoch 3778/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 59.7159 - mae: 60.3887 - val_loss: 1140.5437 - val_mae: 1141.2369\n",
      "Epoch 3779/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 59.7735 - mae: 60.4508 - val_loss: 1140.0071 - val_mae: 1140.7004\n",
      "Epoch 3780/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 67.4155 - mae: 68.0909 - val_loss: 1126.6888 - val_mae: 1127.3817\n",
      "Epoch 3781/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 57.8401 - mae: 58.5131 - val_loss: 1127.8761 - val_mae: 1128.5688\n",
      "Epoch 3782/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 57.8115 - mae: 58.4848 - val_loss: 1121.1312 - val_mae: 1121.8243\n",
      "Epoch 3783/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 61.3997 - mae: 62.0728 - val_loss: 1163.1018 - val_mae: 1163.7944\n",
      "Epoch 3784/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 63.1102 - mae: 63.7908 - val_loss: 1130.2670 - val_mae: 1130.9587\n",
      "Epoch 3785/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 60.5931 - mae: 61.2720 - val_loss: 1112.1188 - val_mae: 1112.8115\n",
      "Epoch 3786/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 62.5161 - mae: 63.1981 - val_loss: 1116.6995 - val_mae: 1117.3926\n",
      "Epoch 3787/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 60.1516 - mae: 60.8331 - val_loss: 1147.5504 - val_mae: 1148.2432\n",
      "Epoch 3788/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 55.0883 - mae: 55.7655 - val_loss: 1125.2955 - val_mae: 1125.9885\n",
      "Epoch 3789/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 57.9188 - mae: 58.5959 - val_loss: 1143.8053 - val_mae: 1144.4985\n",
      "Epoch 3790/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 67.8902 - mae: 68.5702 - val_loss: 1151.7268 - val_mae: 1152.4186\n",
      "Epoch 3791/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 60.5286 - mae: 61.2053 - val_loss: 1126.0403 - val_mae: 1126.7334\n",
      "Epoch 3792/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 56.1338 - mae: 56.8071 - val_loss: 1094.1471 - val_mae: 1094.8384\n",
      "Epoch 3793/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 64.4524 - mae: 65.1300 - val_loss: 1162.6312 - val_mae: 1163.3246\n",
      "Epoch 3794/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 54.8292 - mae: 55.5065 - val_loss: 1121.9052 - val_mae: 1122.5970\n",
      "Epoch 3795/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 61.4591 - mae: 62.1356 - val_loss: 1167.8401 - val_mae: 1168.5331\n",
      "Epoch 3796/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 62.8303 - mae: 63.5081 - val_loss: 1144.9579 - val_mae: 1145.6510\n",
      "Epoch 3797/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 61.8360 - mae: 62.5158 - val_loss: 1131.0535 - val_mae: 1131.7465\n",
      "Epoch 3798/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 61.3310 - mae: 62.0117 - val_loss: 1137.6002 - val_mae: 1138.2936\n",
      "Epoch 3799/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 63.1438 - mae: 63.8242 - val_loss: 1118.0620 - val_mae: 1118.7551\n",
      "Epoch 3800/5000\n",
      "46/46 [==============================] - 2s 31ms/step - loss: 71.5952 - mae: 72.2704 - val_loss: 1112.4480 - val_mae: 1113.1411\n",
      "Epoch 3801/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 59.4779 - mae: 60.1548 - val_loss: 1113.2311 - val_mae: 1113.9241\n",
      "Epoch 3802/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 57.0637 - mae: 57.7356 - val_loss: 1108.1409 - val_mae: 1108.8335\n",
      "Epoch 3803/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 54.0549 - mae: 54.7350 - val_loss: 1123.6935 - val_mae: 1124.3855\n",
      "Epoch 3804/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 57.3529 - mae: 58.0300 - val_loss: 1111.0770 - val_mae: 1111.7703\n",
      "Epoch 3805/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 54.9199 - mae: 55.5916 - val_loss: 1159.6993 - val_mae: 1160.3923\n",
      "Epoch 3806/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 60.1125 - mae: 60.7903 - val_loss: 1117.5919 - val_mae: 1118.2839\n",
      "Epoch 3807/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 54.5050 - mae: 55.1816 - val_loss: 1128.5571 - val_mae: 1129.2500\n",
      "Epoch 3808/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 57.7182 - mae: 58.3972 - val_loss: 1137.3185 - val_mae: 1138.0115\n",
      "Epoch 3809/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 58.9268 - mae: 59.6052 - val_loss: 1120.1200 - val_mae: 1120.8131\n",
      "Epoch 3810/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 54.5995 - mae: 55.2741 - val_loss: 1103.8269 - val_mae: 1104.5192\n",
      "Epoch 3811/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 55.9210 - mae: 56.5961 - val_loss: 1136.6711 - val_mae: 1137.3643\n",
      "Epoch 3812/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 59.0824 - mae: 59.7583 - val_loss: 1151.0944 - val_mae: 1151.7876\n",
      "Epoch 3813/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 59.0772 - mae: 59.7548 - val_loss: 1128.0555 - val_mae: 1128.7488\n",
      "Epoch 3814/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 54.4819 - mae: 55.1631 - val_loss: 1144.3645 - val_mae: 1145.0566\n",
      "Epoch 3815/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 57.5671 - mae: 58.2459 - val_loss: 1151.4438 - val_mae: 1152.1368\n",
      "Epoch 3816/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 61.9951 - mae: 62.6751 - val_loss: 1168.5282 - val_mae: 1169.2200\n",
      "Epoch 3817/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 69.0393 - mae: 69.7167 - val_loss: 1143.8851 - val_mae: 1144.5784\n",
      "Epoch 3818/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 51.9999 - mae: 52.6705 - val_loss: 1142.4535 - val_mae: 1143.1465\n",
      "Epoch 3819/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 54.0971 - mae: 54.7753 - val_loss: 1133.7462 - val_mae: 1134.4395\n",
      "Epoch 3820/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 52.9618 - mae: 53.6376 - val_loss: 1148.6661 - val_mae: 1149.3579\n",
      "Epoch 3821/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 61.1633 - mae: 61.8414 - val_loss: 1144.1649 - val_mae: 1144.8575\n",
      "Epoch 3822/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 59.7448 - mae: 60.4227 - val_loss: 1130.8657 - val_mae: 1131.5583\n",
      "Epoch 3823/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 60.7784 - mae: 61.4554 - val_loss: 1138.3809 - val_mae: 1139.0741\n",
      "Epoch 3824/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 57.8327 - mae: 58.5108 - val_loss: 1132.1766 - val_mae: 1132.8696\n",
      "Epoch 3825/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 65.7636 - mae: 66.4431 - val_loss: 1133.6626 - val_mae: 1134.3558\n",
      "Epoch 3826/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 61.2907 - mae: 61.9667 - val_loss: 1111.2258 - val_mae: 1111.9189\n",
      "Epoch 3827/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 57.4540 - mae: 58.1312 - val_loss: 1193.7963 - val_mae: 1194.4896\n",
      "Epoch 3828/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 59.7908 - mae: 60.4678 - val_loss: 1119.1660 - val_mae: 1119.8579\n",
      "Epoch 3829/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 56.1137 - mae: 56.7868 - val_loss: 1140.3673 - val_mae: 1141.0603\n",
      "Epoch 3830/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 57.4698 - mae: 58.1447 - val_loss: 1115.5198 - val_mae: 1116.2129\n",
      "Epoch 3831/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 58.5075 - mae: 59.1867 - val_loss: 1125.0209 - val_mae: 1125.7141\n",
      "Epoch 3832/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 65.0895 - mae: 65.7635 - val_loss: 1178.6530 - val_mae: 1179.3446\n",
      "Epoch 3833/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 63.4150 - mae: 64.0955 - val_loss: 1113.9619 - val_mae: 1114.6542\n",
      "Epoch 3834/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 64.4181 - mae: 65.0940 - val_loss: 1123.6611 - val_mae: 1124.3541\n",
      "Epoch 3835/5000\n",
      "46/46 [==============================] - 2s 40ms/step - loss: 60.1433 - mae: 60.8233 - val_loss: 1140.4395 - val_mae: 1141.1323\n",
      "Epoch 3836/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 61.3442 - mae: 62.0180 - val_loss: 1115.2672 - val_mae: 1115.9602\n",
      "Epoch 3837/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 58.0666 - mae: 58.7447 - val_loss: 1135.6216 - val_mae: 1136.3143\n",
      "Epoch 3838/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 63.4380 - mae: 64.1125 - val_loss: 1115.9673 - val_mae: 1116.6591\n",
      "Epoch 3839/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 58.4663 - mae: 59.1417 - val_loss: 1134.3522 - val_mae: 1135.0453\n",
      "Epoch 3840/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 59.5312 - mae: 60.2083 - val_loss: 1126.2540 - val_mae: 1126.9468\n",
      "Epoch 3841/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 55.3183 - mae: 55.9983 - val_loss: 1139.1223 - val_mae: 1139.8152\n",
      "Epoch 3842/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 54.9030 - mae: 55.5793 - val_loss: 1143.2983 - val_mae: 1143.9902\n",
      "Epoch 3843/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 61.6043 - mae: 62.2873 - val_loss: 1142.3185 - val_mae: 1143.0116\n",
      "Epoch 3844/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 57.8751 - mae: 58.5506 - val_loss: 1099.8901 - val_mae: 1100.5830\n",
      "Epoch 3845/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 56.0766 - mae: 56.7525 - val_loss: 1155.6573 - val_mae: 1156.3506\n",
      "Epoch 3846/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 68.2086 - mae: 68.8895 - val_loss: 1155.2167 - val_mae: 1155.9099\n",
      "Epoch 3847/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 66.1954 - mae: 66.8745 - val_loss: 1135.7279 - val_mae: 1136.4210\n",
      "Epoch 3848/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 63.5957 - mae: 64.2742 - val_loss: 1124.5255 - val_mae: 1125.2188\n",
      "Epoch 3849/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 60.2972 - mae: 60.9722 - val_loss: 1125.2351 - val_mae: 1125.9283\n",
      "Epoch 3850/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 53.3496 - mae: 54.0235 - val_loss: 1169.4531 - val_mae: 1170.1461\n",
      "Epoch 3851/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 60.6754 - mae: 61.3558 - val_loss: 1127.5177 - val_mae: 1128.2107\n",
      "Epoch 3852/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 61.3856 - mae: 62.0610 - val_loss: 1121.1064 - val_mae: 1121.7996\n",
      "Epoch 3853/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 55.1532 - mae: 55.8272 - val_loss: 1130.1771 - val_mae: 1130.8698\n",
      "Epoch 3854/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 55.5028 - mae: 56.1780 - val_loss: 1118.5449 - val_mae: 1119.2383\n",
      "Epoch 3855/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 57.1543 - mae: 57.8296 - val_loss: 1127.6217 - val_mae: 1128.3136\n",
      "Epoch 3856/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 56.7742 - mae: 57.4492 - val_loss: 1132.6434 - val_mae: 1133.3356\n",
      "Epoch 3857/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 64.3596 - mae: 65.0413 - val_loss: 1127.1091 - val_mae: 1127.8022\n",
      "Epoch 3858/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 71.7057 - mae: 72.3842 - val_loss: 1122.9414 - val_mae: 1123.6345\n",
      "Epoch 3859/5000\n",
      "46/46 [==============================] - 2s 48ms/step - loss: 65.8666 - mae: 66.5439 - val_loss: 1119.0951 - val_mae: 1119.7883\n",
      "Epoch 3860/5000\n",
      "46/46 [==============================] - 2s 40ms/step - loss: 59.8209 - mae: 60.4981 - val_loss: 1129.3199 - val_mae: 1130.0123\n",
      "Epoch 3861/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 69.1544 - mae: 69.8342 - val_loss: 1139.2487 - val_mae: 1139.9417\n",
      "Epoch 3862/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 58.7325 - mae: 59.4086 - val_loss: 1161.9868 - val_mae: 1162.6799\n",
      "Epoch 3863/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 63.5619 - mae: 64.2391 - val_loss: 1122.4211 - val_mae: 1123.1146\n",
      "Epoch 3864/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 59.1362 - mae: 59.8161 - val_loss: 1132.3265 - val_mae: 1133.0188\n",
      "Epoch 3865/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 57.7166 - mae: 58.3916 - val_loss: 1147.5823 - val_mae: 1148.2754\n",
      "Epoch 3866/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 59.3948 - mae: 60.0735 - val_loss: 1107.4259 - val_mae: 1108.1190\n",
      "Epoch 3867/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 55.2847 - mae: 55.9588 - val_loss: 1138.5515 - val_mae: 1139.2435\n",
      "Epoch 3868/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 57.2023 - mae: 57.8807 - val_loss: 1119.8685 - val_mae: 1120.5616\n",
      "Epoch 3869/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 56.7386 - mae: 57.4113 - val_loss: 1106.4546 - val_mae: 1107.1477\n",
      "Epoch 3870/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 61.8434 - mae: 62.5207 - val_loss: 1150.4314 - val_mae: 1151.1245\n",
      "Epoch 3871/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 61.0160 - mae: 61.6891 - val_loss: 1190.5648 - val_mae: 1191.2579\n",
      "Epoch 3872/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 65.5786 - mae: 66.2555 - val_loss: 1126.7770 - val_mae: 1127.4700\n",
      "Epoch 3873/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 54.8839 - mae: 55.5613 - val_loss: 1134.4159 - val_mae: 1135.1088\n",
      "Epoch 3874/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 58.0642 - mae: 58.7428 - val_loss: 1168.3220 - val_mae: 1169.0137\n",
      "Epoch 3875/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 57.4538 - mae: 58.1271 - val_loss: 1142.9253 - val_mae: 1143.6173\n",
      "Epoch 3876/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 56.4928 - mae: 57.1713 - val_loss: 1118.5651 - val_mae: 1119.2576\n",
      "Epoch 3877/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 61.4201 - mae: 62.0953 - val_loss: 1168.1256 - val_mae: 1168.8187\n",
      "Epoch 3878/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 63.0371 - mae: 63.7159 - val_loss: 1143.2738 - val_mae: 1143.9672\n",
      "Epoch 3879/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 58.0688 - mae: 58.7449 - val_loss: 1103.7639 - val_mae: 1104.4564\n",
      "Epoch 3880/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 58.2073 - mae: 58.8853 - val_loss: 1140.5951 - val_mae: 1141.2882\n",
      "Epoch 3881/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 64.3411 - mae: 65.0191 - val_loss: 1125.2076 - val_mae: 1125.9009\n",
      "Epoch 3882/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 62.3704 - mae: 63.0486 - val_loss: 1164.6208 - val_mae: 1165.3136\n",
      "Epoch 3883/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 66.6287 - mae: 67.3080 - val_loss: 1129.9579 - val_mae: 1130.6512\n",
      "Epoch 3884/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 57.7341 - mae: 58.4096 - val_loss: 1123.9902 - val_mae: 1124.6832\n",
      "Epoch 3885/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 62.9092 - mae: 63.5842 - val_loss: 1135.5349 - val_mae: 1136.2281\n",
      "Epoch 3886/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 59.5413 - mae: 60.2219 - val_loss: 1144.6495 - val_mae: 1145.3429\n",
      "Epoch 3887/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 59.0558 - mae: 59.7338 - val_loss: 1127.2467 - val_mae: 1127.9399\n",
      "Epoch 3888/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 60.7369 - mae: 61.4147 - val_loss: 1128.5084 - val_mae: 1129.2007\n",
      "Epoch 3889/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 57.8339 - mae: 58.5130 - val_loss: 1144.1743 - val_mae: 1144.8674\n",
      "Epoch 3890/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 74.2428 - mae: 74.9211 - val_loss: 1128.0829 - val_mae: 1128.7756\n",
      "Epoch 3891/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 61.9151 - mae: 62.5903 - val_loss: 1139.5297 - val_mae: 1140.2217\n",
      "Epoch 3892/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 59.3822 - mae: 60.0601 - val_loss: 1181.8987 - val_mae: 1182.5920\n",
      "Epoch 3893/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 58.3059 - mae: 58.9867 - val_loss: 1140.4124 - val_mae: 1141.1046\n",
      "Epoch 3894/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 58.8082 - mae: 59.4884 - val_loss: 1114.8539 - val_mae: 1115.5470\n",
      "Epoch 3895/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 59.3170 - mae: 59.9912 - val_loss: 1139.8021 - val_mae: 1140.4938\n",
      "Epoch 3896/5000\n",
      "46/46 [==============================] - 2s 41ms/step - loss: 55.0491 - mae: 55.7246 - val_loss: 1128.2047 - val_mae: 1128.8967\n",
      "Epoch 3897/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 60.7104 - mae: 61.3884 - val_loss: 1157.6697 - val_mae: 1158.3629\n",
      "Epoch 3898/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 55.0751 - mae: 55.7505 - val_loss: 1147.8153 - val_mae: 1148.5084\n",
      "Epoch 3899/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 56.7486 - mae: 57.4260 - val_loss: 1130.4702 - val_mae: 1131.1636\n",
      "Epoch 3900/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 60.9822 - mae: 61.6623 - val_loss: 1120.9500 - val_mae: 1121.6432\n",
      "Epoch 3901/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 58.4464 - mae: 59.1233 - val_loss: 1171.7003 - val_mae: 1172.3936\n",
      "Epoch 3902/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 57.7410 - mae: 58.4142 - val_loss: 1105.4319 - val_mae: 1106.1252\n",
      "Epoch 3903/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 57.7736 - mae: 58.4495 - val_loss: 1130.5100 - val_mae: 1131.2032\n",
      "Epoch 3904/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 58.6680 - mae: 59.3464 - val_loss: 1142.5952 - val_mae: 1143.2885\n",
      "Epoch 3905/5000\n",
      "46/46 [==============================] - 2s 54ms/step - loss: 68.5599 - mae: 69.2400 - val_loss: 1105.8102 - val_mae: 1106.5035\n",
      "Epoch 3906/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 68.4249 - mae: 69.1041 - val_loss: 1134.8585 - val_mae: 1135.5516\n",
      "Epoch 3907/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 59.0696 - mae: 59.7470 - val_loss: 1110.7789 - val_mae: 1111.4719\n",
      "Epoch 3908/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 57.4526 - mae: 58.1304 - val_loss: 1139.8574 - val_mae: 1140.5505\n",
      "Epoch 3909/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 57.6373 - mae: 58.3127 - val_loss: 1128.1115 - val_mae: 1128.8046\n",
      "Epoch 3910/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 58.5640 - mae: 59.2418 - val_loss: 1125.3817 - val_mae: 1126.0745\n",
      "Epoch 3911/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 54.9631 - mae: 55.6382 - val_loss: 1132.3676 - val_mae: 1133.0607\n",
      "Epoch 3912/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 54.4527 - mae: 55.1306 - val_loss: 1126.4934 - val_mae: 1127.1859\n",
      "Epoch 3913/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 61.8131 - mae: 62.4910 - val_loss: 1114.1359 - val_mae: 1114.8287\n",
      "Epoch 3914/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 57.5231 - mae: 58.1997 - val_loss: 1146.1862 - val_mae: 1146.8793\n",
      "Epoch 3915/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 57.8638 - mae: 58.5381 - val_loss: 1127.0227 - val_mae: 1127.7158\n",
      "Epoch 3916/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 63.4029 - mae: 64.0802 - val_loss: 1148.9247 - val_mae: 1149.6174\n",
      "Epoch 3917/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 76.7613 - mae: 77.4424 - val_loss: 1176.3063 - val_mae: 1176.9989\n",
      "Epoch 3918/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 75.6428 - mae: 76.3219 - val_loss: 1129.9104 - val_mae: 1130.6034\n",
      "Epoch 3919/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 61.8548 - mae: 62.5320 - val_loss: 1128.1658 - val_mae: 1128.8588\n",
      "Epoch 3920/5000\n",
      "46/46 [==============================] - 2s 48ms/step - loss: 62.0128 - mae: 62.6875 - val_loss: 1157.7346 - val_mae: 1158.4280\n",
      "Epoch 3921/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 62.8144 - mae: 63.4956 - val_loss: 1135.1965 - val_mae: 1135.8899\n",
      "Epoch 3922/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 58.0495 - mae: 58.7289 - val_loss: 1120.2712 - val_mae: 1120.9631\n",
      "Epoch 3923/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 54.1710 - mae: 54.8457 - val_loss: 1197.4109 - val_mae: 1198.1041\n",
      "Epoch 3924/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 58.8750 - mae: 59.5515 - val_loss: 1135.7177 - val_mae: 1136.4108\n",
      "Epoch 3925/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 57.3737 - mae: 58.0457 - val_loss: 1130.1036 - val_mae: 1130.7968\n",
      "Epoch 3926/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 53.5773 - mae: 54.2496 - val_loss: 1159.4861 - val_mae: 1160.1792\n",
      "Epoch 3927/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 60.6155 - mae: 61.2931 - val_loss: 1146.5969 - val_mae: 1147.2900\n",
      "Epoch 3928/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 58.2414 - mae: 58.9173 - val_loss: 1141.1184 - val_mae: 1141.8115\n",
      "Epoch 3929/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 64.9571 - mae: 65.6332 - val_loss: 1150.7839 - val_mae: 1151.4772\n",
      "Epoch 3930/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 72.0652 - mae: 72.7442 - val_loss: 1127.7188 - val_mae: 1128.4115\n",
      "Epoch 3931/5000\n",
      "46/46 [==============================] - 2s 42ms/step - loss: 57.9747 - mae: 58.6527 - val_loss: 1135.1353 - val_mae: 1135.8280\n",
      "Epoch 3932/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 66.6032 - mae: 67.2772 - val_loss: 1140.8232 - val_mae: 1141.5164\n",
      "Epoch 3933/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 60.3082 - mae: 60.9827 - val_loss: 1161.7855 - val_mae: 1162.4789\n",
      "Epoch 3934/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 53.6423 - mae: 54.3138 - val_loss: 1130.3862 - val_mae: 1131.0795\n",
      "Epoch 3935/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 51.8083 - mae: 52.4829 - val_loss: 1142.4039 - val_mae: 1143.0957\n",
      "Epoch 3936/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 59.8781 - mae: 60.5558 - val_loss: 1139.2732 - val_mae: 1139.9666\n",
      "Epoch 3937/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 78.1635 - mae: 78.8457 - val_loss: 1133.8213 - val_mae: 1134.5144\n",
      "Epoch 3938/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 57.4404 - mae: 58.1164 - val_loss: 1133.3965 - val_mae: 1134.0890\n",
      "Epoch 3939/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 61.7125 - mae: 62.3945 - val_loss: 1200.5120 - val_mae: 1201.2037\n",
      "Epoch 3940/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 63.7989 - mae: 64.4762 - val_loss: 1136.2422 - val_mae: 1136.9352\n",
      "Epoch 3941/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 67.9772 - mae: 68.6565 - val_loss: 1129.6425 - val_mae: 1130.3354\n",
      "Epoch 3942/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 59.4009 - mae: 60.0771 - val_loss: 1136.9812 - val_mae: 1137.6747\n",
      "Epoch 3943/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 56.2559 - mae: 56.9302 - val_loss: 1178.5328 - val_mae: 1179.2258\n",
      "Epoch 3944/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 60.4184 - mae: 61.0963 - val_loss: 1103.5031 - val_mae: 1104.1962\n",
      "Epoch 3945/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 57.4476 - mae: 58.1207 - val_loss: 1117.0981 - val_mae: 1117.7914\n",
      "Epoch 3946/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 61.2143 - mae: 61.8885 - val_loss: 1129.8246 - val_mae: 1130.5178\n",
      "Epoch 3947/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 74.0737 - mae: 74.7551 - val_loss: 1225.9592 - val_mae: 1226.6526\n",
      "Epoch 3948/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 66.2803 - mae: 66.9619 - val_loss: 1161.8063 - val_mae: 1162.4991\n",
      "Epoch 3949/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 62.3273 - mae: 63.0050 - val_loss: 1159.5908 - val_mae: 1160.2823\n",
      "Epoch 3950/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 57.4338 - mae: 58.1072 - val_loss: 1126.4332 - val_mae: 1127.1265\n",
      "Epoch 3951/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 60.2071 - mae: 60.8846 - val_loss: 1103.0747 - val_mae: 1103.7673\n",
      "Epoch 3952/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 57.6939 - mae: 58.3703 - val_loss: 1151.9597 - val_mae: 1152.6528\n",
      "Epoch 3953/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 55.3239 - mae: 55.9976 - val_loss: 1126.3141 - val_mae: 1127.0071\n",
      "Epoch 3954/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 54.0352 - mae: 54.7142 - val_loss: 1154.3564 - val_mae: 1155.0498\n",
      "Epoch 3955/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 59.9122 - mae: 60.5920 - val_loss: 1144.1378 - val_mae: 1144.8308\n",
      "Epoch 3956/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 55.0160 - mae: 55.6877 - val_loss: 1146.3882 - val_mae: 1147.0813\n",
      "Epoch 3957/5000\n",
      "46/46 [==============================] - 2s 42ms/step - loss: 60.0402 - mae: 60.7170 - val_loss: 1144.2830 - val_mae: 1144.9763\n",
      "Epoch 3958/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 54.4393 - mae: 55.1172 - val_loss: 1117.2772 - val_mae: 1117.9702\n",
      "Epoch 3959/5000\n",
      "46/46 [==============================] - 2s 48ms/step - loss: 55.0491 - mae: 55.7260 - val_loss: 1124.7462 - val_mae: 1125.4393\n",
      "Epoch 3960/5000\n",
      "46/46 [==============================] - 2s 32ms/step - loss: 60.1726 - mae: 60.8515 - val_loss: 1117.6954 - val_mae: 1118.3883\n",
      "Epoch 3961/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 58.8223 - mae: 59.4995 - val_loss: 1142.9833 - val_mae: 1143.6758\n",
      "Epoch 3962/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 61.4769 - mae: 62.1518 - val_loss: 1110.4318 - val_mae: 1111.1249\n",
      "Epoch 3963/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 68.4978 - mae: 69.1743 - val_loss: 1129.0394 - val_mae: 1129.7321\n",
      "Epoch 3964/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 61.4651 - mae: 62.1458 - val_loss: 1142.6470 - val_mae: 1143.3401\n",
      "Epoch 3965/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 60.2146 - mae: 60.8899 - val_loss: 1116.4850 - val_mae: 1117.1782\n",
      "Epoch 3966/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 54.7832 - mae: 55.4617 - val_loss: 1129.2710 - val_mae: 1129.9639\n",
      "Epoch 3967/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 59.6600 - mae: 60.3369 - val_loss: 1130.2147 - val_mae: 1130.9077\n",
      "Epoch 3968/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 63.7089 - mae: 64.3894 - val_loss: 1176.3508 - val_mae: 1177.0430\n",
      "Epoch 3969/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 59.4769 - mae: 60.1584 - val_loss: 1117.1414 - val_mae: 1117.8331\n",
      "Epoch 3970/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 57.5931 - mae: 58.2649 - val_loss: 1116.4868 - val_mae: 1117.1788\n",
      "Epoch 3971/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 62.3215 - mae: 62.9998 - val_loss: 1109.9479 - val_mae: 1110.6405\n",
      "Epoch 3972/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 59.1976 - mae: 59.8731 - val_loss: 1119.8875 - val_mae: 1120.5792\n",
      "Epoch 3973/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 56.9352 - mae: 57.6117 - val_loss: 1120.7786 - val_mae: 1121.4718\n",
      "Epoch 3974/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 54.4581 - mae: 55.1385 - val_loss: 1112.1125 - val_mae: 1112.8059\n",
      "Epoch 3975/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 54.7040 - mae: 55.3783 - val_loss: 1122.5284 - val_mae: 1123.2216\n",
      "Epoch 3976/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 57.7036 - mae: 58.3825 - val_loss: 1106.7145 - val_mae: 1107.4076\n",
      "Epoch 3977/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 57.2877 - mae: 57.9617 - val_loss: 1124.8938 - val_mae: 1125.5868\n",
      "Epoch 3978/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 54.1001 - mae: 54.7739 - val_loss: 1114.8141 - val_mae: 1115.5073\n",
      "Epoch 3979/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 54.8272 - mae: 55.5021 - val_loss: 1115.5231 - val_mae: 1116.2159\n",
      "Epoch 3980/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 59.1803 - mae: 59.8547 - val_loss: 1154.9067 - val_mae: 1155.5988\n",
      "Epoch 3981/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 66.6998 - mae: 67.3747 - val_loss: 1222.8876 - val_mae: 1223.5800\n",
      "Epoch 3982/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 63.0258 - mae: 63.7053 - val_loss: 1123.7653 - val_mae: 1124.4586\n",
      "Epoch 3983/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 59.4465 - mae: 60.1272 - val_loss: 1195.2731 - val_mae: 1195.9662\n",
      "Epoch 3984/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 55.0902 - mae: 55.7665 - val_loss: 1138.9166 - val_mae: 1139.6093\n",
      "Epoch 3985/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 61.9525 - mae: 62.6310 - val_loss: 1155.0682 - val_mae: 1155.7612\n",
      "Epoch 3986/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 60.2153 - mae: 60.8925 - val_loss: 1125.5474 - val_mae: 1126.2406\n",
      "Epoch 3987/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 55.6603 - mae: 56.3310 - val_loss: 1131.4709 - val_mae: 1132.1642\n",
      "Epoch 3988/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 59.4605 - mae: 60.1349 - val_loss: 1169.2374 - val_mae: 1169.9305\n",
      "Epoch 3989/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 62.6456 - mae: 63.3192 - val_loss: 1137.2850 - val_mae: 1137.9781\n",
      "Epoch 3990/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 64.3312 - mae: 65.0097 - val_loss: 1175.7797 - val_mae: 1176.4728\n",
      "Epoch 3991/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 61.6330 - mae: 62.3133 - val_loss: 1151.4448 - val_mae: 1152.1379\n",
      "Epoch 3992/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 52.5479 - mae: 53.2186 - val_loss: 1125.9417 - val_mae: 1126.6346\n",
      "Epoch 3993/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 52.5313 - mae: 53.2047 - val_loss: 1134.7935 - val_mae: 1135.4867\n",
      "Epoch 3994/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 56.1627 - mae: 56.8406 - val_loss: 1127.8184 - val_mae: 1128.5115\n",
      "Epoch 3995/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 58.8352 - mae: 59.5094 - val_loss: 1156.1661 - val_mae: 1156.8580\n",
      "Epoch 3996/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 55.5416 - mae: 56.2174 - val_loss: 1123.6866 - val_mae: 1124.3789\n",
      "Epoch 3997/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 56.1475 - mae: 56.8288 - val_loss: 1159.0354 - val_mae: 1159.7281\n",
      "Epoch 3998/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 64.5840 - mae: 65.2643 - val_loss: 1196.9001 - val_mae: 1197.5935\n",
      "Epoch 3999/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 65.6153 - mae: 66.2914 - val_loss: 1133.7598 - val_mae: 1134.4530\n",
      "Epoch 4000/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 58.1095 - mae: 58.7852 - val_loss: 1118.6980 - val_mae: 1119.3898\n",
      "Epoch 4001/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 55.1772 - mae: 55.8541 - val_loss: 1143.0964 - val_mae: 1143.7897\n",
      "Epoch 4002/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 58.8374 - mae: 59.5130 - val_loss: 1164.3823 - val_mae: 1165.0754\n",
      "Epoch 4003/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 58.6113 - mae: 59.2912 - val_loss: 1119.3628 - val_mae: 1120.0557\n",
      "Epoch 4004/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 61.5014 - mae: 62.1743 - val_loss: 1128.1476 - val_mae: 1128.8409\n",
      "Epoch 4005/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 60.3018 - mae: 60.9786 - val_loss: 1158.6666 - val_mae: 1159.3599\n",
      "Epoch 4006/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 58.9465 - mae: 59.6245 - val_loss: 1171.9152 - val_mae: 1172.6082\n",
      "Epoch 4007/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 59.9288 - mae: 60.6049 - val_loss: 1161.9670 - val_mae: 1162.6597\n",
      "Epoch 4008/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 63.1991 - mae: 63.8758 - val_loss: 1182.3896 - val_mae: 1183.0826\n",
      "Epoch 4009/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 54.2080 - mae: 54.8880 - val_loss: 1144.8101 - val_mae: 1145.5033\n",
      "Epoch 4010/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 54.9541 - mae: 55.6289 - val_loss: 1166.3829 - val_mae: 1167.0763\n",
      "Epoch 4011/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 54.3065 - mae: 54.9867 - val_loss: 1122.6738 - val_mae: 1123.3671\n",
      "Epoch 4012/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 58.3075 - mae: 58.9861 - val_loss: 1129.2842 - val_mae: 1129.9767\n",
      "Epoch 4013/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 63.0836 - mae: 63.7640 - val_loss: 1146.0139 - val_mae: 1146.7067\n",
      "Epoch 4014/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 57.1947 - mae: 57.8700 - val_loss: 1145.7594 - val_mae: 1146.4520\n",
      "Epoch 4015/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 67.2208 - mae: 67.9038 - val_loss: 1159.5586 - val_mae: 1160.2517\n",
      "Epoch 4016/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 61.0618 - mae: 61.7398 - val_loss: 1137.8127 - val_mae: 1138.5056\n",
      "Epoch 4017/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 68.2766 - mae: 68.9544 - val_loss: 1165.9354 - val_mae: 1166.6287\n",
      "Epoch 4018/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 54.1793 - mae: 54.8557 - val_loss: 1122.8920 - val_mae: 1123.5842\n",
      "Epoch 4019/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 56.5735 - mae: 57.2548 - val_loss: 1135.7385 - val_mae: 1136.4318\n",
      "Epoch 4020/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 57.3406 - mae: 58.0176 - val_loss: 1178.9459 - val_mae: 1179.6393\n",
      "Epoch 4021/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 57.4830 - mae: 58.1629 - val_loss: 1135.7072 - val_mae: 1136.4005\n",
      "Epoch 4022/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 53.1329 - mae: 53.8050 - val_loss: 1128.9233 - val_mae: 1129.6154\n",
      "Epoch 4023/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 53.4412 - mae: 54.1127 - val_loss: 1165.0563 - val_mae: 1165.7491\n",
      "Epoch 4024/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 57.4827 - mae: 58.1588 - val_loss: 1106.8800 - val_mae: 1107.5729\n",
      "Epoch 4025/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 60.7761 - mae: 61.4544 - val_loss: 1141.8396 - val_mae: 1142.5326\n",
      "Epoch 4026/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 57.5736 - mae: 58.2516 - val_loss: 1127.2338 - val_mae: 1127.9269\n",
      "Epoch 4027/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 64.1086 - mae: 64.7913 - val_loss: 1124.4291 - val_mae: 1125.1210\n",
      "Epoch 4028/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 52.7945 - mae: 53.4731 - val_loss: 1150.3241 - val_mae: 1151.0175\n",
      "Epoch 4029/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 57.3673 - mae: 58.0455 - val_loss: 1125.8281 - val_mae: 1126.5212\n",
      "Epoch 4030/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 57.3836 - mae: 58.0599 - val_loss: 1147.6274 - val_mae: 1148.3204\n",
      "Epoch 4031/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 61.3045 - mae: 61.9798 - val_loss: 1136.0712 - val_mae: 1136.7642\n",
      "Epoch 4032/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 60.4260 - mae: 61.1047 - val_loss: 1133.6484 - val_mae: 1134.3414\n",
      "Epoch 4033/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 61.8910 - mae: 62.5719 - val_loss: 1136.6390 - val_mae: 1137.3320\n",
      "Epoch 4034/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 55.6281 - mae: 56.3013 - val_loss: 1141.6326 - val_mae: 1142.3247\n",
      "Epoch 4035/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 57.6328 - mae: 58.3084 - val_loss: 1183.5308 - val_mae: 1184.2236\n",
      "Epoch 4036/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 60.7215 - mae: 61.3987 - val_loss: 1118.2572 - val_mae: 1118.9503\n",
      "Epoch 4037/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 60.6553 - mae: 61.3342 - val_loss: 1126.0620 - val_mae: 1126.7549\n",
      "Epoch 4038/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 59.5279 - mae: 60.2079 - val_loss: 1091.3264 - val_mae: 1092.0197\n",
      "Epoch 4039/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 60.3650 - mae: 61.0410 - val_loss: 1123.1448 - val_mae: 1123.8380\n",
      "Epoch 4040/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 54.6078 - mae: 55.2855 - val_loss: 1109.5162 - val_mae: 1110.2072\n",
      "Epoch 4041/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 65.0916 - mae: 65.7668 - val_loss: 1129.6320 - val_mae: 1130.3247\n",
      "Epoch 4042/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 60.1538 - mae: 60.8297 - val_loss: 1130.5286 - val_mae: 1131.2216\n",
      "Epoch 4043/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 61.9856 - mae: 62.6607 - val_loss: 1102.7809 - val_mae: 1103.4735\n",
      "Epoch 4044/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 56.4210 - mae: 57.0979 - val_loss: 1106.3608 - val_mae: 1107.0537\n",
      "Epoch 4045/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 58.8358 - mae: 59.5130 - val_loss: 1136.0112 - val_mae: 1136.7036\n",
      "Epoch 4046/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 55.4225 - mae: 56.0941 - val_loss: 1113.1271 - val_mae: 1113.8188\n",
      "Epoch 4047/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 57.3521 - mae: 58.0299 - val_loss: 1125.4420 - val_mae: 1126.1342\n",
      "Epoch 4048/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 57.5974 - mae: 58.2674 - val_loss: 1116.5216 - val_mae: 1117.2146\n",
      "Epoch 4049/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 61.7994 - mae: 62.4779 - val_loss: 1124.1447 - val_mae: 1124.8369\n",
      "Epoch 4050/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 55.5808 - mae: 56.2559 - val_loss: 1127.1188 - val_mae: 1127.8119\n",
      "Epoch 4051/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 57.1418 - mae: 57.8169 - val_loss: 1134.7688 - val_mae: 1135.4618\n",
      "Epoch 4052/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 59.8658 - mae: 60.5410 - val_loss: 1108.5342 - val_mae: 1109.2273\n",
      "Epoch 4053/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 53.2925 - mae: 53.9681 - val_loss: 1109.3451 - val_mae: 1110.0382\n",
      "Epoch 4054/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 52.5993 - mae: 53.2749 - val_loss: 1110.1469 - val_mae: 1110.8398\n",
      "Epoch 4055/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 55.9221 - mae: 56.5968 - val_loss: 1118.0769 - val_mae: 1118.7698\n",
      "Epoch 4056/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 54.1777 - mae: 54.8470 - val_loss: 1128.6309 - val_mae: 1129.3234\n",
      "Epoch 4057/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 57.3663 - mae: 58.0405 - val_loss: 1152.3351 - val_mae: 1153.0283\n",
      "Epoch 4058/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 54.6717 - mae: 55.3479 - val_loss: 1109.6951 - val_mae: 1110.3877\n",
      "Epoch 4059/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 57.8111 - mae: 58.4881 - val_loss: 1117.7499 - val_mae: 1118.4431\n",
      "Epoch 4060/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 55.1674 - mae: 55.8425 - val_loss: 1138.4325 - val_mae: 1139.1252\n",
      "Epoch 4061/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 56.2843 - mae: 56.9607 - val_loss: 1168.7542 - val_mae: 1169.4474\n",
      "Epoch 4062/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 62.0125 - mae: 62.6883 - val_loss: 1125.3508 - val_mae: 1126.0439\n",
      "Epoch 4063/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 53.4726 - mae: 54.1473 - val_loss: 1121.2550 - val_mae: 1121.9478\n",
      "Epoch 4064/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 66.2866 - mae: 66.9662 - val_loss: 1125.5074 - val_mae: 1126.2003\n",
      "Epoch 4065/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 72.6676 - mae: 73.3491 - val_loss: 1148.8885 - val_mae: 1149.5814\n",
      "Epoch 4066/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 71.9241 - mae: 72.6029 - val_loss: 1137.9509 - val_mae: 1138.6442\n",
      "Epoch 4067/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 57.3619 - mae: 58.0347 - val_loss: 1159.7255 - val_mae: 1160.4187\n",
      "Epoch 4068/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 59.1026 - mae: 59.7789 - val_loss: 1129.7775 - val_mae: 1130.4703\n",
      "Epoch 4069/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 60.1199 - mae: 60.7997 - val_loss: 1106.5637 - val_mae: 1107.2568\n",
      "Epoch 4070/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 55.1050 - mae: 55.7804 - val_loss: 1129.3171 - val_mae: 1130.0103\n",
      "Epoch 4071/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 61.3893 - mae: 62.0697 - val_loss: 1142.1235 - val_mae: 1142.8169\n",
      "Epoch 4072/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 58.7801 - mae: 59.4515 - val_loss: 1135.9313 - val_mae: 1136.6244\n",
      "Epoch 4073/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 59.3635 - mae: 60.0375 - val_loss: 1128.5205 - val_mae: 1129.2136\n",
      "Epoch 4074/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 53.6053 - mae: 54.2825 - val_loss: 1134.1870 - val_mae: 1134.8787\n",
      "Epoch 4075/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 56.0015 - mae: 56.6738 - val_loss: 1114.8438 - val_mae: 1115.5366\n",
      "Epoch 4076/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 53.2158 - mae: 53.8954 - val_loss: 1128.4137 - val_mae: 1129.1066\n",
      "Epoch 4077/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 56.1954 - mae: 56.8701 - val_loss: 1171.4463 - val_mae: 1172.1395\n",
      "Epoch 4078/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 64.9068 - mae: 65.5833 - val_loss: 1120.8542 - val_mae: 1121.5471\n",
      "Epoch 4079/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 65.2447 - mae: 65.9203 - val_loss: 1117.3212 - val_mae: 1118.0144\n",
      "Epoch 4080/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 56.1017 - mae: 56.7757 - val_loss: 1135.8024 - val_mae: 1136.4955\n",
      "Epoch 4081/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 54.1280 - mae: 54.8031 - val_loss: 1175.7252 - val_mae: 1176.4185\n",
      "Epoch 4082/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 56.1785 - mae: 56.8540 - val_loss: 1133.0952 - val_mae: 1133.7883\n",
      "Epoch 4083/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 64.3977 - mae: 65.0767 - val_loss: 1140.0339 - val_mae: 1140.7263\n",
      "Epoch 4084/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 57.1640 - mae: 57.8387 - val_loss: 1133.5466 - val_mae: 1134.2396\n",
      "Epoch 4085/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 59.0741 - mae: 59.7495 - val_loss: 1134.7472 - val_mae: 1135.4393\n",
      "Epoch 4086/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 51.5838 - mae: 52.2572 - val_loss: 1163.1472 - val_mae: 1163.8400\n",
      "Epoch 4087/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 57.9742 - mae: 58.6529 - val_loss: 1143.9891 - val_mae: 1144.6818\n",
      "Epoch 4088/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 57.8753 - mae: 58.5527 - val_loss: 1160.9133 - val_mae: 1161.6063\n",
      "Epoch 4089/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 55.9392 - mae: 56.6143 - val_loss: 1129.4186 - val_mae: 1130.1117\n",
      "Epoch 4090/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 56.1561 - mae: 56.8354 - val_loss: 1137.4944 - val_mae: 1138.1876\n",
      "Epoch 4091/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 56.4429 - mae: 57.1171 - val_loss: 1148.4611 - val_mae: 1149.1544\n",
      "Epoch 4092/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 60.8062 - mae: 61.4851 - val_loss: 1174.8302 - val_mae: 1175.5232\n",
      "Epoch 4093/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 56.1372 - mae: 56.8128 - val_loss: 1156.7697 - val_mae: 1157.4625\n",
      "Epoch 4094/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 57.8899 - mae: 58.5664 - val_loss: 1117.7637 - val_mae: 1118.4569\n",
      "Epoch 4095/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 57.2837 - mae: 57.9602 - val_loss: 1107.4954 - val_mae: 1108.1882\n",
      "Epoch 4096/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 54.9290 - mae: 55.6065 - val_loss: 1128.0320 - val_mae: 1128.7251\n",
      "Epoch 4097/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 54.7115 - mae: 55.3906 - val_loss: 1123.5143 - val_mae: 1124.2069\n",
      "Epoch 4098/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 56.2707 - mae: 56.9434 - val_loss: 1125.6785 - val_mae: 1126.3716\n",
      "Epoch 4099/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 58.3799 - mae: 59.0537 - val_loss: 1226.0337 - val_mae: 1226.7251\n",
      "Epoch 4100/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 63.9890 - mae: 64.6647 - val_loss: 1142.4982 - val_mae: 1143.1912\n",
      "Epoch 4101/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 68.1086 - mae: 68.7871 - val_loss: 1124.6642 - val_mae: 1125.3572\n",
      "Epoch 4102/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 57.0457 - mae: 57.7242 - val_loss: 1144.4531 - val_mae: 1145.1462\n",
      "Epoch 4103/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 55.7659 - mae: 56.4416 - val_loss: 1142.1315 - val_mae: 1142.8245\n",
      "Epoch 4104/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 58.4948 - mae: 59.1715 - val_loss: 1149.3943 - val_mae: 1150.0875\n",
      "Epoch 4105/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 59.0957 - mae: 59.7703 - val_loss: 1150.4034 - val_mae: 1151.0966\n",
      "Epoch 4106/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 55.8407 - mae: 56.5180 - val_loss: 1142.0591 - val_mae: 1142.7523\n",
      "Epoch 4107/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 58.9648 - mae: 59.6459 - val_loss: 1152.9956 - val_mae: 1153.6881\n",
      "Epoch 4108/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 71.1632 - mae: 71.8421 - val_loss: 1180.6689 - val_mae: 1181.3615\n",
      "Epoch 4109/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 63.3528 - mae: 64.0287 - val_loss: 1156.4829 - val_mae: 1157.1759\n",
      "Epoch 4110/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 58.6482 - mae: 59.3225 - val_loss: 1147.1937 - val_mae: 1147.8868\n",
      "Epoch 4111/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 65.3854 - mae: 66.0684 - val_loss: 1173.0177 - val_mae: 1173.7107\n",
      "Epoch 4112/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 59.3694 - mae: 60.0467 - val_loss: 1130.6084 - val_mae: 1131.3010\n",
      "Epoch 4113/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 63.2075 - mae: 63.8847 - val_loss: 1130.1444 - val_mae: 1130.8365\n",
      "Epoch 4114/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 58.1157 - mae: 58.7918 - val_loss: 1144.6139 - val_mae: 1145.3055\n",
      "Epoch 4115/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 58.3953 - mae: 59.0726 - val_loss: 1152.7167 - val_mae: 1153.4097\n",
      "Epoch 4116/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 59.1322 - mae: 59.8055 - val_loss: 1128.2040 - val_mae: 1128.8969\n",
      "Epoch 4117/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 56.0100 - mae: 56.6870 - val_loss: 1135.0950 - val_mae: 1135.7883\n",
      "Epoch 4118/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 63.7367 - mae: 64.4126 - val_loss: 1153.6477 - val_mae: 1154.3407\n",
      "Epoch 4119/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 63.8665 - mae: 64.5436 - val_loss: 1119.5479 - val_mae: 1120.2408\n",
      "Epoch 4120/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 53.0492 - mae: 53.7226 - val_loss: 1137.8690 - val_mae: 1138.5616\n",
      "Epoch 4121/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 51.2914 - mae: 51.9619 - val_loss: 1149.1871 - val_mae: 1149.8804\n",
      "Epoch 4122/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 57.5907 - mae: 58.2688 - val_loss: 1156.8528 - val_mae: 1157.5459\n",
      "Epoch 4123/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 73.5361 - mae: 74.2194 - val_loss: 1213.8152 - val_mae: 1214.5082\n",
      "Epoch 4124/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 63.1700 - mae: 63.8473 - val_loss: 1148.4340 - val_mae: 1149.1272\n",
      "Epoch 4125/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 60.4555 - mae: 61.1327 - val_loss: 1120.9540 - val_mae: 1121.6462\n",
      "Epoch 4126/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 70.7453 - mae: 71.4285 - val_loss: 1147.6309 - val_mae: 1148.3237\n",
      "Epoch 4127/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 57.7377 - mae: 58.4181 - val_loss: 1129.3706 - val_mae: 1130.0638\n",
      "Epoch 4128/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 60.8114 - mae: 61.4889 - val_loss: 1118.0862 - val_mae: 1118.7794\n",
      "Epoch 4129/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 59.4596 - mae: 60.1343 - val_loss: 1127.0118 - val_mae: 1127.7051\n",
      "Epoch 4130/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 58.7551 - mae: 59.4290 - val_loss: 1153.7598 - val_mae: 1154.4524\n",
      "Epoch 4131/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 50.7477 - mae: 51.4241 - val_loss: 1138.0875 - val_mae: 1138.7805\n",
      "Epoch 4132/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 66.5768 - mae: 67.2543 - val_loss: 1201.4313 - val_mae: 1202.1232\n",
      "Epoch 4133/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 65.3033 - mae: 65.9822 - val_loss: 1151.7826 - val_mae: 1152.4758\n",
      "Epoch 4134/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 62.1835 - mae: 62.8647 - val_loss: 1266.3835 - val_mae: 1267.0763\n",
      "Epoch 4135/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 59.1966 - mae: 59.8753 - val_loss: 1141.7302 - val_mae: 1142.4230\n",
      "Epoch 4136/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 60.4114 - mae: 61.0934 - val_loss: 1120.0990 - val_mae: 1120.7916\n",
      "Epoch 4137/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 57.3155 - mae: 57.9913 - val_loss: 1160.8993 - val_mae: 1161.5917\n",
      "Epoch 4138/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 54.1276 - mae: 54.8104 - val_loss: 1216.4076 - val_mae: 1217.1008\n",
      "Epoch 4139/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 59.0035 - mae: 59.6805 - val_loss: 1158.5325 - val_mae: 1159.2253\n",
      "Epoch 4140/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 60.8680 - mae: 61.5437 - val_loss: 1142.4226 - val_mae: 1143.1158\n",
      "Epoch 4141/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 56.6166 - mae: 57.2918 - val_loss: 1142.9203 - val_mae: 1143.6136\n",
      "Epoch 4142/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 59.3496 - mae: 60.0264 - val_loss: 1142.1393 - val_mae: 1142.8313\n",
      "Epoch 4143/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 55.3772 - mae: 56.0482 - val_loss: 1152.5739 - val_mae: 1153.2662\n",
      "Epoch 4144/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 54.0362 - mae: 54.7084 - val_loss: 1134.1782 - val_mae: 1134.8713\n",
      "Epoch 4145/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 60.7996 - mae: 61.4759 - val_loss: 1126.1017 - val_mae: 1126.7947\n",
      "Epoch 4146/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 55.2853 - mae: 55.9597 - val_loss: 1133.8518 - val_mae: 1134.5449\n",
      "Epoch 4147/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 62.7668 - mae: 63.4449 - val_loss: 1146.7087 - val_mae: 1147.4019\n",
      "Epoch 4148/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 54.1005 - mae: 54.7771 - val_loss: 1158.8569 - val_mae: 1159.5500\n",
      "Epoch 4149/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 53.1855 - mae: 53.8615 - val_loss: 1151.0746 - val_mae: 1151.7677\n",
      "Epoch 4150/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 55.7925 - mae: 56.4687 - val_loss: 1170.9259 - val_mae: 1171.6169\n",
      "Epoch 4151/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 54.6596 - mae: 55.3358 - val_loss: 1152.7684 - val_mae: 1153.4609\n",
      "Epoch 4152/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 51.6159 - mae: 52.2843 - val_loss: 1149.4189 - val_mae: 1150.1123\n",
      "Epoch 4153/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 54.7599 - mae: 55.4322 - val_loss: 1128.1967 - val_mae: 1128.8883\n",
      "Epoch 4154/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 58.7190 - mae: 59.3973 - val_loss: 1139.7971 - val_mae: 1140.4904\n",
      "Epoch 4155/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 62.2394 - mae: 62.9184 - val_loss: 1150.8497 - val_mae: 1151.5426\n",
      "Epoch 4156/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 60.1042 - mae: 60.7839 - val_loss: 1155.1725 - val_mae: 1155.8656\n",
      "Epoch 4157/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 55.0005 - mae: 55.6725 - val_loss: 1130.4193 - val_mae: 1131.1124\n",
      "Epoch 4158/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 59.1368 - mae: 59.8160 - val_loss: 1131.6367 - val_mae: 1132.3300\n",
      "Epoch 4159/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 60.5986 - mae: 61.2767 - val_loss: 1163.8151 - val_mae: 1164.5073\n",
      "Epoch 4160/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 50.8110 - mae: 51.4871 - val_loss: 1184.3068 - val_mae: 1185.0000\n",
      "Epoch 4161/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 55.5830 - mae: 56.2588 - val_loss: 1148.7157 - val_mae: 1149.4088\n",
      "Epoch 4162/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 53.5414 - mae: 54.2144 - val_loss: 1153.7722 - val_mae: 1154.4655\n",
      "Epoch 4163/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 56.1753 - mae: 56.8565 - val_loss: 1177.9200 - val_mae: 1178.6132\n",
      "Epoch 4164/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 52.4365 - mae: 53.1132 - val_loss: 1151.5667 - val_mae: 1152.2598\n",
      "Epoch 4165/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 51.2371 - mae: 51.9102 - val_loss: 1130.6539 - val_mae: 1131.3470\n",
      "Epoch 4166/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 54.7343 - mae: 55.4074 - val_loss: 1139.3260 - val_mae: 1140.0190\n",
      "Epoch 4167/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 54.9782 - mae: 55.6558 - val_loss: 1171.6471 - val_mae: 1172.3403\n",
      "Epoch 4168/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 56.9881 - mae: 57.6669 - val_loss: 1145.2277 - val_mae: 1145.9198\n",
      "Epoch 4169/5000\n",
      "46/46 [==============================] - 2s 51ms/step - loss: 54.0680 - mae: 54.7415 - val_loss: 1127.4027 - val_mae: 1128.0955\n",
      "Epoch 4170/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 55.2392 - mae: 55.9126 - val_loss: 1201.1659 - val_mae: 1201.8582\n",
      "Epoch 4171/5000\n",
      "46/46 [==============================] - 2s 42ms/step - loss: 59.8250 - mae: 60.5003 - val_loss: 1148.9845 - val_mae: 1149.6776\n",
      "Epoch 4172/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 57.7197 - mae: 58.3962 - val_loss: 1140.7218 - val_mae: 1141.4150\n",
      "Epoch 4173/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 54.3602 - mae: 55.0323 - val_loss: 1169.3926 - val_mae: 1170.0858\n",
      "Epoch 4174/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 56.3357 - mae: 57.0107 - val_loss: 1148.9114 - val_mae: 1149.6046\n",
      "Epoch 4175/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 53.6029 - mae: 54.2792 - val_loss: 1155.9723 - val_mae: 1156.6654\n",
      "Epoch 4176/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 51.9282 - mae: 52.5991 - val_loss: 1141.6782 - val_mae: 1142.3711\n",
      "Epoch 4177/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 57.8413 - mae: 58.5188 - val_loss: 1158.1431 - val_mae: 1158.8361\n",
      "Epoch 4178/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 61.5864 - mae: 62.2672 - val_loss: 1134.9139 - val_mae: 1135.6069\n",
      "Epoch 4179/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 55.6970 - mae: 56.3731 - val_loss: 1164.2930 - val_mae: 1164.9860\n",
      "Epoch 4180/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 61.2859 - mae: 61.9585 - val_loss: 1213.9117 - val_mae: 1214.6050\n",
      "Epoch 4181/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 58.0106 - mae: 58.6845 - val_loss: 1146.3685 - val_mae: 1147.0615\n",
      "Epoch 4182/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 56.7829 - mae: 57.4536 - val_loss: 1139.8513 - val_mae: 1140.5433\n",
      "Epoch 4183/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 50.3341 - mae: 51.0050 - val_loss: 1138.6466 - val_mae: 1139.3397\n",
      "Epoch 4184/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 53.1645 - mae: 53.8441 - val_loss: 1148.4000 - val_mae: 1149.0933\n",
      "Epoch 4185/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 53.1982 - mae: 53.8737 - val_loss: 1144.2516 - val_mae: 1144.9446\n",
      "Epoch 4186/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 54.8340 - mae: 55.5096 - val_loss: 1141.2192 - val_mae: 1141.9125\n",
      "Epoch 4187/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 51.0207 - mae: 51.7000 - val_loss: 1136.7321 - val_mae: 1137.4246\n",
      "Epoch 4188/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 57.7466 - mae: 58.4247 - val_loss: 1150.0824 - val_mae: 1150.7755\n",
      "Epoch 4189/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 52.7195 - mae: 53.3948 - val_loss: 1144.8022 - val_mae: 1145.4944\n",
      "Epoch 4190/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 50.9889 - mae: 51.6625 - val_loss: 1150.6808 - val_mae: 1151.3738\n",
      "Epoch 4191/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 55.5095 - mae: 56.1863 - val_loss: 1133.0327 - val_mae: 1133.7252\n",
      "Epoch 4192/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 60.1671 - mae: 60.8416 - val_loss: 1143.1372 - val_mae: 1143.8304\n",
      "Epoch 4193/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 52.2989 - mae: 52.9773 - val_loss: 1145.0184 - val_mae: 1145.7115\n",
      "Epoch 4194/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 51.8471 - mae: 52.5192 - val_loss: 1143.6687 - val_mae: 1144.3617\n",
      "Epoch 4195/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 65.2351 - mae: 65.9160 - val_loss: 1144.6763 - val_mae: 1145.3687\n",
      "Epoch 4196/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 58.1727 - mae: 58.8497 - val_loss: 1156.6996 - val_mae: 1157.3926\n",
      "Epoch 4197/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 61.7719 - mae: 62.4490 - val_loss: 1144.5508 - val_mae: 1145.2429\n",
      "Epoch 4198/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 53.4859 - mae: 54.1597 - val_loss: 1126.4100 - val_mae: 1127.1031\n",
      "Epoch 4199/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 56.2262 - mae: 56.9022 - val_loss: 1159.0260 - val_mae: 1159.7191\n",
      "Epoch 4200/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 61.0888 - mae: 61.7670 - val_loss: 1155.8485 - val_mae: 1156.5415\n",
      "Epoch 4201/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 53.7735 - mae: 54.4507 - val_loss: 1144.7086 - val_mae: 1145.4017\n",
      "Epoch 4202/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 53.7472 - mae: 54.4216 - val_loss: 1137.6201 - val_mae: 1138.3121\n",
      "Epoch 4203/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 52.5877 - mae: 53.2660 - val_loss: 1139.5903 - val_mae: 1140.2833\n",
      "Epoch 4204/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 53.9130 - mae: 54.5880 - val_loss: 1142.1874 - val_mae: 1142.8794\n",
      "Epoch 4205/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 63.5748 - mae: 64.2501 - val_loss: 1158.4213 - val_mae: 1159.1144\n",
      "Epoch 4206/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 63.4863 - mae: 64.1662 - val_loss: 1148.0472 - val_mae: 1148.7405\n",
      "Epoch 4207/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 57.4661 - mae: 58.1429 - val_loss: 1140.5773 - val_mae: 1141.2704\n",
      "Epoch 4208/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 60.1203 - mae: 60.8006 - val_loss: 1137.9659 - val_mae: 1138.6591\n",
      "Epoch 4209/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 60.0844 - mae: 60.7640 - val_loss: 1144.8431 - val_mae: 1145.5363\n",
      "Epoch 4210/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 66.6068 - mae: 67.2857 - val_loss: 1165.3339 - val_mae: 1166.0258\n",
      "Epoch 4211/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 58.8484 - mae: 59.5267 - val_loss: 1136.9197 - val_mae: 1137.6130\n",
      "Epoch 4212/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 58.7976 - mae: 59.4766 - val_loss: 1162.7434 - val_mae: 1163.4353\n",
      "Epoch 4213/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 55.1519 - mae: 55.8306 - val_loss: 1168.3459 - val_mae: 1169.0393\n",
      "Epoch 4214/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 54.7340 - mae: 55.4062 - val_loss: 1140.8569 - val_mae: 1141.5500\n",
      "Epoch 4215/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 62.4366 - mae: 63.1127 - val_loss: 1163.3239 - val_mae: 1164.0167\n",
      "Epoch 4216/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 64.1660 - mae: 64.8407 - val_loss: 1146.5822 - val_mae: 1147.2747\n",
      "Epoch 4217/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 57.0008 - mae: 57.6753 - val_loss: 1143.9474 - val_mae: 1144.6405\n",
      "Epoch 4218/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 54.4014 - mae: 55.0741 - val_loss: 1141.9038 - val_mae: 1142.5962\n",
      "Epoch 4219/5000\n",
      "46/46 [==============================] - 2s 43ms/step - loss: 56.4539 - mae: 57.1216 - val_loss: 1161.0540 - val_mae: 1161.7468\n",
      "Epoch 4220/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 55.6774 - mae: 56.3518 - val_loss: 1143.6672 - val_mae: 1144.3597\n",
      "Epoch 4221/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 64.3094 - mae: 64.9855 - val_loss: 1157.5006 - val_mae: 1158.1934\n",
      "Epoch 4222/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 55.1183 - mae: 55.7938 - val_loss: 1177.5852 - val_mae: 1178.2772\n",
      "Epoch 4223/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 59.1989 - mae: 59.8759 - val_loss: 1134.3936 - val_mae: 1135.0867\n",
      "Epoch 4224/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 59.1371 - mae: 59.8201 - val_loss: 1143.3253 - val_mae: 1144.0184\n",
      "Epoch 4225/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 53.5836 - mae: 54.2553 - val_loss: 1148.4158 - val_mae: 1149.1088\n",
      "Epoch 4226/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 60.0224 - mae: 60.7009 - val_loss: 1148.2124 - val_mae: 1148.9054\n",
      "Epoch 4227/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 58.5168 - mae: 59.1965 - val_loss: 1150.7115 - val_mae: 1151.4047\n",
      "Epoch 4228/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 57.0988 - mae: 57.7769 - val_loss: 1176.5190 - val_mae: 1177.2123\n",
      "Epoch 4229/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 69.6363 - mae: 70.3165 - val_loss: 1154.6543 - val_mae: 1155.3467\n",
      "Epoch 4230/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 56.9921 - mae: 57.6667 - val_loss: 1180.7634 - val_mae: 1181.4565\n",
      "Epoch 4231/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 57.6035 - mae: 58.2845 - val_loss: 1159.4692 - val_mae: 1160.1625\n",
      "Epoch 4232/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 55.2339 - mae: 55.9087 - val_loss: 1145.1522 - val_mae: 1145.8453\n",
      "Epoch 4233/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 51.1916 - mae: 51.8638 - val_loss: 1148.6411 - val_mae: 1149.3340\n",
      "Epoch 4234/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 62.4015 - mae: 63.0777 - val_loss: 1181.9169 - val_mae: 1182.6097\n",
      "Epoch 4235/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 61.8441 - mae: 62.5249 - val_loss: 1146.0211 - val_mae: 1146.7144\n",
      "Epoch 4236/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 58.7857 - mae: 59.4633 - val_loss: 1171.2035 - val_mae: 1171.8951\n",
      "Epoch 4237/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 57.3861 - mae: 58.0680 - val_loss: 1135.2874 - val_mae: 1135.9795\n",
      "Epoch 4238/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 57.2067 - mae: 57.8857 - val_loss: 1153.4412 - val_mae: 1154.1343\n",
      "Epoch 4239/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 55.7239 - mae: 56.4061 - val_loss: 1161.5369 - val_mae: 1162.2300\n",
      "Epoch 4240/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 59.3129 - mae: 59.9873 - val_loss: 1174.2545 - val_mae: 1174.9474\n",
      "Epoch 4241/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 54.0076 - mae: 54.6811 - val_loss: 1165.9777 - val_mae: 1166.6707\n",
      "Epoch 4242/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 53.4172 - mae: 54.0939 - val_loss: 1202.7073 - val_mae: 1203.4005\n",
      "Epoch 4243/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 59.6734 - mae: 60.3504 - val_loss: 1142.5575 - val_mae: 1143.2509\n",
      "Epoch 4244/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 57.3563 - mae: 58.0308 - val_loss: 1140.7526 - val_mae: 1141.4456\n",
      "Epoch 4245/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 59.2486 - mae: 59.9242 - val_loss: 1124.1973 - val_mae: 1124.8904\n",
      "Epoch 4246/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 53.9639 - mae: 54.6345 - val_loss: 1166.6783 - val_mae: 1167.3715\n",
      "Epoch 4247/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 56.7028 - mae: 57.3773 - val_loss: 1174.9904 - val_mae: 1175.6835\n",
      "Epoch 4248/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 60.3745 - mae: 61.0492 - val_loss: 1153.9240 - val_mae: 1154.6173\n",
      "Epoch 4249/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 53.6553 - mae: 54.3297 - val_loss: 1163.5725 - val_mae: 1164.2646\n",
      "Epoch 4250/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 62.2170 - mae: 62.8955 - val_loss: 1155.7874 - val_mae: 1156.4795\n",
      "Epoch 4251/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 60.2156 - mae: 60.8917 - val_loss: 1173.1145 - val_mae: 1173.8077\n",
      "Epoch 4252/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 55.7307 - mae: 56.4044 - val_loss: 1135.8313 - val_mae: 1136.5245\n",
      "Epoch 4253/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 52.5061 - mae: 53.1847 - val_loss: 1147.5243 - val_mae: 1148.2173\n",
      "Epoch 4254/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 59.6282 - mae: 60.3052 - val_loss: 1133.2975 - val_mae: 1133.9906\n",
      "Epoch 4255/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 54.8916 - mae: 55.5665 - val_loss: 1147.4341 - val_mae: 1148.1271\n",
      "Epoch 4256/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 64.1649 - mae: 64.8477 - val_loss: 1186.2035 - val_mae: 1186.8962\n",
      "Epoch 4257/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 62.0686 - mae: 62.7506 - val_loss: 1186.4331 - val_mae: 1187.1260\n",
      "Epoch 4258/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 56.9504 - mae: 57.6240 - val_loss: 1136.0945 - val_mae: 1136.7876\n",
      "Epoch 4259/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 51.8988 - mae: 52.5718 - val_loss: 1153.4525 - val_mae: 1154.1458\n",
      "Epoch 4260/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 65.3803 - mae: 66.0556 - val_loss: 1159.1328 - val_mae: 1159.8256\n",
      "Epoch 4261/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 61.0185 - mae: 61.6953 - val_loss: 1153.5848 - val_mae: 1154.2776\n",
      "Epoch 4262/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 52.4735 - mae: 53.1502 - val_loss: 1148.0453 - val_mae: 1148.7371\n",
      "Epoch 4263/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 53.6849 - mae: 54.3573 - val_loss: 1150.8507 - val_mae: 1151.5438\n",
      "Epoch 4264/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 54.9134 - mae: 55.5966 - val_loss: 1154.1334 - val_mae: 1154.8258\n",
      "Epoch 4265/5000\n",
      "46/46 [==============================] - 2s 32ms/step - loss: 67.5886 - mae: 68.2642 - val_loss: 1152.8950 - val_mae: 1153.5883\n",
      "Epoch 4266/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 53.4993 - mae: 54.1737 - val_loss: 1168.2679 - val_mae: 1168.9611\n",
      "Epoch 4267/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 62.8486 - mae: 63.5224 - val_loss: 1168.3602 - val_mae: 1169.0535\n",
      "Epoch 4268/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 61.3476 - mae: 62.0262 - val_loss: 1160.0917 - val_mae: 1160.7848\n",
      "Epoch 4269/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 71.4440 - mae: 72.1211 - val_loss: 1177.3274 - val_mae: 1178.0203\n",
      "Epoch 4270/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 56.2580 - mae: 56.9389 - val_loss: 1206.3195 - val_mae: 1207.0127\n",
      "Epoch 4271/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 66.0073 - mae: 66.6836 - val_loss: 1152.4325 - val_mae: 1153.1257\n",
      "Epoch 4272/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 49.4611 - mae: 50.1339 - val_loss: 1167.0591 - val_mae: 1167.7521\n",
      "Epoch 4273/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 54.4701 - mae: 55.1450 - val_loss: 1126.6396 - val_mae: 1127.3330\n",
      "Epoch 4274/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 53.1633 - mae: 53.8349 - val_loss: 1144.4406 - val_mae: 1145.1329\n",
      "Epoch 4275/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 53.6005 - mae: 54.2773 - val_loss: 1143.4480 - val_mae: 1144.1412\n",
      "Epoch 4276/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 53.0650 - mae: 53.7372 - val_loss: 1160.6102 - val_mae: 1161.3033\n",
      "Epoch 4277/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 59.7196 - mae: 60.3968 - val_loss: 1167.0757 - val_mae: 1167.7686\n",
      "Epoch 4278/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 55.0079 - mae: 55.6823 - val_loss: 1156.7189 - val_mae: 1157.4120\n",
      "Epoch 4279/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 54.7006 - mae: 55.3796 - val_loss: 1125.7078 - val_mae: 1126.4010\n",
      "Epoch 4280/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 53.0871 - mae: 53.7603 - val_loss: 1118.1538 - val_mae: 1118.8469\n",
      "Epoch 4281/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 57.1614 - mae: 57.8363 - val_loss: 1165.3619 - val_mae: 1166.0548\n",
      "Epoch 4282/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 58.7818 - mae: 59.4571 - val_loss: 1150.6823 - val_mae: 1151.3750\n",
      "Epoch 4283/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 50.6380 - mae: 51.3083 - val_loss: 1155.2072 - val_mae: 1155.8993\n",
      "Epoch 4284/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 56.6002 - mae: 57.2772 - val_loss: 1171.9363 - val_mae: 1172.6282\n",
      "Epoch 4285/5000\n",
      "46/46 [==============================] - 2s 40ms/step - loss: 60.9795 - mae: 61.6597 - val_loss: 1124.1504 - val_mae: 1124.8435\n",
      "Epoch 4286/5000\n",
      "46/46 [==============================] - 2s 42ms/step - loss: 59.5096 - mae: 60.1915 - val_loss: 1139.9657 - val_mae: 1140.6589\n",
      "Epoch 4287/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 57.8502 - mae: 58.5290 - val_loss: 1122.3123 - val_mae: 1123.0039\n",
      "Epoch 4288/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 54.0689 - mae: 54.7455 - val_loss: 1137.0370 - val_mae: 1137.7294\n",
      "Epoch 4289/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 59.3367 - mae: 60.0094 - val_loss: 1146.0785 - val_mae: 1146.7717\n",
      "Epoch 4290/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 63.8595 - mae: 64.5363 - val_loss: 1133.6724 - val_mae: 1134.3654\n",
      "Epoch 4291/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 57.9318 - mae: 58.6062 - val_loss: 1167.8024 - val_mae: 1168.4952\n",
      "Epoch 4292/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 55.4868 - mae: 56.1624 - val_loss: 1143.3163 - val_mae: 1144.0092\n",
      "Epoch 4293/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 60.8232 - mae: 61.4980 - val_loss: 1191.5668 - val_mae: 1192.2593\n",
      "Epoch 4294/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 54.2882 - mae: 54.9667 - val_loss: 1144.1589 - val_mae: 1144.8519\n",
      "Epoch 4295/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 55.6226 - mae: 56.2984 - val_loss: 1132.1458 - val_mae: 1132.8391\n",
      "Epoch 4296/5000\n",
      "46/46 [==============================] - 2s 41ms/step - loss: 52.4239 - mae: 53.0982 - val_loss: 1172.2817 - val_mae: 1172.9746\n",
      "Epoch 4297/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 50.6472 - mae: 51.3175 - val_loss: 1143.1802 - val_mae: 1143.8722\n",
      "Epoch 4298/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 49.0223 - mae: 49.6964 - val_loss: 1143.3479 - val_mae: 1144.0410\n",
      "Epoch 4299/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 57.8583 - mae: 58.5356 - val_loss: 1153.2919 - val_mae: 1153.9843\n",
      "Epoch 4300/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 52.6038 - mae: 53.2805 - val_loss: 1169.3159 - val_mae: 1170.0090\n",
      "Epoch 4301/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 50.6845 - mae: 51.3626 - val_loss: 1161.5305 - val_mae: 1162.2236\n",
      "Epoch 4302/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 54.1066 - mae: 54.7795 - val_loss: 1151.5582 - val_mae: 1152.2506\n",
      "Epoch 4303/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 58.4637 - mae: 59.1369 - val_loss: 1116.2131 - val_mae: 1116.9062\n",
      "Epoch 4304/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 59.6447 - mae: 60.3202 - val_loss: 1159.6351 - val_mae: 1160.3281\n",
      "Epoch 4305/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 58.5322 - mae: 59.2105 - val_loss: 1148.9451 - val_mae: 1149.6383\n",
      "Epoch 4306/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 54.1296 - mae: 54.8072 - val_loss: 1153.8052 - val_mae: 1154.4982\n",
      "Epoch 4307/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 61.4153 - mae: 62.0898 - val_loss: 1132.2522 - val_mae: 1132.9453\n",
      "Epoch 4308/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 57.6803 - mae: 58.3571 - val_loss: 1163.1183 - val_mae: 1163.8114\n",
      "Epoch 4309/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 58.8212 - mae: 59.5002 - val_loss: 1145.0742 - val_mae: 1145.7672\n",
      "Epoch 4310/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 54.3652 - mae: 55.0437 - val_loss: 1138.7078 - val_mae: 1139.4010\n",
      "Epoch 4311/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 70.3865 - mae: 71.0660 - val_loss: 1166.8785 - val_mae: 1167.5718\n",
      "Epoch 4312/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 50.4731 - mae: 51.1469 - val_loss: 1155.4700 - val_mae: 1156.1632\n",
      "Epoch 4313/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 59.3583 - mae: 60.0372 - val_loss: 1162.4912 - val_mae: 1163.1842\n",
      "Epoch 4314/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 59.4087 - mae: 60.0858 - val_loss: 1133.4177 - val_mae: 1134.1106\n",
      "Epoch 4315/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 52.1109 - mae: 52.7854 - val_loss: 1133.4631 - val_mae: 1134.1560\n",
      "Epoch 4316/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 55.6892 - mae: 56.3636 - val_loss: 1135.6233 - val_mae: 1136.3165\n",
      "Epoch 4317/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 63.4750 - mae: 64.1556 - val_loss: 1149.7273 - val_mae: 1150.4194\n",
      "Epoch 4318/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 53.9441 - mae: 54.6194 - val_loss: 1178.8169 - val_mae: 1179.5099\n",
      "Epoch 4319/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 51.7963 - mae: 52.4705 - val_loss: 1153.4109 - val_mae: 1154.1040\n",
      "Epoch 4320/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 58.2950 - mae: 58.9722 - val_loss: 1161.5514 - val_mae: 1162.2438\n",
      "Epoch 4321/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 73.5938 - mae: 74.2711 - val_loss: 1146.4468 - val_mae: 1147.1399\n",
      "Epoch 4322/5000\n",
      "46/46 [==============================] - 2s 42ms/step - loss: 60.9446 - mae: 61.6214 - val_loss: 1127.9158 - val_mae: 1128.6083\n",
      "Epoch 4323/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 55.3112 - mae: 55.9888 - val_loss: 1139.6595 - val_mae: 1140.3525\n",
      "Epoch 4324/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 61.0016 - mae: 61.6763 - val_loss: 1136.5669 - val_mae: 1137.2589\n",
      "Epoch 4325/5000\n",
      "46/46 [==============================] - 2s 32ms/step - loss: 67.2745 - mae: 67.9574 - val_loss: 1154.9344 - val_mae: 1155.6278\n",
      "Epoch 4326/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 52.7911 - mae: 53.4639 - val_loss: 1140.6677 - val_mae: 1141.3607\n",
      "Epoch 4327/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 50.9267 - mae: 51.6017 - val_loss: 1145.9663 - val_mae: 1146.6586\n",
      "Epoch 4328/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 58.6240 - mae: 59.3024 - val_loss: 1158.9355 - val_mae: 1159.6288\n",
      "Epoch 4329/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 53.9679 - mae: 54.6422 - val_loss: 1119.5604 - val_mae: 1120.2534\n",
      "Epoch 4330/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 60.4067 - mae: 61.0817 - val_loss: 1149.0353 - val_mae: 1149.7285\n",
      "Epoch 4331/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 53.2775 - mae: 53.9508 - val_loss: 1138.6715 - val_mae: 1139.3646\n",
      "Epoch 4332/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 58.5870 - mae: 59.2679 - val_loss: 1128.7698 - val_mae: 1129.4629\n",
      "Epoch 4333/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 58.0942 - mae: 58.7763 - val_loss: 1196.3893 - val_mae: 1197.0815\n",
      "Epoch 4334/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 64.2260 - mae: 64.9041 - val_loss: 1156.5717 - val_mae: 1157.2650\n",
      "Epoch 4335/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 59.1992 - mae: 59.8763 - val_loss: 1156.5005 - val_mae: 1157.1937\n",
      "Epoch 4336/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 55.7658 - mae: 56.4411 - val_loss: 1156.6539 - val_mae: 1157.3461\n",
      "Epoch 4337/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 53.7776 - mae: 54.4532 - val_loss: 1143.2573 - val_mae: 1143.9500\n",
      "Epoch 4338/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 55.1570 - mae: 55.8277 - val_loss: 1172.0750 - val_mae: 1172.7672\n",
      "Epoch 4339/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 56.3358 - mae: 57.0137 - val_loss: 1132.6333 - val_mae: 1133.3267\n",
      "Epoch 4340/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 55.6636 - mae: 56.3386 - val_loss: 1148.2509 - val_mae: 1148.9434\n",
      "Epoch 4341/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 63.7118 - mae: 64.3858 - val_loss: 1213.1490 - val_mae: 1213.8418\n",
      "Epoch 4342/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 60.3364 - mae: 61.0110 - val_loss: 1121.9990 - val_mae: 1122.6923\n",
      "Epoch 4343/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 53.2443 - mae: 53.9179 - val_loss: 1137.4730 - val_mae: 1138.1656\n",
      "Epoch 4344/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 65.5589 - mae: 66.2430 - val_loss: 1151.9259 - val_mae: 1152.6191\n",
      "Epoch 4345/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 54.7108 - mae: 55.3846 - val_loss: 1145.2023 - val_mae: 1145.8954\n",
      "Epoch 4346/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 56.5645 - mae: 57.2416 - val_loss: 1146.1754 - val_mae: 1146.8685\n",
      "Epoch 4347/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 62.8489 - mae: 63.5299 - val_loss: 1123.4916 - val_mae: 1124.1841\n",
      "Epoch 4348/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 59.4861 - mae: 60.1603 - val_loss: 1141.2936 - val_mae: 1141.9857\n",
      "Epoch 4349/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 52.0005 - mae: 52.6723 - val_loss: 1137.3818 - val_mae: 1138.0742\n",
      "Epoch 4350/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 49.5798 - mae: 50.2505 - val_loss: 1137.5610 - val_mae: 1138.2543\n",
      "Epoch 4351/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 53.4516 - mae: 54.1266 - val_loss: 1128.8569 - val_mae: 1129.5488\n",
      "Epoch 4352/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 53.0683 - mae: 53.7461 - val_loss: 1130.0609 - val_mae: 1130.7534\n",
      "Epoch 4353/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 52.9950 - mae: 53.6686 - val_loss: 1140.5566 - val_mae: 1141.2496\n",
      "Epoch 4354/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 54.1379 - mae: 54.8167 - val_loss: 1199.1351 - val_mae: 1199.8285\n",
      "Epoch 4355/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 61.0221 - mae: 61.7019 - val_loss: 1155.3877 - val_mae: 1156.0807\n",
      "Epoch 4356/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 50.8721 - mae: 51.5499 - val_loss: 1148.5359 - val_mae: 1149.2281\n",
      "Epoch 4357/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 50.7805 - mae: 51.4498 - val_loss: 1133.5353 - val_mae: 1134.2285\n",
      "Epoch 4358/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 52.8836 - mae: 53.5547 - val_loss: 1155.1323 - val_mae: 1155.8254\n",
      "Epoch 4359/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 61.2630 - mae: 61.9386 - val_loss: 1144.4491 - val_mae: 1145.1420\n",
      "Epoch 4360/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 56.4046 - mae: 57.0842 - val_loss: 1144.0369 - val_mae: 1144.7296\n",
      "Epoch 4361/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 59.5290 - mae: 60.2076 - val_loss: 1165.4501 - val_mae: 1166.1432\n",
      "Epoch 4362/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 52.6712 - mae: 53.3469 - val_loss: 1129.2635 - val_mae: 1129.9565\n",
      "Epoch 4363/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 51.7433 - mae: 52.4176 - val_loss: 1138.7095 - val_mae: 1139.4027\n",
      "Epoch 4364/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 50.5855 - mae: 51.2643 - val_loss: 1156.7701 - val_mae: 1157.4630\n",
      "Epoch 4365/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 54.4483 - mae: 55.1233 - val_loss: 1188.9232 - val_mae: 1189.6163\n",
      "Epoch 4366/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 52.6799 - mae: 53.3565 - val_loss: 1143.6735 - val_mae: 1144.3650\n",
      "Epoch 4367/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 53.0082 - mae: 53.6823 - val_loss: 1119.6729 - val_mae: 1120.3661\n",
      "Epoch 4368/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 52.8923 - mae: 53.5690 - val_loss: 1146.8319 - val_mae: 1147.5253\n",
      "Epoch 4369/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 49.3769 - mae: 50.0484 - val_loss: 1156.5035 - val_mae: 1157.1965\n",
      "Epoch 4370/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 53.8691 - mae: 54.5466 - val_loss: 1179.9094 - val_mae: 1180.6024\n",
      "Epoch 4371/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 53.5592 - mae: 54.2362 - val_loss: 1146.4911 - val_mae: 1147.1833\n",
      "Epoch 4372/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 57.1344 - mae: 57.8098 - val_loss: 1154.1719 - val_mae: 1154.8650\n",
      "Epoch 4373/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 54.2032 - mae: 54.8838 - val_loss: 1154.5809 - val_mae: 1155.2731\n",
      "Epoch 4374/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 56.3845 - mae: 57.0568 - val_loss: 1152.5411 - val_mae: 1153.2343\n",
      "Epoch 4375/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 54.4669 - mae: 55.1471 - val_loss: 1165.9279 - val_mae: 1166.6208\n",
      "Epoch 4376/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 53.1003 - mae: 53.7787 - val_loss: 1171.7217 - val_mae: 1172.4149\n",
      "Epoch 4377/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 54.9488 - mae: 55.6237 - val_loss: 1159.3070 - val_mae: 1160.0002\n",
      "Epoch 4378/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 56.5842 - mae: 57.2612 - val_loss: 1159.8623 - val_mae: 1160.5546\n",
      "Epoch 4379/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 53.0020 - mae: 53.6746 - val_loss: 1136.0248 - val_mae: 1136.7178\n",
      "Epoch 4380/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 57.8882 - mae: 58.5628 - val_loss: 1140.0634 - val_mae: 1140.7567\n",
      "Epoch 4381/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 56.7938 - mae: 57.4711 - val_loss: 1159.6785 - val_mae: 1160.3715\n",
      "Epoch 4382/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 48.8435 - mae: 49.5201 - val_loss: 1152.8611 - val_mae: 1153.5541\n",
      "Epoch 4383/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 53.6540 - mae: 54.3333 - val_loss: 1163.0562 - val_mae: 1163.7493\n",
      "Epoch 4384/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 53.3019 - mae: 53.9737 - val_loss: 1156.3197 - val_mae: 1157.0128\n",
      "Epoch 4385/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 52.1526 - mae: 52.8269 - val_loss: 1152.3680 - val_mae: 1153.0610\n",
      "Epoch 4386/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 57.1211 - mae: 57.7992 - val_loss: 1141.9004 - val_mae: 1142.5936\n",
      "Epoch 4387/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 51.6783 - mae: 52.3494 - val_loss: 1154.8171 - val_mae: 1155.5101\n",
      "Epoch 4388/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 56.9006 - mae: 57.5782 - val_loss: 1148.5538 - val_mae: 1149.2466\n",
      "Epoch 4389/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 59.3662 - mae: 60.0488 - val_loss: 1175.0457 - val_mae: 1175.7390\n",
      "Epoch 4390/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 54.4273 - mae: 55.1051 - val_loss: 1139.6755 - val_mae: 1140.3688\n",
      "Epoch 4391/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 53.0189 - mae: 53.6921 - val_loss: 1153.4375 - val_mae: 1154.1299\n",
      "Epoch 4392/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 55.6100 - mae: 56.2863 - val_loss: 1151.4707 - val_mae: 1152.1628\n",
      "Epoch 4393/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 51.9250 - mae: 52.6011 - val_loss: 1154.1335 - val_mae: 1154.8267\n",
      "Epoch 4394/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 53.3390 - mae: 54.0113 - val_loss: 1148.6133 - val_mae: 1149.3065\n",
      "Epoch 4395/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 48.5909 - mae: 49.2592 - val_loss: 1136.2991 - val_mae: 1136.9910\n",
      "Epoch 4396/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 57.3280 - mae: 58.0054 - val_loss: 1147.5696 - val_mae: 1148.2618\n",
      "Epoch 4397/5000\n",
      "46/46 [==============================] - 2s 41ms/step - loss: 64.1031 - mae: 64.7821 - val_loss: 1155.2692 - val_mae: 1155.9624\n",
      "Epoch 4398/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 55.2996 - mae: 55.9764 - val_loss: 1153.0526 - val_mae: 1153.7457\n",
      "Epoch 4399/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 56.7587 - mae: 57.4406 - val_loss: 1135.9420 - val_mae: 1136.6351\n",
      "Epoch 4400/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 54.9378 - mae: 55.6186 - val_loss: 1169.3687 - val_mae: 1170.0616\n",
      "Epoch 4401/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 63.1136 - mae: 63.7958 - val_loss: 1135.8137 - val_mae: 1136.5067\n",
      "Epoch 4402/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 56.8619 - mae: 57.5361 - val_loss: 1145.3920 - val_mae: 1146.0850\n",
      "Epoch 4403/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 52.9814 - mae: 53.6566 - val_loss: 1155.0255 - val_mae: 1155.7178\n",
      "Epoch 4404/5000\n",
      "46/46 [==============================] - 2s 51ms/step - loss: 53.7580 - mae: 54.4334 - val_loss: 1135.7213 - val_mae: 1136.4142\n",
      "Epoch 4405/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 59.9951 - mae: 60.6705 - val_loss: 1120.3481 - val_mae: 1121.0415\n",
      "Epoch 4406/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 55.2154 - mae: 55.8917 - val_loss: 1135.1759 - val_mae: 1135.8690\n",
      "Epoch 4407/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 52.0300 - mae: 52.7044 - val_loss: 1168.2825 - val_mae: 1168.9753\n",
      "Epoch 4408/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 53.7322 - mae: 54.4068 - val_loss: 1170.0851 - val_mae: 1170.7776\n",
      "Epoch 4409/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 50.0354 - mae: 50.7089 - val_loss: 1153.4229 - val_mae: 1154.1161\n",
      "Epoch 4410/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 68.3056 - mae: 68.9841 - val_loss: 1161.7089 - val_mae: 1162.4017\n",
      "Epoch 4411/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 57.3637 - mae: 58.0359 - val_loss: 1169.4060 - val_mae: 1170.0981\n",
      "Epoch 4412/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 55.4894 - mae: 56.1664 - val_loss: 1157.0741 - val_mae: 1157.7672\n",
      "Epoch 4413/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 50.6195 - mae: 51.2930 - val_loss: 1124.2346 - val_mae: 1124.9280\n",
      "Epoch 4414/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 50.0392 - mae: 50.7155 - val_loss: 1140.0402 - val_mae: 1140.7333\n",
      "Epoch 4415/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 59.0558 - mae: 59.7362 - val_loss: 1163.8521 - val_mae: 1164.5441\n",
      "Epoch 4416/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 62.1079 - mae: 62.7862 - val_loss: 1148.4987 - val_mae: 1149.1919\n",
      "Epoch 4417/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 50.7924 - mae: 51.4653 - val_loss: 1128.5283 - val_mae: 1129.2213\n",
      "Epoch 4418/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 55.2140 - mae: 55.8909 - val_loss: 1164.9910 - val_mae: 1165.6841\n",
      "Epoch 4419/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 60.5382 - mae: 61.2146 - val_loss: 1139.0055 - val_mae: 1139.6978\n",
      "Epoch 4420/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 53.5239 - mae: 54.1947 - val_loss: 1155.1331 - val_mae: 1155.8262\n",
      "Epoch 4421/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 58.1472 - mae: 58.8219 - val_loss: 1127.0674 - val_mae: 1127.7604\n",
      "Epoch 4422/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 61.8565 - mae: 62.5306 - val_loss: 1132.7336 - val_mae: 1133.4258\n",
      "Epoch 4423/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 51.3375 - mae: 52.0075 - val_loss: 1136.8341 - val_mae: 1137.5272\n",
      "Epoch 4424/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 53.9483 - mae: 54.6213 - val_loss: 1141.7551 - val_mae: 1142.4471\n",
      "Epoch 4425/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 54.3295 - mae: 55.0116 - val_loss: 1143.4152 - val_mae: 1144.1079\n",
      "Epoch 4426/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 59.6255 - mae: 60.3030 - val_loss: 1150.1659 - val_mae: 1150.8586\n",
      "Epoch 4427/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 57.5391 - mae: 58.2164 - val_loss: 1151.0344 - val_mae: 1151.7264\n",
      "Epoch 4428/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 58.8221 - mae: 59.5029 - val_loss: 1204.6449 - val_mae: 1205.3379\n",
      "Epoch 4429/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 72.6287 - mae: 73.3113 - val_loss: 1131.1229 - val_mae: 1131.8160\n",
      "Epoch 4430/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 56.5277 - mae: 57.2046 - val_loss: 1138.9843 - val_mae: 1139.6772\n",
      "Epoch 4431/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 53.6014 - mae: 54.2761 - val_loss: 1138.9784 - val_mae: 1139.6707\n",
      "Epoch 4432/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 51.5396 - mae: 52.2194 - val_loss: 1161.8395 - val_mae: 1162.5325\n",
      "Epoch 4433/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 61.0922 - mae: 61.7737 - val_loss: 1176.8005 - val_mae: 1177.4938\n",
      "Epoch 4434/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 57.8737 - mae: 58.5466 - val_loss: 1133.4650 - val_mae: 1134.1578\n",
      "Epoch 4435/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 63.5793 - mae: 64.2597 - val_loss: 1132.2130 - val_mae: 1132.9061\n",
      "Epoch 4436/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 55.7485 - mae: 56.4241 - val_loss: 1121.1322 - val_mae: 1121.8256\n",
      "Epoch 4437/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 51.0971 - mae: 51.7687 - val_loss: 1127.1155 - val_mae: 1127.8086\n",
      "Epoch 4438/5000\n",
      "46/46 [==============================] - 2s 41ms/step - loss: 59.7502 - mae: 60.4261 - val_loss: 1129.0417 - val_mae: 1129.7347\n",
      "Epoch 4439/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 63.0512 - mae: 63.7285 - val_loss: 1139.7771 - val_mae: 1140.4702\n",
      "Epoch 4440/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 69.1578 - mae: 69.8376 - val_loss: 1163.3503 - val_mae: 1164.0421\n",
      "Epoch 4441/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 49.5161 - mae: 50.1877 - val_loss: 1143.6927 - val_mae: 1144.3855\n",
      "Epoch 4442/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 61.8033 - mae: 62.4815 - val_loss: 1140.2760 - val_mae: 1140.9691\n",
      "Epoch 4443/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 53.4126 - mae: 54.0904 - val_loss: 1135.8646 - val_mae: 1136.5576\n",
      "Epoch 4444/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 54.6713 - mae: 55.3441 - val_loss: 1158.3147 - val_mae: 1159.0079\n",
      "Epoch 4445/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 54.7278 - mae: 55.4053 - val_loss: 1178.3855 - val_mae: 1179.0784\n",
      "Epoch 4446/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 68.5748 - mae: 69.2527 - val_loss: 1181.8799 - val_mae: 1182.5715\n",
      "Epoch 4447/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 55.8130 - mae: 56.4926 - val_loss: 1145.5693 - val_mae: 1146.2625\n",
      "Epoch 4448/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 64.2447 - mae: 64.9242 - val_loss: 1176.7102 - val_mae: 1177.4022\n",
      "Epoch 4449/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 55.6160 - mae: 56.2896 - val_loss: 1132.0588 - val_mae: 1132.7518\n",
      "Epoch 4450/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 56.1646 - mae: 56.8375 - val_loss: 1131.6362 - val_mae: 1132.3293\n",
      "Epoch 4451/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 62.7551 - mae: 63.4314 - val_loss: 1146.3313 - val_mae: 1147.0245\n",
      "Epoch 4452/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 56.9267 - mae: 57.5965 - val_loss: 1160.7062 - val_mae: 1161.3992\n",
      "Epoch 4453/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 55.9849 - mae: 56.6641 - val_loss: 1138.2528 - val_mae: 1138.9449\n",
      "Epoch 4454/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 54.9138 - mae: 55.5884 - val_loss: 1136.7773 - val_mae: 1137.4703\n",
      "Epoch 4455/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 59.9086 - mae: 60.5858 - val_loss: 1148.0011 - val_mae: 1148.6941\n",
      "Epoch 4456/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 56.8810 - mae: 57.5588 - val_loss: 1129.4991 - val_mae: 1130.1920\n",
      "Epoch 4457/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 52.7688 - mae: 53.4462 - val_loss: 1129.5936 - val_mae: 1130.2863\n",
      "Epoch 4458/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 54.1204 - mae: 54.7955 - val_loss: 1138.2657 - val_mae: 1138.9586\n",
      "Epoch 4459/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 53.4004 - mae: 54.0788 - val_loss: 1136.9105 - val_mae: 1137.6008\n",
      "Epoch 4460/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 56.9218 - mae: 57.5929 - val_loss: 1140.5715 - val_mae: 1141.2646\n",
      "Epoch 4461/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 60.9238 - mae: 61.5968 - val_loss: 1189.6752 - val_mae: 1190.3684\n",
      "Epoch 4462/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 55.1822 - mae: 55.8589 - val_loss: 1158.3906 - val_mae: 1159.0834\n",
      "Epoch 4463/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 53.6311 - mae: 54.3067 - val_loss: 1128.3970 - val_mae: 1129.0901\n",
      "Epoch 4464/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 51.4091 - mae: 52.0813 - val_loss: 1157.7151 - val_mae: 1158.4080\n",
      "Epoch 4465/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 57.0869 - mae: 57.7643 - val_loss: 1169.8625 - val_mae: 1170.5558\n",
      "Epoch 4466/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 58.4145 - mae: 59.0891 - val_loss: 1136.7341 - val_mae: 1137.4268\n",
      "Epoch 4467/5000\n",
      "46/46 [==============================] - 2s 41ms/step - loss: 54.5861 - mae: 55.2620 - val_loss: 1182.4916 - val_mae: 1183.1818\n",
      "Epoch 4468/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 56.1537 - mae: 56.8287 - val_loss: 1170.6699 - val_mae: 1171.3627\n",
      "Epoch 4469/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 57.2555 - mae: 57.9331 - val_loss: 1139.3511 - val_mae: 1140.0444\n",
      "Epoch 4470/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 51.5205 - mae: 52.1929 - val_loss: 1139.9989 - val_mae: 1140.6923\n",
      "Epoch 4471/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 54.1060 - mae: 54.7849 - val_loss: 1127.4061 - val_mae: 1128.0984\n",
      "Epoch 4472/5000\n",
      "46/46 [==============================] - 2s 45ms/step - loss: 52.7330 - mae: 53.4096 - val_loss: 1179.6227 - val_mae: 1180.3157\n",
      "Epoch 4473/5000\n",
      "46/46 [==============================] - 2s 42ms/step - loss: 56.8897 - mae: 57.5649 - val_loss: 1128.3853 - val_mae: 1129.0781\n",
      "Epoch 4474/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 48.8747 - mae: 49.5472 - val_loss: 1144.6821 - val_mae: 1145.3740\n",
      "Epoch 4475/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 56.0998 - mae: 56.7750 - val_loss: 1155.1538 - val_mae: 1155.8456\n",
      "Epoch 4476/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 60.7375 - mae: 61.4182 - val_loss: 1146.7946 - val_mae: 1147.4878\n",
      "Epoch 4477/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 57.1093 - mae: 57.7864 - val_loss: 1161.2810 - val_mae: 1161.9741\n",
      "Epoch 4478/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 57.1998 - mae: 57.8747 - val_loss: 1249.7148 - val_mae: 1250.4077\n",
      "Epoch 4479/5000\n",
      "46/46 [==============================] - 2s 43ms/step - loss: 61.1244 - mae: 61.8000 - val_loss: 1164.8300 - val_mae: 1165.5217\n",
      "Epoch 4480/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 56.7424 - mae: 57.4234 - val_loss: 1163.0649 - val_mae: 1163.7581\n",
      "Epoch 4481/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 49.7814 - mae: 50.4587 - val_loss: 1177.3531 - val_mae: 1178.0453\n",
      "Epoch 4482/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 61.3057 - mae: 61.9838 - val_loss: 1152.2812 - val_mae: 1152.9736\n",
      "Epoch 4483/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 54.7485 - mae: 55.4259 - val_loss: 1159.6335 - val_mae: 1160.3264\n",
      "Epoch 4484/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 57.2994 - mae: 57.9750 - val_loss: 1134.9641 - val_mae: 1135.6572\n",
      "Epoch 4485/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 57.1650 - mae: 57.8435 - val_loss: 1149.1057 - val_mae: 1149.7974\n",
      "Epoch 4486/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 53.0750 - mae: 53.7504 - val_loss: 1133.7780 - val_mae: 1134.4707\n",
      "Epoch 4487/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 54.3873 - mae: 55.0587 - val_loss: 1130.8926 - val_mae: 1131.5854\n",
      "Epoch 4488/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 57.0075 - mae: 57.6834 - val_loss: 1137.6411 - val_mae: 1138.3315\n",
      "Epoch 4489/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 49.7243 - mae: 50.3971 - val_loss: 1143.1667 - val_mae: 1143.8591\n",
      "Epoch 4490/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 52.6496 - mae: 53.3216 - val_loss: 1133.0585 - val_mae: 1133.7517\n",
      "Epoch 4491/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 49.8183 - mae: 50.4923 - val_loss: 1143.8685 - val_mae: 1144.5612\n",
      "Epoch 4492/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 52.0504 - mae: 52.7263 - val_loss: 1135.7146 - val_mae: 1136.4062\n",
      "Epoch 4493/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 51.7590 - mae: 52.4321 - val_loss: 1183.4905 - val_mae: 1184.1836\n",
      "Epoch 4494/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 52.2690 - mae: 52.9476 - val_loss: 1198.7935 - val_mae: 1199.4866\n",
      "Epoch 4495/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 55.2668 - mae: 55.9390 - val_loss: 1150.0469 - val_mae: 1150.7388\n",
      "Epoch 4496/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 52.8195 - mae: 53.4960 - val_loss: 1162.4525 - val_mae: 1163.1453\n",
      "Epoch 4497/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 50.6364 - mae: 51.3086 - val_loss: 1143.0215 - val_mae: 1143.7147\n",
      "Epoch 4498/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 57.1913 - mae: 57.8698 - val_loss: 1182.6584 - val_mae: 1183.3516\n",
      "Epoch 4499/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 50.3527 - mae: 51.0310 - val_loss: 1152.5388 - val_mae: 1153.2314\n",
      "Epoch 4500/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 58.2889 - mae: 58.9671 - val_loss: 1180.9790 - val_mae: 1181.6721\n",
      "Epoch 4501/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 62.3248 - mae: 63.0043 - val_loss: 1168.5903 - val_mae: 1169.2814\n",
      "Epoch 4502/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 64.5325 - mae: 65.2080 - val_loss: 1157.7985 - val_mae: 1158.4917\n",
      "Epoch 4503/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 58.7926 - mae: 59.4709 - val_loss: 1124.8573 - val_mae: 1125.5505\n",
      "Epoch 4504/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 51.0071 - mae: 51.6818 - val_loss: 1179.4015 - val_mae: 1180.0946\n",
      "Epoch 4505/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 51.0385 - mae: 51.7109 - val_loss: 1144.2660 - val_mae: 1144.9587\n",
      "Epoch 4506/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 50.0109 - mae: 50.6851 - val_loss: 1150.5129 - val_mae: 1151.2061\n",
      "Epoch 4507/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 53.1245 - mae: 53.8058 - val_loss: 1131.7797 - val_mae: 1132.4727\n",
      "Epoch 4508/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 55.5854 - mae: 56.2629 - val_loss: 1135.5968 - val_mae: 1136.2899\n",
      "Epoch 4509/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 49.7398 - mae: 50.4161 - val_loss: 1167.9717 - val_mae: 1168.6649\n",
      "Epoch 4510/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 51.3357 - mae: 52.0108 - val_loss: 1154.2626 - val_mae: 1154.9554\n",
      "Epoch 4511/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 54.3086 - mae: 54.9874 - val_loss: 1159.6199 - val_mae: 1160.3129\n",
      "Epoch 4512/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 66.4024 - mae: 67.0792 - val_loss: 1174.7451 - val_mae: 1175.4382\n",
      "Epoch 4513/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 59.5845 - mae: 60.2616 - val_loss: 1170.5299 - val_mae: 1171.2229\n",
      "Epoch 4514/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 56.1844 - mae: 56.8580 - val_loss: 1174.2985 - val_mae: 1174.9918\n",
      "Epoch 4515/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 60.0552 - mae: 60.7276 - val_loss: 1151.3055 - val_mae: 1151.9985\n",
      "Epoch 4516/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 54.9109 - mae: 55.5851 - val_loss: 1126.2773 - val_mae: 1126.9695\n",
      "Epoch 4517/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 53.3025 - mae: 53.9771 - val_loss: 1151.9932 - val_mae: 1152.6852\n",
      "Epoch 4518/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 58.3015 - mae: 58.9822 - val_loss: 1144.6621 - val_mae: 1145.3551\n",
      "Epoch 4519/5000\n",
      "46/46 [==============================] - 2s 42ms/step - loss: 67.0835 - mae: 67.7632 - val_loss: 1138.7719 - val_mae: 1139.4650\n",
      "Epoch 4520/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 57.9404 - mae: 58.6174 - val_loss: 1151.6034 - val_mae: 1152.2966\n",
      "Epoch 4521/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 57.7574 - mae: 58.4321 - val_loss: 1123.8212 - val_mae: 1124.5143\n",
      "Epoch 4522/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 55.1386 - mae: 55.8107 - val_loss: 1169.2777 - val_mae: 1169.9705\n",
      "Epoch 4523/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 49.7709 - mae: 50.4394 - val_loss: 1138.1934 - val_mae: 1138.8849\n",
      "Epoch 4524/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 49.2268 - mae: 49.9000 - val_loss: 1149.3695 - val_mae: 1150.0625\n",
      "Epoch 4525/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 60.0338 - mae: 60.7102 - val_loss: 1140.0602 - val_mae: 1140.7531\n",
      "Epoch 4526/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 51.9719 - mae: 52.6486 - val_loss: 1156.6370 - val_mae: 1157.3302\n",
      "Epoch 4527/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 54.7925 - mae: 55.4708 - val_loss: 1148.2864 - val_mae: 1148.9796\n",
      "Epoch 4528/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 52.1153 - mae: 52.7929 - val_loss: 1131.3784 - val_mae: 1132.0714\n",
      "Epoch 4529/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 59.9408 - mae: 60.6170 - val_loss: 1149.4183 - val_mae: 1150.1111\n",
      "Epoch 4530/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 64.1348 - mae: 64.8136 - val_loss: 1147.8727 - val_mae: 1148.5658\n",
      "Epoch 4531/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 54.0068 - mae: 54.6803 - val_loss: 1140.1993 - val_mae: 1140.8915\n",
      "Epoch 4532/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 54.1332 - mae: 54.8068 - val_loss: 1161.4058 - val_mae: 1162.0985\n",
      "Epoch 4533/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 53.8508 - mae: 54.5244 - val_loss: 1158.0588 - val_mae: 1158.7510\n",
      "Epoch 4534/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 56.2238 - mae: 56.8979 - val_loss: 1168.6898 - val_mae: 1169.3829\n",
      "Epoch 4535/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 55.5027 - mae: 56.1822 - val_loss: 1159.7903 - val_mae: 1160.4833\n",
      "Epoch 4536/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 54.6096 - mae: 55.2846 - val_loss: 1176.0162 - val_mae: 1176.7094\n",
      "Epoch 4537/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 67.7106 - mae: 68.3875 - val_loss: 1147.2496 - val_mae: 1147.9426\n",
      "Epoch 4538/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 57.5075 - mae: 58.1877 - val_loss: 1147.2137 - val_mae: 1147.9070\n",
      "Epoch 4539/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 56.3800 - mae: 57.0549 - val_loss: 1140.5634 - val_mae: 1141.2565\n",
      "Epoch 4540/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 58.8562 - mae: 59.5313 - val_loss: 1151.5061 - val_mae: 1152.1993\n",
      "Epoch 4541/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 57.9476 - mae: 58.6284 - val_loss: 1135.5350 - val_mae: 1136.2278\n",
      "Epoch 4542/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 55.2588 - mae: 55.9328 - val_loss: 1139.2881 - val_mae: 1139.9812\n",
      "Epoch 4543/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 54.8951 - mae: 55.5714 - val_loss: 1134.2972 - val_mae: 1134.9906\n",
      "Epoch 4544/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 61.0319 - mae: 61.7080 - val_loss: 1168.3285 - val_mae: 1169.0214\n",
      "Epoch 4545/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 56.3678 - mae: 57.0455 - val_loss: 1138.1653 - val_mae: 1138.8571\n",
      "Epoch 4546/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 52.1962 - mae: 52.8706 - val_loss: 1153.7781 - val_mae: 1154.4701\n",
      "Epoch 4547/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 52.0036 - mae: 52.6834 - val_loss: 1125.0156 - val_mae: 1125.7086\n",
      "Epoch 4548/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 54.8778 - mae: 55.5524 - val_loss: 1153.3785 - val_mae: 1154.0718\n",
      "Epoch 4549/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 67.4557 - mae: 68.1287 - val_loss: 1168.5858 - val_mae: 1169.2791\n",
      "Epoch 4550/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 61.6783 - mae: 62.3614 - val_loss: 1141.1919 - val_mae: 1141.8850\n",
      "Epoch 4551/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 55.1517 - mae: 55.8284 - val_loss: 1147.1471 - val_mae: 1147.8392\n",
      "Epoch 4552/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 57.1785 - mae: 57.8527 - val_loss: 1168.6080 - val_mae: 1169.3009\n",
      "Epoch 4553/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 64.2500 - mae: 64.9294 - val_loss: 1174.4330 - val_mae: 1175.1249\n",
      "Epoch 4554/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 52.8422 - mae: 53.5209 - val_loss: 1172.4286 - val_mae: 1173.1215\n",
      "Epoch 4555/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 62.8587 - mae: 63.5347 - val_loss: 1138.2598 - val_mae: 1138.9529\n",
      "Epoch 4556/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 54.4538 - mae: 55.1264 - val_loss: 1157.5726 - val_mae: 1158.2655\n",
      "Epoch 4557/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 58.1003 - mae: 58.7746 - val_loss: 1149.0363 - val_mae: 1149.7294\n",
      "Epoch 4558/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 48.9588 - mae: 49.6333 - val_loss: 1177.6747 - val_mae: 1178.3667\n",
      "Epoch 4559/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 53.7255 - mae: 54.4017 - val_loss: 1201.0677 - val_mae: 1201.7601\n",
      "Epoch 4560/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 61.7422 - mae: 62.4237 - val_loss: 1171.9709 - val_mae: 1172.6641\n",
      "Epoch 4561/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 51.2399 - mae: 51.9156 - val_loss: 1159.0057 - val_mae: 1159.6979\n",
      "Epoch 4562/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 53.4172 - mae: 54.0913 - val_loss: 1153.6161 - val_mae: 1154.3091\n",
      "Epoch 4563/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 51.3075 - mae: 51.9823 - val_loss: 1142.5588 - val_mae: 1143.2516\n",
      "Epoch 4564/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 59.2721 - mae: 59.9509 - val_loss: 1120.7881 - val_mae: 1121.4806\n",
      "Epoch 4565/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 53.5210 - mae: 54.1978 - val_loss: 1147.0559 - val_mae: 1147.7469\n",
      "Epoch 4566/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 52.0448 - mae: 52.7190 - val_loss: 1192.5649 - val_mae: 1193.2568\n",
      "Epoch 4567/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 57.6780 - mae: 58.3550 - val_loss: 1140.8737 - val_mae: 1141.5668\n",
      "Epoch 4568/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 52.2598 - mae: 52.9357 - val_loss: 1151.3966 - val_mae: 1152.0900\n",
      "Epoch 4569/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 53.0060 - mae: 53.6851 - val_loss: 1158.1234 - val_mae: 1158.8164\n",
      "Epoch 4570/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 57.0727 - mae: 57.7516 - val_loss: 1161.1516 - val_mae: 1161.8440\n",
      "Epoch 4571/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 54.7944 - mae: 55.4690 - val_loss: 1187.2516 - val_mae: 1187.9447\n",
      "Epoch 4572/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 59.2764 - mae: 59.9550 - val_loss: 1145.7948 - val_mae: 1146.4882\n",
      "Epoch 4573/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 56.6093 - mae: 57.2886 - val_loss: 1144.5336 - val_mae: 1145.2262\n",
      "Epoch 4574/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 71.3215 - mae: 72.0040 - val_loss: 1149.9204 - val_mae: 1150.6129\n",
      "Epoch 4575/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 57.6750 - mae: 58.3522 - val_loss: 1136.2703 - val_mae: 1136.9635\n",
      "Epoch 4576/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 61.5710 - mae: 62.2495 - val_loss: 1138.7241 - val_mae: 1139.4164\n",
      "Epoch 4577/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 49.0309 - mae: 49.7061 - val_loss: 1153.9452 - val_mae: 1154.6383\n",
      "Epoch 4578/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 51.6778 - mae: 52.3491 - val_loss: 1157.7915 - val_mae: 1158.4847\n",
      "Epoch 4579/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 63.6704 - mae: 64.3497 - val_loss: 1175.4194 - val_mae: 1176.1124\n",
      "Epoch 4580/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 66.2812 - mae: 66.9597 - val_loss: 1165.7102 - val_mae: 1166.4033\n",
      "Epoch 4581/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 54.3630 - mae: 55.0407 - val_loss: 1151.2115 - val_mae: 1151.9047\n",
      "Epoch 4582/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 52.5834 - mae: 53.2605 - val_loss: 1127.6824 - val_mae: 1128.3744\n",
      "Epoch 4583/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 52.0297 - mae: 52.7049 - val_loss: 1132.8811 - val_mae: 1133.5745\n",
      "Epoch 4584/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 53.1421 - mae: 53.8198 - val_loss: 1153.1830 - val_mae: 1153.8762\n",
      "Epoch 4585/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 46.5872 - mae: 47.2604 - val_loss: 1147.6848 - val_mae: 1148.3779\n",
      "Epoch 4586/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 55.1240 - mae: 55.7990 - val_loss: 1148.2253 - val_mae: 1148.9176\n",
      "Epoch 4587/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 62.7781 - mae: 63.4553 - val_loss: 1152.1136 - val_mae: 1152.8069\n",
      "Epoch 4588/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 50.7001 - mae: 51.3754 - val_loss: 1157.8524 - val_mae: 1158.5454\n",
      "Epoch 4589/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 56.6413 - mae: 57.3221 - val_loss: 1170.8251 - val_mae: 1171.5179\n",
      "Epoch 4590/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 51.2053 - mae: 51.8829 - val_loss: 1145.5403 - val_mae: 1146.2335\n",
      "Epoch 4591/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 55.7699 - mae: 56.4418 - val_loss: 1174.2729 - val_mae: 1174.9663\n",
      "Epoch 4592/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 55.1742 - mae: 55.8502 - val_loss: 1174.9420 - val_mae: 1175.6339\n",
      "Epoch 4593/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 58.2812 - mae: 58.9614 - val_loss: 1149.9517 - val_mae: 1150.6445\n",
      "Epoch 4594/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 49.4158 - mae: 50.0911 - val_loss: 1162.9594 - val_mae: 1163.6527\n",
      "Epoch 4595/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 50.5829 - mae: 51.2606 - val_loss: 1167.5873 - val_mae: 1168.2805\n",
      "Epoch 4596/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 54.3246 - mae: 54.9968 - val_loss: 1176.7820 - val_mae: 1177.4724\n",
      "Epoch 4597/5000\n",
      "46/46 [==============================] - 2s 30ms/step - loss: 51.4135 - mae: 52.0881 - val_loss: 1157.7781 - val_mae: 1158.4712\n",
      "Epoch 4598/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 57.5904 - mae: 58.2673 - val_loss: 1187.7065 - val_mae: 1188.3998\n",
      "Epoch 4599/5000\n",
      "46/46 [==============================] - 2s 40ms/step - loss: 56.4852 - mae: 57.1641 - val_loss: 1175.4225 - val_mae: 1176.1157\n",
      "Epoch 4600/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 51.4355 - mae: 52.1076 - val_loss: 1147.8469 - val_mae: 1148.5402\n",
      "Epoch 4601/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 53.3234 - mae: 53.9982 - val_loss: 1149.5781 - val_mae: 1150.2705\n",
      "Epoch 4602/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 57.2563 - mae: 57.9276 - val_loss: 1149.0291 - val_mae: 1149.7214\n",
      "Epoch 4603/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 49.9142 - mae: 50.5878 - val_loss: 1151.0995 - val_mae: 1151.7927\n",
      "Epoch 4604/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 51.1581 - mae: 51.8329 - val_loss: 1158.9132 - val_mae: 1159.6060\n",
      "Epoch 4605/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 53.1539 - mae: 53.8289 - val_loss: 1156.8535 - val_mae: 1157.5466\n",
      "Epoch 4606/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 53.9995 - mae: 54.6770 - val_loss: 1138.5177 - val_mae: 1139.2108\n",
      "Epoch 4607/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 60.7272 - mae: 61.4089 - val_loss: 1148.6335 - val_mae: 1149.3268\n",
      "Epoch 4608/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 60.1382 - mae: 60.8202 - val_loss: 1173.1791 - val_mae: 1173.8716\n",
      "Epoch 4609/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 59.1244 - mae: 59.8034 - val_loss: 1149.1619 - val_mae: 1149.8551\n",
      "Epoch 4610/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 54.1684 - mae: 54.8438 - val_loss: 1144.3986 - val_mae: 1145.0909\n",
      "Epoch 4611/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 59.9347 - mae: 60.6091 - val_loss: 1128.2457 - val_mae: 1128.9390\n",
      "Epoch 4612/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 53.7580 - mae: 54.4352 - val_loss: 1162.7784 - val_mae: 1163.4703\n",
      "Epoch 4613/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 49.6531 - mae: 50.3284 - val_loss: 1180.8656 - val_mae: 1181.5588\n",
      "Epoch 4614/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 56.5990 - mae: 57.2810 - val_loss: 1184.3562 - val_mae: 1185.0493\n",
      "Epoch 4615/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 53.8094 - mae: 54.4841 - val_loss: 1145.3596 - val_mae: 1146.0527\n",
      "Epoch 4616/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 50.6494 - mae: 51.3243 - val_loss: 1171.9623 - val_mae: 1172.6554\n",
      "Epoch 4617/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 55.0667 - mae: 55.7378 - val_loss: 1146.3040 - val_mae: 1146.9969\n",
      "Epoch 4618/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 56.8958 - mae: 57.5742 - val_loss: 1178.8002 - val_mae: 1179.4934\n",
      "Epoch 4619/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 50.6169 - mae: 51.2893 - val_loss: 1177.4152 - val_mae: 1178.1080\n",
      "Epoch 4620/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 53.5964 - mae: 54.2699 - val_loss: 1155.2969 - val_mae: 1155.9899\n",
      "Epoch 4621/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 69.5691 - mae: 70.2493 - val_loss: 1163.0840 - val_mae: 1163.7770\n",
      "Epoch 4622/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 54.9639 - mae: 55.6350 - val_loss: 1162.2507 - val_mae: 1162.9437\n",
      "Epoch 4623/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 53.5750 - mae: 54.2453 - val_loss: 1145.2704 - val_mae: 1145.9635\n",
      "Epoch 4624/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 53.6796 - mae: 54.3615 - val_loss: 1135.5979 - val_mae: 1136.2905\n",
      "Epoch 4625/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 55.4404 - mae: 56.1152 - val_loss: 1128.9678 - val_mae: 1129.6603\n",
      "Epoch 4626/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 63.1318 - mae: 63.8073 - val_loss: 1155.5370 - val_mae: 1156.2295\n",
      "Epoch 4627/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 57.0405 - mae: 57.7193 - val_loss: 1146.9476 - val_mae: 1147.6400\n",
      "Epoch 4628/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 52.2125 - mae: 52.8886 - val_loss: 1151.3157 - val_mae: 1152.0079\n",
      "Epoch 4629/5000\n",
      "46/46 [==============================] - 2s 32ms/step - loss: 56.6853 - mae: 57.3601 - val_loss: 1165.2070 - val_mae: 1165.8997\n",
      "Epoch 4630/5000\n",
      "46/46 [==============================] - 2s 42ms/step - loss: 51.0026 - mae: 51.6791 - val_loss: 1145.2405 - val_mae: 1145.9329\n",
      "Epoch 4631/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 50.1696 - mae: 50.8404 - val_loss: 1157.2921 - val_mae: 1157.9852\n",
      "Epoch 4632/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 53.0001 - mae: 53.6776 - val_loss: 1173.9795 - val_mae: 1174.6724\n",
      "Epoch 4633/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 55.7130 - mae: 56.3865 - val_loss: 1147.1014 - val_mae: 1147.7946\n",
      "Epoch 4634/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 50.6457 - mae: 51.3158 - val_loss: 1180.2970 - val_mae: 1180.9902\n",
      "Epoch 4635/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 54.2771 - mae: 54.9543 - val_loss: 1153.1770 - val_mae: 1153.8691\n",
      "Epoch 4636/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 60.2581 - mae: 60.9311 - val_loss: 1153.3967 - val_mae: 1154.0897\n",
      "Epoch 4637/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 59.2802 - mae: 59.9592 - val_loss: 1166.6195 - val_mae: 1167.3116\n",
      "Epoch 4638/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 54.9493 - mae: 55.6257 - val_loss: 1157.4603 - val_mae: 1158.1537\n",
      "Epoch 4639/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 52.7123 - mae: 53.3859 - val_loss: 1188.3665 - val_mae: 1189.0591\n",
      "Epoch 4640/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 51.1134 - mae: 51.7912 - val_loss: 1164.3276 - val_mae: 1165.0208\n",
      "Epoch 4641/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 66.6497 - mae: 67.3295 - val_loss: 1168.3428 - val_mae: 1169.0359\n",
      "Epoch 4642/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 57.5953 - mae: 58.2729 - val_loss: 1162.0074 - val_mae: 1162.7006\n",
      "Epoch 4643/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 49.3767 - mae: 50.0482 - val_loss: 1150.9985 - val_mae: 1151.6908\n",
      "Epoch 4644/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 54.1767 - mae: 54.8509 - val_loss: 1162.6172 - val_mae: 1163.3103\n",
      "Epoch 4645/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 53.8973 - mae: 54.5736 - val_loss: 1152.4348 - val_mae: 1153.1281\n",
      "Epoch 4646/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 49.0608 - mae: 49.7334 - val_loss: 1130.1832 - val_mae: 1130.8767\n",
      "Epoch 4647/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 53.0605 - mae: 53.7381 - val_loss: 1125.5554 - val_mae: 1126.2484\n",
      "Epoch 4648/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 50.5478 - mae: 51.2201 - val_loss: 1149.3198 - val_mae: 1150.0121\n",
      "Epoch 4649/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 50.4973 - mae: 51.1719 - val_loss: 1141.1781 - val_mae: 1141.8712\n",
      "Epoch 4650/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 51.2079 - mae: 51.8764 - val_loss: 1157.4966 - val_mae: 1158.1898\n",
      "Epoch 4651/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 58.8174 - mae: 59.4909 - val_loss: 1164.0538 - val_mae: 1164.7471\n",
      "Epoch 4652/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 52.1426 - mae: 52.8147 - val_loss: 1144.1611 - val_mae: 1144.8542\n",
      "Epoch 4653/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 49.1723 - mae: 49.8462 - val_loss: 1147.1520 - val_mae: 1147.8450\n",
      "Epoch 4654/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 50.2642 - mae: 50.9390 - val_loss: 1124.9302 - val_mae: 1125.6229\n",
      "Epoch 4655/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 50.5977 - mae: 51.2697 - val_loss: 1141.0767 - val_mae: 1141.7694\n",
      "Epoch 4656/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 50.3733 - mae: 51.0464 - val_loss: 1159.8336 - val_mae: 1160.5256\n",
      "Epoch 4657/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 45.4858 - mae: 46.1585 - val_loss: 1156.7408 - val_mae: 1157.4336\n",
      "Epoch 4658/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 52.3831 - mae: 53.0565 - val_loss: 1147.7108 - val_mae: 1148.4041\n",
      "Epoch 4659/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 53.6832 - mae: 54.3617 - val_loss: 1156.2954 - val_mae: 1156.9888\n",
      "Epoch 4660/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 52.8749 - mae: 53.5562 - val_loss: 1174.6240 - val_mae: 1175.3173\n",
      "Epoch 4661/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 60.0536 - mae: 60.7302 - val_loss: 1149.9542 - val_mae: 1150.6473\n",
      "Epoch 4662/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 54.1318 - mae: 54.8110 - val_loss: 1152.7788 - val_mae: 1153.4718\n",
      "Epoch 4663/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 51.7428 - mae: 52.4164 - val_loss: 1155.6609 - val_mae: 1156.3541\n",
      "Epoch 4664/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 52.2775 - mae: 52.9593 - val_loss: 1145.1896 - val_mae: 1145.8812\n",
      "Epoch 4665/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 54.3161 - mae: 54.9885 - val_loss: 1180.3076 - val_mae: 1181.0007\n",
      "Epoch 4666/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 52.3598 - mae: 53.0375 - val_loss: 1147.5070 - val_mae: 1148.2002\n",
      "Epoch 4667/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 51.6385 - mae: 52.3117 - val_loss: 1160.0215 - val_mae: 1160.7146\n",
      "Epoch 4668/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 50.2830 - mae: 50.9592 - val_loss: 1173.3632 - val_mae: 1174.0563\n",
      "Epoch 4669/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 52.1253 - mae: 52.8031 - val_loss: 1154.3698 - val_mae: 1155.0629\n",
      "Epoch 4670/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 48.9125 - mae: 49.5837 - val_loss: 1153.0891 - val_mae: 1153.7821\n",
      "Epoch 4671/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 48.1807 - mae: 48.8581 - val_loss: 1175.9397 - val_mae: 1176.6326\n",
      "Epoch 4672/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 51.5218 - mae: 52.1941 - val_loss: 1157.1692 - val_mae: 1157.8623\n",
      "Epoch 4673/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 59.0762 - mae: 59.7547 - val_loss: 1148.1123 - val_mae: 1148.8054\n",
      "Epoch 4674/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 54.3938 - mae: 55.0706 - val_loss: 1186.0087 - val_mae: 1186.7015\n",
      "Epoch 4675/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 54.8915 - mae: 55.5708 - val_loss: 1136.5299 - val_mae: 1137.2230\n",
      "Epoch 4676/5000\n",
      "46/46 [==============================] - 2s 40ms/step - loss: 51.3598 - mae: 52.0310 - val_loss: 1150.1554 - val_mae: 1150.8485\n",
      "Epoch 4677/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 57.2034 - mae: 57.8807 - val_loss: 1151.4364 - val_mae: 1152.1296\n",
      "Epoch 4678/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 53.0773 - mae: 53.7524 - val_loss: 1144.3063 - val_mae: 1144.9993\n",
      "Epoch 4679/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 49.7056 - mae: 50.3708 - val_loss: 1173.9657 - val_mae: 1174.6588\n",
      "Epoch 4680/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 62.3613 - mae: 63.0396 - val_loss: 1157.3724 - val_mae: 1158.0657\n",
      "Epoch 4681/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 54.5400 - mae: 55.2160 - val_loss: 1159.8169 - val_mae: 1160.5100\n",
      "Epoch 4682/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 52.3778 - mae: 53.0546 - val_loss: 1148.7976 - val_mae: 1149.4910\n",
      "Epoch 4683/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 54.4851 - mae: 55.1604 - val_loss: 1155.7332 - val_mae: 1156.4264\n",
      "Epoch 4684/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 53.3313 - mae: 54.0130 - val_loss: 1166.0345 - val_mae: 1166.7275\n",
      "Epoch 4685/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 54.9652 - mae: 55.6399 - val_loss: 1193.1344 - val_mae: 1193.8268\n",
      "Epoch 4686/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 52.4890 - mae: 53.1658 - val_loss: 1151.0189 - val_mae: 1151.7098\n",
      "Epoch 4687/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 52.2051 - mae: 52.8832 - val_loss: 1145.9500 - val_mae: 1146.6422\n",
      "Epoch 4688/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 57.7795 - mae: 58.4582 - val_loss: 1157.0858 - val_mae: 1157.7788\n",
      "Epoch 4689/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 55.9395 - mae: 56.6141 - val_loss: 1164.4821 - val_mae: 1165.1738\n",
      "Epoch 4690/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 57.0243 - mae: 57.6967 - val_loss: 1146.5776 - val_mae: 1147.2705\n",
      "Epoch 4691/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 51.0478 - mae: 51.7189 - val_loss: 1170.0243 - val_mae: 1170.7162\n",
      "Epoch 4692/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 46.8703 - mae: 47.5458 - val_loss: 1168.9540 - val_mae: 1169.6461\n",
      "Epoch 4693/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 52.8938 - mae: 53.5635 - val_loss: 1164.9803 - val_mae: 1165.6736\n",
      "Epoch 4694/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 61.7749 - mae: 62.4539 - val_loss: 1212.7831 - val_mae: 1213.4763\n",
      "Epoch 4695/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 57.3371 - mae: 58.0144 - val_loss: 1147.2765 - val_mae: 1147.9689\n",
      "Epoch 4696/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 51.7268 - mae: 52.4004 - val_loss: 1160.4580 - val_mae: 1161.1500\n",
      "Epoch 4697/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 50.4047 - mae: 51.0763 - val_loss: 1180.6898 - val_mae: 1181.3828\n",
      "Epoch 4698/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 57.1699 - mae: 57.8503 - val_loss: 1158.3464 - val_mae: 1159.0396\n",
      "Epoch 4699/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 51.1239 - mae: 51.8006 - val_loss: 1150.8103 - val_mae: 1151.5027\n",
      "Epoch 4700/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 51.0685 - mae: 51.7426 - val_loss: 1145.4375 - val_mae: 1146.1304\n",
      "Epoch 4701/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 50.1892 - mae: 50.8585 - val_loss: 1168.0675 - val_mae: 1168.7605\n",
      "Epoch 4702/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 54.8591 - mae: 55.5348 - val_loss: 1147.8491 - val_mae: 1148.5425\n",
      "Epoch 4703/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 49.4728 - mae: 50.1454 - val_loss: 1171.9432 - val_mae: 1172.6349\n",
      "Epoch 4704/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 49.9991 - mae: 50.6739 - val_loss: 1146.6465 - val_mae: 1147.3397\n",
      "Epoch 4705/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 51.3943 - mae: 52.0715 - val_loss: 1202.9690 - val_mae: 1203.6622\n",
      "Epoch 4706/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 57.7413 - mae: 58.4120 - val_loss: 1168.5938 - val_mae: 1169.2869\n",
      "Epoch 4707/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 51.9822 - mae: 52.6599 - val_loss: 1164.8840 - val_mae: 1165.5773\n",
      "Epoch 4708/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 47.6622 - mae: 48.3298 - val_loss: 1136.5775 - val_mae: 1137.2700\n",
      "Epoch 4709/5000\n",
      "46/46 [==============================] - 2s 41ms/step - loss: 53.1740 - mae: 53.8498 - val_loss: 1158.8199 - val_mae: 1159.5115\n",
      "Epoch 4710/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 59.8124 - mae: 60.4872 - val_loss: 1193.8077 - val_mae: 1194.5007\n",
      "Epoch 4711/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 53.2522 - mae: 53.9246 - val_loss: 1170.6047 - val_mae: 1171.2971\n",
      "Epoch 4712/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 59.9442 - mae: 60.6218 - val_loss: 1196.8679 - val_mae: 1197.5594\n",
      "Epoch 4713/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 55.3106 - mae: 55.9901 - val_loss: 1145.3679 - val_mae: 1146.0602\n",
      "Epoch 4714/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 53.7358 - mae: 54.4110 - val_loss: 1182.9867 - val_mae: 1183.6798\n",
      "Epoch 4715/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 57.4736 - mae: 58.1497 - val_loss: 1191.7546 - val_mae: 1192.4475\n",
      "Epoch 4716/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 51.7962 - mae: 52.4736 - val_loss: 1145.9227 - val_mae: 1146.6158\n",
      "Epoch 4717/5000\n",
      "46/46 [==============================] - 2s 40ms/step - loss: 50.3304 - mae: 51.0048 - val_loss: 1158.2357 - val_mae: 1158.9287\n",
      "Epoch 4718/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 56.0837 - mae: 56.7618 - val_loss: 1182.2175 - val_mae: 1182.9099\n",
      "Epoch 4719/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 58.6972 - mae: 59.3768 - val_loss: 1137.9460 - val_mae: 1138.6381\n",
      "Epoch 4720/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 59.5373 - mae: 60.2147 - val_loss: 1129.9592 - val_mae: 1130.6523\n",
      "Epoch 4721/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 57.8560 - mae: 58.5334 - val_loss: 1168.8680 - val_mae: 1169.5608\n",
      "Epoch 4722/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 50.5649 - mae: 51.2392 - val_loss: 1166.4358 - val_mae: 1167.1285\n",
      "Epoch 4723/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 48.1102 - mae: 48.7841 - val_loss: 1154.4021 - val_mae: 1155.0951\n",
      "Epoch 4724/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 57.2346 - mae: 57.9140 - val_loss: 1159.2371 - val_mae: 1159.9303\n",
      "Epoch 4725/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 51.3877 - mae: 52.0641 - val_loss: 1184.0179 - val_mae: 1184.7098\n",
      "Epoch 4726/5000\n",
      "46/46 [==============================] - 2s 46ms/step - loss: 55.7652 - mae: 56.4449 - val_loss: 1179.6694 - val_mae: 1180.3628\n",
      "Epoch 4727/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 60.7630 - mae: 61.4430 - val_loss: 1166.5726 - val_mae: 1167.2660\n",
      "Epoch 4728/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 57.8814 - mae: 58.5551 - val_loss: 1159.6722 - val_mae: 1160.3645\n",
      "Epoch 4729/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 53.3975 - mae: 54.0711 - val_loss: 1154.5419 - val_mae: 1155.2340\n",
      "Epoch 4730/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 50.9237 - mae: 51.5966 - val_loss: 1156.6482 - val_mae: 1157.3414\n",
      "Epoch 4731/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 47.7437 - mae: 48.4187 - val_loss: 1173.4916 - val_mae: 1174.1833\n",
      "Epoch 4732/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 55.2650 - mae: 55.9399 - val_loss: 1168.4242 - val_mae: 1169.1172\n",
      "Epoch 4733/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 58.1230 - mae: 58.8030 - val_loss: 1178.8372 - val_mae: 1179.5303\n",
      "Epoch 4734/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 51.7215 - mae: 52.3937 - val_loss: 1155.2434 - val_mae: 1155.9353\n",
      "Epoch 4735/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 52.8497 - mae: 53.5291 - val_loss: 1153.4215 - val_mae: 1154.1133\n",
      "Epoch 4736/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 46.2389 - mae: 46.9108 - val_loss: 1148.8447 - val_mae: 1149.5367\n",
      "Epoch 4737/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 49.9295 - mae: 50.6010 - val_loss: 1151.5311 - val_mae: 1152.2240\n",
      "Epoch 4738/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 52.0026 - mae: 52.6751 - val_loss: 1173.3573 - val_mae: 1174.0504\n",
      "Epoch 4739/5000\n",
      "46/46 [==============================] - 2s 44ms/step - loss: 55.6888 - mae: 56.3665 - val_loss: 1161.0192 - val_mae: 1161.7115\n",
      "Epoch 4740/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 52.4149 - mae: 53.0868 - val_loss: 1159.8613 - val_mae: 1160.5546\n",
      "Epoch 4741/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 49.5055 - mae: 50.1760 - val_loss: 1144.4845 - val_mae: 1145.1779\n",
      "Epoch 4742/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 60.8294 - mae: 61.5054 - val_loss: 1148.9142 - val_mae: 1149.6073\n",
      "Epoch 4743/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 57.8609 - mae: 58.5377 - val_loss: 1147.8009 - val_mae: 1148.4940\n",
      "Epoch 4744/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 50.5152 - mae: 51.1912 - val_loss: 1172.3071 - val_mae: 1173.0004\n",
      "Epoch 4745/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 56.9736 - mae: 57.6448 - val_loss: 1149.4003 - val_mae: 1150.0935\n",
      "Epoch 4746/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 55.0653 - mae: 55.7367 - val_loss: 1165.2015 - val_mae: 1165.8948\n",
      "Epoch 4747/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 52.1948 - mae: 52.8707 - val_loss: 1162.0867 - val_mae: 1162.7786\n",
      "Epoch 4748/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 50.7908 - mae: 51.4651 - val_loss: 1187.2323 - val_mae: 1187.9254\n",
      "Epoch 4749/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 52.4689 - mae: 53.1446 - val_loss: 1157.2489 - val_mae: 1157.9421\n",
      "Epoch 4750/5000\n",
      "46/46 [==============================] - 2s 45ms/step - loss: 56.9705 - mae: 57.6496 - val_loss: 1184.6964 - val_mae: 1185.3890\n",
      "Epoch 4751/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 58.4866 - mae: 59.1611 - val_loss: 1164.8546 - val_mae: 1165.5476\n",
      "Epoch 4752/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 58.0932 - mae: 58.7724 - val_loss: 1178.3291 - val_mae: 1179.0221\n",
      "Epoch 4753/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 55.0637 - mae: 55.7385 - val_loss: 1168.5548 - val_mae: 1169.2478\n",
      "Epoch 4754/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 51.4025 - mae: 52.0804 - val_loss: 1162.5889 - val_mae: 1163.2820\n",
      "Epoch 4755/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 49.8237 - mae: 50.4934 - val_loss: 1157.3596 - val_mae: 1158.0527\n",
      "Epoch 4756/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 50.9110 - mae: 51.5824 - val_loss: 1180.6354 - val_mae: 1181.3274\n",
      "Epoch 4757/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 49.4013 - mae: 50.0778 - val_loss: 1175.8772 - val_mae: 1176.5702\n",
      "Epoch 4758/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 57.8026 - mae: 58.4782 - val_loss: 1166.1365 - val_mae: 1166.8296\n",
      "Epoch 4759/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 48.9748 - mae: 49.6512 - val_loss: 1161.9557 - val_mae: 1162.6467\n",
      "Epoch 4760/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 53.1287 - mae: 53.8002 - val_loss: 1173.5316 - val_mae: 1174.2247\n",
      "Epoch 4761/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 52.8068 - mae: 53.4849 - val_loss: 1170.0096 - val_mae: 1170.7029\n",
      "Epoch 4762/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 49.9295 - mae: 50.6012 - val_loss: 1157.2843 - val_mae: 1157.9774\n",
      "Epoch 4763/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 51.6598 - mae: 52.3312 - val_loss: 1190.9578 - val_mae: 1191.6509\n",
      "Epoch 4764/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 53.0712 - mae: 53.7486 - val_loss: 1193.7826 - val_mae: 1194.4758\n",
      "Epoch 4765/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 60.7659 - mae: 61.4406 - val_loss: 1213.3719 - val_mae: 1214.0649\n",
      "Epoch 4766/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 53.6875 - mae: 54.3677 - val_loss: 1175.2091 - val_mae: 1175.9023\n",
      "Epoch 4767/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 48.5868 - mae: 49.2659 - val_loss: 1166.2639 - val_mae: 1166.9563\n",
      "Epoch 4768/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 48.2877 - mae: 48.9610 - val_loss: 1149.9371 - val_mae: 1150.6301\n",
      "Epoch 4769/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 50.4034 - mae: 51.0766 - val_loss: 1156.0836 - val_mae: 1156.7762\n",
      "Epoch 4770/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 57.0523 - mae: 57.7282 - val_loss: 1209.0302 - val_mae: 1209.7219\n",
      "Epoch 4771/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 57.0646 - mae: 57.7409 - val_loss: 1188.9231 - val_mae: 1189.6151\n",
      "Epoch 4772/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 48.9786 - mae: 49.6545 - val_loss: 1176.7374 - val_mae: 1177.4307\n",
      "Epoch 4773/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 62.0501 - mae: 62.7336 - val_loss: 1159.3788 - val_mae: 1160.0719\n",
      "Epoch 4774/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 52.3981 - mae: 53.0731 - val_loss: 1142.7291 - val_mae: 1143.4214\n",
      "Epoch 4775/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 58.4271 - mae: 59.1042 - val_loss: 1150.1532 - val_mae: 1150.8459\n",
      "Epoch 4776/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 52.2633 - mae: 52.9351 - val_loss: 1169.6925 - val_mae: 1170.3854\n",
      "Epoch 4777/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 51.4166 - mae: 52.0921 - val_loss: 1154.0847 - val_mae: 1154.7776\n",
      "Epoch 4778/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 54.5701 - mae: 55.2483 - val_loss: 1150.8344 - val_mae: 1151.5275\n",
      "Epoch 4779/5000\n",
      "46/46 [==============================] - 2s 42ms/step - loss: 47.8680 - mae: 48.5405 - val_loss: 1189.4176 - val_mae: 1190.1106\n",
      "Epoch 4780/5000\n",
      "46/46 [==============================] - 2s 31ms/step - loss: 49.8963 - mae: 50.5664 - val_loss: 1160.1705 - val_mae: 1160.8629\n",
      "Epoch 4781/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 52.5536 - mae: 53.2312 - val_loss: 1189.0663 - val_mae: 1189.7594\n",
      "Epoch 4782/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 53.6448 - mae: 54.3221 - val_loss: 1161.5831 - val_mae: 1162.2755\n",
      "Epoch 4783/5000\n",
      "46/46 [==============================] - 2s 32ms/step - loss: 49.8346 - mae: 50.5056 - val_loss: 1145.8470 - val_mae: 1146.5403\n",
      "Epoch 4784/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 50.0460 - mae: 50.7121 - val_loss: 1142.1616 - val_mae: 1142.8547\n",
      "Epoch 4785/5000\n",
      "46/46 [==============================] - 2s 31ms/step - loss: 53.3950 - mae: 54.0686 - val_loss: 1185.4041 - val_mae: 1186.0970\n",
      "Epoch 4786/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 55.8897 - mae: 56.5646 - val_loss: 1178.2085 - val_mae: 1178.9014\n",
      "Epoch 4787/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 55.7488 - mae: 56.4220 - val_loss: 1175.5990 - val_mae: 1176.2920\n",
      "Epoch 4788/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 55.6193 - mae: 56.2978 - val_loss: 1161.6019 - val_mae: 1162.2938\n",
      "Epoch 4789/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 60.0411 - mae: 60.7211 - val_loss: 1159.9689 - val_mae: 1160.6613\n",
      "Epoch 4790/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 50.6367 - mae: 51.3125 - val_loss: 1167.2217 - val_mae: 1167.9149\n",
      "Epoch 4791/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 51.4318 - mae: 52.1056 - val_loss: 1202.5287 - val_mae: 1203.2211\n",
      "Epoch 4792/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 53.8443 - mae: 54.5230 - val_loss: 1170.9591 - val_mae: 1171.6522\n",
      "Epoch 4793/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 57.4007 - mae: 58.0796 - val_loss: 1159.1500 - val_mae: 1159.8407\n",
      "Epoch 4794/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 53.9861 - mae: 54.6665 - val_loss: 1157.9635 - val_mae: 1158.6566\n",
      "Epoch 4795/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 52.3773 - mae: 53.0478 - val_loss: 1159.1613 - val_mae: 1159.8544\n",
      "Epoch 4796/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 55.6812 - mae: 56.3527 - val_loss: 1165.3070 - val_mae: 1166.0000\n",
      "Epoch 4797/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 49.3870 - mae: 50.0620 - val_loss: 1197.4958 - val_mae: 1198.1890\n",
      "Epoch 4798/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 54.0803 - mae: 54.7585 - val_loss: 1181.8370 - val_mae: 1182.5304\n",
      "Epoch 4799/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 56.3822 - mae: 57.0558 - val_loss: 1156.4291 - val_mae: 1157.1216\n",
      "Epoch 4800/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 53.0002 - mae: 53.6750 - val_loss: 1202.1661 - val_mae: 1202.8591\n",
      "Epoch 4801/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 54.1861 - mae: 54.8657 - val_loss: 1189.6860 - val_mae: 1190.3785\n",
      "Epoch 4802/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 50.5770 - mae: 51.2515 - val_loss: 1161.6506 - val_mae: 1162.3438\n",
      "Epoch 4803/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 53.5040 - mae: 54.1788 - val_loss: 1188.2936 - val_mae: 1188.9865\n",
      "Epoch 4804/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 63.9496 - mae: 64.6290 - val_loss: 1222.0746 - val_mae: 1222.7678\n",
      "Epoch 4805/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 52.4942 - mae: 53.1663 - val_loss: 1183.8289 - val_mae: 1184.5221\n",
      "Epoch 4806/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 57.9750 - mae: 58.6510 - val_loss: 1170.8977 - val_mae: 1171.5907\n",
      "Epoch 4807/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 46.4585 - mae: 47.1325 - val_loss: 1165.7117 - val_mae: 1166.4048\n",
      "Epoch 4808/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 48.8158 - mae: 49.4942 - val_loss: 1195.7533 - val_mae: 1196.4467\n",
      "Epoch 4809/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 57.1610 - mae: 57.8393 - val_loss: 1154.4401 - val_mae: 1155.1324\n",
      "Epoch 4810/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 55.3691 - mae: 56.0433 - val_loss: 1173.1779 - val_mae: 1173.8708\n",
      "Epoch 4811/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 48.8169 - mae: 49.4924 - val_loss: 1179.7870 - val_mae: 1180.4802\n",
      "Epoch 4812/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 55.2366 - mae: 55.9131 - val_loss: 1179.8895 - val_mae: 1180.5826\n",
      "Epoch 4813/5000\n",
      "46/46 [==============================] - 2s 41ms/step - loss: 56.0111 - mae: 56.6872 - val_loss: 1156.2731 - val_mae: 1156.9662\n",
      "Epoch 4814/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 50.1863 - mae: 50.8646 - val_loss: 1154.9552 - val_mae: 1155.6486\n",
      "Epoch 4815/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 48.3285 - mae: 49.0045 - val_loss: 1165.1373 - val_mae: 1165.8303\n",
      "Epoch 4816/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 52.9446 - mae: 53.6178 - val_loss: 1172.3623 - val_mae: 1173.0541\n",
      "Epoch 4817/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 58.2234 - mae: 58.8930 - val_loss: 1158.4209 - val_mae: 1159.1135\n",
      "Epoch 4818/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 51.8647 - mae: 52.5381 - val_loss: 1160.7754 - val_mae: 1161.4684\n",
      "Epoch 4819/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 54.8964 - mae: 55.5704 - val_loss: 1161.2855 - val_mae: 1161.9786\n",
      "Epoch 4820/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 48.5413 - mae: 49.2160 - val_loss: 1171.5504 - val_mae: 1172.2437\n",
      "Epoch 4821/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 55.3846 - mae: 56.0563 - val_loss: 1210.3943 - val_mae: 1211.0858\n",
      "Epoch 4822/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 57.9755 - mae: 58.6492 - val_loss: 1158.6530 - val_mae: 1159.3459\n",
      "Epoch 4823/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 53.6781 - mae: 54.3562 - val_loss: 1175.7129 - val_mae: 1176.4059\n",
      "Epoch 4824/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 60.6080 - mae: 61.2853 - val_loss: 1174.4291 - val_mae: 1175.1221\n",
      "Epoch 4825/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 56.2487 - mae: 56.9173 - val_loss: 1166.3088 - val_mae: 1167.0020\n",
      "Epoch 4826/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 63.8622 - mae: 64.5410 - val_loss: 1168.9979 - val_mae: 1169.6912\n",
      "Epoch 4827/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 49.8057 - mae: 50.4846 - val_loss: 1183.6196 - val_mae: 1184.3129\n",
      "Epoch 4828/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 55.4001 - mae: 56.0750 - val_loss: 1178.2321 - val_mae: 1178.9246\n",
      "Epoch 4829/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 63.6426 - mae: 64.3222 - val_loss: 1187.3960 - val_mae: 1188.0890\n",
      "Epoch 4830/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 49.8829 - mae: 50.5560 - val_loss: 1164.3328 - val_mae: 1165.0259\n",
      "Epoch 4831/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 68.5702 - mae: 69.2483 - val_loss: 1162.0496 - val_mae: 1162.7428\n",
      "Epoch 4832/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 51.8579 - mae: 52.5324 - val_loss: 1149.5219 - val_mae: 1150.2152\n",
      "Epoch 4833/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 57.1362 - mae: 57.8073 - val_loss: 1193.8943 - val_mae: 1194.5864\n",
      "Epoch 4834/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 54.2925 - mae: 54.9664 - val_loss: 1156.2164 - val_mae: 1156.9095\n",
      "Epoch 4835/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 53.5295 - mae: 54.2085 - val_loss: 1183.7743 - val_mae: 1184.4675\n",
      "Epoch 4836/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 52.9695 - mae: 53.6438 - val_loss: 1168.8240 - val_mae: 1169.5171\n",
      "Epoch 4837/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 55.0888 - mae: 55.7611 - val_loss: 1160.1117 - val_mae: 1160.8044\n",
      "Epoch 4838/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 53.1549 - mae: 53.8352 - val_loss: 1174.9601 - val_mae: 1175.6534\n",
      "Epoch 4839/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 53.2525 - mae: 53.9311 - val_loss: 1160.8557 - val_mae: 1161.5483\n",
      "Epoch 4840/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 48.5937 - mae: 49.2693 - val_loss: 1182.7336 - val_mae: 1183.4261\n",
      "Epoch 4841/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 50.5326 - mae: 51.2045 - val_loss: 1155.9774 - val_mae: 1156.6694\n",
      "Epoch 4842/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 51.6772 - mae: 52.3511 - val_loss: 1171.9694 - val_mae: 1172.6625\n",
      "Epoch 4843/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 59.3593 - mae: 60.0399 - val_loss: 1158.2716 - val_mae: 1158.9648\n",
      "Epoch 4844/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 53.3189 - mae: 53.9939 - val_loss: 1172.8457 - val_mae: 1173.5389\n",
      "Epoch 4845/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 55.0200 - mae: 55.6913 - val_loss: 1154.6362 - val_mae: 1155.3293\n",
      "Epoch 4846/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 56.5256 - mae: 57.2054 - val_loss: 1160.6381 - val_mae: 1161.3312\n",
      "Epoch 4847/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 52.8874 - mae: 53.5626 - val_loss: 1173.7964 - val_mae: 1174.4872\n",
      "Epoch 4848/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 48.8222 - mae: 49.4933 - val_loss: 1176.6307 - val_mae: 1177.3240\n",
      "Epoch 4849/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 51.1686 - mae: 51.8466 - val_loss: 1154.7804 - val_mae: 1155.4731\n",
      "Epoch 4850/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 54.3041 - mae: 54.9816 - val_loss: 1182.4332 - val_mae: 1183.1263\n",
      "Epoch 4851/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 60.0730 - mae: 60.7451 - val_loss: 1152.1888 - val_mae: 1152.8820\n",
      "Epoch 4852/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 51.7236 - mae: 52.3972 - val_loss: 1179.6885 - val_mae: 1180.3816\n",
      "Epoch 4853/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 47.9537 - mae: 48.6235 - val_loss: 1154.3484 - val_mae: 1155.0415\n",
      "Epoch 4854/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 54.3844 - mae: 55.0616 - val_loss: 1159.4845 - val_mae: 1160.1766\n",
      "Epoch 4855/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 51.2439 - mae: 51.9174 - val_loss: 1152.9211 - val_mae: 1153.6144\n",
      "Epoch 4856/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 53.8492 - mae: 54.5250 - val_loss: 1179.9453 - val_mae: 1180.6384\n",
      "Epoch 4857/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 54.5080 - mae: 55.1844 - val_loss: 1148.3505 - val_mae: 1149.0437\n",
      "Epoch 4858/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 47.9127 - mae: 48.5911 - val_loss: 1191.8339 - val_mae: 1192.5272\n",
      "Epoch 4859/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 56.8881 - mae: 57.5666 - val_loss: 1169.9774 - val_mae: 1170.6704\n",
      "Epoch 4860/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 48.8739 - mae: 49.5514 - val_loss: 1149.8124 - val_mae: 1150.5055\n",
      "Epoch 4861/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 50.6703 - mae: 51.3418 - val_loss: 1173.5250 - val_mae: 1174.2181\n",
      "Epoch 4862/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 54.8718 - mae: 55.5455 - val_loss: 1169.8066 - val_mae: 1170.4999\n",
      "Epoch 4863/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 51.2842 - mae: 51.9566 - val_loss: 1162.8357 - val_mae: 1163.5287\n",
      "Epoch 4864/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 57.9692 - mae: 58.6447 - val_loss: 1158.8469 - val_mae: 1159.5396\n",
      "Epoch 4865/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 50.0585 - mae: 50.7320 - val_loss: 1179.7961 - val_mae: 1180.4893\n",
      "Epoch 4866/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 47.9131 - mae: 48.5884 - val_loss: 1153.5900 - val_mae: 1154.2830\n",
      "Epoch 4867/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 48.8217 - mae: 49.4973 - val_loss: 1173.3511 - val_mae: 1174.0441\n",
      "Epoch 4868/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 51.6141 - mae: 52.2854 - val_loss: 1173.0272 - val_mae: 1173.7203\n",
      "Epoch 4869/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 52.4020 - mae: 53.0689 - val_loss: 1180.0123 - val_mae: 1180.7054\n",
      "Epoch 4870/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 53.1173 - mae: 53.7919 - val_loss: 1181.3629 - val_mae: 1182.0560\n",
      "Epoch 4871/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 60.5753 - mae: 61.2497 - val_loss: 1206.4288 - val_mae: 1207.1204\n",
      "Epoch 4872/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 69.2471 - mae: 69.9255 - val_loss: 1183.8578 - val_mae: 1184.5499\n",
      "Epoch 4873/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 48.3877 - mae: 49.0636 - val_loss: 1164.0437 - val_mae: 1164.7368\n",
      "Epoch 4874/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 58.5435 - mae: 59.2196 - val_loss: 1177.3682 - val_mae: 1178.0614\n",
      "Epoch 4875/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 55.7834 - mae: 56.4643 - val_loss: 1207.8669 - val_mae: 1208.5596\n",
      "Epoch 4876/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 51.9938 - mae: 52.6663 - val_loss: 1170.7323 - val_mae: 1171.4254\n",
      "Epoch 4877/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 47.4655 - mae: 48.1408 - val_loss: 1163.5444 - val_mae: 1164.2375\n",
      "Epoch 4878/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 51.3137 - mae: 51.9878 - val_loss: 1164.8291 - val_mae: 1165.5221\n",
      "Epoch 4879/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 53.9118 - mae: 54.5879 - val_loss: 1171.9575 - val_mae: 1172.6497\n",
      "Epoch 4880/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 55.3441 - mae: 56.0230 - val_loss: 1146.3600 - val_mae: 1147.0527\n",
      "Epoch 4881/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 52.7265 - mae: 53.4008 - val_loss: 1184.8250 - val_mae: 1185.5182\n",
      "Epoch 4882/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 58.3488 - mae: 59.0271 - val_loss: 1187.5083 - val_mae: 1188.2008\n",
      "Epoch 4883/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 51.0117 - mae: 51.6860 - val_loss: 1174.1989 - val_mae: 1174.8923\n",
      "Epoch 4884/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 53.0496 - mae: 53.7236 - val_loss: 1178.3381 - val_mae: 1179.0305\n",
      "Epoch 4885/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 54.0423 - mae: 54.7155 - val_loss: 1203.3804 - val_mae: 1204.0736\n",
      "Epoch 4886/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 56.4131 - mae: 57.0877 - val_loss: 1174.3691 - val_mae: 1175.0624\n",
      "Epoch 4887/5000\n",
      "46/46 [==============================] - 2s 41ms/step - loss: 59.7207 - mae: 60.3998 - val_loss: 1194.7260 - val_mae: 1195.4189\n",
      "Epoch 4888/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 49.4967 - mae: 50.1656 - val_loss: 1190.2023 - val_mae: 1190.8954\n",
      "Epoch 4889/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 47.7426 - mae: 48.4182 - val_loss: 1209.5338 - val_mae: 1210.2262\n",
      "Epoch 4890/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 52.1765 - mae: 52.8527 - val_loss: 1163.6656 - val_mae: 1164.3589\n",
      "Epoch 4891/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 53.4089 - mae: 54.0880 - val_loss: 1178.9514 - val_mae: 1179.6445\n",
      "Epoch 4892/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 51.1879 - mae: 51.8625 - val_loss: 1184.4449 - val_mae: 1185.1382\n",
      "Epoch 4893/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 59.2224 - mae: 59.8966 - val_loss: 1181.2820 - val_mae: 1181.9752\n",
      "Epoch 4894/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 53.3467 - mae: 54.0242 - val_loss: 1172.9022 - val_mae: 1173.5952\n",
      "Epoch 4895/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 53.8274 - mae: 54.5019 - val_loss: 1190.8030 - val_mae: 1191.4961\n",
      "Epoch 4896/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 48.0669 - mae: 48.7368 - val_loss: 1176.3560 - val_mae: 1177.0490\n",
      "Epoch 4897/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 55.3145 - mae: 55.9925 - val_loss: 1160.4041 - val_mae: 1161.0970\n",
      "Epoch 4898/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 49.3871 - mae: 50.0582 - val_loss: 1210.2469 - val_mae: 1210.9401\n",
      "Epoch 4899/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 59.9926 - mae: 60.6727 - val_loss: 1161.4041 - val_mae: 1162.0970\n",
      "Epoch 4900/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 53.5114 - mae: 54.1897 - val_loss: 1164.2100 - val_mae: 1164.9032\n",
      "Epoch 4901/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 59.4295 - mae: 60.1013 - val_loss: 1164.7531 - val_mae: 1165.4458\n",
      "Epoch 4902/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 55.2836 - mae: 55.9654 - val_loss: 1178.5906 - val_mae: 1179.2833\n",
      "Epoch 4903/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 52.4126 - mae: 53.0895 - val_loss: 1164.9490 - val_mae: 1165.6414\n",
      "Epoch 4904/5000\n",
      "46/46 [==============================] - 2s 42ms/step - loss: 52.2183 - mae: 52.8973 - val_loss: 1181.0302 - val_mae: 1181.7228\n",
      "Epoch 4905/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 55.9485 - mae: 56.6198 - val_loss: 1167.4097 - val_mae: 1168.1022\n",
      "Epoch 4906/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 50.1388 - mae: 50.8147 - val_loss: 1164.1881 - val_mae: 1164.8812\n",
      "Epoch 4907/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 48.0149 - mae: 48.6885 - val_loss: 1166.6362 - val_mae: 1167.3296\n",
      "Epoch 4908/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 45.1468 - mae: 45.8202 - val_loss: 1158.7883 - val_mae: 1159.4808\n",
      "Epoch 4909/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 51.2686 - mae: 51.9426 - val_loss: 1173.5695 - val_mae: 1174.2626\n",
      "Epoch 4910/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 47.2696 - mae: 47.9461 - val_loss: 1150.9448 - val_mae: 1151.6367\n",
      "Epoch 4911/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 48.0847 - mae: 48.7601 - val_loss: 1163.3063 - val_mae: 1163.9996\n",
      "Epoch 4912/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 53.8781 - mae: 54.5532 - val_loss: 1144.9716 - val_mae: 1145.6647\n",
      "Epoch 4913/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 64.3645 - mae: 65.0394 - val_loss: 1177.7059 - val_mae: 1178.3989\n",
      "Epoch 4914/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 49.5846 - mae: 50.2563 - val_loss: 1155.3044 - val_mae: 1155.9976\n",
      "Epoch 4915/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 52.2721 - mae: 52.9456 - val_loss: 1153.3108 - val_mae: 1154.0042\n",
      "Epoch 4916/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 53.3887 - mae: 54.0628 - val_loss: 1237.0182 - val_mae: 1237.7113\n",
      "Epoch 4917/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 49.0710 - mae: 49.7456 - val_loss: 1157.6289 - val_mae: 1158.3219\n",
      "Epoch 4918/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 49.7541 - mae: 50.4280 - val_loss: 1163.7006 - val_mae: 1164.3936\n",
      "Epoch 4919/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 51.7815 - mae: 52.4532 - val_loss: 1190.5664 - val_mae: 1191.2598\n",
      "Epoch 4920/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 57.6566 - mae: 58.3343 - val_loss: 1150.0741 - val_mae: 1150.7672\n",
      "Epoch 4921/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 53.0188 - mae: 53.6934 - val_loss: 1164.2236 - val_mae: 1164.9166\n",
      "Epoch 4922/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 51.7613 - mae: 52.4352 - val_loss: 1157.3455 - val_mae: 1158.0386\n",
      "Epoch 4923/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 47.3364 - mae: 48.0133 - val_loss: 1181.1562 - val_mae: 1181.8469\n",
      "Epoch 4924/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 52.5412 - mae: 53.2118 - val_loss: 1219.6652 - val_mae: 1220.3580\n",
      "Epoch 4925/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 63.3798 - mae: 64.0586 - val_loss: 1176.7614 - val_mae: 1177.4545\n",
      "Epoch 4926/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 67.7383 - mae: 68.4191 - val_loss: 1184.2726 - val_mae: 1184.9658\n",
      "Epoch 4927/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 50.4346 - mae: 51.1065 - val_loss: 1177.7231 - val_mae: 1178.4160\n",
      "Epoch 4928/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 49.8290 - mae: 50.5041 - val_loss: 1165.9169 - val_mae: 1166.6101\n",
      "Epoch 4929/5000\n",
      "46/46 [==============================] - 2s 40ms/step - loss: 47.9259 - mae: 48.6025 - val_loss: 1160.0696 - val_mae: 1160.7627\n",
      "Epoch 4930/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 50.4046 - mae: 51.0761 - val_loss: 1152.7056 - val_mae: 1153.3964\n",
      "Epoch 4931/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 48.3720 - mae: 49.0446 - val_loss: 1162.6328 - val_mae: 1163.3259\n",
      "Epoch 4932/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 55.9669 - mae: 56.6435 - val_loss: 1210.4987 - val_mae: 1211.1908\n",
      "Epoch 4933/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 53.5637 - mae: 54.2403 - val_loss: 1157.7192 - val_mae: 1158.4122\n",
      "Epoch 4934/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 51.4432 - mae: 52.1163 - val_loss: 1179.7640 - val_mae: 1180.4569\n",
      "Epoch 4935/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 51.5827 - mae: 52.2609 - val_loss: 1169.7278 - val_mae: 1170.4209\n",
      "Epoch 4936/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 47.6506 - mae: 48.3203 - val_loss: 1182.6924 - val_mae: 1183.3842\n",
      "Epoch 4937/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 51.8773 - mae: 52.5551 - val_loss: 1169.8402 - val_mae: 1170.5332\n",
      "Epoch 4938/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 50.1370 - mae: 50.8063 - val_loss: 1174.3381 - val_mae: 1175.0308\n",
      "Epoch 4939/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 51.9412 - mae: 52.6118 - val_loss: 1170.6012 - val_mae: 1171.2942\n",
      "Epoch 4940/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 54.2724 - mae: 54.9487 - val_loss: 1189.0685 - val_mae: 1189.7616\n",
      "Epoch 4941/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 56.3196 - mae: 56.9990 - val_loss: 1152.3883 - val_mae: 1153.0809\n",
      "Epoch 4942/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 45.8983 - mae: 46.5676 - val_loss: 1181.5782 - val_mae: 1182.2715\n",
      "Epoch 4943/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 53.3356 - mae: 54.0133 - val_loss: 1175.2297 - val_mae: 1175.9227\n",
      "Epoch 4944/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 53.8419 - mae: 54.5200 - val_loss: 1168.3459 - val_mae: 1169.0393\n",
      "Epoch 4945/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 56.3444 - mae: 57.0189 - val_loss: 1199.1748 - val_mae: 1199.8665\n",
      "Epoch 4946/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 51.9918 - mae: 52.6699 - val_loss: 1181.5636 - val_mae: 1182.2566\n",
      "Epoch 4947/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 53.1118 - mae: 53.7874 - val_loss: 1174.8242 - val_mae: 1175.5173\n",
      "Epoch 4948/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 51.8691 - mae: 52.5428 - val_loss: 1185.8478 - val_mae: 1186.5409\n",
      "Epoch 4949/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 61.7094 - mae: 62.3867 - val_loss: 1178.0247 - val_mae: 1178.7172\n",
      "Epoch 4950/5000\n",
      "46/46 [==============================] - 2s 32ms/step - loss: 52.6263 - mae: 53.3020 - val_loss: 1175.4867 - val_mae: 1176.1799\n",
      "Epoch 4951/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 53.8285 - mae: 54.4997 - val_loss: 1175.7527 - val_mae: 1176.4457\n",
      "Epoch 4952/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 51.5759 - mae: 52.2520 - val_loss: 1172.3701 - val_mae: 1173.0634\n",
      "Epoch 4953/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 48.4611 - mae: 49.1373 - val_loss: 1169.7003 - val_mae: 1170.3937\n",
      "Epoch 4954/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 60.3188 - mae: 60.9981 - val_loss: 1167.2911 - val_mae: 1167.9828\n",
      "Epoch 4955/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 50.8298 - mae: 51.5075 - val_loss: 1181.9883 - val_mae: 1182.6809\n",
      "Epoch 4956/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 53.4867 - mae: 54.1671 - val_loss: 1184.0341 - val_mae: 1184.7273\n",
      "Epoch 4957/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 56.7656 - mae: 57.4388 - val_loss: 1176.2201 - val_mae: 1176.9121\n",
      "Epoch 4958/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 48.5570 - mae: 49.2329 - val_loss: 1183.9695 - val_mae: 1184.6628\n",
      "Epoch 4959/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 54.7298 - mae: 55.4081 - val_loss: 1255.8700 - val_mae: 1256.5632\n",
      "Epoch 4960/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 55.4850 - mae: 56.1626 - val_loss: 1177.8229 - val_mae: 1178.5148\n",
      "Epoch 4961/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 56.0193 - mae: 56.6955 - val_loss: 1190.6819 - val_mae: 1191.3752\n",
      "Epoch 4962/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 55.3058 - mae: 55.9820 - val_loss: 1175.1501 - val_mae: 1175.8434\n",
      "Epoch 4963/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 60.0260 - mae: 60.7030 - val_loss: 1182.3044 - val_mae: 1182.9973\n",
      "Epoch 4964/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 55.8703 - mae: 56.5474 - val_loss: 1196.0114 - val_mae: 1196.7046\n",
      "Epoch 4965/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 49.9812 - mae: 50.6585 - val_loss: 1159.4207 - val_mae: 1160.1132\n",
      "Epoch 4966/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 52.3505 - mae: 53.0275 - val_loss: 1173.9154 - val_mae: 1174.6085\n",
      "Epoch 4967/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 50.2420 - mae: 50.9131 - val_loss: 1175.0697 - val_mae: 1175.7629\n",
      "Epoch 4968/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 54.5264 - mae: 55.2059 - val_loss: 1162.4203 - val_mae: 1163.1134\n",
      "Epoch 4969/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 54.9523 - mae: 55.6273 - val_loss: 1171.6952 - val_mae: 1172.3881\n",
      "Epoch 4970/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 57.9557 - mae: 58.6316 - val_loss: 1168.4260 - val_mae: 1169.1182\n",
      "Epoch 4971/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 52.7054 - mae: 53.3797 - val_loss: 1213.4911 - val_mae: 1214.1825\n",
      "Epoch 4972/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 60.9997 - mae: 61.6765 - val_loss: 1207.8561 - val_mae: 1208.5487\n",
      "Epoch 4973/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 55.9808 - mae: 56.6522 - val_loss: 1185.5259 - val_mae: 1186.2186\n",
      "Epoch 4974/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 48.4623 - mae: 49.1361 - val_loss: 1192.4991 - val_mae: 1193.1920\n",
      "Epoch 4975/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 48.7158 - mae: 49.3892 - val_loss: 1171.8208 - val_mae: 1172.5142\n",
      "Epoch 4976/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 51.3184 - mae: 51.9954 - val_loss: 1176.6150 - val_mae: 1177.3079\n",
      "Epoch 4977/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 52.4201 - mae: 53.0879 - val_loss: 1157.4485 - val_mae: 1158.1416\n",
      "Epoch 4978/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 60.0155 - mae: 60.6948 - val_loss: 1153.4801 - val_mae: 1154.1729\n",
      "Epoch 4979/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 52.9302 - mae: 53.6109 - val_loss: 1170.2852 - val_mae: 1170.9783\n",
      "Epoch 4980/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 47.8614 - mae: 48.5297 - val_loss: 1174.4299 - val_mae: 1175.1232\n",
      "Epoch 4981/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 46.8100 - mae: 47.4819 - val_loss: 1175.0363 - val_mae: 1175.7285\n",
      "Epoch 4982/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 49.3707 - mae: 50.0480 - val_loss: 1188.5272 - val_mae: 1189.2202\n",
      "Epoch 4983/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 50.8439 - mae: 51.5198 - val_loss: 1173.3917 - val_mae: 1174.0850\n",
      "Epoch 4984/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 54.0827 - mae: 54.7572 - val_loss: 1182.5087 - val_mae: 1183.2019\n",
      "Epoch 4985/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 49.5208 - mae: 50.1967 - val_loss: 1192.4132 - val_mae: 1193.1064\n",
      "Epoch 4986/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 60.5998 - mae: 61.2820 - val_loss: 1166.2063 - val_mae: 1166.8995\n",
      "Epoch 4987/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 53.3723 - mae: 54.0450 - val_loss: 1171.8235 - val_mae: 1172.5167\n",
      "Epoch 4988/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 53.0163 - mae: 53.6927 - val_loss: 1182.0360 - val_mae: 1182.7292\n",
      "Epoch 4989/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 54.6488 - mae: 55.3234 - val_loss: 1172.0745 - val_mae: 1172.7676\n",
      "Epoch 4990/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 58.5924 - mae: 59.2673 - val_loss: 1182.5774 - val_mae: 1183.2704\n",
      "Epoch 4991/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 54.5392 - mae: 55.2184 - val_loss: 1183.9131 - val_mae: 1184.6062\n",
      "Epoch 4992/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 51.8278 - mae: 52.5069 - val_loss: 1177.6597 - val_mae: 1178.3522\n",
      "Epoch 4993/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 47.6112 - mae: 48.2838 - val_loss: 1187.4146 - val_mae: 1188.1077\n",
      "Epoch 4994/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 44.8519 - mae: 45.5294 - val_loss: 1177.3989 - val_mae: 1178.0920\n",
      "Epoch 4995/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 51.3153 - mae: 51.9911 - val_loss: 1197.4122 - val_mae: 1198.1055\n",
      "Epoch 4996/5000\n",
      "46/46 [==============================] - 2s 43ms/step - loss: 53.3856 - mae: 54.0565 - val_loss: 1174.1819 - val_mae: 1174.8745\n",
      "Epoch 4997/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 47.3973 - mae: 48.0709 - val_loss: 1195.2705 - val_mae: 1195.9637\n",
      "Epoch 4998/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 53.1327 - mae: 53.8052 - val_loss: 1157.7291 - val_mae: 1158.4219\n",
      "Epoch 4999/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 47.3038 - mae: 47.9747 - val_loss: 1172.6973 - val_mae: 1173.3894\n",
      "Epoch 5000/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 48.9119 - mae: 49.5882 - val_loss: 1194.2994 - val_mae: 1194.9918\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_mae',\n",
    "    directory='btc_tune',\n",
    "    project_name='ANN_TUNE'\n",
    ")\n",
    "\n",
    "tuner.search(X_train, Y_train, epochs=5000, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9884fdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters.\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "# Build the model with the best hp.\n",
    "regressor = build_model(best_hp)\n",
    "# Fit with the entire dataset.\n",
    "X_all = np.concatenate((X_train, X_val))\n",
    "Y_all = np.concatenate((Y_train, Y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46c3b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best hyperparameters\n",
    "best_hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a7de56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72db04c9",
   "metadata": {},
   "source": [
    "val_loss is the value of cost function for your cross-validation data and loss is the value of cost function for your training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85653fe",
   "metadata": {},
   "source": [
    "stop training when val_loss is not increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48b24b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=1000,verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85174a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26820/1361768653.py:1: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  regressor=KerasRegressor(build_fn=sequential_model,epochs=5000,verbose=1, use_multiprocessing=True)\n"
     ]
    }
   ],
   "source": [
    "regressor.fit(x=X_all, y=Y_all, epochs=5000, use_multiprocessing=True, callbacks=[earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf8814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred=regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0f090b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 127us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9966428561796283"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for check\n",
    "Y_train_pred=regressor.predict(X_train)\n",
    "r2_score(Y_train, Y_train_pred) #training score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e757ef47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2:0.3289147112445364\n"
     ]
    }
   ],
   "source": [
    "r2=r2_score(Y_test[:-30],y_pred[:-30]) #score/ r^2\n",
    "print(f'r2:{r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ef1c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2_oos\n",
    "def r2_oos(ret, pred):\n",
    "    sum_of_sq_res = np.nansum(np.power((ret-pred), 2))\n",
    "    sum_of_sq_total = np.nansum(np.power(ret, 2))\n",
    "    \n",
    "    return 1-sum_of_sq_res/sum_of_sq_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8969a6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_oos:0.9181680230673898\n"
     ]
    }
   ],
   "source": [
    "r2_oos = r2_oos(Y_test[:-30], y_pred[:-30])\n",
    "print(f'r2_oos:{r2_oos}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b87143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae:8630.756097837935\n",
      "rmse:11084.832551513839\n",
      "mape:24.996031025506138\n"
     ]
    }
   ],
   "source": [
    "mae=mean_absolute_error(Y_test[:-30],y_pred[:-30]) #mae\n",
    "print(f'mae:{mae}')\n",
    "\n",
    "rmse=np.sqrt(mean_squared_error(Y_test[:-30],y_pred[:-30])) #rmse\n",
    "print(f'rmse:{rmse}')\n",
    "\n",
    "mape=mean_absolute_percentage_error(Y_test[:-30],y_pred[:-30]) #mape\n",
    "print(f'mape:{mape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb36caf",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5641115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546/546 [==============================] - 0s 83us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-06-01</th>\n",
       "      <td>33731.0</td>\n",
       "      <td>34394.320312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-02</th>\n",
       "      <td>33285.0</td>\n",
       "      <td>34062.855469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-03</th>\n",
       "      <td>34298.0</td>\n",
       "      <td>33482.195312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-04</th>\n",
       "      <td>35271.0</td>\n",
       "      <td>34105.074219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-05</th>\n",
       "      <td>34100.0</td>\n",
       "      <td>33992.710938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>24159.792969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>23844.636719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>23678.773438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>23188.109375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>22679.246094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>546 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             y_test        y_pred\n",
       "Date                             \n",
       "2021-06-01  33731.0  34394.320312\n",
       "2021-06-02  33285.0  34062.855469\n",
       "2021-06-03  34298.0  33482.195312\n",
       "2021-06-04  35271.0  34105.074219\n",
       "2021-06-05  34100.0  33992.710938\n",
       "...             ...           ...\n",
       "2022-11-24      NaN  24159.792969\n",
       "2022-11-25      NaN  23844.636719\n",
       "2022-11-26      NaN  23678.773438\n",
       "2022-11-27      NaN  23188.109375\n",
       "2022-11-28      NaN  22679.246094\n",
       "\n",
       "[546 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_df = pd.DataFrame(zip(Y_test,y_pred),columns=['Y_test','y_pred'])\n",
    "pre_df.index = Y_test.index\n",
    "pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4bd2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df['pred_returns'] = pre_df['y_pred'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b94aa95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>pred_returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-06-01</th>\n",
       "      <td>33731.0</td>\n",
       "      <td>34394.320312</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-02</th>\n",
       "      <td>33285.0</td>\n",
       "      <td>34062.855469</td>\n",
       "      <td>-0.009637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-03</th>\n",
       "      <td>34298.0</td>\n",
       "      <td>33482.195312</td>\n",
       "      <td>-0.017047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-04</th>\n",
       "      <td>35271.0</td>\n",
       "      <td>34105.074219</td>\n",
       "      <td>0.018603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-05</th>\n",
       "      <td>34100.0</td>\n",
       "      <td>33992.710938</td>\n",
       "      <td>-0.003295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>24159.792969</td>\n",
       "      <td>-0.012123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>23844.636719</td>\n",
       "      <td>-0.013045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>23678.773438</td>\n",
       "      <td>-0.006956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>23188.109375</td>\n",
       "      <td>-0.020722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>22679.246094</td>\n",
       "      <td>-0.021945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>546 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             y_test        y_pred  pred_returns\n",
       "Date                                           \n",
       "2021-06-01  33731.0  34394.320312           NaN\n",
       "2021-06-02  33285.0  34062.855469     -0.009637\n",
       "2021-06-03  34298.0  33482.195312     -0.017047\n",
       "2021-06-04  35271.0  34105.074219      0.018603\n",
       "2021-06-05  34100.0  33992.710938     -0.003295\n",
       "...             ...           ...           ...\n",
       "2022-11-24      NaN  24159.792969     -0.012123\n",
       "2022-11-25      NaN  23844.636719     -0.013045\n",
       "2022-11-26      NaN  23678.773438     -0.006956\n",
       "2022-11-27      NaN  23188.109375     -0.020722\n",
       "2022-11-28      NaN  22679.246094     -0.021945\n",
       "\n",
       "[546 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9674c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(zip(Y_test,y_pred),columns=['Y_test','y_pred']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732393ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df.to_csv(\"../result/ANN/btc_NN.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e12ff04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kdeconnect-cli -n TAS-AN00 --ping-msg 'Script complete!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b42805c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('tensorplustorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "efc08374433b8d8e4a9fd8a0a66f7295c7ce37eceb639810a945045512ff181b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
