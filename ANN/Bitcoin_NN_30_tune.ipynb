{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5b918ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-03 11:06:49.423855: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# conda install keras-tuner\n",
    "import keras_tuner as kt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from datetime import datetime\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n",
    "from keras import losses\n",
    "from keras.constraints import maxnorm\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_validate\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from collections import Counter\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import randint\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, accuracy_score, f1_score, classification_report\n",
    "# import pickle\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, roc_auc_score\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ded3682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535a3fac",
   "metadata": {},
   "source": [
    "### Bitcoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1467391e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13680/2421463472.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  btc['Date'][i]  =  datetime.strptime(btc['Date'][i], '%Y/%m/%d')\n"
     ]
    }
   ],
   "source": [
    "regs = pd.read_csv(\"../Data/train_btc_selected_features.csv\")\n",
    "btc = pd.read_csv(\"../Data/btc_Data.csv\")\n",
    "for i in range(len(btc['Date'])):\n",
    "    btc['Date'][i]  =  datetime.strptime(btc['Date'][i], '%Y/%m/%d')\n",
    "\n",
    "btc = btc.set_index(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d665a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "btcData = btc[regs.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8227b01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13680/751970969.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  btcData['returns'] = btcData['priceUSD'].pct_change()\n"
     ]
    }
   ],
   "source": [
    "btcData['returns'] = btcData['priceUSD'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0204bfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = btcData.drop(columns=['priceUSD'])\n",
    "Data = Data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a926d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>difficulty14mom</th>\n",
       "      <th>difficulty30mom</th>\n",
       "      <th>difficulty3rsi</th>\n",
       "      <th>difficulty90mom</th>\n",
       "      <th>difficulty90sma</th>\n",
       "      <th>hashrate90std</th>\n",
       "      <th>hashrate90var</th>\n",
       "      <th>median_transaction_fee90rocUSD</th>\n",
       "      <th>median_transaction_feeUSD</th>\n",
       "      <th>mining_profitability</th>\n",
       "      <th>...</th>\n",
       "      <th>price90wmaUSD</th>\n",
       "      <th>sentinusd90emaUSD</th>\n",
       "      <th>sentinusdUSD</th>\n",
       "      <th>top100cap</th>\n",
       "      <th>top100cap14trx</th>\n",
       "      <th>top100cap30rsi</th>\n",
       "      <th>top100cap30trx</th>\n",
       "      <th>top100cap90rsi</th>\n",
       "      <th>top100cap90trx</th>\n",
       "      <th>returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-07-18</th>\n",
       "      <td>1.621380e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.737230e+02</td>\n",
       "      <td>2.055500e+01</td>\n",
       "      <td>8.384389e+08</td>\n",
       "      <td>1.757449e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>401834.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.612000e+03</td>\n",
       "      <td>25.782</td>\n",
       "      <td>0.139</td>\n",
       "      <td>71.191</td>\n",
       "      <td>0.308</td>\n",
       "      <td>79.756</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-19</th>\n",
       "      <td>1.621380e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.737230e+02</td>\n",
       "      <td>2.248600e+01</td>\n",
       "      <td>8.819952e+08</td>\n",
       "      <td>1.944789e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>481473.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.047000e+03</td>\n",
       "      <td>25.685</td>\n",
       "      <td>0.123</td>\n",
       "      <td>68.863</td>\n",
       "      <td>0.300</td>\n",
       "      <td>78.999</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.183196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-20</th>\n",
       "      <td>1.584060e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.735570e+02</td>\n",
       "      <td>2.441400e+01</td>\n",
       "      <td>9.280542e+08</td>\n",
       "      <td>2.153212e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>431831.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.341000e+03</td>\n",
       "      <td>25.602</td>\n",
       "      <td>0.107</td>\n",
       "      <td>66.923</td>\n",
       "      <td>0.292</td>\n",
       "      <td>78.355</td>\n",
       "      <td>0.544</td>\n",
       "      <td>-0.088475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-21</th>\n",
       "      <td>1.580420e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.700790e+02</td>\n",
       "      <td>2.630400e+01</td>\n",
       "      <td>9.761949e+08</td>\n",
       "      <td>2.382391e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>460783.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.122000e+03</td>\n",
       "      <td>25.638</td>\n",
       "      <td>0.092</td>\n",
       "      <td>67.334</td>\n",
       "      <td>0.284</td>\n",
       "      <td>78.431</td>\n",
       "      <td>0.541</td>\n",
       "      <td>-0.020434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-22</th>\n",
       "      <td>1.580420e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.700790e+02</td>\n",
       "      <td>2.819400e+01</td>\n",
       "      <td>1.019474e+09</td>\n",
       "      <td>2.598318e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>334641.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.570000e+03</td>\n",
       "      <td>25.604</td>\n",
       "      <td>0.078</td>\n",
       "      <td>66.526</td>\n",
       "      <td>0.275</td>\n",
       "      <td>78.166</td>\n",
       "      <td>0.537</td>\n",
       "      <td>-0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-24</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.384949e+13</td>\n",
       "      <td>4.612156e+19</td>\n",
       "      <td>5.317997e+38</td>\n",
       "      <td>165.380</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.0577</td>\n",
       "      <td>...</td>\n",
       "      <td>18820.0</td>\n",
       "      <td>28544682257</td>\n",
       "      <td>6.114577e+09</td>\n",
       "      <td>15.236</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>40.118</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>45.140</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.008075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-25</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.394503e+13</td>\n",
       "      <td>4.542600e+19</td>\n",
       "      <td>5.158804e+38</td>\n",
       "      <td>214.464</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.0541</td>\n",
       "      <td>...</td>\n",
       "      <td>18759.0</td>\n",
       "      <td>28076113671</td>\n",
       "      <td>7.224812e+09</td>\n",
       "      <td>15.231</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>39.880</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>45.047</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.006565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-26</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.404058e+13</td>\n",
       "      <td>4.624135e+19</td>\n",
       "      <td>5.345655e+38</td>\n",
       "      <td>255.004</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.0571</td>\n",
       "      <td>...</td>\n",
       "      <td>18701.0</td>\n",
       "      <td>27566714445</td>\n",
       "      <td>4.898449e+09</td>\n",
       "      <td>15.233</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>40.069</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>45.106</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.004001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-27</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.413612e+13</td>\n",
       "      <td>4.576227e+19</td>\n",
       "      <td>5.235464e+38</td>\n",
       "      <td>113.430</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.0618</td>\n",
       "      <td>...</td>\n",
       "      <td>18642.0</td>\n",
       "      <td>27053515674</td>\n",
       "      <td>4.216170e+09</td>\n",
       "      <td>15.259</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>41.833</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>45.656</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.423166e+13</td>\n",
       "      <td>4.552126e+19</td>\n",
       "      <td>5.180463e+38</td>\n",
       "      <td>104.294</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.0592</td>\n",
       "      <td>...</td>\n",
       "      <td>18578.0</td>\n",
       "      <td>26631320968</td>\n",
       "      <td>7.843657e+09</td>\n",
       "      <td>15.219</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>39.965</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>44.957</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.019828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4517 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            difficulty14mom  difficulty30mom  difficulty3rsi  difficulty90mom  \\\n",
       "Date                                                                            \n",
       "2010-07-18     1.621380e+02     1.641620e+02          100.00     1.737230e+02   \n",
       "2010-07-19     1.621380e+02     1.641620e+02          100.00     1.737230e+02   \n",
       "2010-07-20     1.584060e+02     1.641620e+02          100.00     1.735570e+02   \n",
       "2010-07-21     1.580420e+02     1.641620e+02          100.00     1.700790e+02   \n",
       "2010-07-22     1.580420e+02     1.641620e+02          100.00     1.700790e+02   \n",
       "...                     ...              ...             ...              ...   \n",
       "2022-11-24     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-25     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-26     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-27     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-28     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "\n",
       "            difficulty90sma  hashrate90std  hashrate90var  \\\n",
       "Date                                                        \n",
       "2010-07-18     2.055500e+01   8.384389e+08   1.757449e+17   \n",
       "2010-07-19     2.248600e+01   8.819952e+08   1.944789e+17   \n",
       "2010-07-20     2.441400e+01   9.280542e+08   2.153212e+17   \n",
       "2010-07-21     2.630400e+01   9.761949e+08   2.382391e+17   \n",
       "2010-07-22     2.819400e+01   1.019474e+09   2.598318e+17   \n",
       "...                     ...            ...            ...   \n",
       "2022-11-24     3.384949e+13   4.612156e+19   5.317997e+38   \n",
       "2022-11-25     3.394503e+13   4.542600e+19   5.158804e+38   \n",
       "2022-11-26     3.404058e+13   4.624135e+19   5.345655e+38   \n",
       "2022-11-27     3.413612e+13   4.576227e+19   5.235464e+38   \n",
       "2022-11-28     3.423166e+13   4.552126e+19   5.180463e+38   \n",
       "\n",
       "            median_transaction_fee90rocUSD  median_transaction_feeUSD  \\\n",
       "Date                                                                    \n",
       "2010-07-18                           0.000                      0.000   \n",
       "2010-07-19                           0.000                      0.000   \n",
       "2010-07-20                           0.000                      0.000   \n",
       "2010-07-21                           0.000                      0.000   \n",
       "2010-07-22                           0.000                      0.000   \n",
       "...                                    ...                        ...   \n",
       "2022-11-24                         165.380                      0.818   \n",
       "2022-11-25                         214.464                      0.812   \n",
       "2022-11-26                         255.004                      0.809   \n",
       "2022-11-27                         113.430                      0.619   \n",
       "2022-11-28                         104.294                      0.622   \n",
       "\n",
       "            mining_profitability  ...  price90wmaUSD  sentinusd90emaUSD  \\\n",
       "Date                              ...                                     \n",
       "2010-07-18           401834.0000  ...            0.0                  0   \n",
       "2010-07-19           481473.0000  ...            0.0                  0   \n",
       "2010-07-20           431831.0000  ...            0.0                  0   \n",
       "2010-07-21           460783.0000  ...            0.0                  0   \n",
       "2010-07-22           334641.0000  ...            0.0                  0   \n",
       "...                          ...  ...            ...                ...   \n",
       "2022-11-24                0.0577  ...        18820.0        28544682257   \n",
       "2022-11-25                0.0541  ...        18759.0        28076113671   \n",
       "2022-11-26                0.0571  ...        18701.0        27566714445   \n",
       "2022-11-27                0.0618  ...        18642.0        27053515674   \n",
       "2022-11-28                0.0592  ...        18578.0        26631320968   \n",
       "\n",
       "            sentinusdUSD  top100cap  top100cap14trx  top100cap30rsi  \\\n",
       "Date                                                                  \n",
       "2010-07-18  2.612000e+03     25.782           0.139          71.191   \n",
       "2010-07-19  4.047000e+03     25.685           0.123          68.863   \n",
       "2010-07-20  2.341000e+03     25.602           0.107          66.923   \n",
       "2010-07-21  2.122000e+03     25.638           0.092          67.334   \n",
       "2010-07-22  2.570000e+03     25.604           0.078          66.526   \n",
       "...                  ...        ...             ...             ...   \n",
       "2022-11-24  6.114577e+09     15.236          -0.080          40.118   \n",
       "2022-11-25  7.224812e+09     15.231          -0.075          39.880   \n",
       "2022-11-26  4.898449e+09     15.233          -0.070          40.069   \n",
       "2022-11-27  4.216170e+09     15.259          -0.065          41.833   \n",
       "2022-11-28  7.843657e+09     15.219          -0.061          39.965   \n",
       "\n",
       "            top100cap30trx  top100cap90rsi  top100cap90trx   returns  \n",
       "Date                                                                  \n",
       "2010-07-18           0.308          79.756           0.550  0.466667  \n",
       "2010-07-19           0.300          78.999           0.547  0.183196  \n",
       "2010-07-20           0.292          78.355           0.544 -0.088475  \n",
       "2010-07-21           0.284          78.431           0.541 -0.020434  \n",
       "2010-07-22           0.275          78.166           0.537 -0.153846  \n",
       "...                    ...             ...             ...       ...  \n",
       "2022-11-24          -0.042          45.140           0.001  0.008075  \n",
       "2022-11-25          -0.043          45.047           0.001 -0.006565  \n",
       "2022-11-26          -0.043          45.106           0.001  0.004001  \n",
       "2022-11-27          -0.044          45.656           0.000 -0.001147  \n",
       "2022-11-28          -0.044          44.957           0.000 -0.019828  \n",
       "\n",
       "[4517 rows x 32 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5d4f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide X and Y\n",
    "X = Data.iloc[:,0:]\n",
    "#Y = Data['returns']   # 用returns的话就用这一行，然后把下一行comment掉\n",
    "Y = btcData['priceUSD'].shift(-30)[1:] # 反之亦然"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb3275b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>difficulty14mom</th>\n",
       "      <th>difficulty30mom</th>\n",
       "      <th>difficulty3rsi</th>\n",
       "      <th>difficulty90mom</th>\n",
       "      <th>difficulty90sma</th>\n",
       "      <th>hashrate90std</th>\n",
       "      <th>hashrate90var</th>\n",
       "      <th>median_transaction_fee90rocUSD</th>\n",
       "      <th>median_transaction_feeUSD</th>\n",
       "      <th>mining_profitability</th>\n",
       "      <th>...</th>\n",
       "      <th>price90wmaUSD</th>\n",
       "      <th>sentinusd90emaUSD</th>\n",
       "      <th>sentinusdUSD</th>\n",
       "      <th>top100cap</th>\n",
       "      <th>top100cap14trx</th>\n",
       "      <th>top100cap30rsi</th>\n",
       "      <th>top100cap30trx</th>\n",
       "      <th>top100cap90rsi</th>\n",
       "      <th>top100cap90trx</th>\n",
       "      <th>returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-07-18</th>\n",
       "      <td>162.138</td>\n",
       "      <td>164.162</td>\n",
       "      <td>100.0</td>\n",
       "      <td>173.723</td>\n",
       "      <td>20.555</td>\n",
       "      <td>838438881.0</td>\n",
       "      <td>1.757449e+17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>401834.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2612.0</td>\n",
       "      <td>25.782</td>\n",
       "      <td>0.139</td>\n",
       "      <td>71.191</td>\n",
       "      <td>0.308</td>\n",
       "      <td>79.756</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-19</th>\n",
       "      <td>162.138</td>\n",
       "      <td>164.162</td>\n",
       "      <td>100.0</td>\n",
       "      <td>173.723</td>\n",
       "      <td>22.486</td>\n",
       "      <td>881995244.0</td>\n",
       "      <td>1.944789e+17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>481473.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4047.0</td>\n",
       "      <td>25.685</td>\n",
       "      <td>0.123</td>\n",
       "      <td>68.863</td>\n",
       "      <td>0.300</td>\n",
       "      <td>78.999</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.183196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-20</th>\n",
       "      <td>158.406</td>\n",
       "      <td>164.162</td>\n",
       "      <td>100.0</td>\n",
       "      <td>173.557</td>\n",
       "      <td>24.414</td>\n",
       "      <td>928054231.0</td>\n",
       "      <td>2.153212e+17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>431831.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2341.0</td>\n",
       "      <td>25.602</td>\n",
       "      <td>0.107</td>\n",
       "      <td>66.923</td>\n",
       "      <td>0.292</td>\n",
       "      <td>78.355</td>\n",
       "      <td>0.544</td>\n",
       "      <td>-0.088475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            difficulty14mom  difficulty30mom  difficulty3rsi  difficulty90mom  \\\n",
       "Date                                                                            \n",
       "2010-07-18          162.138          164.162           100.0          173.723   \n",
       "2010-07-19          162.138          164.162           100.0          173.723   \n",
       "2010-07-20          158.406          164.162           100.0          173.557   \n",
       "\n",
       "            difficulty90sma  hashrate90std  hashrate90var  \\\n",
       "Date                                                        \n",
       "2010-07-18           20.555    838438881.0   1.757449e+17   \n",
       "2010-07-19           22.486    881995244.0   1.944789e+17   \n",
       "2010-07-20           24.414    928054231.0   2.153212e+17   \n",
       "\n",
       "            median_transaction_fee90rocUSD  median_transaction_feeUSD  \\\n",
       "Date                                                                    \n",
       "2010-07-18                             0.0                        0.0   \n",
       "2010-07-19                             0.0                        0.0   \n",
       "2010-07-20                             0.0                        0.0   \n",
       "\n",
       "            mining_profitability  ...  price90wmaUSD  sentinusd90emaUSD  \\\n",
       "Date                              ...                                     \n",
       "2010-07-18              401834.0  ...            0.0                  0   \n",
       "2010-07-19              481473.0  ...            0.0                  0   \n",
       "2010-07-20              431831.0  ...            0.0                  0   \n",
       "\n",
       "            sentinusdUSD  top100cap  top100cap14trx  top100cap30rsi  \\\n",
       "Date                                                                  \n",
       "2010-07-18        2612.0     25.782           0.139          71.191   \n",
       "2010-07-19        4047.0     25.685           0.123          68.863   \n",
       "2010-07-20        2341.0     25.602           0.107          66.923   \n",
       "\n",
       "            top100cap30trx  top100cap90rsi  top100cap90trx   returns  \n",
       "Date                                                                  \n",
       "2010-07-18           0.308          79.756           0.550  0.466667  \n",
       "2010-07-19           0.300          78.999           0.547  0.183196  \n",
       "2010-07-20           0.292          78.355           0.544 -0.088475  \n",
       "\n",
       "[3 rows x 32 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd589ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into three data sets\n",
    "X_train = X['2016-01-01':'2019-12-31']\n",
    "X_val = X['2020-01-01':'2021-05-31']\n",
    "X_test = X['2021-06-01':'2023-01-01']\n",
    "\n",
    "Y_train = Y['2016-01-01':'2019-12-31']\n",
    "Y_val = Y['2020-01-01':'2021-05-31']\n",
    "Y_test = Y['2021-06-01':'2023-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e5e9fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............ (step 1 of 2) Processing mixmax, total=   0.0s\n",
      "[Pipeline] ............ (step 2 of 2) Processing robust, total=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;mixmax&#x27;, MinMaxScaler()), [&#x27;robust&#x27;, RobustScaler()]],\n",
       "         verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;mixmax&#x27;, MinMaxScaler()), [&#x27;robust&#x27;, RobustScaler()]],\n",
       "         verbose=True)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RobustScaler</label><div class=\"sk-toggleable__content\"><pre>RobustScaler()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('mixmax', MinMaxScaler()), ['robust', RobustScaler()]],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators=[]\n",
    "estimators.append(['mixmax',MinMaxScaler()])\n",
    "estimators.append(['robust',RobustScaler()])\n",
    "scale=Pipeline(estimators,verbose=True)\n",
    "scale.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6fc0859",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=scale.transform(X_train)\n",
    "X_test=scale.transform(X_test)\n",
    "X_val = scale.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8a84d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f6916ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define neural network model\n",
    "shape=X.shape[1]\n",
    "def build_model(hp, initializer='normal', activation='relu', NUM_FEATURES=shape):\n",
    "    # create model\n",
    "    hp_units1 = hp.Int('units1', min_value=32, max_value=512, step=32)\n",
    "    hp_units2 = hp.Int('units2', min_value=32, max_value=512, step=32)\n",
    "    hp_units3 = hp.Int('units3', min_value=32, max_value=512, step=32)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hp_units1, input_shape=(NUM_FEATURES,), kernel_initializer=initializer, activation=activation))\n",
    "    model.add(Dense(hp_units2, activation=activation))\n",
    "    model.add(Dense(hp_units3, activation=activation))\n",
    "    model.add(Dense(1, activation=activation))\n",
    "    # Compile model\n",
    "    adam=keras.optimizers.Adam(lr=lr_schedule(0), amsgrad=True)\n",
    "    #sgd=keras.optimizers.SGD(learning_rate=0.08, momentum=0.9, nesterov=False)\n",
    "    model.compile(loss='logcosh', optimizer=adam, metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112c8df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-03 11:06:51.935927: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-03 11:06:51.936722: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "/home/spectre/anaconda3/envs/tensorplustorch/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "480               |?                 |units1\n",
      "384               |?                 |units2\n",
      "320               |?                 |units3\n",
      "\n",
      "Learning rate:  0.001\n",
      "Epoch 1/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 4638.6973 - mae: 4639.3906 - val_loss: 18311.5977 - val_mae: 18312.2891\n",
      "Epoch 2/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2424.6301 - mae: 2425.3230 - val_loss: 6763.4771 - val_mae: 6764.1709\n",
      "Epoch 3/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 1432.9191 - mae: 1433.6121 - val_loss: 6482.1313 - val_mae: 6482.8242\n",
      "Epoch 4/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 1198.4069 - mae: 1199.1002 - val_loss: 6855.1562 - val_mae: 6855.8496\n",
      "Epoch 5/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 1068.0933 - mae: 1068.7859 - val_loss: 7274.4834 - val_mae: 7275.1763\n",
      "Epoch 6/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 980.4193 - mae: 981.1119 - val_loss: 7074.3228 - val_mae: 7075.0166\n",
      "Epoch 7/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 886.7975 - mae: 887.4900 - val_loss: 6609.4785 - val_mae: 6610.1729\n",
      "Epoch 8/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 862.2034 - mae: 862.8959 - val_loss: 7054.0176 - val_mae: 7054.7104\n",
      "Epoch 9/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 798.8704 - mae: 799.5617 - val_loss: 7178.0146 - val_mae: 7178.7075\n",
      "Epoch 10/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 741.4408 - mae: 742.1318 - val_loss: 7485.6953 - val_mae: 7486.3887\n",
      "Epoch 11/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 711.5714 - mae: 712.2643 - val_loss: 7358.3999 - val_mae: 7359.0928\n",
      "Epoch 12/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 685.5760 - mae: 686.2681 - val_loss: 7116.7690 - val_mae: 7117.4619\n",
      "Epoch 13/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 684.0607 - mae: 684.7527 - val_loss: 8093.3691 - val_mae: 8094.0625\n",
      "Epoch 14/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 667.1904 - mae: 667.8831 - val_loss: 7378.2251 - val_mae: 7378.9194\n",
      "Epoch 15/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 619.8024 - mae: 620.4939 - val_loss: 7541.0225 - val_mae: 7541.7158\n",
      "Epoch 16/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 617.0598 - mae: 617.7520 - val_loss: 7595.2783 - val_mae: 7595.9727\n",
      "Epoch 17/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 599.5854 - mae: 600.2772 - val_loss: 7793.6113 - val_mae: 7794.3042\n",
      "Epoch 18/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 584.7150 - mae: 585.4074 - val_loss: 7859.1836 - val_mae: 7859.8770\n",
      "Epoch 19/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 557.7863 - mae: 558.4775 - val_loss: 7906.2280 - val_mae: 7906.9209\n",
      "Epoch 20/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 572.8237 - mae: 573.5148 - val_loss: 7967.0430 - val_mae: 7967.7363\n",
      "Epoch 21/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 551.6910 - mae: 552.3834 - val_loss: 7342.0938 - val_mae: 7342.7866\n",
      "Epoch 22/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 518.2421 - mae: 518.9338 - val_loss: 7241.9111 - val_mae: 7242.6050\n",
      "Epoch 23/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 530.3719 - mae: 531.0638 - val_loss: 7584.4038 - val_mae: 7585.0967\n",
      "Epoch 24/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 515.1329 - mae: 515.8240 - val_loss: 7334.2104 - val_mae: 7334.9038\n",
      "Epoch 25/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 497.5851 - mae: 498.2772 - val_loss: 7290.3535 - val_mae: 7291.0454\n",
      "Epoch 26/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 478.9437 - mae: 479.6362 - val_loss: 7349.6953 - val_mae: 7350.3892\n",
      "Epoch 27/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 499.8893 - mae: 500.5814 - val_loss: 6879.6074 - val_mae: 6880.3008\n",
      "Epoch 28/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 458.5591 - mae: 459.2512 - val_loss: 7213.8594 - val_mae: 7214.5518\n",
      "Epoch 29/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 464.2992 - mae: 464.9902 - val_loss: 7557.2603 - val_mae: 7557.9521\n",
      "Epoch 30/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 466.3418 - mae: 467.0330 - val_loss: 7575.7983 - val_mae: 7576.4917\n",
      "Epoch 31/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 461.5216 - mae: 462.2138 - val_loss: 7788.4155 - val_mae: 7789.1084\n",
      "Epoch 32/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 432.0183 - mae: 432.7101 - val_loss: 7669.2617 - val_mae: 7669.9541\n",
      "Epoch 33/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 456.8873 - mae: 457.5797 - val_loss: 7515.3594 - val_mae: 7516.0518\n",
      "Epoch 34/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 445.5250 - mae: 446.2168 - val_loss: 7737.6606 - val_mae: 7738.3535\n",
      "Epoch 35/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 447.6469 - mae: 448.3389 - val_loss: 7542.2129 - val_mae: 7542.9062\n",
      "Epoch 36/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 420.4012 - mae: 421.0935 - val_loss: 7925.2236 - val_mae: 7925.9175\n",
      "Epoch 37/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 417.1743 - mae: 417.8658 - val_loss: 7329.5005 - val_mae: 7330.1934\n",
      "Epoch 38/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 405.4299 - mae: 406.1220 - val_loss: 8075.8599 - val_mae: 8076.5537\n",
      "Epoch 39/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 398.7752 - mae: 399.4669 - val_loss: 7847.0586 - val_mae: 7847.7515\n",
      "Epoch 40/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 413.2866 - mae: 413.9791 - val_loss: 7198.2607 - val_mae: 7198.9546\n",
      "Epoch 41/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 399.2147 - mae: 399.9057 - val_loss: 7736.8086 - val_mae: 7737.5020\n",
      "Epoch 42/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 415.7198 - mae: 416.4114 - val_loss: 7180.8101 - val_mae: 7181.5029\n",
      "Epoch 43/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 399.8567 - mae: 400.5486 - val_loss: 7276.1963 - val_mae: 7276.8901\n",
      "Epoch 44/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 374.7497 - mae: 375.4420 - val_loss: 7930.0474 - val_mae: 7930.7402\n",
      "Epoch 45/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 378.8269 - mae: 379.5167 - val_loss: 7362.5415 - val_mae: 7363.2339\n",
      "Epoch 46/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 386.8334 - mae: 387.5246 - val_loss: 8166.0317 - val_mae: 8166.7251\n",
      "Epoch 47/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 369.9940 - mae: 370.6831 - val_loss: 7980.2939 - val_mae: 7980.9878\n",
      "Epoch 48/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 363.5000 - mae: 364.1911 - val_loss: 8490.7041 - val_mae: 8491.3975\n",
      "Epoch 49/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 376.1326 - mae: 376.8246 - val_loss: 8508.4629 - val_mae: 8509.1553\n",
      "Epoch 50/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 371.6500 - mae: 372.3410 - val_loss: 7940.9263 - val_mae: 7941.6201\n",
      "Epoch 51/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 367.5061 - mae: 368.1963 - val_loss: 8411.5420 - val_mae: 8412.2363\n",
      "Epoch 52/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 351.8095 - mae: 352.5001 - val_loss: 7969.3965 - val_mae: 7970.0884\n",
      "Epoch 53/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 357.9570 - mae: 358.6481 - val_loss: 8928.4736 - val_mae: 8929.1680\n",
      "Epoch 54/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 411.2164 - mae: 411.9074 - val_loss: 7846.5410 - val_mae: 7847.2329\n",
      "Epoch 55/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 346.7432 - mae: 347.4332 - val_loss: 8508.8545 - val_mae: 8509.5498\n",
      "Epoch 56/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 6ms/step - loss: 337.9890 - mae: 338.6802 - val_loss: 8157.9189 - val_mae: 8158.6123\n",
      "Epoch 57/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 352.1619 - mae: 352.8517 - val_loss: 8479.8291 - val_mae: 8480.5225\n",
      "Epoch 58/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 387.1444 - mae: 387.8362 - val_loss: 8302.6279 - val_mae: 8303.3203\n",
      "Epoch 59/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 317.6659 - mae: 318.3555 - val_loss: 7943.5581 - val_mae: 7944.2524\n",
      "Epoch 60/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 317.6264 - mae: 318.3179 - val_loss: 7777.7842 - val_mae: 7778.4771\n",
      "Epoch 61/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 315.1986 - mae: 315.8893 - val_loss: 7719.6948 - val_mae: 7720.3877\n",
      "Epoch 62/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 314.0262 - mae: 314.7172 - val_loss: 8104.6504 - val_mae: 8105.3433\n",
      "Epoch 63/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 327.2100 - mae: 327.9013 - val_loss: 7898.3882 - val_mae: 7899.0815\n",
      "Epoch 64/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 320.0847 - mae: 320.7758 - val_loss: 7847.5107 - val_mae: 7848.2031\n",
      "Epoch 65/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 329.8798 - mae: 330.5706 - val_loss: 8153.3057 - val_mae: 8153.9990\n",
      "Epoch 66/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 329.6586 - mae: 330.3497 - val_loss: 7783.7378 - val_mae: 7784.4307\n",
      "Epoch 67/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 313.8366 - mae: 314.5266 - val_loss: 7881.0913 - val_mae: 7881.7842\n",
      "Epoch 68/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 302.4082 - mae: 303.0999 - val_loss: 7873.9053 - val_mae: 7874.5962\n",
      "Epoch 69/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 300.4578 - mae: 301.1491 - val_loss: 7952.5620 - val_mae: 7953.2549\n",
      "Epoch 70/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 296.9265 - mae: 297.6162 - val_loss: 8404.6484 - val_mae: 8405.3418\n",
      "Epoch 71/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 303.4037 - mae: 304.0943 - val_loss: 8123.3945 - val_mae: 8124.0879\n",
      "Epoch 72/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 330.4535 - mae: 331.1447 - val_loss: 7990.5581 - val_mae: 7991.2520\n",
      "Epoch 73/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 304.1220 - mae: 304.8122 - val_loss: 8100.5918 - val_mae: 8101.2856\n",
      "Epoch 74/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 307.3842 - mae: 308.0759 - val_loss: 7525.5186 - val_mae: 7526.2124\n",
      "Epoch 75/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 334.9207 - mae: 335.6109 - val_loss: 8405.7266 - val_mae: 8406.4209\n",
      "Epoch 76/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 300.2975 - mae: 300.9879 - val_loss: 8493.4219 - val_mae: 8494.1162\n",
      "Epoch 77/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 284.3284 - mae: 285.0193 - val_loss: 8139.0308 - val_mae: 8139.7246\n",
      "Epoch 78/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 306.8942 - mae: 307.5836 - val_loss: 7812.6924 - val_mae: 7813.3853\n",
      "Epoch 79/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 347.7971 - mae: 348.4885 - val_loss: 8013.3379 - val_mae: 8014.0308\n",
      "Epoch 80/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 305.0196 - mae: 305.7099 - val_loss: 7525.1592 - val_mae: 7525.8525\n",
      "Epoch 81/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 304.2925 - mae: 304.9840 - val_loss: 7939.4165 - val_mae: 7940.1094\n",
      "Epoch 82/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 300.9207 - mae: 301.6091 - val_loss: 7231.9717 - val_mae: 7232.6646\n",
      "Epoch 83/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 307.3009 - mae: 307.9917 - val_loss: 8319.3896 - val_mae: 8320.0830\n",
      "Epoch 84/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 281.7773 - mae: 282.4684 - val_loss: 8547.1152 - val_mae: 8547.8086\n",
      "Epoch 85/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 282.1290 - mae: 282.8199 - val_loss: 7945.5693 - val_mae: 7946.2632\n",
      "Epoch 86/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 292.2811 - mae: 292.9715 - val_loss: 8565.8418 - val_mae: 8566.5361\n",
      "Epoch 87/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 294.1006 - mae: 294.7919 - val_loss: 7633.3887 - val_mae: 7634.0815\n",
      "Epoch 88/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 272.5056 - mae: 273.1953 - val_loss: 7741.1382 - val_mae: 7741.8306\n",
      "Epoch 89/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 276.2750 - mae: 276.9666 - val_loss: 7810.9727 - val_mae: 7811.6660\n",
      "Epoch 90/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 280.7497 - mae: 281.4405 - val_loss: 8012.1694 - val_mae: 8012.8618\n",
      "Epoch 91/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 279.5276 - mae: 280.2171 - val_loss: 8035.4883 - val_mae: 8036.1816\n",
      "Epoch 92/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 272.4784 - mae: 273.1692 - val_loss: 7801.9663 - val_mae: 7802.6587\n",
      "Epoch 93/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 271.2470 - mae: 271.9361 - val_loss: 8011.7847 - val_mae: 8012.4785\n",
      "Epoch 94/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 267.0608 - mae: 267.7506 - val_loss: 7762.7671 - val_mae: 7763.4604\n",
      "Epoch 95/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 282.9766 - mae: 283.6672 - val_loss: 8126.9111 - val_mae: 8127.6055\n",
      "Epoch 96/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 287.3184 - mae: 288.0081 - val_loss: 8296.2969 - val_mae: 8296.9902\n",
      "Epoch 97/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 266.9609 - mae: 267.6508 - val_loss: 7805.0127 - val_mae: 7805.7051\n",
      "Epoch 98/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 280.1554 - mae: 280.8457 - val_loss: 8269.2861 - val_mae: 8269.9795\n",
      "Epoch 99/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 282.3264 - mae: 283.0162 - val_loss: 8158.4814 - val_mae: 8159.1743\n",
      "Epoch 100/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 268.0888 - mae: 268.7790 - val_loss: 7950.7285 - val_mae: 7951.4229\n",
      "Epoch 101/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 318.4068 - mae: 319.0975 - val_loss: 7276.4395 - val_mae: 7277.1313\n",
      "Epoch 102/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 292.3983 - mae: 293.0887 - val_loss: 8050.4629 - val_mae: 8051.1558\n",
      "Epoch 103/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 263.9505 - mae: 264.6409 - val_loss: 8112.0347 - val_mae: 8112.7280\n",
      "Epoch 104/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 307.1266 - mae: 307.8163 - val_loss: 8310.0527 - val_mae: 8310.7480\n",
      "Epoch 105/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 320.1874 - mae: 320.8777 - val_loss: 8212.6104 - val_mae: 8213.3037\n",
      "Epoch 106/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 283.5980 - mae: 284.2874 - val_loss: 8427.1572 - val_mae: 8427.8516\n",
      "Epoch 107/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 286.3447 - mae: 287.0350 - val_loss: 8140.7319 - val_mae: 8141.4248\n",
      "Epoch 108/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 284.2557 - mae: 284.9460 - val_loss: 7933.4644 - val_mae: 7934.1572\n",
      "Epoch 109/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 260.1237 - mae: 260.8145 - val_loss: 7711.1279 - val_mae: 7711.8218\n",
      "Epoch 110/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 291.8514 - mae: 292.5407 - val_loss: 7906.8296 - val_mae: 7907.5239\n",
      "Epoch 111/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 253.2331 - mae: 253.9227 - val_loss: 8195.0000 - val_mae: 8195.6934\n",
      "Epoch 112/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 268.3617 - mae: 269.0525 - val_loss: 8151.9624 - val_mae: 8152.6567\n",
      "Epoch 113/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 6ms/step - loss: 264.9640 - mae: 265.6530 - val_loss: 7763.3340 - val_mae: 7764.0264\n",
      "Epoch 114/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 261.1137 - mae: 261.8031 - val_loss: 8087.6191 - val_mae: 8088.3125\n",
      "Epoch 115/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 288.4123 - mae: 289.1009 - val_loss: 7967.3672 - val_mae: 7968.0605\n",
      "Epoch 116/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 281.8394 - mae: 282.5279 - val_loss: 8966.3154 - val_mae: 8967.0088\n",
      "Epoch 117/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 302.9285 - mae: 303.6194 - val_loss: 8310.2188 - val_mae: 8310.9111\n",
      "Epoch 118/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 254.8239 - mae: 255.5132 - val_loss: 8052.4336 - val_mae: 8053.1270\n",
      "Epoch 119/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 251.9321 - mae: 252.6208 - val_loss: 8159.4331 - val_mae: 8160.1265\n",
      "Epoch 120/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 246.4630 - mae: 247.1519 - val_loss: 7937.3174 - val_mae: 7938.0112\n",
      "Epoch 121/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 267.6290 - mae: 268.3176 - val_loss: 8298.9297 - val_mae: 8299.6240\n",
      "Epoch 122/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 266.8184 - mae: 267.5092 - val_loss: 8039.9517 - val_mae: 8040.6445\n",
      "Epoch 123/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 270.0167 - mae: 270.7062 - val_loss: 8284.8174 - val_mae: 8285.5098\n",
      "Epoch 124/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 264.7963 - mae: 265.4863 - val_loss: 7930.1006 - val_mae: 7930.7939\n",
      "Epoch 125/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 273.1318 - mae: 273.8232 - val_loss: 7529.0688 - val_mae: 7529.7622\n",
      "Epoch 126/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 258.4821 - mae: 259.1725 - val_loss: 7966.4697 - val_mae: 7967.1631\n",
      "Epoch 127/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 239.6143 - mae: 240.3028 - val_loss: 8112.4004 - val_mae: 8113.0938\n",
      "Epoch 128/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 259.0368 - mae: 259.7249 - val_loss: 8080.4092 - val_mae: 8081.1016\n",
      "Epoch 129/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 244.1362 - mae: 244.8264 - val_loss: 7583.8682 - val_mae: 7584.5620\n",
      "Epoch 130/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 245.2789 - mae: 245.9705 - val_loss: 8120.3657 - val_mae: 8121.0581\n",
      "Epoch 131/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 261.0969 - mae: 261.7874 - val_loss: 8231.0039 - val_mae: 8231.6973\n",
      "Epoch 132/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 263.3660 - mae: 264.0543 - val_loss: 8224.2373 - val_mae: 8224.9307\n",
      "Epoch 133/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 253.7635 - mae: 254.4546 - val_loss: 7976.7104 - val_mae: 7977.4033\n",
      "Epoch 134/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 237.2665 - mae: 237.9541 - val_loss: 8374.7422 - val_mae: 8375.4355\n",
      "Epoch 135/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 251.3579 - mae: 252.0484 - val_loss: 8342.1299 - val_mae: 8342.8232\n",
      "Epoch 136/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 252.1294 - mae: 252.8197 - val_loss: 7534.7700 - val_mae: 7535.4629\n",
      "Epoch 137/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 252.2522 - mae: 252.9410 - val_loss: 7605.5298 - val_mae: 7606.2231\n",
      "Epoch 138/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 240.5257 - mae: 241.2141 - val_loss: 7836.4170 - val_mae: 7837.1104\n",
      "Epoch 139/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 245.0379 - mae: 245.7257 - val_loss: 7658.8721 - val_mae: 7659.5654\n",
      "Epoch 140/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 259.2046 - mae: 259.8950 - val_loss: 7914.2515 - val_mae: 7914.9453\n",
      "Epoch 141/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 237.2511 - mae: 237.9384 - val_loss: 7412.8940 - val_mae: 7413.5874\n",
      "Epoch 142/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 257.2581 - mae: 257.9472 - val_loss: 7963.4893 - val_mae: 7964.1812\n",
      "Epoch 143/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 247.0188 - mae: 247.7070 - val_loss: 8103.4658 - val_mae: 8104.1602\n",
      "Epoch 144/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 246.6850 - mae: 247.3735 - val_loss: 7964.9526 - val_mae: 7965.6460\n",
      "Epoch 145/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 238.6548 - mae: 239.3459 - val_loss: 7832.3359 - val_mae: 7833.0303\n",
      "Epoch 146/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 256.9065 - mae: 257.5971 - val_loss: 8284.4541 - val_mae: 8285.1484\n",
      "Epoch 147/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 243.4228 - mae: 244.1116 - val_loss: 8079.0005 - val_mae: 8079.6938\n",
      "Epoch 148/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 229.8302 - mae: 230.5190 - val_loss: 7803.7104 - val_mae: 7804.4028\n",
      "Epoch 149/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 246.1529 - mae: 246.8430 - val_loss: 7812.9780 - val_mae: 7813.6724\n",
      "Epoch 150/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 239.5764 - mae: 240.2670 - val_loss: 7677.0889 - val_mae: 7677.7827\n",
      "Epoch 151/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 256.9381 - mae: 257.6281 - val_loss: 8108.2847 - val_mae: 8108.9766\n",
      "Epoch 152/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 245.3375 - mae: 246.0285 - val_loss: 8081.4736 - val_mae: 8082.1675\n",
      "Epoch 153/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 245.8240 - mae: 246.5134 - val_loss: 7686.6494 - val_mae: 7687.3428\n",
      "Epoch 154/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 229.1190 - mae: 229.8082 - val_loss: 7682.0522 - val_mae: 7682.7466\n",
      "Epoch 155/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 231.3593 - mae: 232.0478 - val_loss: 7503.7212 - val_mae: 7504.4136\n",
      "Epoch 156/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 271.1358 - mae: 271.8249 - val_loss: 7490.3228 - val_mae: 7491.0166\n",
      "Epoch 157/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 255.4609 - mae: 256.1508 - val_loss: 7664.6514 - val_mae: 7665.3447\n",
      "Epoch 158/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 262.6924 - mae: 263.3816 - val_loss: 7821.5444 - val_mae: 7822.2373\n",
      "Epoch 159/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 233.5755 - mae: 234.2656 - val_loss: 8143.5327 - val_mae: 8144.2261\n",
      "Epoch 160/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 225.9974 - mae: 226.6871 - val_loss: 7795.9053 - val_mae: 7796.5981\n",
      "Epoch 161/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 248.0535 - mae: 248.7445 - val_loss: 7875.9697 - val_mae: 7876.6626\n",
      "Epoch 162/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 229.9436 - mae: 230.6328 - val_loss: 7958.8101 - val_mae: 7959.5034\n",
      "Epoch 163/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 248.5706 - mae: 249.2605 - val_loss: 7955.8965 - val_mae: 7956.5889\n",
      "Epoch 164/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 237.8075 - mae: 238.4986 - val_loss: 8500.8271 - val_mae: 8501.5205\n",
      "Epoch 165/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 226.7404 - mae: 227.4303 - val_loss: 7941.3677 - val_mae: 7942.0615\n",
      "Epoch 166/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 217.9919 - mae: 218.6806 - val_loss: 7651.5410 - val_mae: 7652.2329\n",
      "Epoch 167/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 248.0045 - mae: 248.6939 - val_loss: 7803.0688 - val_mae: 7803.7617\n",
      "Epoch 168/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 231.5490 - mae: 232.2384 - val_loss: 7915.2290 - val_mae: 7915.9224\n",
      "Epoch 169/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 220.8877 - mae: 221.5751 - val_loss: 8261.5117 - val_mae: 8262.2041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 224.6377 - mae: 225.3265 - val_loss: 8115.2651 - val_mae: 8115.9575\n",
      "Epoch 171/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 243.3749 - mae: 244.0625 - val_loss: 8089.0762 - val_mae: 8089.7690\n",
      "Epoch 172/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 231.7369 - mae: 232.4268 - val_loss: 7702.0298 - val_mae: 7702.7236\n",
      "Epoch 173/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 231.4895 - mae: 232.1786 - val_loss: 7578.2485 - val_mae: 7578.9419\n",
      "Epoch 174/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 217.8037 - mae: 218.4916 - val_loss: 7627.5400 - val_mae: 7628.2324\n",
      "Epoch 175/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 246.2366 - mae: 246.9255 - val_loss: 8094.6821 - val_mae: 8095.3755\n",
      "Epoch 176/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 231.2961 - mae: 231.9837 - val_loss: 8364.7500 - val_mae: 8365.4434\n",
      "Epoch 177/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 226.1026 - mae: 226.7914 - val_loss: 8003.1011 - val_mae: 8003.7939\n",
      "Epoch 178/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 231.6446 - mae: 232.3336 - val_loss: 7763.7720 - val_mae: 7764.4648\n",
      "Epoch 179/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 257.6084 - mae: 258.2997 - val_loss: 7928.4600 - val_mae: 7929.1523\n",
      "Epoch 180/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 229.6680 - mae: 230.3574 - val_loss: 7769.7852 - val_mae: 7770.4775\n",
      "Epoch 181/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 216.3043 - mae: 216.9933 - val_loss: 7961.6011 - val_mae: 7962.2954\n",
      "Epoch 182/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 235.2042 - mae: 235.8937 - val_loss: 7583.7554 - val_mae: 7584.4492\n",
      "Epoch 183/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 239.3094 - mae: 239.9991 - val_loss: 7783.3560 - val_mae: 7784.0493\n",
      "Epoch 184/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 215.6359 - mae: 216.3245 - val_loss: 7496.2295 - val_mae: 7496.9209\n",
      "Epoch 185/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 213.6619 - mae: 214.3510 - val_loss: 8338.5889 - val_mae: 8339.2812\n",
      "Epoch 186/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 242.2090 - mae: 242.8981 - val_loss: 7495.0791 - val_mae: 7495.7725\n",
      "Epoch 187/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 215.6662 - mae: 216.3546 - val_loss: 8114.7573 - val_mae: 8115.4497\n",
      "Epoch 188/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 231.6110 - mae: 232.3001 - val_loss: 7859.3003 - val_mae: 7859.9927\n",
      "Epoch 189/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 229.7304 - mae: 230.4212 - val_loss: 7976.6885 - val_mae: 7977.3813\n",
      "Epoch 190/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 216.7605 - mae: 217.4504 - val_loss: 7709.8735 - val_mae: 7710.5664\n",
      "Epoch 191/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 214.3294 - mae: 215.0195 - val_loss: 7766.9116 - val_mae: 7767.6040\n",
      "Epoch 192/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 221.2707 - mae: 221.9589 - val_loss: 8102.8657 - val_mae: 8103.5591\n",
      "Epoch 193/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 217.2757 - mae: 217.9644 - val_loss: 8058.8188 - val_mae: 8059.5122\n",
      "Epoch 194/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 219.1207 - mae: 219.8107 - val_loss: 7687.4385 - val_mae: 7688.1323\n",
      "Epoch 195/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 219.1483 - mae: 219.8377 - val_loss: 7554.5439 - val_mae: 7555.2378\n",
      "Epoch 196/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 213.4629 - mae: 214.1524 - val_loss: 7741.9536 - val_mae: 7742.6470\n",
      "Epoch 197/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 217.2776 - mae: 217.9675 - val_loss: 7973.3779 - val_mae: 7974.0713\n",
      "Epoch 198/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 211.6318 - mae: 212.3190 - val_loss: 8092.0342 - val_mae: 8092.7275\n",
      "Epoch 199/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 219.5852 - mae: 220.2755 - val_loss: 8176.6548 - val_mae: 8177.3472\n",
      "Epoch 200/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 205.3640 - mae: 206.0524 - val_loss: 7766.0796 - val_mae: 7766.7739\n",
      "Epoch 201/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 208.4033 - mae: 209.0913 - val_loss: 7890.3164 - val_mae: 7891.0093\n",
      "Epoch 202/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 200.2000 - mae: 200.8900 - val_loss: 7874.8896 - val_mae: 7875.5820\n",
      "Epoch 203/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 213.5754 - mae: 214.2647 - val_loss: 7632.5967 - val_mae: 7633.2900\n",
      "Epoch 204/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 243.1965 - mae: 243.8873 - val_loss: 7946.7969 - val_mae: 7947.4897\n",
      "Epoch 205/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 232.3659 - mae: 233.0560 - val_loss: 7936.4985 - val_mae: 7937.1909\n",
      "Epoch 206/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 209.6652 - mae: 210.3543 - val_loss: 7830.9434 - val_mae: 7831.6357\n",
      "Epoch 207/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 199.1039 - mae: 199.7922 - val_loss: 7916.8569 - val_mae: 7917.5498\n",
      "Epoch 208/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 197.9321 - mae: 198.6208 - val_loss: 7768.6069 - val_mae: 7769.2993\n",
      "Epoch 209/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 200.5364 - mae: 201.2257 - val_loss: 7628.1304 - val_mae: 7628.8247\n",
      "Epoch 210/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 201.6994 - mae: 202.3888 - val_loss: 7839.9434 - val_mae: 7840.6362\n",
      "Epoch 211/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 196.2267 - mae: 196.9163 - val_loss: 8034.6431 - val_mae: 8035.3350\n",
      "Epoch 212/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 201.6743 - mae: 202.3638 - val_loss: 7790.1880 - val_mae: 7790.8809\n",
      "Epoch 213/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 208.8800 - mae: 209.5684 - val_loss: 7786.4561 - val_mae: 7787.1479\n",
      "Epoch 214/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 217.8345 - mae: 218.5243 - val_loss: 7805.5981 - val_mae: 7806.2915\n",
      "Epoch 215/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 218.5956 - mae: 219.2836 - val_loss: 7599.7358 - val_mae: 7600.4297\n",
      "Epoch 216/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 203.6974 - mae: 204.3850 - val_loss: 7749.5923 - val_mae: 7750.2861\n",
      "Epoch 217/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 205.4086 - mae: 206.0985 - val_loss: 7731.5659 - val_mae: 7732.2588\n",
      "Epoch 218/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 223.3965 - mae: 224.0851 - val_loss: 8167.8096 - val_mae: 8168.5029\n",
      "Epoch 219/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 199.3041 - mae: 199.9926 - val_loss: 7588.3711 - val_mae: 7589.0649\n",
      "Epoch 220/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 203.4439 - mae: 204.1332 - val_loss: 7907.6670 - val_mae: 7908.3599\n",
      "Epoch 221/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 217.5401 - mae: 218.2292 - val_loss: 8225.8037 - val_mae: 8226.4961\n",
      "Epoch 222/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 217.4102 - mae: 218.1003 - val_loss: 7705.7642 - val_mae: 7706.4565\n",
      "Epoch 223/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 193.2371 - mae: 193.9258 - val_loss: 7903.8301 - val_mae: 7904.5244\n",
      "Epoch 224/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 202.5771 - mae: 203.2671 - val_loss: 7945.1792 - val_mae: 7945.8735\n",
      "Epoch 225/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 207.5987 - mae: 208.2881 - val_loss: 7816.9805 - val_mae: 7817.6729\n",
      "Epoch 226/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 205.8101 - mae: 206.4975 - val_loss: 8261.2275 - val_mae: 8261.9199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 211.2043 - mae: 211.8939 - val_loss: 8098.8862 - val_mae: 8099.5791\n",
      "Epoch 228/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 226.8756 - mae: 227.5653 - val_loss: 7695.0757 - val_mae: 7695.7695\n",
      "Epoch 229/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 196.6791 - mae: 197.3688 - val_loss: 7616.0127 - val_mae: 7616.7061\n",
      "Epoch 230/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 204.4451 - mae: 205.1358 - val_loss: 7672.0732 - val_mae: 7672.7666\n",
      "Epoch 231/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 196.6972 - mae: 197.3852 - val_loss: 7701.6094 - val_mae: 7702.3027\n",
      "Epoch 232/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 200.0918 - mae: 200.7802 - val_loss: 7943.3296 - val_mae: 7944.0225\n",
      "Epoch 233/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 192.9461 - mae: 193.6339 - val_loss: 7927.5659 - val_mae: 7928.2588\n",
      "Epoch 234/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 196.8858 - mae: 197.5761 - val_loss: 7769.4697 - val_mae: 7770.1626\n",
      "Epoch 235/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 215.3388 - mae: 216.0296 - val_loss: 7674.3506 - val_mae: 7675.0439\n",
      "Epoch 236/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 190.9612 - mae: 191.6504 - val_loss: 7588.6885 - val_mae: 7589.3818\n",
      "Epoch 237/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 211.4576 - mae: 212.1476 - val_loss: 7963.8438 - val_mae: 7964.5371\n",
      "Epoch 238/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 228.4910 - mae: 229.1816 - val_loss: 7227.1743 - val_mae: 7227.8677\n",
      "Epoch 239/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 214.3041 - mae: 214.9933 - val_loss: 7319.4531 - val_mae: 7320.1470\n",
      "Epoch 240/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 213.4500 - mae: 214.1398 - val_loss: 7615.8335 - val_mae: 7616.5249\n",
      "Epoch 241/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 207.1388 - mae: 207.8281 - val_loss: 7907.8115 - val_mae: 7908.5049\n",
      "Epoch 242/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 193.6931 - mae: 194.3796 - val_loss: 7721.9678 - val_mae: 7722.6611\n",
      "Epoch 243/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 195.1830 - mae: 195.8729 - val_loss: 7663.8086 - val_mae: 7664.5020\n",
      "Epoch 244/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 193.4561 - mae: 194.1458 - val_loss: 7774.4097 - val_mae: 7775.1030\n",
      "Epoch 245/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 182.4451 - mae: 183.1349 - val_loss: 7649.0752 - val_mae: 7649.7686\n",
      "Epoch 246/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 198.2609 - mae: 198.9490 - val_loss: 7992.3418 - val_mae: 7993.0352\n",
      "Epoch 247/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 200.4993 - mae: 201.1885 - val_loss: 7792.0264 - val_mae: 7792.7202\n",
      "Epoch 248/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 199.8744 - mae: 200.5625 - val_loss: 7829.9849 - val_mae: 7830.6787\n",
      "Epoch 249/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 190.4807 - mae: 191.1693 - val_loss: 8011.9111 - val_mae: 8012.6030\n",
      "Epoch 250/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 200.0915 - mae: 200.7797 - val_loss: 7768.4976 - val_mae: 7769.1904\n",
      "Epoch 251/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 216.0742 - mae: 216.7650 - val_loss: 7580.7075 - val_mae: 7581.4009\n",
      "Epoch 252/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 206.2679 - mae: 206.9558 - val_loss: 7974.3198 - val_mae: 7975.0127\n",
      "Epoch 253/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 189.4590 - mae: 190.1464 - val_loss: 7648.8955 - val_mae: 7649.5884\n",
      "Epoch 254/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 199.0685 - mae: 199.7576 - val_loss: 7959.9951 - val_mae: 7960.6880\n",
      "Epoch 255/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 203.4060 - mae: 204.0942 - val_loss: 7819.1001 - val_mae: 7819.7939\n",
      "Epoch 256/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 203.2647 - mae: 203.9550 - val_loss: 7721.5459 - val_mae: 7722.2393\n",
      "Epoch 257/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 195.6280 - mae: 196.3166 - val_loss: 7714.4541 - val_mae: 7715.1479\n",
      "Epoch 258/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 185.4675 - mae: 186.1570 - val_loss: 7748.4170 - val_mae: 7749.1084\n",
      "Epoch 259/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 198.7434 - mae: 199.4328 - val_loss: 7645.7002 - val_mae: 7646.3931\n",
      "Epoch 260/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 189.9948 - mae: 190.6832 - val_loss: 7921.1641 - val_mae: 7921.8579\n",
      "Epoch 261/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 204.5288 - mae: 205.2183 - val_loss: 7705.7656 - val_mae: 7706.4590\n",
      "Epoch 262/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 187.1566 - mae: 187.8455 - val_loss: 7538.8076 - val_mae: 7539.5000\n",
      "Epoch 263/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 200.4075 - mae: 201.0960 - val_loss: 7547.4639 - val_mae: 7548.1577\n",
      "Epoch 264/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 205.4634 - mae: 206.1519 - val_loss: 7758.4263 - val_mae: 7759.1206\n",
      "Epoch 265/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 204.9608 - mae: 205.6491 - val_loss: 7617.3193 - val_mae: 7618.0122\n",
      "Epoch 266/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 221.9502 - mae: 222.6398 - val_loss: 7901.3604 - val_mae: 7902.0542\n",
      "Epoch 267/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 184.4482 - mae: 185.1353 - val_loss: 7743.6372 - val_mae: 7744.3301\n",
      "Epoch 268/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 186.4190 - mae: 187.1068 - val_loss: 7582.1230 - val_mae: 7582.8149\n",
      "Epoch 269/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 187.0288 - mae: 187.7182 - val_loss: 7574.0586 - val_mae: 7574.7520\n",
      "Epoch 270/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 194.1227 - mae: 194.8118 - val_loss: 7840.7012 - val_mae: 7841.3945\n",
      "Epoch 271/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 176.3448 - mae: 177.0332 - val_loss: 7859.4438 - val_mae: 7860.1372\n",
      "Epoch 272/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 209.3298 - mae: 210.0182 - val_loss: 7607.9111 - val_mae: 7608.6040\n",
      "Epoch 273/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 196.9237 - mae: 197.6111 - val_loss: 8061.6401 - val_mae: 8062.3330\n",
      "Epoch 274/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 179.6924 - mae: 180.3803 - val_loss: 7603.1455 - val_mae: 7603.8394\n",
      "Epoch 275/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 194.0641 - mae: 194.7519 - val_loss: 8336.8105 - val_mae: 8337.5039\n",
      "Epoch 276/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 194.4718 - mae: 195.1614 - val_loss: 7924.5830 - val_mae: 7925.2759\n",
      "Epoch 277/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 191.0426 - mae: 191.7302 - val_loss: 7693.5044 - val_mae: 7694.1978\n",
      "Epoch 278/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 195.8803 - mae: 196.5695 - val_loss: 8149.2368 - val_mae: 8149.9282\n",
      "Epoch 279/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 194.1780 - mae: 194.8682 - val_loss: 7718.5356 - val_mae: 7719.2290\n",
      "Epoch 280/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 186.2273 - mae: 186.9165 - val_loss: 7873.7725 - val_mae: 7874.4658\n",
      "Epoch 281/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 196.8994 - mae: 197.5878 - val_loss: 7555.1143 - val_mae: 7555.8076\n",
      "Epoch 282/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 194.5984 - mae: 195.2868 - val_loss: 7711.6724 - val_mae: 7712.3647\n",
      "Epoch 283/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 198.4631 - mae: 199.1518 - val_loss: 7745.4697 - val_mae: 7746.1626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 188.5768 - mae: 189.2663 - val_loss: 8052.7773 - val_mae: 8053.4717\n",
      "Epoch 285/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 186.6504 - mae: 187.3386 - val_loss: 7779.9551 - val_mae: 7780.6489\n",
      "Epoch 286/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 188.6732 - mae: 189.3617 - val_loss: 7963.3101 - val_mae: 7964.0029\n",
      "Epoch 287/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 215.9458 - mae: 216.6329 - val_loss: 7572.5205 - val_mae: 7573.2139\n",
      "Epoch 288/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 213.1202 - mae: 213.8086 - val_loss: 7450.4741 - val_mae: 7451.1665\n",
      "Epoch 289/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 191.3243 - mae: 192.0121 - val_loss: 7397.8560 - val_mae: 7398.5483\n",
      "Epoch 290/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 188.8580 - mae: 189.5486 - val_loss: 7571.2876 - val_mae: 7571.9810\n",
      "Epoch 291/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 179.9214 - mae: 180.6102 - val_loss: 7636.2358 - val_mae: 7636.9297\n",
      "Epoch 292/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 193.0182 - mae: 193.7080 - val_loss: 7860.1357 - val_mae: 7860.8291\n",
      "Epoch 293/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 193.2351 - mae: 193.9250 - val_loss: 7498.2285 - val_mae: 7498.9224\n",
      "Epoch 294/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 198.6104 - mae: 199.2990 - val_loss: 7903.7993 - val_mae: 7904.4922\n",
      "Epoch 295/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 197.7625 - mae: 198.4501 - val_loss: 7574.4746 - val_mae: 7575.1680\n",
      "Epoch 296/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 198.4787 - mae: 199.1701 - val_loss: 7617.5830 - val_mae: 7618.2754\n",
      "Epoch 297/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 197.6952 - mae: 198.3824 - val_loss: 7778.2988 - val_mae: 7778.9922\n",
      "Epoch 298/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 231.0943 - mae: 231.7851 - val_loss: 7629.3252 - val_mae: 7630.0186\n",
      "Epoch 299/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 180.1614 - mae: 180.8510 - val_loss: 7695.2607 - val_mae: 7695.9526\n",
      "Epoch 300/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 192.7005 - mae: 193.3905 - val_loss: 7973.2471 - val_mae: 7973.9404\n",
      "Epoch 301/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 235.6037 - mae: 236.2935 - val_loss: 7464.7412 - val_mae: 7465.4346\n",
      "Epoch 302/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 194.9701 - mae: 195.6583 - val_loss: 8035.0430 - val_mae: 8035.7368\n",
      "Epoch 303/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 199.7783 - mae: 200.4674 - val_loss: 7807.2505 - val_mae: 7807.9434\n",
      "Epoch 304/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 190.4851 - mae: 191.1732 - val_loss: 7763.2095 - val_mae: 7763.9033\n",
      "Epoch 305/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 186.6523 - mae: 187.3418 - val_loss: 7780.2568 - val_mae: 7780.9502\n",
      "Epoch 306/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 180.6614 - mae: 181.3482 - val_loss: 7498.5806 - val_mae: 7499.2734\n",
      "Epoch 307/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 180.1278 - mae: 180.8143 - val_loss: 7672.0234 - val_mae: 7672.7158\n",
      "Epoch 308/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 176.6917 - mae: 177.3825 - val_loss: 7930.7583 - val_mae: 7931.4517\n",
      "Epoch 309/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 180.2284 - mae: 180.9179 - val_loss: 7814.4375 - val_mae: 7815.1309\n",
      "Epoch 310/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 181.7569 - mae: 182.4461 - val_loss: 7742.1982 - val_mae: 7742.8921\n",
      "Epoch 311/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 177.5323 - mae: 178.2200 - val_loss: 7643.9521 - val_mae: 7644.6455\n",
      "Epoch 312/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 187.1577 - mae: 187.8475 - val_loss: 8227.8721 - val_mae: 8228.5654\n",
      "Epoch 313/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 179.9670 - mae: 180.6561 - val_loss: 7439.4551 - val_mae: 7440.1484\n",
      "Epoch 314/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 178.5744 - mae: 179.2628 - val_loss: 8256.8418 - val_mae: 8257.5342\n",
      "Epoch 315/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 207.7783 - mae: 208.4656 - val_loss: 7670.0752 - val_mae: 7670.7686\n",
      "Epoch 316/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 188.4823 - mae: 189.1697 - val_loss: 8215.5762 - val_mae: 8216.2686\n",
      "Epoch 317/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 193.2059 - mae: 193.8937 - val_loss: 7724.8237 - val_mae: 7725.5166\n",
      "Epoch 318/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 171.3577 - mae: 172.0470 - val_loss: 7985.6045 - val_mae: 7986.2969\n",
      "Epoch 319/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 191.2042 - mae: 191.8941 - val_loss: 7933.8691 - val_mae: 7934.5620\n",
      "Epoch 320/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 187.4446 - mae: 188.1319 - val_loss: 7867.2812 - val_mae: 7867.9746\n",
      "Epoch 321/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 171.9196 - mae: 172.6071 - val_loss: 7743.3281 - val_mae: 7744.0225\n",
      "Epoch 322/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 170.9170 - mae: 171.6055 - val_loss: 7778.5249 - val_mae: 7779.2173\n",
      "Epoch 323/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 185.7129 - mae: 186.4019 - val_loss: 7960.9897 - val_mae: 7961.6831\n",
      "Epoch 324/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 204.5851 - mae: 205.2741 - val_loss: 7721.6758 - val_mae: 7722.3687\n",
      "Epoch 325/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 163.9108 - mae: 164.5966 - val_loss: 8049.4390 - val_mae: 8050.1313\n",
      "Epoch 326/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 172.2270 - mae: 172.9142 - val_loss: 7690.6523 - val_mae: 7691.3452\n",
      "Epoch 327/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 185.2305 - mae: 185.9180 - val_loss: 7711.9844 - val_mae: 7712.6768\n",
      "Epoch 328/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 168.0323 - mae: 168.7198 - val_loss: 7601.5547 - val_mae: 7602.2476\n",
      "Epoch 329/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 166.7917 - mae: 167.4774 - val_loss: 8115.8462 - val_mae: 8116.5405\n",
      "Epoch 330/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 173.0606 - mae: 173.7474 - val_loss: 8285.0273 - val_mae: 8285.7217\n",
      "Epoch 331/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 193.4459 - mae: 194.1352 - val_loss: 7608.3184 - val_mae: 7609.0112\n",
      "Epoch 332/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 180.9796 - mae: 181.6701 - val_loss: 7573.5435 - val_mae: 7574.2358\n",
      "Epoch 333/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 194.7354 - mae: 195.4239 - val_loss: 7926.1465 - val_mae: 7926.8398\n",
      "Epoch 334/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 186.6053 - mae: 187.2924 - val_loss: 7393.9360 - val_mae: 7394.6284\n",
      "Epoch 335/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 178.3993 - mae: 179.0885 - val_loss: 7856.1343 - val_mae: 7856.8281\n",
      "Epoch 336/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 194.5608 - mae: 195.2514 - val_loss: 7402.2388 - val_mae: 7402.9312\n",
      "Epoch 337/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 192.1857 - mae: 192.8753 - val_loss: 7973.2148 - val_mae: 7973.9072\n",
      "Epoch 338/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 188.2484 - mae: 188.9383 - val_loss: 8043.4893 - val_mae: 8044.1821\n",
      "Epoch 339/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 164.2491 - mae: 164.9374 - val_loss: 7681.2866 - val_mae: 7681.9805\n",
      "Epoch 340/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 166.0881 - mae: 166.7765 - val_loss: 7865.9331 - val_mae: 7866.6265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 341/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 169.9061 - mae: 170.5919 - val_loss: 7706.5176 - val_mae: 7707.2109\n",
      "Epoch 342/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 180.4171 - mae: 181.1069 - val_loss: 7582.8389 - val_mae: 7583.5327\n",
      "Epoch 343/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 168.4848 - mae: 169.1720 - val_loss: 7972.8750 - val_mae: 7973.5688\n",
      "Epoch 344/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 180.9745 - mae: 181.6637 - val_loss: 7579.5732 - val_mae: 7580.2642\n",
      "Epoch 345/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 182.9649 - mae: 183.6531 - val_loss: 7775.4160 - val_mae: 7776.1099\n",
      "Epoch 346/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 160.8487 - mae: 161.5374 - val_loss: 7887.8730 - val_mae: 7888.5659\n",
      "Epoch 347/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 159.6997 - mae: 160.3838 - val_loss: 7736.4526 - val_mae: 7737.1460\n",
      "Epoch 348/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 161.0549 - mae: 161.7422 - val_loss: 7673.0913 - val_mae: 7673.7847\n",
      "Epoch 349/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 166.7785 - mae: 167.4632 - val_loss: 7729.9321 - val_mae: 7730.6250\n",
      "Epoch 350/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 173.2503 - mae: 173.9367 - val_loss: 7955.1714 - val_mae: 7955.8657\n",
      "Epoch 351/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 190.7553 - mae: 191.4452 - val_loss: 7934.4712 - val_mae: 7935.1646\n",
      "Epoch 352/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 168.7516 - mae: 169.4399 - val_loss: 7557.5044 - val_mae: 7558.1978\n",
      "Epoch 353/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 183.7230 - mae: 184.4109 - val_loss: 7466.0088 - val_mae: 7466.7026\n",
      "Epoch 354/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 175.4179 - mae: 176.1061 - val_loss: 7689.6929 - val_mae: 7690.3848\n",
      "Epoch 355/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 171.4235 - mae: 172.1113 - val_loss: 8318.2949 - val_mae: 8318.9883\n",
      "Epoch 356/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 182.9183 - mae: 183.6076 - val_loss: 7937.1685 - val_mae: 7937.8618\n",
      "Epoch 357/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 169.0445 - mae: 169.7326 - val_loss: 8350.8975 - val_mae: 8351.5898\n",
      "Epoch 358/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 192.2696 - mae: 192.9584 - val_loss: 7858.1836 - val_mae: 7858.8774\n",
      "Epoch 359/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 171.7699 - mae: 172.4595 - val_loss: 7777.3491 - val_mae: 7778.0425\n",
      "Epoch 360/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 180.7905 - mae: 181.4789 - val_loss: 7808.5166 - val_mae: 7809.2100\n",
      "Epoch 361/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 178.0929 - mae: 178.7831 - val_loss: 7988.1733 - val_mae: 7988.8657\n",
      "Epoch 362/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 182.2764 - mae: 182.9665 - val_loss: 7643.9541 - val_mae: 7644.6460\n",
      "Epoch 363/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 167.3311 - mae: 168.0182 - val_loss: 8001.0615 - val_mae: 8001.7554\n",
      "Epoch 364/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 173.0880 - mae: 173.7755 - val_loss: 7843.1484 - val_mae: 7843.8403\n",
      "Epoch 365/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 164.7295 - mae: 165.4181 - val_loss: 7866.5723 - val_mae: 7867.2661\n",
      "Epoch 366/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 164.6915 - mae: 165.3791 - val_loss: 8024.9131 - val_mae: 8025.6069\n",
      "Epoch 367/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 177.7378 - mae: 178.4252 - val_loss: 7536.8770 - val_mae: 7537.5718\n",
      "Epoch 368/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 181.3364 - mae: 182.0243 - val_loss: 8231.2266 - val_mae: 8231.9199\n",
      "Epoch 369/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 181.7875 - mae: 182.4766 - val_loss: 8098.2422 - val_mae: 8098.9355\n",
      "Epoch 370/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 197.5277 - mae: 198.2169 - val_loss: 8206.1387 - val_mae: 8206.8320\n",
      "Epoch 371/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 185.5491 - mae: 186.2394 - val_loss: 7571.7725 - val_mae: 7572.4658\n",
      "Epoch 372/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 203.1364 - mae: 203.8265 - val_loss: 7938.9883 - val_mae: 7939.6812\n",
      "Epoch 373/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 181.7283 - mae: 182.4144 - val_loss: 7813.0479 - val_mae: 7813.7407\n",
      "Epoch 374/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 154.3644 - mae: 155.0518 - val_loss: 7844.6611 - val_mae: 7845.3540\n",
      "Epoch 375/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 158.7848 - mae: 159.4705 - val_loss: 7858.2515 - val_mae: 7858.9443\n",
      "Epoch 376/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 166.8724 - mae: 167.5627 - val_loss: 7827.3354 - val_mae: 7828.0283\n",
      "Epoch 377/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 169.0123 - mae: 169.7023 - val_loss: 7664.7231 - val_mae: 7665.4165\n",
      "Epoch 378/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 163.6674 - mae: 164.3535 - val_loss: 7592.6694 - val_mae: 7593.3623\n",
      "Epoch 379/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 166.4212 - mae: 167.1090 - val_loss: 8154.0830 - val_mae: 8154.7764\n",
      "Epoch 380/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 168.1911 - mae: 168.8778 - val_loss: 7895.2808 - val_mae: 7895.9736\n",
      "Epoch 381/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 154.8284 - mae: 155.5168 - val_loss: 8058.4521 - val_mae: 8059.1455\n",
      "Epoch 382/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 165.3907 - mae: 166.0782 - val_loss: 7566.6670 - val_mae: 7567.3604\n",
      "Epoch 383/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 168.9742 - mae: 169.6625 - val_loss: 7428.9966 - val_mae: 7429.6899\n",
      "Epoch 384/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 223.0131 - mae: 223.7028 - val_loss: 7756.8364 - val_mae: 7757.5293\n",
      "Epoch 385/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 177.7177 - mae: 178.4051 - val_loss: 7726.0376 - val_mae: 7726.7310\n",
      "Epoch 386/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 169.4018 - mae: 170.0904 - val_loss: 7932.4883 - val_mae: 7933.1812\n",
      "Epoch 387/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 153.4363 - mae: 154.1234 - val_loss: 7680.8896 - val_mae: 7681.5830\n",
      "Epoch 388/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 178.3934 - mae: 179.0846 - val_loss: 7745.9795 - val_mae: 7746.6733\n",
      "Epoch 389/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 154.2755 - mae: 154.9637 - val_loss: 7862.7998 - val_mae: 7863.4927\n",
      "Epoch 390/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 159.3825 - mae: 160.0712 - val_loss: 7861.0889 - val_mae: 7861.7817\n",
      "Epoch 391/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 151.7938 - mae: 152.4816 - val_loss: 7708.1250 - val_mae: 7708.8179\n",
      "Epoch 392/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 155.2846 - mae: 155.9723 - val_loss: 7638.7124 - val_mae: 7639.4053\n",
      "Epoch 393/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 156.7175 - mae: 157.4054 - val_loss: 7730.6836 - val_mae: 7731.3770\n",
      "Epoch 394/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 162.4397 - mae: 163.1273 - val_loss: 7656.8179 - val_mae: 7657.5112\n",
      "Epoch 395/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 175.0952 - mae: 175.7842 - val_loss: 7733.6382 - val_mae: 7734.3320\n",
      "Epoch 396/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 158.3482 - mae: 159.0352 - val_loss: 7614.6924 - val_mae: 7615.3853\n",
      "Epoch 397/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 177.7814 - mae: 178.4695 - val_loss: 7784.7017 - val_mae: 7785.3945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 398/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 154.2186 - mae: 154.9074 - val_loss: 7891.0195 - val_mae: 7891.7129\n",
      "Epoch 399/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 156.5526 - mae: 157.2415 - val_loss: 8176.8105 - val_mae: 8177.5029\n",
      "Epoch 400/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 156.5879 - mae: 157.2758 - val_loss: 7777.0962 - val_mae: 7777.7905\n",
      "Epoch 401/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 164.8488 - mae: 165.5374 - val_loss: 7771.9741 - val_mae: 7772.6670\n",
      "Epoch 402/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 167.3668 - mae: 168.0572 - val_loss: 7527.1650 - val_mae: 7527.8584\n",
      "Epoch 403/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 178.0990 - mae: 178.7857 - val_loss: 7689.6343 - val_mae: 7690.3271\n",
      "Epoch 404/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 160.2930 - mae: 160.9821 - val_loss: 7875.4795 - val_mae: 7876.1729\n",
      "Epoch 405/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 158.2950 - mae: 158.9826 - val_loss: 7824.6665 - val_mae: 7825.3589\n",
      "Epoch 406/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 149.4691 - mae: 150.1593 - val_loss: 7808.7051 - val_mae: 7809.3984\n",
      "Epoch 407/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 145.3663 - mae: 146.0517 - val_loss: 7743.9160 - val_mae: 7744.6089\n",
      "Epoch 408/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 162.9891 - mae: 163.6782 - val_loss: 7968.8652 - val_mae: 7969.5581\n",
      "Epoch 409/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 157.1813 - mae: 157.8689 - val_loss: 7700.4692 - val_mae: 7701.1626\n",
      "Epoch 410/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 158.2926 - mae: 158.9810 - val_loss: 7659.1494 - val_mae: 7659.8423\n",
      "Epoch 411/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 149.8330 - mae: 150.5197 - val_loss: 7963.1670 - val_mae: 7963.8604\n",
      "Epoch 412/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 159.3222 - mae: 160.0114 - val_loss: 7707.2729 - val_mae: 7707.9668\n",
      "Epoch 413/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 165.5490 - mae: 166.2385 - val_loss: 8020.0039 - val_mae: 8020.6973\n",
      "Epoch 414/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 165.5165 - mae: 166.2060 - val_loss: 7374.2021 - val_mae: 7374.8945\n",
      "Epoch 415/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 168.8336 - mae: 169.5229 - val_loss: 7616.0278 - val_mae: 7616.7217\n",
      "Epoch 416/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 152.0735 - mae: 152.7589 - val_loss: 7637.2998 - val_mae: 7637.9937\n",
      "Epoch 417/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 160.2964 - mae: 160.9825 - val_loss: 7776.7373 - val_mae: 7777.4302\n",
      "Epoch 418/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 165.7623 - mae: 166.4501 - val_loss: 7649.1226 - val_mae: 7649.8154\n",
      "Epoch 419/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 152.4984 - mae: 153.1858 - val_loss: 7684.4722 - val_mae: 7685.1650\n",
      "Epoch 420/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 167.2331 - mae: 167.9216 - val_loss: 7899.7422 - val_mae: 7900.4360\n",
      "Epoch 421/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 155.2408 - mae: 155.9288 - val_loss: 7686.5879 - val_mae: 7687.2803\n",
      "Epoch 422/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 158.6055 - mae: 159.2938 - val_loss: 7818.3477 - val_mae: 7819.0410\n",
      "Epoch 423/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 148.3302 - mae: 149.0154 - val_loss: 7936.0571 - val_mae: 7936.7500\n",
      "Epoch 424/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 172.0249 - mae: 172.7128 - val_loss: 7873.3945 - val_mae: 7874.0884\n",
      "Epoch 425/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 166.1133 - mae: 166.8019 - val_loss: 7783.2539 - val_mae: 7783.9473\n",
      "Epoch 426/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 145.4330 - mae: 146.1208 - val_loss: 7722.2778 - val_mae: 7722.9722\n",
      "Epoch 427/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 145.8304 - mae: 146.5167 - val_loss: 7940.6274 - val_mae: 7941.3213\n",
      "Epoch 428/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 148.9380 - mae: 149.6250 - val_loss: 7467.1714 - val_mae: 7467.8652\n",
      "Epoch 429/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 185.0399 - mae: 185.7263 - val_loss: 7680.5811 - val_mae: 7681.2749\n",
      "Epoch 430/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 151.3244 - mae: 152.0137 - val_loss: 7844.6631 - val_mae: 7845.3560\n",
      "Epoch 431/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 173.8069 - mae: 174.4930 - val_loss: 7727.8716 - val_mae: 7728.5654\n",
      "Epoch 432/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 168.0033 - mae: 168.6922 - val_loss: 8016.7510 - val_mae: 8017.4424\n",
      "Epoch 433/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 177.2264 - mae: 177.9149 - val_loss: 7932.7139 - val_mae: 7933.4067\n",
      "Epoch 434/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 163.8078 - mae: 164.4947 - val_loss: 7765.6616 - val_mae: 7766.3550\n",
      "Epoch 435/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 165.9833 - mae: 166.6695 - val_loss: 7831.2949 - val_mae: 7831.9878\n",
      "Epoch 436/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 159.7996 - mae: 160.4865 - val_loss: 7625.7778 - val_mae: 7626.4717\n",
      "Epoch 437/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 159.9393 - mae: 160.6285 - val_loss: 7761.9805 - val_mae: 7762.6738\n",
      "Epoch 438/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 168.7651 - mae: 169.4551 - val_loss: 7877.5410 - val_mae: 7878.2344\n",
      "Epoch 439/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 161.3295 - mae: 162.0177 - val_loss: 7868.4595 - val_mae: 7869.1509\n",
      "Epoch 440/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 167.0090 - mae: 167.6971 - val_loss: 8178.2803 - val_mae: 8178.9731\n",
      "Epoch 441/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 153.4106 - mae: 154.0992 - val_loss: 7633.1338 - val_mae: 7633.8262\n",
      "Epoch 442/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 152.9590 - mae: 153.6470 - val_loss: 7718.7075 - val_mae: 7719.4014\n",
      "Epoch 443/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 151.3184 - mae: 152.0050 - val_loss: 7905.5459 - val_mae: 7906.2388\n",
      "Epoch 444/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 141.2975 - mae: 141.9825 - val_loss: 7706.8926 - val_mae: 7707.5854\n",
      "Epoch 445/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 167.9374 - mae: 168.6253 - val_loss: 7738.6045 - val_mae: 7739.2969\n",
      "Epoch 446/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 158.6485 - mae: 159.3365 - val_loss: 7616.9111 - val_mae: 7617.6035\n",
      "Epoch 447/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 166.2758 - mae: 166.9611 - val_loss: 7719.2485 - val_mae: 7719.9419\n",
      "Epoch 448/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 155.4442 - mae: 156.1331 - val_loss: 7426.7461 - val_mae: 7427.4399\n",
      "Epoch 449/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 182.9319 - mae: 183.6197 - val_loss: 7676.7896 - val_mae: 7677.4819\n",
      "Epoch 450/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 148.1488 - mae: 148.8347 - val_loss: 7845.5645 - val_mae: 7846.2583\n",
      "Epoch 451/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 154.5322 - mae: 155.2177 - val_loss: 7857.5469 - val_mae: 7858.2402\n",
      "Epoch 452/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 151.5133 - mae: 152.2011 - val_loss: 7733.1367 - val_mae: 7733.8301\n",
      "Epoch 453/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 156.0003 - mae: 156.6880 - val_loss: 7865.3203 - val_mae: 7866.0122\n",
      "Epoch 454/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 149.4870 - mae: 150.1723 - val_loss: 7816.8096 - val_mae: 7817.5024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 455/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 161.8270 - mae: 162.5121 - val_loss: 7906.4609 - val_mae: 7907.1548\n",
      "Epoch 456/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 165.7821 - mae: 166.4706 - val_loss: 7678.8091 - val_mae: 7679.5029\n",
      "Epoch 457/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 154.2616 - mae: 154.9478 - val_loss: 7742.8501 - val_mae: 7743.5425\n",
      "Epoch 458/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 144.8305 - mae: 145.5153 - val_loss: 7781.0073 - val_mae: 7781.7002\n",
      "Epoch 459/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 142.6981 - mae: 143.3856 - val_loss: 7743.0117 - val_mae: 7743.7051\n",
      "Epoch 460/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 142.6256 - mae: 143.3128 - val_loss: 7817.0107 - val_mae: 7817.7036\n",
      "Epoch 461/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 157.1944 - mae: 157.8825 - val_loss: 7614.8901 - val_mae: 7615.5840\n",
      "Epoch 462/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 142.6746 - mae: 143.3602 - val_loss: 7871.4355 - val_mae: 7872.1279\n",
      "Epoch 463/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 167.1076 - mae: 167.7971 - val_loss: 7783.4946 - val_mae: 7784.1885\n",
      "Epoch 464/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 148.6606 - mae: 149.3495 - val_loss: 7751.6353 - val_mae: 7752.3286\n",
      "Epoch 465/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 146.7436 - mae: 147.4331 - val_loss: 7481.1021 - val_mae: 7481.7954\n",
      "Epoch 466/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 149.7906 - mae: 150.4786 - val_loss: 8137.3599 - val_mae: 8138.0532\n",
      "Epoch 467/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 171.1830 - mae: 171.8708 - val_loss: 7742.8477 - val_mae: 7743.5405\n",
      "Epoch 468/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 160.9498 - mae: 161.6388 - val_loss: 7761.7876 - val_mae: 7762.4810\n",
      "Epoch 469/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 142.5799 - mae: 143.2670 - val_loss: 7838.6240 - val_mae: 7839.3169\n",
      "Epoch 470/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 156.0831 - mae: 156.7695 - val_loss: 7473.7222 - val_mae: 7474.4160\n",
      "Epoch 471/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 153.2731 - mae: 153.9588 - val_loss: 7611.3682 - val_mae: 7612.0610\n",
      "Epoch 472/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 147.8596 - mae: 148.5454 - val_loss: 7759.5005 - val_mae: 7760.1929\n",
      "Epoch 473/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 156.4749 - mae: 157.1646 - val_loss: 7501.5728 - val_mae: 7502.2661\n",
      "Epoch 474/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 155.1916 - mae: 155.8777 - val_loss: 7877.6040 - val_mae: 7878.2969\n",
      "Epoch 475/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 163.7716 - mae: 164.4605 - val_loss: 7711.7764 - val_mae: 7712.4702\n",
      "Epoch 476/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 156.3228 - mae: 157.0132 - val_loss: 7860.1021 - val_mae: 7860.7959\n",
      "Epoch 477/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 157.3390 - mae: 158.0262 - val_loss: 7583.4766 - val_mae: 7584.1709\n",
      "Epoch 478/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 145.6873 - mae: 146.3736 - val_loss: 7656.5190 - val_mae: 7657.2119\n",
      "Epoch 479/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 137.9836 - mae: 138.6698 - val_loss: 7699.6982 - val_mae: 7700.3916\n",
      "Epoch 480/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 152.5485 - mae: 153.2342 - val_loss: 7792.9414 - val_mae: 7793.6353\n",
      "Epoch 481/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 147.3508 - mae: 148.0370 - val_loss: 7721.2773 - val_mae: 7721.9712\n",
      "Epoch 482/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 141.5169 - mae: 142.2052 - val_loss: 7715.9634 - val_mae: 7716.6558\n",
      "Epoch 483/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 140.5418 - mae: 141.2291 - val_loss: 7915.8115 - val_mae: 7916.5049\n",
      "Epoch 484/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 147.8680 - mae: 148.5551 - val_loss: 7876.2544 - val_mae: 7876.9463\n",
      "Epoch 485/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 146.9331 - mae: 147.6189 - val_loss: 8055.0273 - val_mae: 8055.7217\n",
      "Epoch 486/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 145.9297 - mae: 146.6190 - val_loss: 7801.6177 - val_mae: 7802.3101\n",
      "Epoch 487/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 161.3386 - mae: 162.0241 - val_loss: 7740.1318 - val_mae: 7740.8242\n",
      "Epoch 488/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 153.6656 - mae: 154.3523 - val_loss: 7658.0264 - val_mae: 7658.7202\n",
      "Epoch 489/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 146.3382 - mae: 147.0258 - val_loss: 7706.3018 - val_mae: 7706.9956\n",
      "Epoch 490/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 140.5762 - mae: 141.2649 - val_loss: 7851.0532 - val_mae: 7851.7461\n",
      "Epoch 491/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 153.2847 - mae: 153.9726 - val_loss: 7522.4272 - val_mae: 7523.1216\n",
      "Epoch 492/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 179.7047 - mae: 180.3951 - val_loss: 7726.5522 - val_mae: 7727.2456\n",
      "Epoch 493/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 139.1065 - mae: 139.7946 - val_loss: 7684.9595 - val_mae: 7685.6523\n",
      "Epoch 494/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 140.9186 - mae: 141.6074 - val_loss: 7788.8262 - val_mae: 7789.5205\n",
      "Epoch 495/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 158.9431 - mae: 159.6319 - val_loss: 7522.1924 - val_mae: 7522.8848\n",
      "Epoch 496/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 149.1970 - mae: 149.8846 - val_loss: 7869.6108 - val_mae: 7870.3037\n",
      "Epoch 497/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 143.0486 - mae: 143.7360 - val_loss: 7892.8442 - val_mae: 7893.5371\n",
      "Epoch 498/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 142.8965 - mae: 143.5823 - val_loss: 7669.4756 - val_mae: 7670.1694\n",
      "Epoch 499/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 161.3304 - mae: 162.0172 - val_loss: 7736.8066 - val_mae: 7737.5000\n",
      "Epoch 500/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 143.5988 - mae: 144.2855 - val_loss: 7889.0815 - val_mae: 7889.7744\n",
      "Epoch 501/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 140.3295 - mae: 141.0181 - val_loss: 7764.8657 - val_mae: 7765.5581\n",
      "Epoch 502/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 146.5539 - mae: 147.2416 - val_loss: 7652.0537 - val_mae: 7652.7476\n",
      "Epoch 503/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 149.9161 - mae: 150.6062 - val_loss: 7668.5742 - val_mae: 7669.2666\n",
      "Epoch 504/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 142.3106 - mae: 142.9983 - val_loss: 7691.6978 - val_mae: 7692.3906\n",
      "Epoch 505/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 139.8646 - mae: 140.5537 - val_loss: 7696.0146 - val_mae: 7696.7080\n",
      "Epoch 506/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 145.9690 - mae: 146.6599 - val_loss: 8234.0801 - val_mae: 8234.7734\n",
      "Epoch 507/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 136.5371 - mae: 137.2250 - val_loss: 7688.4175 - val_mae: 7689.1118\n",
      "Epoch 508/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 147.2708 - mae: 147.9588 - val_loss: 7950.9028 - val_mae: 7951.5967\n",
      "Epoch 509/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 155.7392 - mae: 156.4278 - val_loss: 7760.9697 - val_mae: 7761.6631\n",
      "Epoch 510/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 152.0381 - mae: 152.7264 - val_loss: 7908.3521 - val_mae: 7909.0449\n",
      "Epoch 511/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 147.3459 - mae: 148.0329 - val_loss: 7774.9165 - val_mae: 7775.6099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 512/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 146.3175 - mae: 147.0014 - val_loss: 7910.2598 - val_mae: 7910.9531\n",
      "Epoch 513/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 143.1572 - mae: 143.8424 - val_loss: 7811.4121 - val_mae: 7812.1055\n",
      "Epoch 514/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 177.0748 - mae: 177.7610 - val_loss: 7664.3350 - val_mae: 7665.0269\n",
      "Epoch 515/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 150.4872 - mae: 151.1754 - val_loss: 7778.6328 - val_mae: 7779.3252\n",
      "Epoch 516/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 146.7914 - mae: 147.4768 - val_loss: 8039.7749 - val_mae: 8040.4678\n",
      "Epoch 517/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 150.0044 - mae: 150.6915 - val_loss: 7826.4746 - val_mae: 7827.1680\n",
      "Epoch 518/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 146.8408 - mae: 147.5284 - val_loss: 7820.8545 - val_mae: 7821.5483\n",
      "Epoch 519/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 156.5363 - mae: 157.2254 - val_loss: 7678.3442 - val_mae: 7679.0371\n",
      "Epoch 520/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 142.2693 - mae: 142.9578 - val_loss: 7661.8457 - val_mae: 7662.5386\n",
      "Epoch 521/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 151.0914 - mae: 151.7796 - val_loss: 7690.3433 - val_mae: 7691.0371\n",
      "Epoch 522/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 143.9025 - mae: 144.5858 - val_loss: 7852.8042 - val_mae: 7853.4971\n",
      "Epoch 523/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 160.7232 - mae: 161.4113 - val_loss: 7830.5220 - val_mae: 7831.2148\n",
      "Epoch 524/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 136.0248 - mae: 136.7114 - val_loss: 7866.0986 - val_mae: 7866.7925\n",
      "Epoch 525/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 149.7976 - mae: 150.4861 - val_loss: 7616.8599 - val_mae: 7617.5522\n",
      "Epoch 526/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 138.2319 - mae: 138.9186 - val_loss: 7756.3091 - val_mae: 7757.0034\n",
      "Epoch 527/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 130.4338 - mae: 131.1199 - val_loss: 7909.7461 - val_mae: 7910.4395\n",
      "Epoch 528/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 143.1627 - mae: 143.8490 - val_loss: 7560.0518 - val_mae: 7560.7456\n",
      "Epoch 529/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 139.5746 - mae: 140.2598 - val_loss: 7779.5991 - val_mae: 7780.2925\n",
      "Epoch 530/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 149.3203 - mae: 150.0055 - val_loss: 7754.4189 - val_mae: 7755.1123\n",
      "Epoch 531/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 150.8986 - mae: 151.5875 - val_loss: 7721.5410 - val_mae: 7722.2344\n",
      "Epoch 532/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 132.9568 - mae: 133.6421 - val_loss: 7617.7188 - val_mae: 7618.4121\n",
      "Epoch 533/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 140.3655 - mae: 141.0545 - val_loss: 7653.8784 - val_mae: 7654.5728\n",
      "Epoch 534/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 136.5144 - mae: 137.2001 - val_loss: 7847.6753 - val_mae: 7848.3691\n",
      "Epoch 535/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 136.2975 - mae: 136.9836 - val_loss: 7573.9014 - val_mae: 7574.5942\n",
      "Epoch 536/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 141.5576 - mae: 142.2438 - val_loss: 7710.1318 - val_mae: 7710.8252\n",
      "Epoch 537/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 139.6897 - mae: 140.3772 - val_loss: 7975.9424 - val_mae: 7976.6357\n",
      "Epoch 538/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 149.9701 - mae: 150.6571 - val_loss: 7862.6685 - val_mae: 7863.3618\n",
      "Epoch 539/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 136.5161 - mae: 137.2025 - val_loss: 7466.2866 - val_mae: 7466.9800\n",
      "Epoch 540/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 135.5719 - mae: 136.2588 - val_loss: 7874.5410 - val_mae: 7875.2334\n",
      "Epoch 541/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 134.7184 - mae: 135.4057 - val_loss: 7911.0303 - val_mae: 7911.7241\n",
      "Epoch 542/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 153.0890 - mae: 153.7778 - val_loss: 7782.2812 - val_mae: 7782.9746\n",
      "Epoch 543/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 144.7430 - mae: 145.4309 - val_loss: 7878.2061 - val_mae: 7878.8994\n",
      "Epoch 544/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 152.3439 - mae: 153.0326 - val_loss: 7870.3535 - val_mae: 7871.0459\n",
      "Epoch 545/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 156.3267 - mae: 157.0144 - val_loss: 7533.5752 - val_mae: 7534.2690\n",
      "Epoch 546/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 158.6303 - mae: 159.3197 - val_loss: 7824.7148 - val_mae: 7825.4082\n",
      "Epoch 547/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 141.1175 - mae: 141.8043 - val_loss: 7812.5200 - val_mae: 7813.2129\n",
      "Epoch 548/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 138.9889 - mae: 139.6771 - val_loss: 7782.7964 - val_mae: 7783.4902\n",
      "Epoch 549/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 131.2513 - mae: 131.9386 - val_loss: 7881.3213 - val_mae: 7882.0146\n",
      "Epoch 550/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 141.7912 - mae: 142.4768 - val_loss: 7951.7236 - val_mae: 7952.4165\n",
      "Epoch 551/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 137.7733 - mae: 138.4602 - val_loss: 8023.9536 - val_mae: 8024.6470\n",
      "Epoch 552/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 141.0278 - mae: 141.7174 - val_loss: 7662.6167 - val_mae: 7663.3101\n",
      "Epoch 553/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 139.7063 - mae: 140.3905 - val_loss: 7951.9482 - val_mae: 7952.6406\n",
      "Epoch 554/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 142.8649 - mae: 143.5519 - val_loss: 7863.9844 - val_mae: 7864.6782\n",
      "Epoch 555/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 126.3200 - mae: 127.0055 - val_loss: 8008.7388 - val_mae: 8009.4307\n",
      "Epoch 556/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 168.8766 - mae: 169.5635 - val_loss: 7677.3057 - val_mae: 7677.9980\n",
      "Epoch 557/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 149.9651 - mae: 150.6517 - val_loss: 7855.2432 - val_mae: 7855.9370\n",
      "Epoch 558/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 137.8240 - mae: 138.5094 - val_loss: 7646.2983 - val_mae: 7646.9912\n",
      "Epoch 559/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 148.9933 - mae: 149.6814 - val_loss: 7665.1899 - val_mae: 7665.8833\n",
      "Epoch 560/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 142.2627 - mae: 142.9507 - val_loss: 7493.9072 - val_mae: 7494.6006\n",
      "Epoch 561/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 135.6809 - mae: 136.3656 - val_loss: 7757.4692 - val_mae: 7758.1631\n",
      "Epoch 562/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 139.6837 - mae: 140.3714 - val_loss: 7761.1743 - val_mae: 7761.8677\n",
      "Epoch 563/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 128.9927 - mae: 129.6790 - val_loss: 7931.6318 - val_mae: 7932.3252\n",
      "Epoch 564/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 134.2761 - mae: 134.9649 - val_loss: 7847.2427 - val_mae: 7847.9355\n",
      "Epoch 565/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 140.9182 - mae: 141.6071 - val_loss: 7671.2178 - val_mae: 7671.9106\n",
      "Epoch 566/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 138.4724 - mae: 139.1563 - val_loss: 7831.5669 - val_mae: 7832.2607\n",
      "Epoch 567/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 137.3457 - mae: 138.0318 - val_loss: 7769.9717 - val_mae: 7770.6641\n",
      "Epoch 568/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 133.5997 - mae: 134.2862 - val_loss: 7808.5601 - val_mae: 7809.2539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 569/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 149.2371 - mae: 149.9245 - val_loss: 7927.6011 - val_mae: 7928.2944\n",
      "Epoch 570/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 145.2807 - mae: 145.9686 - val_loss: 8011.9585 - val_mae: 8012.6509\n",
      "Epoch 571/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 134.0166 - mae: 134.7023 - val_loss: 7720.8633 - val_mae: 7721.5566\n",
      "Epoch 572/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 136.9904 - mae: 137.6793 - val_loss: 7725.1392 - val_mae: 7725.8320\n",
      "Epoch 573/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 137.0912 - mae: 137.7809 - val_loss: 7610.0215 - val_mae: 7610.7148\n",
      "Epoch 574/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 129.9266 - mae: 130.6135 - val_loss: 7429.5327 - val_mae: 7430.2266\n",
      "Epoch 575/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 144.3375 - mae: 145.0241 - val_loss: 8284.9209 - val_mae: 8285.6143\n",
      "Epoch 576/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 163.4944 - mae: 164.1815 - val_loss: 7813.1963 - val_mae: 7813.8892\n",
      "Epoch 577/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 145.1230 - mae: 145.8100 - val_loss: 7798.8198 - val_mae: 7799.5117\n",
      "Epoch 578/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 158.4608 - mae: 159.1486 - val_loss: 7783.2148 - val_mae: 7783.9082\n",
      "Epoch 579/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 149.8925 - mae: 150.5795 - val_loss: 7778.3633 - val_mae: 7779.0562\n",
      "Epoch 580/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 152.3961 - mae: 153.0823 - val_loss: 7779.6626 - val_mae: 7780.3560\n",
      "Epoch 581/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 140.6069 - mae: 141.2949 - val_loss: 7799.9541 - val_mae: 7800.6470\n",
      "Epoch 582/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 133.3562 - mae: 134.0438 - val_loss: 7884.0566 - val_mae: 7884.7495\n",
      "Epoch 583/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 134.1761 - mae: 134.8614 - val_loss: 7399.3218 - val_mae: 7400.0151\n",
      "Epoch 584/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 142.0667 - mae: 142.7551 - val_loss: 7672.3516 - val_mae: 7673.0444\n",
      "Epoch 585/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 130.5430 - mae: 131.2269 - val_loss: 7435.9243 - val_mae: 7436.6177\n",
      "Epoch 586/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 147.1800 - mae: 147.8691 - val_loss: 7857.1807 - val_mae: 7857.8740\n",
      "Epoch 587/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 140.6349 - mae: 141.3225 - val_loss: 7925.5283 - val_mae: 7926.2222\n",
      "Epoch 588/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 127.3635 - mae: 128.0517 - val_loss: 7691.6729 - val_mae: 7692.3657\n",
      "Epoch 589/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 129.9630 - mae: 130.6492 - val_loss: 7880.6221 - val_mae: 7881.3159\n",
      "Epoch 590/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 127.0851 - mae: 127.7724 - val_loss: 7846.5864 - val_mae: 7847.2788\n",
      "Epoch 591/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 131.8597 - mae: 132.5465 - val_loss: 7747.0947 - val_mae: 7747.7881\n",
      "Epoch 592/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 156.7804 - mae: 157.4691 - val_loss: 7978.6787 - val_mae: 7979.3716\n",
      "Epoch 593/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 152.7524 - mae: 153.4401 - val_loss: 7740.5713 - val_mae: 7741.2637\n",
      "Epoch 594/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 127.8250 - mae: 128.5101 - val_loss: 7860.0879 - val_mae: 7860.7803\n",
      "Epoch 595/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 134.8197 - mae: 135.5080 - val_loss: 7795.8154 - val_mae: 7796.5088\n",
      "Epoch 596/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 127.2260 - mae: 127.9140 - val_loss: 7765.0229 - val_mae: 7765.7158\n",
      "Epoch 597/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 133.1617 - mae: 133.8457 - val_loss: 7814.8809 - val_mae: 7815.5742\n",
      "Epoch 598/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 143.5177 - mae: 144.2066 - val_loss: 7678.5249 - val_mae: 7679.2192\n",
      "Epoch 599/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 135.6534 - mae: 136.3404 - val_loss: 7812.2651 - val_mae: 7812.9590\n",
      "Epoch 600/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 149.4861 - mae: 150.1753 - val_loss: 7796.5449 - val_mae: 7797.2378\n",
      "Epoch 601/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 134.5757 - mae: 135.2622 - val_loss: 7805.9463 - val_mae: 7806.6396\n",
      "Epoch 602/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 141.8204 - mae: 142.5080 - val_loss: 8042.0522 - val_mae: 8042.7461\n",
      "Epoch 603/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 127.4038 - mae: 128.0876 - val_loss: 7856.4482 - val_mae: 7857.1411\n",
      "Epoch 604/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 140.1374 - mae: 140.8240 - val_loss: 7868.1094 - val_mae: 7868.8022\n",
      "Epoch 605/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 142.0900 - mae: 142.7773 - val_loss: 7656.9131 - val_mae: 7657.6069\n",
      "Epoch 606/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 130.9337 - mae: 131.6219 - val_loss: 7746.6689 - val_mae: 7747.3623\n",
      "Epoch 607/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 135.5439 - mae: 136.2297 - val_loss: 8065.8438 - val_mae: 8066.5371\n",
      "Epoch 608/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 130.0636 - mae: 130.7519 - val_loss: 7840.5576 - val_mae: 7841.2505\n",
      "Epoch 609/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 129.0974 - mae: 129.7837 - val_loss: 7866.0913 - val_mae: 7866.7847\n",
      "Epoch 610/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 152.4918 - mae: 153.1790 - val_loss: 7808.0723 - val_mae: 7808.7651\n",
      "Epoch 611/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 142.4674 - mae: 143.1541 - val_loss: 7780.9990 - val_mae: 7781.6924\n",
      "Epoch 612/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 144.0375 - mae: 144.7267 - val_loss: 7698.6621 - val_mae: 7699.3555\n",
      "Epoch 613/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 130.5090 - mae: 131.1940 - val_loss: 7875.0981 - val_mae: 7875.7910\n",
      "Epoch 614/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 150.9557 - mae: 151.6407 - val_loss: 7964.8677 - val_mae: 7965.5610\n",
      "Epoch 615/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 142.4433 - mae: 143.1322 - val_loss: 7933.8838 - val_mae: 7934.5762\n",
      "Epoch 616/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 126.3338 - mae: 127.0200 - val_loss: 7675.9644 - val_mae: 7676.6577\n",
      "Epoch 617/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 132.9167 - mae: 133.6035 - val_loss: 7885.9722 - val_mae: 7886.6655\n",
      "Epoch 618/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 134.4971 - mae: 135.1836 - val_loss: 7766.4819 - val_mae: 7767.1748\n",
      "Epoch 619/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 136.0006 - mae: 136.6858 - val_loss: 7858.5073 - val_mae: 7859.2002\n",
      "Epoch 620/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 141.4893 - mae: 142.1782 - val_loss: 7668.7202 - val_mae: 7669.4131\n",
      "Epoch 621/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 129.7370 - mae: 130.4207 - val_loss: 7840.3887 - val_mae: 7841.0820\n",
      "Epoch 622/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 137.6435 - mae: 138.3325 - val_loss: 7699.1279 - val_mae: 7699.8218\n",
      "Epoch 623/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 132.6712 - mae: 133.3563 - val_loss: 7906.0347 - val_mae: 7906.7280\n",
      "Epoch 624/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 141.1742 - mae: 141.8628 - val_loss: 7622.4673 - val_mae: 7623.1611\n",
      "Epoch 625/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 16ms/step - loss: 146.0391 - mae: 146.7266 - val_loss: 7897.3198 - val_mae: 7898.0132\n",
      "Epoch 626/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 145.8232 - mae: 146.5107 - val_loss: 7856.9282 - val_mae: 7857.6216\n",
      "Epoch 627/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 129.6860 - mae: 130.3733 - val_loss: 7862.7456 - val_mae: 7863.4380\n",
      "Epoch 628/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 133.8087 - mae: 134.4969 - val_loss: 7811.2900 - val_mae: 7811.9839\n",
      "Epoch 629/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 131.5978 - mae: 132.2852 - val_loss: 7667.0229 - val_mae: 7667.7148\n",
      "Epoch 630/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 124.6997 - mae: 125.3854 - val_loss: 7921.1201 - val_mae: 7921.8140\n",
      "Epoch 631/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 138.5968 - mae: 139.2858 - val_loss: 7970.4575 - val_mae: 7971.1509\n",
      "Epoch 632/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 141.6581 - mae: 142.3475 - val_loss: 7967.9023 - val_mae: 7968.5942\n",
      "Epoch 633/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 133.7388 - mae: 134.4236 - val_loss: 7632.8232 - val_mae: 7633.5161\n",
      "Epoch 634/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 151.7171 - mae: 152.4052 - val_loss: 7873.3887 - val_mae: 7874.0806\n",
      "Epoch 635/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 132.0329 - mae: 132.7183 - val_loss: 7988.1392 - val_mae: 7988.8320\n",
      "Epoch 636/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 138.2368 - mae: 138.9232 - val_loss: 7751.5718 - val_mae: 7752.2646\n",
      "Epoch 637/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 123.9473 - mae: 124.6351 - val_loss: 7859.1206 - val_mae: 7859.8140\n",
      "Epoch 638/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 130.8081 - mae: 131.4944 - val_loss: 7973.6997 - val_mae: 7974.3926\n",
      "Epoch 639/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 124.7153 - mae: 125.4004 - val_loss: 7692.4614 - val_mae: 7693.1548\n",
      "Epoch 640/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 134.2296 - mae: 134.9174 - val_loss: 8091.0117 - val_mae: 8091.7051\n",
      "Epoch 641/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 141.9654 - mae: 142.6497 - val_loss: 7642.2207 - val_mae: 7642.9131\n",
      "Epoch 642/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 141.3275 - mae: 142.0152 - val_loss: 7951.8984 - val_mae: 7952.5918\n",
      "Epoch 643/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 123.2762 - mae: 123.9641 - val_loss: 7736.8604 - val_mae: 7737.5532\n",
      "Epoch 644/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 126.3559 - mae: 127.0420 - val_loss: 7993.4414 - val_mae: 7994.1343\n",
      "Epoch 645/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 138.1406 - mae: 138.8288 - val_loss: 7757.9058 - val_mae: 7758.5991\n",
      "Epoch 646/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 127.9111 - mae: 128.5975 - val_loss: 7538.9912 - val_mae: 7539.6851\n",
      "Epoch 647/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 142.6015 - mae: 143.2871 - val_loss: 7836.1162 - val_mae: 7836.8091\n",
      "Epoch 648/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 136.8277 - mae: 137.5164 - val_loss: 7737.7642 - val_mae: 7738.4570\n",
      "Epoch 649/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 135.0210 - mae: 135.7073 - val_loss: 8011.5981 - val_mae: 8012.2920\n",
      "Epoch 650/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 131.5445 - mae: 132.2325 - val_loss: 8117.9751 - val_mae: 8118.6675\n",
      "Epoch 651/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 130.6172 - mae: 131.3047 - val_loss: 7780.2085 - val_mae: 7780.9009\n",
      "Epoch 652/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 120.8496 - mae: 121.5371 - val_loss: 7928.0493 - val_mae: 7928.7417\n",
      "Epoch 653/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 128.3002 - mae: 128.9876 - val_loss: 8103.8252 - val_mae: 8104.5186\n",
      "Epoch 654/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 125.1585 - mae: 125.8437 - val_loss: 7918.3438 - val_mae: 7919.0366\n",
      "Epoch 655/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 131.9492 - mae: 132.6379 - val_loss: 7839.6421 - val_mae: 7840.3350\n",
      "Epoch 656/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 128.6543 - mae: 129.3431 - val_loss: 7807.3267 - val_mae: 7808.0195\n",
      "Epoch 657/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 130.0639 - mae: 130.7498 - val_loss: 7853.2588 - val_mae: 7853.9512\n",
      "Epoch 658/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 123.6096 - mae: 124.2981 - val_loss: 7779.3457 - val_mae: 7780.0386\n",
      "Epoch 659/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 124.6699 - mae: 125.3569 - val_loss: 7934.4805 - val_mae: 7935.1748\n",
      "Epoch 660/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 128.3203 - mae: 129.0087 - val_loss: 7967.0093 - val_mae: 7967.7026\n",
      "Epoch 661/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 129.2555 - mae: 129.9392 - val_loss: 7794.2773 - val_mae: 7794.9707\n",
      "Epoch 662/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 125.5154 - mae: 126.2000 - val_loss: 7900.9619 - val_mae: 7901.6538\n",
      "Epoch 663/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 124.9625 - mae: 125.6483 - val_loss: 7785.8872 - val_mae: 7786.5801\n",
      "Epoch 664/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 133.0574 - mae: 133.7431 - val_loss: 7972.8027 - val_mae: 7973.4966\n",
      "Epoch 665/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 124.3025 - mae: 124.9915 - val_loss: 7982.9443 - val_mae: 7983.6382\n",
      "Epoch 666/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 122.6969 - mae: 123.3853 - val_loss: 7761.2651 - val_mae: 7761.9585\n",
      "Epoch 667/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 116.9119 - mae: 117.5979 - val_loss: 8032.5552 - val_mae: 8033.2480\n",
      "Epoch 668/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 122.2489 - mae: 122.9344 - val_loss: 7997.6836 - val_mae: 7998.3770\n",
      "Epoch 669/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 128.2532 - mae: 128.9406 - val_loss: 7858.9009 - val_mae: 7859.5933\n",
      "Epoch 670/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 135.8579 - mae: 136.5446 - val_loss: 7525.0298 - val_mae: 7525.7231\n",
      "Epoch 671/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 146.7676 - mae: 147.4538 - val_loss: 7780.9634 - val_mae: 7781.6567\n",
      "Epoch 672/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 140.5029 - mae: 141.1872 - val_loss: 7792.0288 - val_mae: 7792.7222\n",
      "Epoch 673/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 127.1508 - mae: 127.8388 - val_loss: 7790.7285 - val_mae: 7791.4214\n",
      "Epoch 674/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 130.7817 - mae: 131.4698 - val_loss: 7831.0142 - val_mae: 7831.7080\n",
      "Epoch 675/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 128.9980 - mae: 129.6846 - val_loss: 7939.0430 - val_mae: 7939.7358\n",
      "Epoch 676/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 149.5665 - mae: 150.2532 - val_loss: 7598.7910 - val_mae: 7599.4849\n",
      "Epoch 677/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 128.3812 - mae: 129.0691 - val_loss: 7827.8159 - val_mae: 7828.5083\n",
      "Epoch 678/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 130.4949 - mae: 131.1813 - val_loss: 7856.2568 - val_mae: 7856.9497\n",
      "Epoch 679/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 122.0636 - mae: 122.7514 - val_loss: 7947.1416 - val_mae: 7947.8345\n",
      "Epoch 680/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 135.5189 - mae: 136.2056 - val_loss: 7853.2764 - val_mae: 7853.9697\n",
      "Epoch 681/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 120.2011 - mae: 120.8868 - val_loss: 7740.4663 - val_mae: 7741.1592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 682/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 123.9998 - mae: 124.6861 - val_loss: 7991.9453 - val_mae: 7992.6377\n",
      "Epoch 683/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 132.4108 - mae: 133.0967 - val_loss: 7773.1401 - val_mae: 7773.8325\n",
      "Epoch 684/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 118.1627 - mae: 118.8508 - val_loss: 7970.4844 - val_mae: 7971.1787\n",
      "Epoch 685/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 127.6785 - mae: 128.3632 - val_loss: 7903.7842 - val_mae: 7904.4771\n",
      "Epoch 686/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 129.7513 - mae: 130.4385 - val_loss: 7885.8018 - val_mae: 7886.4946\n",
      "Epoch 687/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 127.9291 - mae: 128.6159 - val_loss: 7748.6562 - val_mae: 7749.3501\n",
      "Epoch 688/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 124.2974 - mae: 124.9837 - val_loss: 7717.7368 - val_mae: 7718.4302\n",
      "Epoch 689/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 132.5710 - mae: 133.2587 - val_loss: 7862.2856 - val_mae: 7862.9795\n",
      "Epoch 690/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 118.5107 - mae: 119.1950 - val_loss: 7829.3599 - val_mae: 7830.0513\n",
      "Epoch 691/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 120.0442 - mae: 120.7294 - val_loss: 7877.6406 - val_mae: 7878.3340\n",
      "Epoch 692/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 128.4554 - mae: 129.1427 - val_loss: 7882.8638 - val_mae: 7883.5566\n",
      "Epoch 693/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 146.0359 - mae: 146.7238 - val_loss: 7741.1436 - val_mae: 7741.8364\n",
      "Epoch 694/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 118.6249 - mae: 119.3104 - val_loss: 7974.0762 - val_mae: 7974.7700\n",
      "Epoch 695/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 133.0156 - mae: 133.7038 - val_loss: 7971.8940 - val_mae: 7972.5874\n",
      "Epoch 696/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 115.8028 - mae: 116.4904 - val_loss: 7956.4307 - val_mae: 7957.1245\n",
      "Epoch 697/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 148.0235 - mae: 148.7116 - val_loss: 8075.9766 - val_mae: 8076.6699\n",
      "Epoch 698/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 130.4775 - mae: 131.1631 - val_loss: 7665.6128 - val_mae: 7666.3066\n",
      "Epoch 699/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 130.7723 - mae: 131.4581 - val_loss: 7989.3179 - val_mae: 7990.0107\n",
      "Epoch 700/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 129.8516 - mae: 130.5378 - val_loss: 7968.5850 - val_mae: 7969.2778\n",
      "Epoch 701/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 116.6894 - mae: 117.3759 - val_loss: 8123.8818 - val_mae: 8124.5747\n",
      "Epoch 702/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 131.2540 - mae: 131.9412 - val_loss: 7871.5498 - val_mae: 7872.2427\n",
      "Epoch 703/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 122.9650 - mae: 123.6499 - val_loss: 7986.0649 - val_mae: 7986.7583\n",
      "Epoch 704/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 111.5928 - mae: 112.2791 - val_loss: 7697.7104 - val_mae: 7698.4038\n",
      "Epoch 705/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 138.4528 - mae: 139.1401 - val_loss: 8019.2104 - val_mae: 8019.9043\n",
      "Epoch 706/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 130.6472 - mae: 131.3329 - val_loss: 7797.8320 - val_mae: 7798.5249\n",
      "Epoch 707/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 118.8638 - mae: 119.5519 - val_loss: 7746.6411 - val_mae: 7747.3320\n",
      "Epoch 708/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 130.3515 - mae: 131.0409 - val_loss: 7783.4248 - val_mae: 7784.1187\n",
      "Epoch 709/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 118.7156 - mae: 119.4015 - val_loss: 7916.6953 - val_mae: 7917.3887\n",
      "Epoch 710/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 111.5809 - mae: 112.2672 - val_loss: 7850.5952 - val_mae: 7851.2876\n",
      "Epoch 711/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 127.5941 - mae: 128.2814 - val_loss: 7958.8438 - val_mae: 7959.5366\n",
      "Epoch 712/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 128.4027 - mae: 129.0893 - val_loss: 8131.4912 - val_mae: 8132.1836\n",
      "Epoch 713/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 140.1706 - mae: 140.8601 - val_loss: 7892.1533 - val_mae: 7892.8462\n",
      "Epoch 714/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 127.0373 - mae: 127.7243 - val_loss: 7982.5078 - val_mae: 7983.2012\n",
      "Epoch 715/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 124.3730 - mae: 125.0622 - val_loss: 7868.0293 - val_mae: 7868.7227\n",
      "Epoch 716/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 125.1564 - mae: 125.8436 - val_loss: 8137.6992 - val_mae: 8138.3926\n",
      "Epoch 717/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 165.9083 - mae: 166.5974 - val_loss: 7862.9385 - val_mae: 7863.6323\n",
      "Epoch 718/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 124.5568 - mae: 125.2428 - val_loss: 7905.2158 - val_mae: 7905.9097\n",
      "Epoch 719/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 119.2319 - mae: 119.9188 - val_loss: 7937.0840 - val_mae: 7937.7778\n",
      "Epoch 720/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 125.5179 - mae: 126.2048 - val_loss: 8002.6855 - val_mae: 8003.3789\n",
      "Epoch 721/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 133.2903 - mae: 133.9773 - val_loss: 8095.4131 - val_mae: 8096.1064\n",
      "Epoch 722/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 114.9473 - mae: 115.6344 - val_loss: 7967.3521 - val_mae: 7968.0454\n",
      "Epoch 723/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 118.6706 - mae: 119.3558 - val_loss: 7960.7578 - val_mae: 7961.4502\n",
      "Epoch 724/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 111.7981 - mae: 112.4802 - val_loss: 8068.4775 - val_mae: 8069.1704\n",
      "Epoch 725/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 119.9192 - mae: 120.6033 - val_loss: 7870.0806 - val_mae: 7870.7739\n",
      "Epoch 726/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 127.0968 - mae: 127.7833 - val_loss: 7999.2910 - val_mae: 7999.9844\n",
      "Epoch 727/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 137.0697 - mae: 137.7545 - val_loss: 8073.9907 - val_mae: 8074.6851\n",
      "Epoch 728/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 137.2859 - mae: 137.9749 - val_loss: 7697.3931 - val_mae: 7698.0864\n",
      "Epoch 729/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 122.4838 - mae: 123.1700 - val_loss: 7964.8384 - val_mae: 7965.5312\n",
      "Epoch 730/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 117.0470 - mae: 117.7333 - val_loss: 7618.3477 - val_mae: 7619.0400\n",
      "Epoch 731/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 126.2864 - mae: 126.9704 - val_loss: 7652.7993 - val_mae: 7653.4927\n",
      "Epoch 732/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 123.9147 - mae: 124.6023 - val_loss: 7810.0200 - val_mae: 7810.7134\n",
      "Epoch 733/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 123.2576 - mae: 123.9431 - val_loss: 7899.1211 - val_mae: 7899.8140\n",
      "Epoch 734/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 111.1387 - mae: 111.8247 - val_loss: 7894.3755 - val_mae: 7895.0688\n",
      "Epoch 735/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 145.7012 - mae: 146.3871 - val_loss: 7907.0840 - val_mae: 7907.7764\n",
      "Epoch 736/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 119.1869 - mae: 119.8723 - val_loss: 7917.9751 - val_mae: 7918.6675\n",
      "Epoch 737/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 123.4691 - mae: 124.1564 - val_loss: 7908.9766 - val_mae: 7909.6709\n",
      "Epoch 738/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 116.3826 - mae: 117.0680 - val_loss: 8055.6714 - val_mae: 8056.3643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 739/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 125.5468 - mae: 126.2318 - val_loss: 7804.6069 - val_mae: 7805.3003\n",
      "Epoch 740/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 135.5355 - mae: 136.2231 - val_loss: 7744.7251 - val_mae: 7745.4185\n",
      "Epoch 741/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 122.7591 - mae: 123.4448 - val_loss: 7823.0161 - val_mae: 7823.7085\n",
      "Epoch 742/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 121.6111 - mae: 122.2981 - val_loss: 7924.0928 - val_mae: 7924.7866\n",
      "Epoch 743/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 127.8614 - mae: 128.5494 - val_loss: 8052.3589 - val_mae: 8053.0513\n",
      "Epoch 744/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 127.3721 - mae: 128.0597 - val_loss: 7793.7773 - val_mae: 7794.4707\n",
      "Epoch 745/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 129.3334 - mae: 130.0205 - val_loss: 7802.4160 - val_mae: 7803.1089\n",
      "Epoch 746/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 122.3987 - mae: 123.0845 - val_loss: 7785.2886 - val_mae: 7785.9829\n",
      "Epoch 747/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 132.0479 - mae: 132.7347 - val_loss: 7923.4756 - val_mae: 7924.1699\n",
      "Epoch 748/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 128.5940 - mae: 129.2834 - val_loss: 7973.5796 - val_mae: 7974.2734\n",
      "Epoch 749/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 116.0043 - mae: 116.6891 - val_loss: 7903.1001 - val_mae: 7903.7935\n",
      "Epoch 750/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 130.8632 - mae: 131.5506 - val_loss: 7801.5088 - val_mae: 7802.2026\n",
      "Epoch 751/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 142.8341 - mae: 143.5207 - val_loss: 7861.1860 - val_mae: 7861.8770\n",
      "Epoch 752/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 130.1006 - mae: 130.7871 - val_loss: 7944.7417 - val_mae: 7945.4341\n",
      "Epoch 753/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 141.2772 - mae: 141.9632 - val_loss: 7805.7021 - val_mae: 7806.3945\n",
      "Epoch 754/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 116.0848 - mae: 116.7708 - val_loss: 7850.2476 - val_mae: 7850.9404\n",
      "Epoch 755/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 118.8035 - mae: 119.4937 - val_loss: 7885.9819 - val_mae: 7886.6748\n",
      "Epoch 756/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 122.0986 - mae: 122.7831 - val_loss: 7816.7256 - val_mae: 7817.4194\n",
      "Epoch 757/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 109.3689 - mae: 110.0552 - val_loss: 8036.0605 - val_mae: 8036.7534\n",
      "Epoch 758/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 131.9843 - mae: 132.6692 - val_loss: 7970.5122 - val_mae: 7971.2041\n",
      "Epoch 759/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 117.4964 - mae: 118.1810 - val_loss: 7989.0308 - val_mae: 7989.7236\n",
      "Epoch 760/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 117.6948 - mae: 118.3807 - val_loss: 7995.0566 - val_mae: 7995.7505\n",
      "Epoch 761/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 124.0045 - mae: 124.6903 - val_loss: 7838.5610 - val_mae: 7839.2539\n",
      "Epoch 762/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 127.6428 - mae: 128.3300 - val_loss: 7786.6914 - val_mae: 7787.3848\n",
      "Epoch 763/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 120.0323 - mae: 120.7178 - val_loss: 7847.4258 - val_mae: 7848.1191\n",
      "Epoch 764/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 115.2330 - mae: 115.9206 - val_loss: 8078.1987 - val_mae: 8078.8921\n",
      "Epoch 765/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 127.9447 - mae: 128.6301 - val_loss: 8006.2227 - val_mae: 8006.9155\n",
      "Epoch 766/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 131.6465 - mae: 132.3323 - val_loss: 7939.0942 - val_mae: 7939.7871\n",
      "Epoch 767/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 123.8475 - mae: 124.5336 - val_loss: 7826.9609 - val_mae: 7827.6533\n",
      "Epoch 768/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 119.8138 - mae: 120.5029 - val_loss: 7956.8506 - val_mae: 7957.5430\n",
      "Epoch 769/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 138.6183 - mae: 139.3047 - val_loss: 7889.5337 - val_mae: 7890.2261\n",
      "Epoch 770/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 126.5221 - mae: 127.2096 - val_loss: 7964.2144 - val_mae: 7964.9072\n",
      "Epoch 771/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 123.4920 - mae: 124.1781 - val_loss: 7859.1680 - val_mae: 7859.8608\n",
      "Epoch 772/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 126.5563 - mae: 127.2430 - val_loss: 7887.1514 - val_mae: 7887.8452\n",
      "Epoch 773/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 114.8719 - mae: 115.5565 - val_loss: 8056.5688 - val_mae: 8057.2617\n",
      "Epoch 774/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 117.2008 - mae: 117.8886 - val_loss: 7883.9258 - val_mae: 7884.6196\n",
      "Epoch 775/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 119.2382 - mae: 119.9243 - val_loss: 8002.1270 - val_mae: 8002.8208\n",
      "Epoch 776/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 122.4325 - mae: 123.1186 - val_loss: 7904.3901 - val_mae: 7905.0835\n",
      "Epoch 777/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 119.8707 - mae: 120.5570 - val_loss: 7871.8027 - val_mae: 7872.4956\n",
      "Epoch 778/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 120.4560 - mae: 121.1412 - val_loss: 7874.5918 - val_mae: 7875.2852\n",
      "Epoch 779/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 132.1136 - mae: 132.7984 - val_loss: 7624.9985 - val_mae: 7625.6909\n",
      "Epoch 780/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 135.1631 - mae: 135.8497 - val_loss: 7946.3286 - val_mae: 7947.0220\n",
      "Epoch 781/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 115.4882 - mae: 116.1748 - val_loss: 7771.2769 - val_mae: 7771.9707\n",
      "Epoch 782/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 130.2789 - mae: 130.9634 - val_loss: 8055.2251 - val_mae: 8055.9180\n",
      "Epoch 783/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 130.6603 - mae: 131.3467 - val_loss: 7975.0469 - val_mae: 7975.7388\n",
      "Epoch 784/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 136.2493 - mae: 136.9374 - val_loss: 7994.3687 - val_mae: 7995.0620\n",
      "Epoch 785/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 118.9154 - mae: 119.6008 - val_loss: 8129.1104 - val_mae: 8129.8037\n",
      "Epoch 786/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 127.0130 - mae: 127.7010 - val_loss: 7953.1323 - val_mae: 7953.8247\n",
      "Epoch 787/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 122.8497 - mae: 123.5368 - val_loss: 7965.0288 - val_mae: 7965.7227\n",
      "Epoch 788/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 113.0061 - mae: 113.6917 - val_loss: 7935.4243 - val_mae: 7936.1177\n",
      "Epoch 789/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 107.9149 - mae: 108.5992 - val_loss: 7875.4893 - val_mae: 7876.1821\n",
      "Epoch 790/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 115.1609 - mae: 115.8462 - val_loss: 7760.3032 - val_mae: 7760.9956\n",
      "Epoch 791/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 122.3142 - mae: 123.0023 - val_loss: 7865.2407 - val_mae: 7865.9346\n",
      "Epoch 792/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 118.2771 - mae: 118.9651 - val_loss: 7774.9824 - val_mae: 7775.6753\n",
      "Epoch 793/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 123.3608 - mae: 124.0488 - val_loss: 7921.7339 - val_mae: 7922.4277\n",
      "Epoch 794/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 125.8294 - mae: 126.5142 - val_loss: 7953.0728 - val_mae: 7953.7656\n",
      "Epoch 795/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 7ms/step - loss: 126.9027 - mae: 127.5921 - val_loss: 7718.4702 - val_mae: 7719.1641\n",
      "Epoch 796/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 118.8728 - mae: 119.5593 - val_loss: 7694.3735 - val_mae: 7695.0659\n",
      "Epoch 797/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 125.5054 - mae: 126.1930 - val_loss: 7930.7690 - val_mae: 7931.4624\n",
      "Epoch 798/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 114.3048 - mae: 114.9878 - val_loss: 7811.8110 - val_mae: 7812.5054\n",
      "Epoch 799/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 120.1499 - mae: 120.8375 - val_loss: 7755.0991 - val_mae: 7755.7915\n",
      "Epoch 800/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 117.5593 - mae: 118.2464 - val_loss: 7891.8789 - val_mae: 7892.5728\n",
      "Epoch 801/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 114.1149 - mae: 114.8020 - val_loss: 7982.3628 - val_mae: 7983.0562\n",
      "Epoch 802/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 124.7584 - mae: 125.4454 - val_loss: 8090.5674 - val_mae: 8091.2612\n",
      "Epoch 803/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 112.6137 - mae: 113.3021 - val_loss: 7776.2236 - val_mae: 7776.9160\n",
      "Epoch 804/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 128.3143 - mae: 129.0002 - val_loss: 7886.1045 - val_mae: 7886.7983\n",
      "Epoch 805/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 117.6676 - mae: 118.3546 - val_loss: 7805.3716 - val_mae: 7806.0645\n",
      "Epoch 806/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 119.4675 - mae: 120.1544 - val_loss: 7695.1265 - val_mae: 7695.8203\n",
      "Epoch 807/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 114.0473 - mae: 114.7303 - val_loss: 8026.2075 - val_mae: 8026.9004\n",
      "Epoch 808/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 112.7314 - mae: 113.4161 - val_loss: 8053.9175 - val_mae: 8054.6108\n",
      "Epoch 809/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 122.6377 - mae: 123.3207 - val_loss: 7849.6226 - val_mae: 7850.3159\n",
      "Epoch 810/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 108.6963 - mae: 109.3837 - val_loss: 7951.5083 - val_mae: 7952.2017\n",
      "Epoch 811/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 111.4914 - mae: 112.1802 - val_loss: 7918.3643 - val_mae: 7919.0571\n",
      "Epoch 812/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 120.3679 - mae: 121.0534 - val_loss: 8003.2808 - val_mae: 8003.9741\n",
      "Epoch 813/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 120.2272 - mae: 120.9144 - val_loss: 7812.0513 - val_mae: 7812.7432\n",
      "Epoch 814/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 125.4381 - mae: 126.1245 - val_loss: 8003.9468 - val_mae: 8004.6401\n",
      "Epoch 815/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 136.7637 - mae: 137.4517 - val_loss: 8005.8696 - val_mae: 8006.5610\n",
      "Epoch 816/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 122.2451 - mae: 122.9303 - val_loss: 7940.5190 - val_mae: 7941.2129\n",
      "Epoch 817/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 111.9278 - mae: 112.6130 - val_loss: 7771.4302 - val_mae: 7772.1230\n",
      "Epoch 818/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 116.6773 - mae: 117.3632 - val_loss: 7882.9639 - val_mae: 7883.6567\n",
      "Epoch 819/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 115.5352 - mae: 116.2223 - val_loss: 7875.9116 - val_mae: 7876.6045\n",
      "Epoch 820/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 116.2276 - mae: 116.9146 - val_loss: 8024.0796 - val_mae: 8024.7734\n",
      "Epoch 821/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 133.6260 - mae: 134.3119 - val_loss: 7884.5508 - val_mae: 7885.2446\n",
      "Epoch 822/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 151.1642 - mae: 151.8517 - val_loss: 8175.9585 - val_mae: 8176.6519\n",
      "Epoch 823/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 117.4686 - mae: 118.1515 - val_loss: 7975.5732 - val_mae: 7976.2671\n",
      "Epoch 824/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 106.7270 - mae: 107.4102 - val_loss: 7944.7451 - val_mae: 7945.4380\n",
      "Epoch 825/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 120.1326 - mae: 120.8210 - val_loss: 8036.7036 - val_mae: 8037.3975\n",
      "Epoch 826/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 130.8863 - mae: 131.5708 - val_loss: 7936.2705 - val_mae: 7936.9634\n",
      "Epoch 827/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 126.7897 - mae: 127.4758 - val_loss: 7953.3936 - val_mae: 7954.0869\n",
      "Epoch 828/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 119.3530 - mae: 120.0402 - val_loss: 7999.0576 - val_mae: 7999.7505\n",
      "Epoch 829/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 114.1648 - mae: 114.8511 - val_loss: 8144.5298 - val_mae: 8145.2236\n",
      "Epoch 830/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 114.0715 - mae: 114.7582 - val_loss: 7822.0859 - val_mae: 7822.7793\n",
      "Epoch 831/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 117.0517 - mae: 117.7376 - val_loss: 7886.7856 - val_mae: 7887.4790\n",
      "Epoch 832/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 131.9909 - mae: 132.6770 - val_loss: 7968.4067 - val_mae: 7969.1001\n",
      "Epoch 833/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 114.7829 - mae: 115.4702 - val_loss: 8027.3394 - val_mae: 8028.0322\n",
      "Epoch 834/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 114.4711 - mae: 115.1579 - val_loss: 8128.6323 - val_mae: 8129.3257\n",
      "Epoch 835/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 138.6584 - mae: 139.3438 - val_loss: 7777.7466 - val_mae: 7778.4395\n",
      "Epoch 836/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 115.2278 - mae: 115.9122 - val_loss: 8051.7817 - val_mae: 8052.4751\n",
      "Epoch 837/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 126.2675 - mae: 126.9535 - val_loss: 8005.9097 - val_mae: 8006.6030\n",
      "Epoch 838/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 115.2476 - mae: 115.9357 - val_loss: 8098.2437 - val_mae: 8098.9370\n",
      "Epoch 839/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 120.7883 - mae: 121.4744 - val_loss: 8318.5078 - val_mae: 8319.2012\n",
      "Epoch 840/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 133.3494 - mae: 134.0365 - val_loss: 8003.1997 - val_mae: 8003.8931\n",
      "Epoch 841/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 132.1573 - mae: 132.8428 - val_loss: 7876.2017 - val_mae: 7876.8955\n",
      "Epoch 842/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 131.5484 - mae: 132.2341 - val_loss: 8118.2437 - val_mae: 8118.9360\n",
      "Epoch 843/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 111.0408 - mae: 111.7271 - val_loss: 7861.3467 - val_mae: 7862.0396\n",
      "Epoch 844/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 109.5015 - mae: 110.1860 - val_loss: 8116.6733 - val_mae: 8117.3667\n",
      "Epoch 845/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 120.5216 - mae: 121.2048 - val_loss: 7895.5024 - val_mae: 7896.1953\n",
      "Epoch 846/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 117.2236 - mae: 117.9074 - val_loss: 7859.3530 - val_mae: 7860.0464\n",
      "Epoch 847/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 130.7037 - mae: 131.3855 - val_loss: 7920.5415 - val_mae: 7921.2349\n",
      "Epoch 848/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 121.5192 - mae: 122.2066 - val_loss: 7880.4590 - val_mae: 7881.1514\n",
      "Epoch 849/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 118.3213 - mae: 119.0081 - val_loss: 7979.8140 - val_mae: 7980.5073\n",
      "Epoch 850/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 108.3800 - mae: 109.0638 - val_loss: 7868.3857 - val_mae: 7869.0796\n",
      "Epoch 851/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 123.5396 - mae: 124.2238 - val_loss: 7952.4009 - val_mae: 7953.0938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 852/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 110.2570 - mae: 110.9441 - val_loss: 7918.3408 - val_mae: 7919.0327\n",
      "Epoch 853/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 106.2471 - mae: 106.9316 - val_loss: 8063.7905 - val_mae: 8064.4839\n",
      "Epoch 854/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 122.5298 - mae: 123.2165 - val_loss: 7970.8306 - val_mae: 7971.5239\n",
      "Epoch 855/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 145.1313 - mae: 145.8205 - val_loss: 7912.8716 - val_mae: 7913.5649\n",
      "Epoch 856/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 114.2562 - mae: 114.9415 - val_loss: 7898.9512 - val_mae: 7899.6440\n",
      "Epoch 857/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 108.3147 - mae: 109.0023 - val_loss: 7870.8369 - val_mae: 7871.5288\n",
      "Epoch 858/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 107.5477 - mae: 108.2346 - val_loss: 7997.8428 - val_mae: 7998.5361\n",
      "Epoch 859/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 132.6835 - mae: 133.3716 - val_loss: 7808.2417 - val_mae: 7808.9346\n",
      "Epoch 860/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 127.2215 - mae: 127.9075 - val_loss: 7723.6665 - val_mae: 7724.3599\n",
      "Epoch 861/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 110.1895 - mae: 110.8736 - val_loss: 8025.9995 - val_mae: 8026.6919\n",
      "Epoch 862/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 117.8198 - mae: 118.5031 - val_loss: 7992.1372 - val_mae: 7992.8301\n",
      "Epoch 863/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 103.8663 - mae: 104.5504 - val_loss: 8110.6636 - val_mae: 8111.3560\n",
      "Epoch 864/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 107.0702 - mae: 107.7587 - val_loss: 7860.2764 - val_mae: 7860.9702\n",
      "Epoch 865/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 130.5716 - mae: 131.2592 - val_loss: 8076.8125 - val_mae: 8077.5059\n",
      "Epoch 866/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 122.4534 - mae: 123.1421 - val_loss: 8027.4058 - val_mae: 8028.0981\n",
      "Epoch 867/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 126.8303 - mae: 127.5170 - val_loss: 7912.0693 - val_mae: 7912.7637\n",
      "Epoch 868/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 116.1416 - mae: 116.8284 - val_loss: 7986.2681 - val_mae: 7986.9604\n",
      "Epoch 869/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 114.9851 - mae: 115.6725 - val_loss: 8092.5513 - val_mae: 8093.2437\n",
      "Epoch 870/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 116.6006 - mae: 117.2853 - val_loss: 7959.6514 - val_mae: 7960.3457\n",
      "Epoch 871/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 103.4693 - mae: 104.1542 - val_loss: 7899.8101 - val_mae: 7900.5039\n",
      "Epoch 872/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 118.0081 - mae: 118.6935 - val_loss: 8029.1602 - val_mae: 8029.8535\n",
      "Epoch 873/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 110.6329 - mae: 111.3203 - val_loss: 7881.1099 - val_mae: 7881.8037\n",
      "Epoch 874/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 108.0744 - mae: 108.7602 - val_loss: 7950.7920 - val_mae: 7951.4839\n",
      "Epoch 875/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 114.8967 - mae: 115.5823 - val_loss: 7903.0884 - val_mae: 7903.7803\n",
      "Epoch 876/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 126.9497 - mae: 127.6367 - val_loss: 8104.3784 - val_mae: 8105.0723\n",
      "Epoch 877/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 122.6152 - mae: 123.3029 - val_loss: 8094.0166 - val_mae: 8094.7104\n",
      "Epoch 878/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 144.6241 - mae: 145.3104 - val_loss: 7855.4570 - val_mae: 7856.1509\n",
      "Epoch 879/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 112.6289 - mae: 113.3140 - val_loss: 7978.1606 - val_mae: 7978.8540\n",
      "Epoch 880/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 102.2665 - mae: 102.9526 - val_loss: 8178.1743 - val_mae: 8178.8687\n",
      "Epoch 881/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 120.8811 - mae: 121.5705 - val_loss: 8024.4160 - val_mae: 8025.1099\n",
      "Epoch 882/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 107.9569 - mae: 108.6430 - val_loss: 8033.1714 - val_mae: 8033.8633\n",
      "Epoch 883/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 111.0207 - mae: 111.7062 - val_loss: 8047.5537 - val_mae: 8048.2456\n",
      "Epoch 884/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 118.0920 - mae: 118.7772 - val_loss: 8154.3267 - val_mae: 8155.0205\n",
      "Epoch 885/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 133.6737 - mae: 134.3614 - val_loss: 8043.4766 - val_mae: 8044.1709\n",
      "Epoch 886/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 111.4927 - mae: 112.1763 - val_loss: 7945.5708 - val_mae: 7946.2642\n",
      "Epoch 887/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 110.8817 - mae: 111.5677 - val_loss: 7855.9595 - val_mae: 7856.6523\n",
      "Epoch 888/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 113.6139 - mae: 114.3010 - val_loss: 8230.3701 - val_mae: 8231.0645\n",
      "Epoch 889/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 120.2923 - mae: 120.9781 - val_loss: 8111.1113 - val_mae: 8111.8052\n",
      "Epoch 890/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 129.1302 - mae: 129.8195 - val_loss: 8218.5645 - val_mae: 8219.2578\n",
      "Epoch 891/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 132.2611 - mae: 132.9490 - val_loss: 8063.9038 - val_mae: 8064.5952\n",
      "Epoch 892/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 113.1939 - mae: 113.8794 - val_loss: 8063.4272 - val_mae: 8064.1211\n",
      "Epoch 893/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 113.7826 - mae: 114.4688 - val_loss: 8063.8896 - val_mae: 8064.5830\n",
      "Epoch 894/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 133.0045 - mae: 133.6905 - val_loss: 8052.9429 - val_mae: 8053.6367\n",
      "Epoch 895/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 118.6132 - mae: 119.2974 - val_loss: 7951.1079 - val_mae: 7951.8018\n",
      "Epoch 896/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 108.2370 - mae: 108.9221 - val_loss: 7840.4062 - val_mae: 7841.0996\n",
      "Epoch 897/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 104.9877 - mae: 105.6753 - val_loss: 7990.2207 - val_mae: 7990.9141\n",
      "Epoch 898/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 106.2208 - mae: 106.9069 - val_loss: 7992.3789 - val_mae: 7993.0723\n",
      "Epoch 899/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 107.2205 - mae: 107.9056 - val_loss: 7979.1890 - val_mae: 7979.8823\n",
      "Epoch 900/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 111.7739 - mae: 112.4596 - val_loss: 8032.3096 - val_mae: 8033.0010\n",
      "Epoch 901/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 108.8696 - mae: 109.5555 - val_loss: 7974.8252 - val_mae: 7975.5190\n",
      "Epoch 902/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 128.7242 - mae: 129.4118 - val_loss: 7746.2070 - val_mae: 7746.9004\n",
      "Epoch 903/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 144.0233 - mae: 144.7117 - val_loss: 8145.1528 - val_mae: 8145.8452\n",
      "Epoch 904/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 119.4894 - mae: 120.1730 - val_loss: 7674.1997 - val_mae: 7674.8936\n",
      "Epoch 905/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 129.2684 - mae: 129.9555 - val_loss: 8010.4570 - val_mae: 8011.1499\n",
      "Epoch 906/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 120.2114 - mae: 120.8966 - val_loss: 7885.2490 - val_mae: 7885.9424\n",
      "Epoch 907/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 116.1230 - mae: 116.8102 - val_loss: 7970.2114 - val_mae: 7970.9048\n",
      "Epoch 908/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 113.3487 - mae: 114.0360 - val_loss: 8037.6426 - val_mae: 8038.3350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 909/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 123.4745 - mae: 124.1616 - val_loss: 7943.8071 - val_mae: 7944.5010\n",
      "Epoch 910/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 118.8402 - mae: 119.5256 - val_loss: 7972.2026 - val_mae: 7972.8955\n",
      "Epoch 911/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 101.1191 - mae: 101.8037 - val_loss: 8001.4805 - val_mae: 8002.1743\n",
      "Epoch 912/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 108.6957 - mae: 109.3804 - val_loss: 7958.7729 - val_mae: 7959.4668\n",
      "Epoch 913/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 100.6451 - mae: 101.3278 - val_loss: 7903.1489 - val_mae: 7903.8423\n",
      "Epoch 914/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 105.6156 - mae: 106.3045 - val_loss: 7951.8335 - val_mae: 7952.5273\n",
      "Epoch 915/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 113.3667 - mae: 114.0531 - val_loss: 7774.0083 - val_mae: 7774.7012\n",
      "Epoch 916/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 117.8813 - mae: 118.5701 - val_loss: 8040.5586 - val_mae: 8041.2515\n",
      "Epoch 917/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 108.2262 - mae: 108.9112 - val_loss: 7932.3828 - val_mae: 7933.0767\n",
      "Epoch 918/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 105.3177 - mae: 106.0040 - val_loss: 8159.2236 - val_mae: 8159.9170\n",
      "Epoch 919/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 109.0670 - mae: 109.7536 - val_loss: 7913.9546 - val_mae: 7914.6475\n",
      "Epoch 920/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 123.7236 - mae: 124.4110 - val_loss: 7983.5054 - val_mae: 7984.1982\n",
      "Epoch 921/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 121.5091 - mae: 122.1941 - val_loss: 7979.1216 - val_mae: 7979.8135\n",
      "Epoch 922/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 105.0322 - mae: 105.7175 - val_loss: 7756.2388 - val_mae: 7756.9326\n",
      "Epoch 923/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 119.8442 - mae: 120.5282 - val_loss: 8109.9268 - val_mae: 8110.6206\n",
      "Epoch 924/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 113.1841 - mae: 113.8684 - val_loss: 7886.8477 - val_mae: 7887.5405\n",
      "Epoch 925/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 112.2987 - mae: 112.9847 - val_loss: 7967.2764 - val_mae: 7967.9697\n",
      "Epoch 926/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 120.6536 - mae: 121.3381 - val_loss: 7903.2373 - val_mae: 7903.9307\n",
      "Epoch 927/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 107.0214 - mae: 107.7084 - val_loss: 7974.8242 - val_mae: 7975.5156\n",
      "Epoch 928/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 105.8937 - mae: 106.5783 - val_loss: 8186.3325 - val_mae: 8187.0259\n",
      "Epoch 929/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 113.3957 - mae: 114.0824 - val_loss: 8196.2959 - val_mae: 8196.9883\n",
      "Epoch 930/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 113.3557 - mae: 114.0390 - val_loss: 8239.1387 - val_mae: 8239.8320\n",
      "Epoch 931/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 124.4732 - mae: 125.1594 - val_loss: 8061.1753 - val_mae: 8061.8687\n",
      "Epoch 932/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 109.1083 - mae: 109.7913 - val_loss: 8019.6660 - val_mae: 8020.3589\n",
      "Epoch 933/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 110.7010 - mae: 111.3898 - val_loss: 7868.7944 - val_mae: 7869.4868\n",
      "Epoch 934/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 109.9262 - mae: 110.6145 - val_loss: 8109.5962 - val_mae: 8110.2891\n",
      "Epoch 935/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 109.6464 - mae: 110.3315 - val_loss: 7969.5889 - val_mae: 7970.2812\n",
      "Epoch 936/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 121.8755 - mae: 122.5642 - val_loss: 7813.0703 - val_mae: 7813.7632\n",
      "Epoch 937/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 117.7430 - mae: 118.4270 - val_loss: 7861.1147 - val_mae: 7861.8081\n",
      "Epoch 938/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 117.2264 - mae: 117.9121 - val_loss: 8224.2861 - val_mae: 8224.9795\n",
      "Epoch 939/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 109.0954 - mae: 109.7815 - val_loss: 7912.3291 - val_mae: 7913.0229\n",
      "Epoch 940/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 109.0398 - mae: 109.7293 - val_loss: 8103.4795 - val_mae: 8104.1724\n",
      "Epoch 941/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 118.5762 - mae: 119.2616 - val_loss: 7954.3110 - val_mae: 7955.0039\n",
      "Epoch 942/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 105.2634 - mae: 105.9482 - val_loss: 8032.1699 - val_mae: 8032.8623\n",
      "Epoch 943/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 109.8377 - mae: 110.5223 - val_loss: 8189.1548 - val_mae: 8189.8472\n",
      "Epoch 944/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 108.4624 - mae: 109.1476 - val_loss: 7923.2080 - val_mae: 7923.9004\n",
      "Epoch 945/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 102.7551 - mae: 103.4377 - val_loss: 7925.1284 - val_mae: 7925.8218\n",
      "Epoch 946/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 124.9744 - mae: 125.6607 - val_loss: 7795.1162 - val_mae: 7795.8096\n",
      "Epoch 947/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 126.0308 - mae: 126.7167 - val_loss: 7985.6528 - val_mae: 7986.3457\n",
      "Epoch 948/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 105.2335 - mae: 105.9194 - val_loss: 8130.6045 - val_mae: 8131.2969\n",
      "Epoch 949/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 107.6484 - mae: 108.3345 - val_loss: 7943.3853 - val_mae: 7944.0776\n",
      "Epoch 950/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 116.4871 - mae: 117.1716 - val_loss: 8119.0820 - val_mae: 8119.7754\n",
      "Epoch 951/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 115.5543 - mae: 116.2416 - val_loss: 7855.8242 - val_mae: 7856.5186\n",
      "Epoch 952/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 116.3285 - mae: 117.0139 - val_loss: 7988.2261 - val_mae: 7988.9199\n",
      "Epoch 953/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 109.9915 - mae: 110.6774 - val_loss: 7943.3306 - val_mae: 7944.0249\n",
      "Epoch 954/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 105.7713 - mae: 106.4570 - val_loss: 8219.0957 - val_mae: 8219.7881\n",
      "Epoch 955/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 111.3906 - mae: 112.0757 - val_loss: 7869.2856 - val_mae: 7869.9785\n",
      "Epoch 956/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 100.7730 - mae: 101.4572 - val_loss: 8011.3975 - val_mae: 8012.0913\n",
      "Epoch 957/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 109.8252 - mae: 110.5104 - val_loss: 8016.4351 - val_mae: 8017.1274\n",
      "Epoch 958/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 107.3657 - mae: 108.0524 - val_loss: 7880.7451 - val_mae: 7881.4385\n",
      "Epoch 959/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 110.6128 - mae: 111.2981 - val_loss: 8138.2573 - val_mae: 8138.9507\n",
      "Epoch 960/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 109.2658 - mae: 109.9502 - val_loss: 8016.9932 - val_mae: 8017.6865\n",
      "Epoch 961/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 100.8589 - mae: 101.5409 - val_loss: 8150.6665 - val_mae: 8151.3599\n",
      "Epoch 962/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 113.7854 - mae: 114.4688 - val_loss: 7991.7637 - val_mae: 7992.4575\n",
      "Epoch 963/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 98.0077 - mae: 98.6917 - val_loss: 8152.4546 - val_mae: 8153.1479\n",
      "Epoch 964/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 113.7465 - mae: 114.4308 - val_loss: 7941.5088 - val_mae: 7942.2021\n",
      "Epoch 965/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 111.8424 - mae: 112.5291 - val_loss: 7923.2251 - val_mae: 7923.9189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 966/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 96.9456 - mae: 97.6301 - val_loss: 7963.3628 - val_mae: 7964.0557\n",
      "Epoch 967/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 109.8130 - mae: 110.4981 - val_loss: 7953.2686 - val_mae: 7953.9614\n",
      "Epoch 968/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 111.8521 - mae: 112.5380 - val_loss: 8133.2593 - val_mae: 8133.9536\n",
      "Epoch 969/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 118.3937 - mae: 119.0810 - val_loss: 8038.5601 - val_mae: 8039.2534\n",
      "Epoch 970/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 118.3079 - mae: 118.9954 - val_loss: 8106.5977 - val_mae: 8107.2896\n",
      "Epoch 971/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 103.9794 - mae: 104.6632 - val_loss: 7955.9731 - val_mae: 7956.6650\n",
      "Epoch 972/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 108.5750 - mae: 109.2590 - val_loss: 7962.3164 - val_mae: 7963.0093\n",
      "Epoch 973/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 102.3449 - mae: 103.0308 - val_loss: 8053.5757 - val_mae: 8054.2705\n",
      "Epoch 974/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 111.2332 - mae: 111.9193 - val_loss: 8085.5024 - val_mae: 8086.1958\n",
      "Epoch 975/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 109.8790 - mae: 110.5638 - val_loss: 8109.0405 - val_mae: 8109.7329\n",
      "Epoch 976/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 115.1500 - mae: 115.8370 - val_loss: 8247.1650 - val_mae: 8247.8584\n",
      "Epoch 977/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 114.5756 - mae: 115.2642 - val_loss: 7992.8984 - val_mae: 7993.5918\n",
      "Epoch 978/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 108.1613 - mae: 108.8475 - val_loss: 8055.9766 - val_mae: 8056.6694\n",
      "Epoch 979/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 106.9793 - mae: 107.6660 - val_loss: 8137.2485 - val_mae: 8137.9409\n",
      "Epoch 980/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 104.9899 - mae: 105.6763 - val_loss: 7919.6592 - val_mae: 7920.3516\n",
      "Epoch 981/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 96.9776 - mae: 97.6623 - val_loss: 7849.0742 - val_mae: 7849.7661\n",
      "Epoch 982/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 121.2284 - mae: 121.9117 - val_loss: 7994.3750 - val_mae: 7995.0684\n",
      "Epoch 983/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 140.6770 - mae: 141.3641 - val_loss: 7725.6196 - val_mae: 7726.3130\n",
      "Epoch 984/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 134.4078 - mae: 135.0952 - val_loss: 7889.7432 - val_mae: 7890.4365\n",
      "Epoch 985/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 102.4920 - mae: 103.1767 - val_loss: 8019.6255 - val_mae: 8020.3184\n",
      "Epoch 986/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 103.7054 - mae: 104.3900 - val_loss: 8030.6929 - val_mae: 8031.3862\n",
      "Epoch 987/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 121.7571 - mae: 122.4429 - val_loss: 7862.6572 - val_mae: 7863.3501\n",
      "Epoch 988/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 116.9893 - mae: 117.6753 - val_loss: 8107.4497 - val_mae: 8108.1431\n",
      "Epoch 989/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 103.5734 - mae: 104.2609 - val_loss: 8007.8467 - val_mae: 8008.5396\n",
      "Epoch 990/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 123.9712 - mae: 124.6568 - val_loss: 8085.2153 - val_mae: 8085.9087\n",
      "Epoch 991/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 120.7774 - mae: 121.4641 - val_loss: 7799.2603 - val_mae: 7799.9531\n",
      "Epoch 992/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 116.3209 - mae: 117.0080 - val_loss: 8044.1128 - val_mae: 8044.8066\n",
      "Epoch 993/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 103.2180 - mae: 103.9026 - val_loss: 8118.3384 - val_mae: 8119.0327\n",
      "Epoch 994/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 107.6967 - mae: 108.3835 - val_loss: 8179.5928 - val_mae: 8180.2861\n",
      "Epoch 995/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 108.0269 - mae: 108.7135 - val_loss: 7910.8179 - val_mae: 7911.5098\n",
      "Epoch 996/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 99.0854 - mae: 99.7689 - val_loss: 8009.9897 - val_mae: 8010.6826\n",
      "Epoch 997/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 100.6645 - mae: 101.3505 - val_loss: 8084.5244 - val_mae: 8085.2173\n",
      "Epoch 998/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 119.2032 - mae: 119.8906 - val_loss: 7844.3120 - val_mae: 7845.0049\n",
      "Epoch 999/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 115.7690 - mae: 116.4563 - val_loss: 7987.4165 - val_mae: 7988.1089\n",
      "Epoch 1000/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 95.8414 - mae: 96.5254 - val_loss: 7961.0093 - val_mae: 7961.7031\n",
      "Epoch 1001/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 100.4534 - mae: 101.1400 - val_loss: 7944.8794 - val_mae: 7945.5728\n",
      "Epoch 1002/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 103.9358 - mae: 104.6217 - val_loss: 7841.4614 - val_mae: 7842.1543\n",
      "Epoch 1003/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 107.4708 - mae: 108.1552 - val_loss: 8055.2759 - val_mae: 8055.9692\n",
      "Epoch 1004/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 98.7561 - mae: 99.4397 - val_loss: 7894.2817 - val_mae: 7894.9751\n",
      "Epoch 1005/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 113.9974 - mae: 114.6805 - val_loss: 7956.8906 - val_mae: 7957.5835\n",
      "Epoch 1006/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 109.2355 - mae: 109.9215 - val_loss: 8033.2578 - val_mae: 8033.9507\n",
      "Epoch 1007/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 106.0446 - mae: 106.7303 - val_loss: 8023.7866 - val_mae: 8024.4795\n",
      "Epoch 1008/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 113.3417 - mae: 114.0261 - val_loss: 8187.6758 - val_mae: 8188.3706\n",
      "Epoch 1009/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 105.9688 - mae: 106.6555 - val_loss: 8002.5322 - val_mae: 8003.2261\n",
      "Epoch 1010/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 104.9390 - mae: 105.6233 - val_loss: 7835.4238 - val_mae: 7836.1157\n",
      "Epoch 1011/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 113.7142 - mae: 114.3991 - val_loss: 8132.8433 - val_mae: 8133.5366\n",
      "Epoch 1012/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 119.2823 - mae: 119.9659 - val_loss: 8021.5356 - val_mae: 8022.2290\n",
      "Epoch 1013/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 105.5125 - mae: 106.1990 - val_loss: 8124.9854 - val_mae: 8125.6768\n",
      "Epoch 1014/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 112.9321 - mae: 113.6189 - val_loss: 8072.5918 - val_mae: 8073.2847\n",
      "Epoch 1015/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 113.9564 - mae: 114.6432 - val_loss: 8177.8569 - val_mae: 8178.5503\n",
      "Epoch 1016/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 115.2328 - mae: 115.9165 - val_loss: 8129.1294 - val_mae: 8129.8232\n",
      "Epoch 1017/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 103.1529 - mae: 103.8373 - val_loss: 8019.4385 - val_mae: 8020.1313\n",
      "Epoch 1018/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 106.1812 - mae: 106.8656 - val_loss: 7959.2026 - val_mae: 7959.8960\n",
      "Epoch 1019/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 111.3658 - mae: 112.0493 - val_loss: 7920.9937 - val_mae: 7921.6865\n",
      "Epoch 1020/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 123.0755 - mae: 123.7627 - val_loss: 7946.5396 - val_mae: 7947.2329\n",
      "Epoch 1021/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 113.8405 - mae: 114.5257 - val_loss: 7973.9595 - val_mae: 7974.6523\n",
      "Epoch 1022/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 8ms/step - loss: 99.2078 - mae: 99.8918 - val_loss: 8054.1040 - val_mae: 8054.7983\n",
      "Epoch 1023/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 98.9616 - mae: 99.6489 - val_loss: 7972.5039 - val_mae: 7973.1968\n",
      "Epoch 1024/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 103.7284 - mae: 104.4155 - val_loss: 7826.0903 - val_mae: 7826.7827\n",
      "Epoch 1025/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 104.6318 - mae: 105.3134 - val_loss: 8075.3975 - val_mae: 8076.0908\n",
      "Epoch 1026/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 96.8737 - mae: 97.5566 - val_loss: 8072.6519 - val_mae: 8073.3447\n",
      "Epoch 1027/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 94.8293 - mae: 95.5114 - val_loss: 7938.4897 - val_mae: 7939.1826\n",
      "Epoch 1028/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 104.6283 - mae: 105.3099 - val_loss: 8045.1841 - val_mae: 8045.8770\n",
      "Epoch 1029/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 107.8582 - mae: 108.5438 - val_loss: 7865.7207 - val_mae: 7866.4141\n",
      "Epoch 1030/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 94.7586 - mae: 95.4447 - val_loss: 7966.6602 - val_mae: 7967.3535\n",
      "Epoch 1031/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 110.0193 - mae: 110.7011 - val_loss: 7877.9062 - val_mae: 7878.5996\n",
      "Epoch 1032/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 105.2730 - mae: 105.9587 - val_loss: 8188.1411 - val_mae: 8188.8345\n",
      "Epoch 1033/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 100.1202 - mae: 100.8050 - val_loss: 7968.1328 - val_mae: 7968.8252\n",
      "Epoch 1034/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 121.1286 - mae: 121.8116 - val_loss: 7929.9941 - val_mae: 7930.6870\n",
      "Epoch 1035/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 116.3060 - mae: 116.9933 - val_loss: 8239.4688 - val_mae: 8240.1611\n",
      "Epoch 1036/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 108.5536 - mae: 109.2359 - val_loss: 7968.2671 - val_mae: 7968.9600\n",
      "Epoch 1037/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 111.6828 - mae: 112.3680 - val_loss: 7829.9297 - val_mae: 7830.6221\n",
      "Epoch 1038/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 118.3052 - mae: 118.9918 - val_loss: 8012.9146 - val_mae: 8013.6089\n",
      "Epoch 1039/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 103.7701 - mae: 104.4558 - val_loss: 8088.1987 - val_mae: 8088.8911\n",
      "Epoch 1040/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 102.2701 - mae: 102.9544 - val_loss: 8105.0088 - val_mae: 8105.7021\n",
      "Epoch 1041/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 95.8548 - mae: 96.5374 - val_loss: 8223.8301 - val_mae: 8224.5225\n",
      "Epoch 1042/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 95.9442 - mae: 96.6287 - val_loss: 7941.9468 - val_mae: 7942.6406\n",
      "Epoch 1043/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 99.8856 - mae: 100.5718 - val_loss: 8062.0259 - val_mae: 8062.7197\n",
      "Epoch 1044/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 106.3157 - mae: 107.0014 - val_loss: 8215.5459 - val_mae: 8216.2402\n",
      "Epoch 1045/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 108.8316 - mae: 109.5168 - val_loss: 8024.5112 - val_mae: 8025.2046\n",
      "Epoch 1046/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 99.9191 - mae: 100.6043 - val_loss: 8053.5820 - val_mae: 8054.2744\n",
      "Epoch 1047/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 99.7007 - mae: 100.3854 - val_loss: 8029.3457 - val_mae: 8030.0386\n",
      "Epoch 1048/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 100.6440 - mae: 101.3264 - val_loss: 8082.0361 - val_mae: 8082.7295\n",
      "Epoch 1049/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 108.1238 - mae: 108.8076 - val_loss: 8236.7676 - val_mae: 8237.4609\n",
      "Epoch 1050/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 110.5964 - mae: 111.2827 - val_loss: 7876.1055 - val_mae: 7876.7993\n",
      "Epoch 1051/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 108.2313 - mae: 108.9166 - val_loss: 7912.7515 - val_mae: 7913.4434\n",
      "Epoch 1052/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 101.4607 - mae: 102.1472 - val_loss: 8142.5771 - val_mae: 8143.2710\n",
      "Epoch 1053/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 103.4657 - mae: 104.1497 - val_loss: 8143.3193 - val_mae: 8144.0127\n",
      "Epoch 1054/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 106.5912 - mae: 107.2769 - val_loss: 8263.6797 - val_mae: 8264.3721\n",
      "Epoch 1055/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 116.9063 - mae: 117.5897 - val_loss: 8057.4805 - val_mae: 8058.1748\n",
      "Epoch 1056/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 106.5640 - mae: 107.2488 - val_loss: 8065.9395 - val_mae: 8066.6328\n",
      "Epoch 1057/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 109.3280 - mae: 110.0134 - val_loss: 8252.3555 - val_mae: 8253.0498\n",
      "Epoch 1058/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 107.3317 - mae: 108.0181 - val_loss: 8149.8877 - val_mae: 8150.5811\n",
      "Epoch 1059/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 103.2175 - mae: 103.9034 - val_loss: 8237.9170 - val_mae: 8238.6104\n",
      "Epoch 1060/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 99.5170 - mae: 100.2017 - val_loss: 8163.9824 - val_mae: 8164.6758\n",
      "Epoch 1061/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 106.2610 - mae: 106.9463 - val_loss: 8433.5840 - val_mae: 8434.2764\n",
      "Epoch 1062/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 112.8075 - mae: 113.4941 - val_loss: 8015.7690 - val_mae: 8016.4624\n",
      "Epoch 1063/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 101.6779 - mae: 102.3620 - val_loss: 7906.4268 - val_mae: 7907.1206\n",
      "Epoch 1064/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 98.1520 - mae: 98.8380 - val_loss: 8008.5503 - val_mae: 8009.2441\n",
      "Epoch 1065/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 97.6721 - mae: 98.3551 - val_loss: 7998.4658 - val_mae: 7999.1597\n",
      "Epoch 1066/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 100.5247 - mae: 101.2101 - val_loss: 8009.8755 - val_mae: 8010.5684\n",
      "Epoch 1067/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 104.3857 - mae: 105.0696 - val_loss: 7827.7549 - val_mae: 7828.4478\n",
      "Epoch 1068/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 93.8667 - mae: 94.5507 - val_loss: 8113.8501 - val_mae: 8114.5435\n",
      "Epoch 1069/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 101.4399 - mae: 102.1271 - val_loss: 8181.5337 - val_mae: 8182.2271\n",
      "Epoch 1070/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 116.8722 - mae: 117.5572 - val_loss: 8095.2935 - val_mae: 8095.9868\n",
      "Epoch 1071/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 116.3708 - mae: 117.0531 - val_loss: 8013.1006 - val_mae: 8013.7944\n",
      "Epoch 1072/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 105.4747 - mae: 106.1628 - val_loss: 8031.8579 - val_mae: 8032.5503\n",
      "Epoch 1073/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 110.1370 - mae: 110.8233 - val_loss: 8064.6958 - val_mae: 8065.3887\n",
      "Epoch 1074/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 105.9663 - mae: 106.6516 - val_loss: 8222.1045 - val_mae: 8222.7969\n",
      "Epoch 1075/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 114.1477 - mae: 114.8360 - val_loss: 8087.6670 - val_mae: 8088.3594\n",
      "Epoch 1076/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 98.4741 - mae: 99.1572 - val_loss: 8016.2910 - val_mae: 8016.9844\n",
      "Epoch 1077/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 111.9595 - mae: 112.6435 - val_loss: 8132.7871 - val_mae: 8133.4805\n",
      "Epoch 1078/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 12ms/step - loss: 95.1750 - mae: 95.8614 - val_loss: 8175.0850 - val_mae: 8175.7783\n",
      "Epoch 1079/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 98.3087 - mae: 98.9948 - val_loss: 8106.1929 - val_mae: 8106.8857\n",
      "Epoch 1080/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 100.7211 - mae: 101.4074 - val_loss: 8082.7871 - val_mae: 8083.4795\n",
      "Epoch 1081/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 107.6753 - mae: 108.3614 - val_loss: 8030.7168 - val_mae: 8031.4097\n",
      "Epoch 1082/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 118.4845 - mae: 119.1704 - val_loss: 7964.7778 - val_mae: 7965.4712\n",
      "Epoch 1083/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 102.2693 - mae: 102.9568 - val_loss: 7901.6782 - val_mae: 7902.3721\n",
      "Epoch 1084/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 94.0268 - mae: 94.7113 - val_loss: 8083.5972 - val_mae: 8084.2891\n",
      "Epoch 1085/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 97.5393 - mae: 98.2234 - val_loss: 8074.9990 - val_mae: 8075.6929\n",
      "Epoch 1086/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 101.4906 - mae: 102.1752 - val_loss: 8217.8623 - val_mae: 8218.5557\n",
      "Epoch 1087/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 103.7032 - mae: 104.3903 - val_loss: 7940.7412 - val_mae: 7941.4341\n",
      "Epoch 1088/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 118.4379 - mae: 119.1228 - val_loss: 8128.5137 - val_mae: 8129.2070\n",
      "Epoch 1089/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 119.1702 - mae: 119.8546 - val_loss: 8100.4263 - val_mae: 8101.1201\n",
      "Epoch 1090/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 100.5892 - mae: 101.2740 - val_loss: 8099.7607 - val_mae: 8100.4536\n",
      "Epoch 1091/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 101.3928 - mae: 102.0790 - val_loss: 8026.1025 - val_mae: 8026.7949\n",
      "Epoch 1092/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 106.1172 - mae: 106.8008 - val_loss: 8082.5142 - val_mae: 8083.2065\n",
      "Epoch 1093/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 99.2830 - mae: 99.9690 - val_loss: 8072.8418 - val_mae: 8073.5361\n",
      "Epoch 1094/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 102.8111 - mae: 103.4949 - val_loss: 8122.9238 - val_mae: 8123.6182\n",
      "Epoch 1095/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 111.1999 - mae: 111.8840 - val_loss: 8282.2734 - val_mae: 8282.9668\n",
      "Epoch 1096/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 103.9746 - mae: 104.6588 - val_loss: 8180.2188 - val_mae: 8180.9111\n",
      "Epoch 1097/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 99.6345 - mae: 100.3219 - val_loss: 7994.4536 - val_mae: 7995.1460\n",
      "Epoch 1098/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 102.1877 - mae: 102.8713 - val_loss: 7966.6826 - val_mae: 7967.3760\n",
      "Epoch 1099/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 96.9749 - mae: 97.6588 - val_loss: 8001.6743 - val_mae: 8002.3682\n",
      "Epoch 1100/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 114.3036 - mae: 114.9883 - val_loss: 8179.1211 - val_mae: 8179.8154\n",
      "Epoch 1101/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 99.5443 - mae: 100.2305 - val_loss: 8123.2388 - val_mae: 8123.9331\n",
      "Epoch 1102/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 96.0839 - mae: 96.7713 - val_loss: 8258.6611 - val_mae: 8259.3545\n",
      "Epoch 1103/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 111.7140 - mae: 112.4001 - val_loss: 7990.0161 - val_mae: 7990.7095\n",
      "Epoch 1104/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 99.4118 - mae: 100.0958 - val_loss: 8101.0615 - val_mae: 8101.7534\n",
      "Epoch 1105/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 115.6495 - mae: 116.3358 - val_loss: 8296.6387 - val_mae: 8297.3320\n",
      "Epoch 1106/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 130.2897 - mae: 130.9776 - val_loss: 8275.0117 - val_mae: 8275.7041\n",
      "Epoch 1107/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 95.8902 - mae: 96.5760 - val_loss: 8083.5972 - val_mae: 8084.2905\n",
      "Epoch 1108/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 112.3940 - mae: 113.0771 - val_loss: 8199.4502 - val_mae: 8200.1436\n",
      "Epoch 1109/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 116.7263 - mae: 117.4096 - val_loss: 7949.6987 - val_mae: 7950.3916\n",
      "Epoch 1110/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 115.7095 - mae: 116.3968 - val_loss: 7842.1055 - val_mae: 7842.7979\n",
      "Epoch 1111/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 112.2481 - mae: 112.9315 - val_loss: 8099.5298 - val_mae: 8100.2236\n",
      "Epoch 1112/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 102.7725 - mae: 103.4558 - val_loss: 8012.2520 - val_mae: 8012.9458\n",
      "Epoch 1113/5000\n",
      "46/46 [==============================] - 3s 71ms/step - loss: 95.4933 - mae: 96.1783 - val_loss: 7985.8413 - val_mae: 7986.5347\n",
      "Epoch 1114/5000\n",
      "46/46 [==============================] - 2s 40ms/step - loss: 96.3442 - mae: 97.0309 - val_loss: 8007.7998 - val_mae: 8008.4937\n",
      "Epoch 1115/5000\n",
      "46/46 [==============================] - 2s 45ms/step - loss: 103.4055 - mae: 104.0904 - val_loss: 8262.4727 - val_mae: 8263.1660\n",
      "Epoch 1116/5000\n",
      "46/46 [==============================] - 2s 40ms/step - loss: 106.8127 - mae: 107.4979 - val_loss: 8231.6084 - val_mae: 8232.3027\n",
      "Epoch 1117/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 108.1217 - mae: 108.8086 - val_loss: 7982.4575 - val_mae: 7983.1514\n",
      "Epoch 1118/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 96.9932 - mae: 97.6766 - val_loss: 8069.8208 - val_mae: 8070.5137\n",
      "Epoch 1119/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 107.0431 - mae: 107.7261 - val_loss: 8065.2554 - val_mae: 8065.9492\n",
      "Epoch 1120/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 103.7959 - mae: 104.4813 - val_loss: 8189.2852 - val_mae: 8189.9785\n",
      "Epoch 1121/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 102.1454 - mae: 102.8321 - val_loss: 7987.5474 - val_mae: 7988.2407\n",
      "Epoch 1122/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 111.2436 - mae: 111.9263 - val_loss: 8124.7969 - val_mae: 8125.4902\n",
      "Epoch 1123/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 101.9878 - mae: 102.6705 - val_loss: 7925.8989 - val_mae: 7926.5918\n",
      "Epoch 1124/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 107.9744 - mae: 108.6581 - val_loss: 7964.7246 - val_mae: 7965.4170\n",
      "Epoch 1125/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 102.1106 - mae: 102.7931 - val_loss: 7869.9658 - val_mae: 7870.6577\n",
      "Epoch 1126/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 102.0173 - mae: 102.7055 - val_loss: 7987.9399 - val_mae: 7988.6333\n",
      "Epoch 1127/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 96.7133 - mae: 97.4002 - val_loss: 7902.6133 - val_mae: 7903.3062\n",
      "Epoch 1128/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 98.0298 - mae: 98.7130 - val_loss: 7801.5293 - val_mae: 7802.2241\n",
      "Epoch 1129/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 102.5456 - mae: 103.2300 - val_loss: 8072.5723 - val_mae: 8073.2656\n",
      "Epoch 1130/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 96.6968 - mae: 97.3828 - val_loss: 8137.3364 - val_mae: 8138.0288\n",
      "Epoch 1131/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 94.2234 - mae: 94.9069 - val_loss: 8091.5752 - val_mae: 8092.2686\n",
      "Epoch 1132/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 91.9302 - mae: 92.6159 - val_loss: 8114.1616 - val_mae: 8114.8550\n",
      "Epoch 1133/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 85.8277 - mae: 86.5105 - val_loss: 8045.5708 - val_mae: 8046.2642\n",
      "Epoch 1134/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 14ms/step - loss: 95.5629 - mae: 96.2451 - val_loss: 7961.5674 - val_mae: 7962.2612\n",
      "Epoch 1135/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 109.3632 - mae: 110.0480 - val_loss: 8299.4941 - val_mae: 8300.1875\n",
      "Epoch 1136/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 114.5876 - mae: 115.2722 - val_loss: 8125.8413 - val_mae: 8126.5347\n",
      "Epoch 1137/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 115.7072 - mae: 116.3914 - val_loss: 8013.5898 - val_mae: 8014.2822\n",
      "Epoch 1138/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 106.1780 - mae: 106.8591 - val_loss: 8006.2354 - val_mae: 8006.9287\n",
      "Epoch 1139/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 95.2672 - mae: 95.9501 - val_loss: 8091.8789 - val_mae: 8092.5728\n",
      "Epoch 1140/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 101.7281 - mae: 102.4116 - val_loss: 7928.6250 - val_mae: 7929.3179\n",
      "Epoch 1141/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 105.1891 - mae: 105.8724 - val_loss: 7897.5947 - val_mae: 7898.2886\n",
      "Epoch 1142/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 97.9132 - mae: 98.5969 - val_loss: 8059.3760 - val_mae: 8060.0693\n",
      "Epoch 1143/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 111.4709 - mae: 112.1555 - val_loss: 8036.8687 - val_mae: 8037.5620\n",
      "Epoch 1144/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 109.8503 - mae: 110.5380 - val_loss: 8129.9526 - val_mae: 8130.6460\n",
      "Epoch 1145/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 96.6595 - mae: 97.3458 - val_loss: 8225.4414 - val_mae: 8226.1348\n",
      "Epoch 1146/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 118.6636 - mae: 119.3491 - val_loss: 7999.0591 - val_mae: 7999.7529\n",
      "Epoch 1147/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 112.3363 - mae: 113.0237 - val_loss: 8034.0864 - val_mae: 8034.7798\n",
      "Epoch 1148/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 106.4445 - mae: 107.1264 - val_loss: 8033.5322 - val_mae: 8034.2251\n",
      "Epoch 1149/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 103.2688 - mae: 103.9542 - val_loss: 7970.7695 - val_mae: 7971.4629\n",
      "Epoch 1150/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 99.4458 - mae: 100.1279 - val_loss: 8152.2485 - val_mae: 8152.9409\n",
      "Epoch 1151/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 93.4566 - mae: 94.1433 - val_loss: 8250.2021 - val_mae: 8250.8945\n",
      "Epoch 1152/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 104.3456 - mae: 105.0350 - val_loss: 7978.2148 - val_mae: 7978.9082\n",
      "Epoch 1153/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 105.9191 - mae: 106.6036 - val_loss: 8080.6582 - val_mae: 8081.3511\n",
      "Epoch 1154/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 99.8361 - mae: 100.5215 - val_loss: 7984.0977 - val_mae: 7984.7910\n",
      "Epoch 1155/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 106.0076 - mae: 106.6920 - val_loss: 7965.7651 - val_mae: 7966.4590\n",
      "Epoch 1156/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 106.0705 - mae: 106.7558 - val_loss: 8046.7656 - val_mae: 8047.4585\n",
      "Epoch 1157/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 96.0878 - mae: 96.7716 - val_loss: 8063.7388 - val_mae: 8064.4316\n",
      "Epoch 1158/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 97.8516 - mae: 98.5332 - val_loss: 7954.5337 - val_mae: 7955.2275\n",
      "Epoch 1159/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 110.4134 - mae: 111.1011 - val_loss: 7903.5723 - val_mae: 7904.2651\n",
      "Epoch 1160/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 101.5701 - mae: 102.2540 - val_loss: 8012.2354 - val_mae: 8012.9292\n",
      "Epoch 1161/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 99.9729 - mae: 100.6605 - val_loss: 7920.5581 - val_mae: 7921.2515\n",
      "Epoch 1162/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 115.6571 - mae: 116.3466 - val_loss: 7984.0962 - val_mae: 7984.7891\n",
      "Epoch 1163/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 101.4340 - mae: 102.1198 - val_loss: 8137.8354 - val_mae: 8138.5269\n",
      "Epoch 1164/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 107.1177 - mae: 107.8021 - val_loss: 8061.5898 - val_mae: 8062.2832\n",
      "Epoch 1165/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 111.6828 - mae: 112.3656 - val_loss: 8164.0503 - val_mae: 8164.7446\n",
      "Epoch 1166/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 100.4262 - mae: 101.1103 - val_loss: 8141.6074 - val_mae: 8142.3018\n",
      "Epoch 1167/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 99.1892 - mae: 99.8710 - val_loss: 7980.4458 - val_mae: 7981.1387\n",
      "Epoch 1168/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 110.4072 - mae: 111.0919 - val_loss: 7951.1753 - val_mae: 7951.8687\n",
      "Epoch 1169/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 110.1084 - mae: 110.7927 - val_loss: 7839.3418 - val_mae: 7840.0332\n",
      "Epoch 1170/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 96.9710 - mae: 97.6579 - val_loss: 8075.5186 - val_mae: 8076.2109\n",
      "Epoch 1171/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 103.3192 - mae: 104.0062 - val_loss: 7956.7993 - val_mae: 7957.4927\n",
      "Epoch 1172/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 102.2643 - mae: 102.9487 - val_loss: 8264.1123 - val_mae: 8264.8057\n",
      "Epoch 1173/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 93.7439 - mae: 94.4274 - val_loss: 8170.6953 - val_mae: 8171.3887\n",
      "Epoch 1174/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 101.8688 - mae: 102.5548 - val_loss: 8067.7114 - val_mae: 8068.4043\n",
      "Epoch 1175/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 109.5134 - mae: 110.1978 - val_loss: 8396.9121 - val_mae: 8397.6055\n",
      "Epoch 1176/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 113.2414 - mae: 113.9271 - val_loss: 8197.2314 - val_mae: 8197.9258\n",
      "Epoch 1177/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 100.3140 - mae: 100.9951 - val_loss: 8102.2715 - val_mae: 8102.9653\n",
      "Epoch 1178/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 105.5109 - mae: 106.1963 - val_loss: 8104.7095 - val_mae: 8105.4033\n",
      "Epoch 1179/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 104.3486 - mae: 105.0321 - val_loss: 8166.6362 - val_mae: 8167.3296\n",
      "Epoch 1180/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 103.7457 - mae: 104.4321 - val_loss: 8299.8291 - val_mae: 8300.5234\n",
      "Epoch 1181/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 102.1010 - mae: 102.7852 - val_loss: 7944.0938 - val_mae: 7944.7866\n",
      "Epoch 1182/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 99.2097 - mae: 99.8936 - val_loss: 8111.9941 - val_mae: 8112.6870\n",
      "Epoch 1183/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 93.5023 - mae: 94.1867 - val_loss: 8093.1289 - val_mae: 8093.8228\n",
      "Epoch 1184/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 106.8840 - mae: 107.5698 - val_loss: 7932.1631 - val_mae: 7932.8564\n",
      "Epoch 1185/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 103.0018 - mae: 103.6882 - val_loss: 8092.9790 - val_mae: 8093.6729\n",
      "Epoch 1186/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 95.2209 - mae: 95.9030 - val_loss: 8248.3545 - val_mae: 8249.0498\n",
      "Epoch 1187/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 127.4628 - mae: 128.1476 - val_loss: 7901.9282 - val_mae: 7902.6216\n",
      "Epoch 1188/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 101.4664 - mae: 102.1532 - val_loss: 8111.4429 - val_mae: 8112.1357\n",
      "Epoch 1189/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 96.1891 - mae: 96.8711 - val_loss: 8042.1021 - val_mae: 8042.7959\n",
      "Epoch 1190/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 10ms/step - loss: 102.2739 - mae: 102.9582 - val_loss: 7946.2739 - val_mae: 7946.9668\n",
      "Epoch 1191/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 112.9587 - mae: 113.6482 - val_loss: 8268.8906 - val_mae: 8269.5830\n",
      "Epoch 1192/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 108.8794 - mae: 109.5658 - val_loss: 8073.3511 - val_mae: 8074.0444\n",
      "Epoch 1193/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 107.1282 - mae: 107.8145 - val_loss: 8112.0889 - val_mae: 8112.7822\n",
      "Epoch 1194/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 108.2036 - mae: 108.8900 - val_loss: 8057.8657 - val_mae: 8058.5586\n",
      "Epoch 1195/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 106.0878 - mae: 106.7740 - val_loss: 8062.0981 - val_mae: 8062.7910\n",
      "Epoch 1196/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 107.1661 - mae: 107.8517 - val_loss: 7972.4106 - val_mae: 7973.1040\n",
      "Epoch 1197/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 114.0508 - mae: 114.7364 - val_loss: 8119.5117 - val_mae: 8120.2051\n",
      "Epoch 1198/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 111.2614 - mae: 111.9466 - val_loss: 8050.2866 - val_mae: 8050.9795\n",
      "Epoch 1199/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 93.3699 - mae: 94.0541 - val_loss: 8061.5674 - val_mae: 8062.2593\n",
      "Epoch 1200/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 94.9005 - mae: 95.5829 - val_loss: 7804.1279 - val_mae: 7804.8213\n",
      "Epoch 1201/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 109.7614 - mae: 110.4441 - val_loss: 8032.6431 - val_mae: 8033.3364\n",
      "Epoch 1202/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 112.3508 - mae: 113.0347 - val_loss: 7954.9224 - val_mae: 7955.6162\n",
      "Epoch 1203/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 108.2478 - mae: 108.9346 - val_loss: 7994.5420 - val_mae: 7995.2358\n",
      "Epoch 1204/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 120.5663 - mae: 121.2489 - val_loss: 7955.9268 - val_mae: 7956.6211\n",
      "Epoch 1205/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 113.4431 - mae: 114.1296 - val_loss: 8076.3018 - val_mae: 8076.9951\n",
      "Epoch 1206/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 102.7646 - mae: 103.4496 - val_loss: 7997.6582 - val_mae: 7998.3511\n",
      "Epoch 1207/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 105.9103 - mae: 106.5983 - val_loss: 8081.0859 - val_mae: 8081.7788\n",
      "Epoch 1208/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 108.9996 - mae: 109.6838 - val_loss: 8087.3320 - val_mae: 8088.0254\n",
      "Epoch 1209/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 100.1499 - mae: 100.8309 - val_loss: 8025.7441 - val_mae: 8026.4375\n",
      "Epoch 1210/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 92.1486 - mae: 92.8335 - val_loss: 8098.1729 - val_mae: 8098.8657\n",
      "Epoch 1211/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 87.8608 - mae: 88.5440 - val_loss: 8298.9414 - val_mae: 8299.6338\n",
      "Epoch 1212/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 97.3797 - mae: 98.0662 - val_loss: 8072.0645 - val_mae: 8072.7578\n",
      "Epoch 1213/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 93.2053 - mae: 93.8895 - val_loss: 8152.2783 - val_mae: 8152.9722\n",
      "Epoch 1214/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 111.5920 - mae: 112.2748 - val_loss: 8290.7607 - val_mae: 8291.4551\n",
      "Epoch 1215/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 105.3283 - mae: 106.0154 - val_loss: 8112.6621 - val_mae: 8113.3550\n",
      "Epoch 1216/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 95.6013 - mae: 96.2849 - val_loss: 8038.8311 - val_mae: 8039.5239\n",
      "Epoch 1217/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 94.8607 - mae: 95.5473 - val_loss: 7987.2075 - val_mae: 7987.9019\n",
      "Epoch 1218/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 103.3004 - mae: 103.9876 - val_loss: 8030.0376 - val_mae: 8030.7305\n",
      "Epoch 1219/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 93.3855 - mae: 94.0689 - val_loss: 8124.3599 - val_mae: 8125.0522\n",
      "Epoch 1220/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 97.3963 - mae: 98.0817 - val_loss: 8158.9487 - val_mae: 8159.6421\n",
      "Epoch 1221/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 96.6620 - mae: 97.3468 - val_loss: 8086.8857 - val_mae: 8087.5781\n",
      "Epoch 1222/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 101.4148 - mae: 102.1008 - val_loss: 8023.1841 - val_mae: 8023.8770\n",
      "Epoch 1223/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 94.1560 - mae: 94.8398 - val_loss: 8076.5815 - val_mae: 8077.2749\n",
      "Epoch 1224/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 96.4348 - mae: 97.1171 - val_loss: 8012.0977 - val_mae: 8012.7905\n",
      "Epoch 1225/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 89.5756 - mae: 90.2582 - val_loss: 8263.6748 - val_mae: 8264.3652\n",
      "Epoch 1226/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 93.0399 - mae: 93.7257 - val_loss: 8245.5029 - val_mae: 8246.1963\n",
      "Epoch 1227/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 101.1211 - mae: 101.8072 - val_loss: 8099.5718 - val_mae: 8100.2651\n",
      "Epoch 1228/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 96.6649 - mae: 97.3478 - val_loss: 8128.0581 - val_mae: 8128.7515\n",
      "Epoch 1229/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 94.3759 - mae: 95.0577 - val_loss: 8262.9326 - val_mae: 8263.6250\n",
      "Epoch 1230/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 89.5551 - mae: 90.2391 - val_loss: 8061.4448 - val_mae: 8062.1387\n",
      "Epoch 1231/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 87.4085 - mae: 88.0899 - val_loss: 8114.6392 - val_mae: 8115.3325\n",
      "Epoch 1232/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 91.6594 - mae: 92.3415 - val_loss: 8119.7251 - val_mae: 8120.4189\n",
      "Epoch 1233/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 83.9874 - mae: 84.6669 - val_loss: 8109.4434 - val_mae: 8110.1362\n",
      "Epoch 1234/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 110.5356 - mae: 111.2245 - val_loss: 8036.0615 - val_mae: 8036.7534\n",
      "Epoch 1235/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 99.3976 - mae: 100.0802 - val_loss: 8037.4028 - val_mae: 8038.0952\n",
      "Epoch 1236/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 114.9494 - mae: 115.6343 - val_loss: 8090.8535 - val_mae: 8091.5459\n",
      "Epoch 1237/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 97.1648 - mae: 97.8514 - val_loss: 8265.6182 - val_mae: 8266.3105\n",
      "Epoch 1238/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 94.6288 - mae: 95.3117 - val_loss: 8030.0615 - val_mae: 8030.7554\n",
      "Epoch 1239/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 103.9499 - mae: 104.6357 - val_loss: 8027.7900 - val_mae: 8028.4834\n",
      "Epoch 1240/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 111.7424 - mae: 112.4297 - val_loss: 8165.9272 - val_mae: 8166.6211\n",
      "Epoch 1241/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 101.6783 - mae: 102.3639 - val_loss: 8166.1084 - val_mae: 8166.8018\n",
      "Epoch 1242/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 95.2470 - mae: 95.9296 - val_loss: 7914.2383 - val_mae: 7914.9307\n",
      "Epoch 1243/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 99.3186 - mae: 100.0040 - val_loss: 8169.4092 - val_mae: 8170.1025\n",
      "Epoch 1244/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 104.3414 - mae: 105.0254 - val_loss: 8148.2930 - val_mae: 8148.9863\n",
      "Epoch 1245/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 98.8443 - mae: 99.5283 - val_loss: 8079.3970 - val_mae: 8080.0903\n",
      "Epoch 1246/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 108.7374 - mae: 109.4249 - val_loss: 8244.9258 - val_mae: 8245.6191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1247/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 109.4211 - mae: 110.1067 - val_loss: 8139.4351 - val_mae: 8140.1274\n",
      "Epoch 1248/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 104.9647 - mae: 105.6509 - val_loss: 8182.1963 - val_mae: 8182.8896\n",
      "Epoch 1249/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 112.4359 - mae: 113.1206 - val_loss: 8153.8726 - val_mae: 8154.5659\n",
      "Epoch 1250/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 103.6702 - mae: 104.3527 - val_loss: 8139.8540 - val_mae: 8140.5474\n",
      "Epoch 1251/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 92.9348 - mae: 93.6201 - val_loss: 8137.0176 - val_mae: 8137.7109\n",
      "Epoch 1252/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 108.9734 - mae: 109.6584 - val_loss: 8076.5957 - val_mae: 8077.2891\n",
      "Epoch 1253/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 96.8806 - mae: 97.5645 - val_loss: 8085.4141 - val_mae: 8086.1069\n",
      "Epoch 1254/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 93.7500 - mae: 94.4331 - val_loss: 8241.2656 - val_mae: 8241.9580\n",
      "Epoch 1255/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 97.5754 - mae: 98.2609 - val_loss: 7973.4673 - val_mae: 7974.1606\n",
      "Epoch 1256/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 101.1313 - mae: 101.8178 - val_loss: 8050.4238 - val_mae: 8051.1172\n",
      "Epoch 1257/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 106.4688 - mae: 107.1539 - val_loss: 8111.0361 - val_mae: 8111.7285\n",
      "Epoch 1258/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 92.2770 - mae: 92.9589 - val_loss: 8080.6323 - val_mae: 8081.3247\n",
      "Epoch 1259/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 101.0793 - mae: 101.7656 - val_loss: 8194.4668 - val_mae: 8195.1592\n",
      "Epoch 1260/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 99.1641 - mae: 99.8486 - val_loss: 8180.4189 - val_mae: 8181.1123\n",
      "Epoch 1261/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 93.8802 - mae: 94.5658 - val_loss: 8117.0288 - val_mae: 8117.7227\n",
      "Epoch 1262/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 97.0696 - mae: 97.7523 - val_loss: 8140.8608 - val_mae: 8141.5532\n",
      "Epoch 1263/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 100.6756 - mae: 101.3604 - val_loss: 8018.2319 - val_mae: 8018.9248\n",
      "Epoch 1264/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 90.5098 - mae: 91.1925 - val_loss: 7941.5449 - val_mae: 7942.2383\n",
      "Epoch 1265/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 125.6622 - mae: 126.3450 - val_loss: 8116.9971 - val_mae: 8117.6914\n",
      "Epoch 1266/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 106.9316 - mae: 107.6184 - val_loss: 8034.9429 - val_mae: 8035.6362\n",
      "Epoch 1267/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 89.3088 - mae: 89.9882 - val_loss: 8259.3945 - val_mae: 8260.0869\n",
      "Epoch 1268/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 97.3649 - mae: 98.0466 - val_loss: 8032.8994 - val_mae: 8033.5923\n",
      "Epoch 1269/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 94.3075 - mae: 94.9919 - val_loss: 8042.3081 - val_mae: 8043.0000\n",
      "Epoch 1270/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 93.6620 - mae: 94.3450 - val_loss: 8091.1768 - val_mae: 8091.8701\n",
      "Epoch 1271/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 92.5594 - mae: 93.2428 - val_loss: 8195.4229 - val_mae: 8196.1172\n",
      "Epoch 1272/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 105.8842 - mae: 106.5711 - val_loss: 8362.8301 - val_mae: 8363.5234\n",
      "Epoch 1273/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 92.1934 - mae: 92.8782 - val_loss: 8250.3115 - val_mae: 8251.0049\n",
      "Epoch 1274/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 103.9072 - mae: 104.5932 - val_loss: 8140.5146 - val_mae: 8141.2070\n",
      "Epoch 1275/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 92.4111 - mae: 93.0967 - val_loss: 8228.0625 - val_mae: 8228.7559\n",
      "Epoch 1276/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 94.0943 - mae: 94.7783 - val_loss: 8013.7666 - val_mae: 8014.4595\n",
      "Epoch 1277/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 92.5158 - mae: 93.2020 - val_loss: 8168.7622 - val_mae: 8169.4556\n",
      "Epoch 1278/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 100.3110 - mae: 100.9962 - val_loss: 8149.2378 - val_mae: 8149.9302\n",
      "Epoch 1279/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 96.8875 - mae: 97.5700 - val_loss: 8271.7725 - val_mae: 8272.4648\n",
      "Epoch 1280/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 100.0852 - mae: 100.7699 - val_loss: 8077.8813 - val_mae: 8078.5742\n",
      "Epoch 1281/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 85.8004 - mae: 86.4841 - val_loss: 8220.7510 - val_mae: 8221.4453\n",
      "Epoch 1282/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 90.6275 - mae: 91.3110 - val_loss: 8191.2690 - val_mae: 8191.9624\n",
      "Epoch 1283/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 103.1086 - mae: 103.7939 - val_loss: 8174.3242 - val_mae: 8175.0186\n",
      "Epoch 1284/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 100.4669 - mae: 101.1508 - val_loss: 8091.6147 - val_mae: 8092.3086\n",
      "Epoch 1285/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 92.3511 - mae: 93.0341 - val_loss: 8188.7881 - val_mae: 8189.4814\n",
      "Epoch 1286/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 100.2718 - mae: 100.9541 - val_loss: 8273.2656 - val_mae: 8273.9590\n",
      "Epoch 1287/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 105.4431 - mae: 106.1277 - val_loss: 8194.8740 - val_mae: 8195.5674\n",
      "Epoch 1288/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 101.9593 - mae: 102.6414 - val_loss: 8133.4160 - val_mae: 8134.1084\n",
      "Epoch 1289/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 101.1636 - mae: 101.8494 - val_loss: 8094.3418 - val_mae: 8095.0352\n",
      "Epoch 1290/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 87.1112 - mae: 87.7957 - val_loss: 8201.6133 - val_mae: 8202.3057\n",
      "Epoch 1291/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 91.0598 - mae: 91.7449 - val_loss: 8214.2363 - val_mae: 8214.9287\n",
      "Epoch 1292/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 99.0418 - mae: 99.7272 - val_loss: 8277.7744 - val_mae: 8278.4668\n",
      "Epoch 1293/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 97.3248 - mae: 98.0113 - val_loss: 8128.5068 - val_mae: 8129.2002\n",
      "Epoch 1294/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 93.5740 - mae: 94.2551 - val_loss: 8160.8076 - val_mae: 8161.5020\n",
      "Epoch 1295/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 86.1448 - mae: 86.8259 - val_loss: 8190.1016 - val_mae: 8190.7949\n",
      "Epoch 1296/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 95.4684 - mae: 96.1525 - val_loss: 8206.4795 - val_mae: 8207.1719\n",
      "Epoch 1297/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 98.9397 - mae: 99.6231 - val_loss: 8129.3242 - val_mae: 8130.0176\n",
      "Epoch 1298/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 94.8520 - mae: 95.5372 - val_loss: 8314.4727 - val_mae: 8315.1660\n",
      "Epoch 1299/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 91.0609 - mae: 91.7447 - val_loss: 8326.4795 - val_mae: 8327.1719\n",
      "Epoch 1300/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 103.8792 - mae: 104.5628 - val_loss: 7987.1362 - val_mae: 7987.8296\n",
      "Epoch 1301/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 98.4937 - mae: 99.1778 - val_loss: 8019.6191 - val_mae: 8020.3130\n",
      "Epoch 1302/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 100.4372 - mae: 101.1222 - val_loss: 8173.4062 - val_mae: 8174.0986\n",
      "Epoch 1303/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 90.4379 - mae: 91.1234 - val_loss: 8088.7944 - val_mae: 8089.4868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1304/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 91.3090 - mae: 91.9948 - val_loss: 8237.7637 - val_mae: 8238.4570\n",
      "Epoch 1305/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 93.2205 - mae: 93.9037 - val_loss: 8113.3896 - val_mae: 8114.0830\n",
      "Epoch 1306/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 97.7292 - mae: 98.4156 - val_loss: 8024.0425 - val_mae: 8024.7358\n",
      "Epoch 1307/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 99.1963 - mae: 99.8815 - val_loss: 7998.0024 - val_mae: 7998.6953\n",
      "Epoch 1308/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 110.4471 - mae: 111.1251 - val_loss: 8302.7031 - val_mae: 8303.3945\n",
      "Epoch 1309/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 121.6295 - mae: 122.3142 - val_loss: 8064.1973 - val_mae: 8064.8901\n",
      "Epoch 1310/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 106.4161 - mae: 107.0994 - val_loss: 8175.8809 - val_mae: 8176.5747\n",
      "Epoch 1311/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 93.4090 - mae: 94.0925 - val_loss: 8169.5347 - val_mae: 8170.2280\n",
      "Epoch 1312/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 101.3307 - mae: 102.0139 - val_loss: 8136.6382 - val_mae: 8137.3315\n",
      "Epoch 1313/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 100.6372 - mae: 101.3229 - val_loss: 8115.5977 - val_mae: 8116.2910\n",
      "Epoch 1314/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 92.3013 - mae: 92.9877 - val_loss: 8058.5259 - val_mae: 8059.2197\n",
      "Epoch 1315/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 105.0715 - mae: 105.7589 - val_loss: 8144.1313 - val_mae: 8144.8252\n",
      "Epoch 1316/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 104.0601 - mae: 104.7471 - val_loss: 8125.0259 - val_mae: 8125.7197\n",
      "Epoch 1317/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 91.1244 - mae: 91.8110 - val_loss: 8260.5938 - val_mae: 8261.2871\n",
      "Epoch 1318/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 98.8591 - mae: 99.5433 - val_loss: 8149.1230 - val_mae: 8149.8174\n",
      "Epoch 1319/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 90.8002 - mae: 91.4854 - val_loss: 8300.3271 - val_mae: 8301.0205\n",
      "Epoch 1320/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 109.8651 - mae: 110.5495 - val_loss: 8083.2412 - val_mae: 8083.9336\n",
      "Epoch 1321/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 94.5953 - mae: 95.2798 - val_loss: 8256.6787 - val_mae: 8257.3730\n",
      "Epoch 1322/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 109.4972 - mae: 110.1814 - val_loss: 8007.4521 - val_mae: 8008.1455\n",
      "Epoch 1323/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 91.5789 - mae: 92.2626 - val_loss: 7955.6904 - val_mae: 7956.3838\n",
      "Epoch 1324/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 106.7167 - mae: 107.4050 - val_loss: 8535.7109 - val_mae: 8536.4043\n",
      "Epoch 1325/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 123.7461 - mae: 124.4317 - val_loss: 8181.9351 - val_mae: 8182.6294\n",
      "Epoch 1326/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 94.0698 - mae: 94.7539 - val_loss: 8197.9893 - val_mae: 8198.6836\n",
      "Epoch 1327/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 85.8286 - mae: 86.5147 - val_loss: 8161.3608 - val_mae: 8162.0542\n",
      "Epoch 1328/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 83.6417 - mae: 84.3222 - val_loss: 8092.1113 - val_mae: 8092.8047\n",
      "Epoch 1329/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 88.6473 - mae: 89.3298 - val_loss: 8075.3452 - val_mae: 8076.0396\n",
      "Epoch 1330/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 104.4946 - mae: 105.1779 - val_loss: 8159.7310 - val_mae: 8160.4238\n",
      "Epoch 1331/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 110.6304 - mae: 111.3182 - val_loss: 8148.8906 - val_mae: 8149.5830\n",
      "Epoch 1332/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 90.1398 - mae: 90.8244 - val_loss: 8245.0967 - val_mae: 8245.7900\n",
      "Epoch 1333/5000\n",
      "16/46 [=========>....................] - ETA: 0s - loss: 88.6559 - mae: 89.3418"
     ]
    }
   ],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_mae',\n",
    "    directory='btc_tune',\n",
    "    project_name='ANN_TUNE'\n",
    ")\n",
    "\n",
    "tuner.search(X_train, Y_train, epochs=5000, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9884fdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters.\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "# Build the model with the best hp.\n",
    "regressor = build_model(best_hp)\n",
    "# Fit with the entire dataset.\n",
    "X_all = np.concatenate((X_train, X_val))\n",
    "Y_all = np.concatenate((Y_train, Y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46c3b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best hyperparameters\n",
    "best_hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a7de56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a85653fe",
   "metadata": {},
   "source": [
    "stop training when val_loss is not increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48b24b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='loss', patience=1000,verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85174a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(x=X_all, y=Y_all, epochs=5000, use_multiprocessing=True, callbacks=[earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf8814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred=regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0f090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for check\n",
    "Y_train_pred=regressor.predict(X_train)\n",
    "r2_score(Y_train, Y_train_pred) #training score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e757ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2=r2_score(Y_test[:-30],y_pred[:-30]) #score/ r^2\n",
    "print(f'r2:{r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ef1c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2_oos\n",
    "def r2_oos(ret, pred):\n",
    "    sum_of_sq_res = np.nansum(np.power((ret-pred), 2))\n",
    "    sum_of_sq_total = np.nansum(np.power(ret, 2))\n",
    "    \n",
    "    return 1-sum_of_sq_res/sum_of_sq_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8969a6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_oos = r2_oos(Y_test[:-30], y_pred[:-30])\n",
    "print(f'r2_oos:{r2_oos}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b87143",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae=mean_absolute_error(Y_test[:-30],y_pred[:-30]) #mae\n",
    "print(f'mae:{mae}')\n",
    "\n",
    "rmse=np.sqrt(mean_squared_error(Y_test[:-30],y_pred[:-30])) #rmse\n",
    "print(f'rmse:{rmse}')\n",
    "\n",
    "mape=mean_absolute_percentage_error(Y_test[:-30],y_pred[:-30]) #mape\n",
    "print(f'mape:{mape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb36caf",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5641115",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df = pd.DataFrame(zip(Y_test,y_pred),columns=['Y_test','y_pred'])\n",
    "pre_df.index = Y_test.index\n",
    "pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4bd2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df['pred_returns'] = pre_df['y_pred'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b94aa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9674c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(zip(Y_test,y_pred),columns=['Y_test','y_pred']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732393ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df.to_csv(\"../result/ANN/btc_NN.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12ff04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kdeconnect-cli -n TAS-AN00 --ping-msg 'Script complete!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b42805c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "efc08374433b8d8e4a9fd8a0a66f7295c7ce37eceb639810a945045512ff181b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
