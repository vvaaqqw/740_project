{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c5b918ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from datetime import datetime\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n",
    "from keras import losses\n",
    "from keras.constraints import maxnorm\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_validate\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from collections import Counter\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import randint\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, accuracy_score, f1_score, classification_report\n",
    "# import pickle\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, roc_auc_score\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ded3682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535a3fac",
   "metadata": {},
   "source": [
    "### Bitcoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1467391e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66099/2421463472.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  btc['Date'][i]  =  datetime.strptime(btc['Date'][i], '%Y/%m/%d')\n"
     ]
    }
   ],
   "source": [
    "regs = pd.read_csv(\"../Data/train_btc_selected_features.csv\")\n",
    "btc = pd.read_csv(\"../Data/btc_Data.csv\")\n",
    "for i in range(len(btc['Date'])):\n",
    "    btc['Date'][i]  =  datetime.strptime(btc['Date'][i], '%Y/%m/%d')\n",
    "\n",
    "btc = btc.set_index(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0d665a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "btcData = btc[regs.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8227b01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66099/751970969.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  btcData['returns'] = btcData['priceUSD'].pct_change()\n"
     ]
    }
   ],
   "source": [
    "btcData['returns'] = btcData['priceUSD'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0204bfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = btcData.drop(columns=['priceUSD'])\n",
    "Data = Data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8a926d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>difficulty14mom</th>\n",
       "      <th>difficulty30mom</th>\n",
       "      <th>difficulty3rsi</th>\n",
       "      <th>difficulty90mom</th>\n",
       "      <th>difficulty90sma</th>\n",
       "      <th>hashrate90std</th>\n",
       "      <th>hashrate90var</th>\n",
       "      <th>median_transaction_fee90rocUSD</th>\n",
       "      <th>median_transaction_feeUSD</th>\n",
       "      <th>mining_profitability</th>\n",
       "      <th>...</th>\n",
       "      <th>price90wmaUSD</th>\n",
       "      <th>sentinusd90emaUSD</th>\n",
       "      <th>sentinusdUSD</th>\n",
       "      <th>top100cap</th>\n",
       "      <th>top100cap14trx</th>\n",
       "      <th>top100cap30rsi</th>\n",
       "      <th>top100cap30trx</th>\n",
       "      <th>top100cap90rsi</th>\n",
       "      <th>top100cap90trx</th>\n",
       "      <th>returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-07-18</th>\n",
       "      <td>1.621380e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.737230e+02</td>\n",
       "      <td>2.055500e+01</td>\n",
       "      <td>8.384389e+08</td>\n",
       "      <td>1.757449e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>401834.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.612000e+03</td>\n",
       "      <td>25.782</td>\n",
       "      <td>0.139</td>\n",
       "      <td>71.191</td>\n",
       "      <td>0.308</td>\n",
       "      <td>79.756</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-19</th>\n",
       "      <td>1.621380e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.737230e+02</td>\n",
       "      <td>2.248600e+01</td>\n",
       "      <td>8.819952e+08</td>\n",
       "      <td>1.944789e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>481473.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.047000e+03</td>\n",
       "      <td>25.685</td>\n",
       "      <td>0.123</td>\n",
       "      <td>68.863</td>\n",
       "      <td>0.300</td>\n",
       "      <td>78.999</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.183196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-20</th>\n",
       "      <td>1.584060e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.735570e+02</td>\n",
       "      <td>2.441400e+01</td>\n",
       "      <td>9.280542e+08</td>\n",
       "      <td>2.153212e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>431831.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.341000e+03</td>\n",
       "      <td>25.602</td>\n",
       "      <td>0.107</td>\n",
       "      <td>66.923</td>\n",
       "      <td>0.292</td>\n",
       "      <td>78.355</td>\n",
       "      <td>0.544</td>\n",
       "      <td>-0.088475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-21</th>\n",
       "      <td>1.580420e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.700790e+02</td>\n",
       "      <td>2.630400e+01</td>\n",
       "      <td>9.761949e+08</td>\n",
       "      <td>2.382391e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>460783.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.122000e+03</td>\n",
       "      <td>25.638</td>\n",
       "      <td>0.092</td>\n",
       "      <td>67.334</td>\n",
       "      <td>0.284</td>\n",
       "      <td>78.431</td>\n",
       "      <td>0.541</td>\n",
       "      <td>-0.020434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-22</th>\n",
       "      <td>1.580420e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.700790e+02</td>\n",
       "      <td>2.819400e+01</td>\n",
       "      <td>1.019474e+09</td>\n",
       "      <td>2.598318e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>334641.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.570000e+03</td>\n",
       "      <td>25.604</td>\n",
       "      <td>0.078</td>\n",
       "      <td>66.526</td>\n",
       "      <td>0.275</td>\n",
       "      <td>78.166</td>\n",
       "      <td>0.537</td>\n",
       "      <td>-0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-24</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.384949e+13</td>\n",
       "      <td>4.612156e+19</td>\n",
       "      <td>5.317997e+38</td>\n",
       "      <td>165.380</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.0577</td>\n",
       "      <td>...</td>\n",
       "      <td>18820.0</td>\n",
       "      <td>28544682257</td>\n",
       "      <td>6.114577e+09</td>\n",
       "      <td>15.236</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>40.118</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>45.140</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.008075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-25</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.394503e+13</td>\n",
       "      <td>4.542600e+19</td>\n",
       "      <td>5.158804e+38</td>\n",
       "      <td>214.464</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.0541</td>\n",
       "      <td>...</td>\n",
       "      <td>18759.0</td>\n",
       "      <td>28076113671</td>\n",
       "      <td>7.224812e+09</td>\n",
       "      <td>15.231</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>39.880</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>45.047</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.006565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-26</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.404058e+13</td>\n",
       "      <td>4.624135e+19</td>\n",
       "      <td>5.345655e+38</td>\n",
       "      <td>255.004</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.0571</td>\n",
       "      <td>...</td>\n",
       "      <td>18701.0</td>\n",
       "      <td>27566714445</td>\n",
       "      <td>4.898449e+09</td>\n",
       "      <td>15.233</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>40.069</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>45.106</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.004001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-27</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.413612e+13</td>\n",
       "      <td>4.576227e+19</td>\n",
       "      <td>5.235464e+38</td>\n",
       "      <td>113.430</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.0618</td>\n",
       "      <td>...</td>\n",
       "      <td>18642.0</td>\n",
       "      <td>27053515674</td>\n",
       "      <td>4.216170e+09</td>\n",
       "      <td>15.259</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>41.833</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>45.656</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.423166e+13</td>\n",
       "      <td>4.552126e+19</td>\n",
       "      <td>5.180463e+38</td>\n",
       "      <td>104.294</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.0592</td>\n",
       "      <td>...</td>\n",
       "      <td>18578.0</td>\n",
       "      <td>26631320968</td>\n",
       "      <td>7.843657e+09</td>\n",
       "      <td>15.219</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>39.965</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>44.957</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.019828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4517 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            difficulty14mom  difficulty30mom  difficulty3rsi  difficulty90mom  \\\n",
       "Date                                                                            \n",
       "2010-07-18     1.621380e+02     1.641620e+02          100.00     1.737230e+02   \n",
       "2010-07-19     1.621380e+02     1.641620e+02          100.00     1.737230e+02   \n",
       "2010-07-20     1.584060e+02     1.641620e+02          100.00     1.735570e+02   \n",
       "2010-07-21     1.580420e+02     1.641620e+02          100.00     1.700790e+02   \n",
       "2010-07-22     1.580420e+02     1.641620e+02          100.00     1.700790e+02   \n",
       "...                     ...              ...             ...              ...   \n",
       "2022-11-24     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-25     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-26     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-27     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-28     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "\n",
       "            difficulty90sma  hashrate90std  hashrate90var  \\\n",
       "Date                                                        \n",
       "2010-07-18     2.055500e+01   8.384389e+08   1.757449e+17   \n",
       "2010-07-19     2.248600e+01   8.819952e+08   1.944789e+17   \n",
       "2010-07-20     2.441400e+01   9.280542e+08   2.153212e+17   \n",
       "2010-07-21     2.630400e+01   9.761949e+08   2.382391e+17   \n",
       "2010-07-22     2.819400e+01   1.019474e+09   2.598318e+17   \n",
       "...                     ...            ...            ...   \n",
       "2022-11-24     3.384949e+13   4.612156e+19   5.317997e+38   \n",
       "2022-11-25     3.394503e+13   4.542600e+19   5.158804e+38   \n",
       "2022-11-26     3.404058e+13   4.624135e+19   5.345655e+38   \n",
       "2022-11-27     3.413612e+13   4.576227e+19   5.235464e+38   \n",
       "2022-11-28     3.423166e+13   4.552126e+19   5.180463e+38   \n",
       "\n",
       "            median_transaction_fee90rocUSD  median_transaction_feeUSD  \\\n",
       "Date                                                                    \n",
       "2010-07-18                           0.000                      0.000   \n",
       "2010-07-19                           0.000                      0.000   \n",
       "2010-07-20                           0.000                      0.000   \n",
       "2010-07-21                           0.000                      0.000   \n",
       "2010-07-22                           0.000                      0.000   \n",
       "...                                    ...                        ...   \n",
       "2022-11-24                         165.380                      0.818   \n",
       "2022-11-25                         214.464                      0.812   \n",
       "2022-11-26                         255.004                      0.809   \n",
       "2022-11-27                         113.430                      0.619   \n",
       "2022-11-28                         104.294                      0.622   \n",
       "\n",
       "            mining_profitability  ...  price90wmaUSD  sentinusd90emaUSD  \\\n",
       "Date                              ...                                     \n",
       "2010-07-18           401834.0000  ...            0.0                  0   \n",
       "2010-07-19           481473.0000  ...            0.0                  0   \n",
       "2010-07-20           431831.0000  ...            0.0                  0   \n",
       "2010-07-21           460783.0000  ...            0.0                  0   \n",
       "2010-07-22           334641.0000  ...            0.0                  0   \n",
       "...                          ...  ...            ...                ...   \n",
       "2022-11-24                0.0577  ...        18820.0        28544682257   \n",
       "2022-11-25                0.0541  ...        18759.0        28076113671   \n",
       "2022-11-26                0.0571  ...        18701.0        27566714445   \n",
       "2022-11-27                0.0618  ...        18642.0        27053515674   \n",
       "2022-11-28                0.0592  ...        18578.0        26631320968   \n",
       "\n",
       "            sentinusdUSD  top100cap  top100cap14trx  top100cap30rsi  \\\n",
       "Date                                                                  \n",
       "2010-07-18  2.612000e+03     25.782           0.139          71.191   \n",
       "2010-07-19  4.047000e+03     25.685           0.123          68.863   \n",
       "2010-07-20  2.341000e+03     25.602           0.107          66.923   \n",
       "2010-07-21  2.122000e+03     25.638           0.092          67.334   \n",
       "2010-07-22  2.570000e+03     25.604           0.078          66.526   \n",
       "...                  ...        ...             ...             ...   \n",
       "2022-11-24  6.114577e+09     15.236          -0.080          40.118   \n",
       "2022-11-25  7.224812e+09     15.231          -0.075          39.880   \n",
       "2022-11-26  4.898449e+09     15.233          -0.070          40.069   \n",
       "2022-11-27  4.216170e+09     15.259          -0.065          41.833   \n",
       "2022-11-28  7.843657e+09     15.219          -0.061          39.965   \n",
       "\n",
       "            top100cap30trx  top100cap90rsi  top100cap90trx   returns  \n",
       "Date                                                                  \n",
       "2010-07-18           0.308          79.756           0.550  0.466667  \n",
       "2010-07-19           0.300          78.999           0.547  0.183196  \n",
       "2010-07-20           0.292          78.355           0.544 -0.088475  \n",
       "2010-07-21           0.284          78.431           0.541 -0.020434  \n",
       "2010-07-22           0.275          78.166           0.537 -0.153846  \n",
       "...                    ...             ...             ...       ...  \n",
       "2022-11-24          -0.042          45.140           0.001  0.008075  \n",
       "2022-11-25          -0.043          45.047           0.001 -0.006565  \n",
       "2022-11-26          -0.043          45.106           0.001  0.004001  \n",
       "2022-11-27          -0.044          45.656           0.000 -0.001147  \n",
       "2022-11-28          -0.044          44.957           0.000 -0.019828  \n",
       "\n",
       "[4517 rows x 32 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b5d4f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide X and Y\n",
    "X = Data.iloc[:,0:]\n",
    "#Y = Data['returns']   # 用returns的话就用这一行，然后把下一行comment掉\n",
    "Y = btcData['priceUSD'].shift(-30)[1:] # 反之亦然"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "eb3275b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>difficulty14mom</th>\n",
       "      <th>difficulty30mom</th>\n",
       "      <th>difficulty3rsi</th>\n",
       "      <th>difficulty90mom</th>\n",
       "      <th>difficulty90sma</th>\n",
       "      <th>hashrate90std</th>\n",
       "      <th>hashrate90var</th>\n",
       "      <th>median_transaction_fee90rocUSD</th>\n",
       "      <th>median_transaction_feeUSD</th>\n",
       "      <th>mining_profitability</th>\n",
       "      <th>...</th>\n",
       "      <th>price90wmaUSD</th>\n",
       "      <th>sentinusd90emaUSD</th>\n",
       "      <th>sentinusdUSD</th>\n",
       "      <th>top100cap</th>\n",
       "      <th>top100cap14trx</th>\n",
       "      <th>top100cap30rsi</th>\n",
       "      <th>top100cap30trx</th>\n",
       "      <th>top100cap90rsi</th>\n",
       "      <th>top100cap90trx</th>\n",
       "      <th>returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-07-18</th>\n",
       "      <td>162.138</td>\n",
       "      <td>164.162</td>\n",
       "      <td>100.0</td>\n",
       "      <td>173.723</td>\n",
       "      <td>20.555</td>\n",
       "      <td>838438881.0</td>\n",
       "      <td>1.757449e+17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>401834.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2612.0</td>\n",
       "      <td>25.782</td>\n",
       "      <td>0.139</td>\n",
       "      <td>71.191</td>\n",
       "      <td>0.308</td>\n",
       "      <td>79.756</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-19</th>\n",
       "      <td>162.138</td>\n",
       "      <td>164.162</td>\n",
       "      <td>100.0</td>\n",
       "      <td>173.723</td>\n",
       "      <td>22.486</td>\n",
       "      <td>881995244.0</td>\n",
       "      <td>1.944789e+17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>481473.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4047.0</td>\n",
       "      <td>25.685</td>\n",
       "      <td>0.123</td>\n",
       "      <td>68.863</td>\n",
       "      <td>0.300</td>\n",
       "      <td>78.999</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.183196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-20</th>\n",
       "      <td>158.406</td>\n",
       "      <td>164.162</td>\n",
       "      <td>100.0</td>\n",
       "      <td>173.557</td>\n",
       "      <td>24.414</td>\n",
       "      <td>928054231.0</td>\n",
       "      <td>2.153212e+17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>431831.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2341.0</td>\n",
       "      <td>25.602</td>\n",
       "      <td>0.107</td>\n",
       "      <td>66.923</td>\n",
       "      <td>0.292</td>\n",
       "      <td>78.355</td>\n",
       "      <td>0.544</td>\n",
       "      <td>-0.088475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            difficulty14mom  difficulty30mom  difficulty3rsi  difficulty90mom  \\\n",
       "Date                                                                            \n",
       "2010-07-18          162.138          164.162           100.0          173.723   \n",
       "2010-07-19          162.138          164.162           100.0          173.723   \n",
       "2010-07-20          158.406          164.162           100.0          173.557   \n",
       "\n",
       "            difficulty90sma  hashrate90std  hashrate90var  \\\n",
       "Date                                                        \n",
       "2010-07-18           20.555    838438881.0   1.757449e+17   \n",
       "2010-07-19           22.486    881995244.0   1.944789e+17   \n",
       "2010-07-20           24.414    928054231.0   2.153212e+17   \n",
       "\n",
       "            median_transaction_fee90rocUSD  median_transaction_feeUSD  \\\n",
       "Date                                                                    \n",
       "2010-07-18                             0.0                        0.0   \n",
       "2010-07-19                             0.0                        0.0   \n",
       "2010-07-20                             0.0                        0.0   \n",
       "\n",
       "            mining_profitability  ...  price90wmaUSD  sentinusd90emaUSD  \\\n",
       "Date                              ...                                     \n",
       "2010-07-18              401834.0  ...            0.0                  0   \n",
       "2010-07-19              481473.0  ...            0.0                  0   \n",
       "2010-07-20              431831.0  ...            0.0                  0   \n",
       "\n",
       "            sentinusdUSD  top100cap  top100cap14trx  top100cap30rsi  \\\n",
       "Date                                                                  \n",
       "2010-07-18        2612.0     25.782           0.139          71.191   \n",
       "2010-07-19        4047.0     25.685           0.123          68.863   \n",
       "2010-07-20        2341.0     25.602           0.107          66.923   \n",
       "\n",
       "            top100cap30trx  top100cap90rsi  top100cap90trx   returns  \n",
       "Date                                                                  \n",
       "2010-07-18           0.308          79.756           0.550  0.466667  \n",
       "2010-07-19           0.300          78.999           0.547  0.183196  \n",
       "2010-07-20           0.292          78.355           0.544 -0.088475  \n",
       "\n",
       "[3 rows x 32 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bd589ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into three data sets\n",
    "X_train = X['2016-01-01':'2019-12-31']\n",
    "X_val = X['2020-01-01':'2021-05-31']\n",
    "X_test = X['2021-06-01':'2023-01-01']\n",
    "\n",
    "Y_train = Y['2016-01-01':'2019-12-31']\n",
    "Y_val = Y['2020-01-01':'2021-05-31']\n",
    "Y_test = Y['2021-06-01':'2023-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0e5e9fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............ (step 1 of 2) Processing mixmax, total=   0.0s\n",
      "[Pipeline] ............ (step 2 of 2) Processing robust, total=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;mixmax&#x27;, MinMaxScaler()), [&#x27;robust&#x27;, RobustScaler()]],\n",
       "         verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;mixmax&#x27;, MinMaxScaler()), [&#x27;robust&#x27;, RobustScaler()]],\n",
       "         verbose=True)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RobustScaler</label><div class=\"sk-toggleable__content\"><pre>RobustScaler()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('mixmax', MinMaxScaler()), ['robust', RobustScaler()]],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators=[]\n",
    "estimators.append(['mixmax',MinMaxScaler()])\n",
    "estimators.append(['robust',RobustScaler()])\n",
    "scale=Pipeline(estimators,verbose=True)\n",
    "scale.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c6fc0859",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=scale.transform(X_train)\n",
    "X_test=scale.transform(X_test)\n",
    "X_val = scale.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c8a84d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8f6916ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "# define neural network model\n",
    "shape=X.shape[1]\n",
    "def sequential_model(initializer='normal', activation='relu', neurons=300, NUM_FEATURES=shape):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_shape=(NUM_FEATURES,), kernel_initializer=initializer, activation=activation))\n",
    "    model.add(Dense(416, activation=activation))\n",
    "    model.add(Dense(32, activation=activation))\n",
    "    model.add(Dense(1, activation=activation))\n",
    "    # Compile model\n",
    "    adam=keras.optimizers.Adam(lr=lr_schedule(0), amsgrad=True)\n",
    "    #sgd=keras.optimizers.SGD(learning_rate=0.08, momentum=0.9, nesterov=False)\n",
    "    model.compile(loss='logcosh', optimizer=adam, metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72db04c9",
   "metadata": {},
   "source": [
    "val_loss is the value of cost function for your cross-validation data and loss is the value of cost function for your training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85653fe",
   "metadata": {},
   "source": [
    "stop training when val_loss is not increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b48b24b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# earlyStopping = EarlyStopping(monitor='val_loss', patience=100,verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "85174a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor=KerasRegressor(build_fn=sequential_model,epochs=5000,verbose=1, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a17646de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Epoch 1/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 4930.3219 - mae: 4931.0150 - val_loss: 20615.4844 - val_mae: 20616.1738\n",
      "Epoch 2/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 4195.6872 - mae: 4196.3801 - val_loss: 10935.8018 - val_mae: 10936.4951\n",
      "Epoch 3/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 2076.6388 - mae: 2077.3314 - val_loss: 6104.1504 - val_mae: 6104.8428\n",
      "Epoch 4/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 1581.2286 - mae: 1581.9214 - val_loss: 6220.6577 - val_mae: 6221.3511\n",
      "Epoch 5/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 1418.0112 - mae: 1418.7039 - val_loss: 6302.5093 - val_mae: 6303.2017\n",
      "Epoch 6/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1296.1853 - mae: 1296.8784 - val_loss: 6419.7935 - val_mae: 6420.4863\n",
      "Epoch 7/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 1184.6349 - mae: 1185.3274 - val_loss: 6707.9614 - val_mae: 6708.6548\n",
      "Epoch 8/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1072.1930 - mae: 1072.8843 - val_loss: 6916.9487 - val_mae: 6917.6416\n",
      "Epoch 9/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1053.2620 - mae: 1053.9548 - val_loss: 7141.5464 - val_mae: 7142.2397\n",
      "Epoch 10/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 966.1841 - mae: 966.8770 - val_loss: 6942.4810 - val_mae: 6943.1743\n",
      "Epoch 11/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 954.2971 - mae: 954.9897 - val_loss: 7143.5820 - val_mae: 7144.2749\n",
      "Epoch 12/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 896.4999 - mae: 897.1923 - val_loss: 7133.3701 - val_mae: 7134.0625\n",
      "Epoch 13/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 919.9607 - mae: 920.6534 - val_loss: 7130.0151 - val_mae: 7130.7075\n",
      "Epoch 14/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 852.2812 - mae: 852.9727 - val_loss: 7240.8071 - val_mae: 7241.5005\n",
      "Epoch 15/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 823.4899 - mae: 824.1826 - val_loss: 7646.7646 - val_mae: 7647.4580\n",
      "Epoch 16/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 759.6970 - mae: 760.3896 - val_loss: 7260.9014 - val_mae: 7261.5947\n",
      "Epoch 17/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 772.1184 - mae: 772.8098 - val_loss: 7581.8579 - val_mae: 7582.5513\n",
      "Epoch 18/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 725.9376 - mae: 726.6301 - val_loss: 7462.1157 - val_mae: 7462.8081\n",
      "Epoch 19/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 699.1475 - mae: 699.8403 - val_loss: 7716.3760 - val_mae: 7717.0693\n",
      "Epoch 20/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 696.7044 - mae: 697.3966 - val_loss: 7514.4209 - val_mae: 7515.1133\n",
      "Epoch 21/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 725.5599 - mae: 726.2528 - val_loss: 7618.6855 - val_mae: 7619.3784\n",
      "Epoch 22/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 706.5078 - mae: 707.2006 - val_loss: 7502.2139 - val_mae: 7502.9077\n",
      "Epoch 23/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 647.5873 - mae: 648.2791 - val_loss: 8107.2964 - val_mae: 8107.9883\n",
      "Epoch 24/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 677.0694 - mae: 677.7617 - val_loss: 7433.4263 - val_mae: 7434.1201\n",
      "Epoch 25/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 661.4917 - mae: 662.1838 - val_loss: 7757.8940 - val_mae: 7758.5869\n",
      "Epoch 26/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 600.1971 - mae: 600.8895 - val_loss: 8005.4033 - val_mae: 8006.0972\n",
      "Epoch 27/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 597.6620 - mae: 598.3538 - val_loss: 7978.2744 - val_mae: 7978.9678\n",
      "Epoch 28/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 602.3775 - mae: 603.0693 - val_loss: 7791.8447 - val_mae: 7792.5381\n",
      "Epoch 29/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 573.1519 - mae: 573.8437 - val_loss: 8103.3462 - val_mae: 8104.0391\n",
      "Epoch 30/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 593.0793 - mae: 593.7707 - val_loss: 7943.6089 - val_mae: 7944.3018\n",
      "Epoch 31/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 521.8578 - mae: 522.5498 - val_loss: 7880.5522 - val_mae: 7881.2451\n",
      "Epoch 32/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 556.9793 - mae: 557.6718 - val_loss: 7735.6167 - val_mae: 7736.3101\n",
      "Epoch 33/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 506.8877 - mae: 507.5798 - val_loss: 7715.3652 - val_mae: 7716.0586\n",
      "Epoch 34/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 567.2664 - mae: 567.9569 - val_loss: 7629.7397 - val_mae: 7630.4316\n",
      "Epoch 35/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 558.6759 - mae: 559.3685 - val_loss: 8294.6504 - val_mae: 8295.3447\n",
      "Epoch 36/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 562.0908 - mae: 562.7823 - val_loss: 8309.5918 - val_mae: 8310.2842\n",
      "Epoch 37/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 550.3891 - mae: 551.0816 - val_loss: 7954.7456 - val_mae: 7955.4390\n",
      "Epoch 38/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 494.8846 - mae: 495.5770 - val_loss: 7851.8638 - val_mae: 7852.5562\n",
      "Epoch 39/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 508.2493 - mae: 508.9412 - val_loss: 7606.6392 - val_mae: 7607.3325\n",
      "Epoch 40/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 498.7046 - mae: 499.3954 - val_loss: 8647.7441 - val_mae: 8648.4365\n",
      "Epoch 41/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 498.9007 - mae: 499.5921 - val_loss: 8293.2324 - val_mae: 8293.9248\n",
      "Epoch 42/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 445.2518 - mae: 445.9400 - val_loss: 8634.1699 - val_mae: 8634.8643\n",
      "Epoch 43/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 468.5610 - mae: 469.2533 - val_loss: 8216.2100 - val_mae: 8216.9033\n",
      "Epoch 44/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 435.6762 - mae: 436.3648 - val_loss: 7778.3809 - val_mae: 7779.0742\n",
      "Epoch 45/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 499.2163 - mae: 499.9082 - val_loss: 8652.0117 - val_mae: 8652.7051\n",
      "Epoch 46/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 459.6235 - mae: 460.3157 - val_loss: 7663.7515 - val_mae: 7664.4448\n",
      "Epoch 47/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 460.0230 - mae: 460.7135 - val_loss: 7776.9082 - val_mae: 7777.6016\n",
      "Epoch 48/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 479.1514 - mae: 479.8419 - val_loss: 8179.6655 - val_mae: 8180.3579\n",
      "Epoch 49/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 464.3740 - mae: 465.0638 - val_loss: 8057.2168 - val_mae: 8057.9106\n",
      "Epoch 50/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 443.9871 - mae: 444.6789 - val_loss: 8113.4380 - val_mae: 8114.1313\n",
      "Epoch 51/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 455.5093 - mae: 456.2021 - val_loss: 8079.8345 - val_mae: 8080.5283\n",
      "Epoch 52/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 434.0707 - mae: 434.7627 - val_loss: 7811.7676 - val_mae: 7812.4614\n",
      "Epoch 53/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 412.3460 - mae: 413.0370 - val_loss: 8050.8740 - val_mae: 8051.5674\n",
      "Epoch 54/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 407.0779 - mae: 407.7702 - val_loss: 8630.6426 - val_mae: 8631.3340\n",
      "Epoch 55/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 443.9496 - mae: 444.6407 - val_loss: 8408.2295 - val_mae: 8408.9229\n",
      "Epoch 56/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 435.5551 - mae: 436.2467 - val_loss: 8199.7314 - val_mae: 8200.4238\n",
      "Epoch 57/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 367.6192 - mae: 368.3098 - val_loss: 8665.8330 - val_mae: 8666.5254\n",
      "Epoch 58/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 424.8524 - mae: 425.5430 - val_loss: 8234.0410 - val_mae: 8234.7344\n",
      "Epoch 59/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 429.3119 - mae: 430.0021 - val_loss: 8499.3965 - val_mae: 8500.0898\n",
      "Epoch 60/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 417.1372 - mae: 417.8290 - val_loss: 8307.2236 - val_mae: 8307.9170\n",
      "Epoch 61/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 368.0340 - mae: 368.7244 - val_loss: 8393.1113 - val_mae: 8393.8047\n",
      "Epoch 62/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 422.3031 - mae: 422.9931 - val_loss: 8480.8203 - val_mae: 8481.5146\n",
      "Epoch 63/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 373.2227 - mae: 373.9142 - val_loss: 9055.1201 - val_mae: 9055.8125\n",
      "Epoch 64/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 374.7223 - mae: 375.4113 - val_loss: 8562.8682 - val_mae: 8563.5596\n",
      "Epoch 65/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 416.8937 - mae: 417.5866 - val_loss: 9089.2529 - val_mae: 9089.9482\n",
      "Epoch 66/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 418.3393 - mae: 419.0306 - val_loss: 8618.7002 - val_mae: 8619.3945\n",
      "Epoch 67/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 386.5823 - mae: 387.2709 - val_loss: 8980.8672 - val_mae: 8981.5605\n",
      "Epoch 68/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 365.5077 - mae: 366.1996 - val_loss: 9063.7148 - val_mae: 9064.4062\n",
      "Epoch 69/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 392.5182 - mae: 393.2104 - val_loss: 8620.7061 - val_mae: 8621.3994\n",
      "Epoch 70/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 376.2141 - mae: 376.9061 - val_loss: 8446.3789 - val_mae: 8447.0713\n",
      "Epoch 71/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 365.5983 - mae: 366.2904 - val_loss: 8801.3887 - val_mae: 8802.0820\n",
      "Epoch 72/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 382.6904 - mae: 383.3819 - val_loss: 8845.8955 - val_mae: 8846.5869\n",
      "Epoch 73/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 343.1134 - mae: 343.8051 - val_loss: 8999.1592 - val_mae: 8999.8535\n",
      "Epoch 74/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 425.1427 - mae: 425.8323 - val_loss: 9215.0459 - val_mae: 9215.7402\n",
      "Epoch 75/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 364.8690 - mae: 365.5609 - val_loss: 9055.8896 - val_mae: 9056.5820\n",
      "Epoch 76/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 427.3083 - mae: 427.9984 - val_loss: 9139.6553 - val_mae: 9140.3486\n",
      "Epoch 77/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 324.0314 - mae: 324.7214 - val_loss: 8801.9941 - val_mae: 8802.6855\n",
      "Epoch 78/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 386.3621 - mae: 387.0510 - val_loss: 8982.6758 - val_mae: 8983.3691\n",
      "Epoch 79/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 359.7213 - mae: 360.4120 - val_loss: 9151.9004 - val_mae: 9152.5938\n",
      "Epoch 80/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 367.4565 - mae: 368.1455 - val_loss: 9115.6416 - val_mae: 9116.3340\n",
      "Epoch 81/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 368.8434 - mae: 369.5336 - val_loss: 8385.2334 - val_mae: 8385.9258\n",
      "Epoch 82/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 360.6635 - mae: 361.3533 - val_loss: 9134.1953 - val_mae: 9134.8887\n",
      "Epoch 83/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 330.9143 - mae: 331.6064 - val_loss: 9168.6035 - val_mae: 9169.2969\n",
      "Epoch 84/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 362.8015 - mae: 363.4920 - val_loss: 9479.4287 - val_mae: 9480.1230\n",
      "Epoch 85/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 367.3023 - mae: 367.9935 - val_loss: 9018.3174 - val_mae: 9019.0098\n",
      "Epoch 86/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 354.0651 - mae: 354.7568 - val_loss: 9450.8477 - val_mae: 9451.5400\n",
      "Epoch 87/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 354.0000 - mae: 354.6912 - val_loss: 9825.4248 - val_mae: 9826.1172\n",
      "Epoch 88/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 354.3004 - mae: 354.9918 - val_loss: 9581.9385 - val_mae: 9582.6309\n",
      "Epoch 89/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 318.4997 - mae: 319.1894 - val_loss: 9422.6826 - val_mae: 9423.3750\n",
      "Epoch 90/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 303.9005 - mae: 304.5884 - val_loss: 9588.2168 - val_mae: 9588.9102\n",
      "Epoch 91/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 376.1204 - mae: 376.8118 - val_loss: 9661.2500 - val_mae: 9661.9434\n",
      "Epoch 92/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 306.1159 - mae: 306.8060 - val_loss: 9159.1973 - val_mae: 9159.8896\n",
      "Epoch 93/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 355.0988 - mae: 355.7909 - val_loss: 9656.7012 - val_mae: 9657.3945\n",
      "Epoch 94/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 314.0996 - mae: 314.7922 - val_loss: 9623.6572 - val_mae: 9624.3516\n",
      "Epoch 95/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 317.2128 - mae: 317.9017 - val_loss: 9733.8691 - val_mae: 9734.5615\n",
      "Epoch 96/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 334.3509 - mae: 335.0405 - val_loss: 8657.9258 - val_mae: 8658.6201\n",
      "Epoch 97/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 332.8914 - mae: 333.5822 - val_loss: 9424.9385 - val_mae: 9425.6328\n",
      "Epoch 98/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 340.1048 - mae: 340.7955 - val_loss: 9343.8564 - val_mae: 9344.5498\n",
      "Epoch 99/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 338.5545 - mae: 339.2446 - val_loss: 9707.2100 - val_mae: 9707.9033\n",
      "Epoch 100/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 288.7449 - mae: 289.4358 - val_loss: 9838.2529 - val_mae: 9838.9473\n",
      "Epoch 101/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 338.7145 - mae: 339.4049 - val_loss: 9372.6816 - val_mae: 9373.3750\n",
      "Epoch 102/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 324.9218 - mae: 325.6141 - val_loss: 9590.0479 - val_mae: 9590.7393\n",
      "Epoch 103/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 336.3160 - mae: 337.0068 - val_loss: 9707.1318 - val_mae: 9707.8262\n",
      "Epoch 104/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 304.6496 - mae: 305.3393 - val_loss: 9683.6738 - val_mae: 9684.3672\n",
      "Epoch 105/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 321.2928 - mae: 321.9834 - val_loss: 9929.7539 - val_mae: 9930.4482\n",
      "Epoch 106/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 283.6543 - mae: 284.3451 - val_loss: 9274.6807 - val_mae: 9275.3740\n",
      "Epoch 107/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 356.4447 - mae: 357.1359 - val_loss: 10130.1934 - val_mae: 10130.8867\n",
      "Epoch 108/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 331.4224 - mae: 332.1129 - val_loss: 9786.0654 - val_mae: 9786.7598\n",
      "Epoch 109/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 322.1202 - mae: 322.8125 - val_loss: 9584.0303 - val_mae: 9584.7236\n",
      "Epoch 110/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 288.9574 - mae: 289.6473 - val_loss: 9387.7900 - val_mae: 9388.4834\n",
      "Epoch 111/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 311.9225 - mae: 312.6143 - val_loss: 9683.1045 - val_mae: 9683.7979\n",
      "Epoch 112/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 287.3674 - mae: 288.0563 - val_loss: 9914.2969 - val_mae: 9914.9893\n",
      "Epoch 113/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 317.8857 - mae: 318.5772 - val_loss: 9093.5586 - val_mae: 9094.2510\n",
      "Epoch 114/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 297.0661 - mae: 297.7571 - val_loss: 9779.1768 - val_mae: 9779.8682\n",
      "Epoch 115/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 302.4784 - mae: 303.1664 - val_loss: 9388.7568 - val_mae: 9389.4502\n",
      "Epoch 116/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 293.6429 - mae: 294.3316 - val_loss: 9530.0977 - val_mae: 9530.7910\n",
      "Epoch 117/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 300.5191 - mae: 301.2096 - val_loss: 9901.6562 - val_mae: 9902.3496\n",
      "Epoch 118/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 297.7776 - mae: 298.4682 - val_loss: 9853.5791 - val_mae: 9854.2715\n",
      "Epoch 119/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 305.2886 - mae: 305.9782 - val_loss: 9698.0068 - val_mae: 9698.7002\n",
      "Epoch 120/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 287.2333 - mae: 287.9241 - val_loss: 9606.9121 - val_mae: 9607.6055\n",
      "Epoch 121/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 263.0402 - mae: 263.7310 - val_loss: 9720.5615 - val_mae: 9721.2549\n",
      "Epoch 122/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 272.5254 - mae: 273.2155 - val_loss: 9665.0557 - val_mae: 9665.7500\n",
      "Epoch 123/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 272.7168 - mae: 273.4078 - val_loss: 9659.7930 - val_mae: 9660.4863\n",
      "Epoch 124/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 280.4940 - mae: 281.1836 - val_loss: 9704.3945 - val_mae: 9705.0869\n",
      "Epoch 125/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 298.6556 - mae: 299.3454 - val_loss: 9393.3311 - val_mae: 9394.0234\n",
      "Epoch 126/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 320.7104 - mae: 321.3983 - val_loss: 9309.8066 - val_mae: 9310.5010\n",
      "Epoch 127/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 278.6675 - mae: 279.3574 - val_loss: 9749.9873 - val_mae: 9750.6797\n",
      "Epoch 128/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 279.6845 - mae: 280.3748 - val_loss: 9804.5244 - val_mae: 9805.2188\n",
      "Epoch 129/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 291.1470 - mae: 291.8373 - val_loss: 9533.8760 - val_mae: 9534.5693\n",
      "Epoch 130/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 290.9311 - mae: 291.6204 - val_loss: 9606.2012 - val_mae: 9606.8955\n",
      "Epoch 131/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 264.8412 - mae: 265.5314 - val_loss: 9491.1104 - val_mae: 9491.8037\n",
      "Epoch 132/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 259.1539 - mae: 259.8425 - val_loss: 9582.8555 - val_mae: 9583.5498\n",
      "Epoch 133/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 251.3923 - mae: 252.0820 - val_loss: 9690.3525 - val_mae: 9691.0469\n",
      "Epoch 134/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 362.8735 - mae: 363.5653 - val_loss: 9426.7412 - val_mae: 9427.4355\n",
      "Epoch 135/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 298.6772 - mae: 299.3681 - val_loss: 9394.6797 - val_mae: 9395.3730\n",
      "Epoch 136/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 303.1116 - mae: 303.8017 - val_loss: 9641.4160 - val_mae: 9642.1084\n",
      "Epoch 137/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 262.5003 - mae: 263.1900 - val_loss: 9647.6562 - val_mae: 9648.3496\n",
      "Epoch 138/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 309.9039 - mae: 310.5937 - val_loss: 9964.5615 - val_mae: 9965.2549\n",
      "Epoch 139/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 288.0548 - mae: 288.7465 - val_loss: 9229.0117 - val_mae: 9229.7051\n",
      "Epoch 140/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 305.5942 - mae: 306.2844 - val_loss: 9349.3066 - val_mae: 9350.0010\n",
      "Epoch 141/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 275.7291 - mae: 276.4159 - val_loss: 9874.3076 - val_mae: 9875.0029\n",
      "Epoch 142/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 260.1503 - mae: 260.8410 - val_loss: 9480.7383 - val_mae: 9481.4316\n",
      "Epoch 143/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 288.7064 - mae: 289.3971 - val_loss: 9740.0723 - val_mae: 9740.7637\n",
      "Epoch 144/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 267.2981 - mae: 267.9886 - val_loss: 10114.6045 - val_mae: 10115.2979\n",
      "Epoch 145/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 293.3522 - mae: 294.0436 - val_loss: 9575.3125 - val_mae: 9576.0049\n",
      "Epoch 146/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 293.9003 - mae: 294.5919 - val_loss: 9786.1475 - val_mae: 9786.8398\n",
      "Epoch 147/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 272.0020 - mae: 272.6917 - val_loss: 9663.2744 - val_mae: 9663.9688\n",
      "Epoch 148/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 266.0706 - mae: 266.7612 - val_loss: 9215.2471 - val_mae: 9215.9395\n",
      "Epoch 149/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 281.4636 - mae: 282.1545 - val_loss: 9566.6055 - val_mae: 9567.2988\n",
      "Epoch 150/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 277.3591 - mae: 278.0477 - val_loss: 9598.9834 - val_mae: 9599.6758\n",
      "Epoch 151/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 257.1595 - mae: 257.8496 - val_loss: 9756.7793 - val_mae: 9757.4727\n",
      "Epoch 152/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 303.6209 - mae: 304.3121 - val_loss: 9389.8936 - val_mae: 9390.5869\n",
      "Epoch 153/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 281.9573 - mae: 282.6494 - val_loss: 9528.4131 - val_mae: 9529.1055\n",
      "Epoch 154/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 308.3488 - mae: 309.0387 - val_loss: 9769.3291 - val_mae: 9770.0225\n",
      "Epoch 155/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 272.5834 - mae: 273.2700 - val_loss: 9544.3291 - val_mae: 9545.0225\n",
      "Epoch 156/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 260.3242 - mae: 261.0121 - val_loss: 9408.7197 - val_mae: 9409.4141\n",
      "Epoch 157/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 282.5507 - mae: 283.2383 - val_loss: 9600.2939 - val_mae: 9600.9863\n",
      "Epoch 158/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 303.2943 - mae: 303.9852 - val_loss: 9944.5332 - val_mae: 9945.2256\n",
      "Epoch 159/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 270.9805 - mae: 271.6707 - val_loss: 9950.6602 - val_mae: 9951.3535\n",
      "Epoch 160/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 261.1980 - mae: 261.8899 - val_loss: 9720.0674 - val_mae: 9720.7607\n",
      "Epoch 161/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 282.1930 - mae: 282.8833 - val_loss: 9830.1104 - val_mae: 9830.8037\n",
      "Epoch 162/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 267.6726 - mae: 268.3625 - val_loss: 9528.3691 - val_mae: 9529.0615\n",
      "Epoch 163/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 282.4876 - mae: 283.1784 - val_loss: 9794.8311 - val_mae: 9795.5254\n",
      "Epoch 164/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 258.0486 - mae: 258.7381 - val_loss: 9539.7178 - val_mae: 9540.4121\n",
      "Epoch 165/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 257.4148 - mae: 258.1057 - val_loss: 9993.1807 - val_mae: 9993.8730\n",
      "Epoch 166/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 246.6614 - mae: 247.3479 - val_loss: 9874.4727 - val_mae: 9875.1650\n",
      "Epoch 167/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 270.0120 - mae: 270.7016 - val_loss: 9972.3379 - val_mae: 9973.0303\n",
      "Epoch 168/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 268.2875 - mae: 268.9765 - val_loss: 9566.0938 - val_mae: 9566.7881\n",
      "Epoch 169/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 320.0072 - mae: 320.6985 - val_loss: 10070.2568 - val_mae: 10070.9502\n",
      "Epoch 170/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 271.1439 - mae: 271.8329 - val_loss: 9507.5283 - val_mae: 9508.2207\n",
      "Epoch 171/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 266.5073 - mae: 267.1960 - val_loss: 9563.8281 - val_mae: 9564.5215\n",
      "Epoch 172/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 287.0739 - mae: 287.7645 - val_loss: 9706.0273 - val_mae: 9706.7207\n",
      "Epoch 173/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 259.8325 - mae: 260.5208 - val_loss: 9819.4922 - val_mae: 9820.1846\n",
      "Epoch 174/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 276.6919 - mae: 277.3820 - val_loss: 9551.4766 - val_mae: 9552.1709\n",
      "Epoch 175/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 236.6819 - mae: 237.3723 - val_loss: 9779.1025 - val_mae: 9779.7949\n",
      "Epoch 176/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 263.2678 - mae: 263.9552 - val_loss: 9565.3994 - val_mae: 9566.0918\n",
      "Epoch 177/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 247.3495 - mae: 248.0396 - val_loss: 9671.8271 - val_mae: 9672.5215\n",
      "Epoch 178/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 257.8974 - mae: 258.5866 - val_loss: 9417.9521 - val_mae: 9418.6465\n",
      "Epoch 179/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 272.8962 - mae: 273.5850 - val_loss: 9804.7891 - val_mae: 9805.4814\n",
      "Epoch 180/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 226.0100 - mae: 226.6987 - val_loss: 10079.7930 - val_mae: 10080.4844\n",
      "Epoch 181/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 229.6464 - mae: 230.3357 - val_loss: 10044.1006 - val_mae: 10044.7930\n",
      "Epoch 182/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 260.6672 - mae: 261.3543 - val_loss: 9515.6602 - val_mae: 9516.3535\n",
      "Epoch 183/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 226.4939 - mae: 227.1839 - val_loss: 9323.7539 - val_mae: 9324.4482\n",
      "Epoch 184/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 285.5206 - mae: 286.2119 - val_loss: 9612.2559 - val_mae: 9612.9502\n",
      "Epoch 185/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 247.3688 - mae: 248.0575 - val_loss: 9750.6152 - val_mae: 9751.3096\n",
      "Epoch 186/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 244.4421 - mae: 245.1332 - val_loss: 9847.1943 - val_mae: 9847.8867\n",
      "Epoch 187/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 263.7251 - mae: 264.4168 - val_loss: 9815.2607 - val_mae: 9815.9531\n",
      "Epoch 188/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 258.5422 - mae: 259.2286 - val_loss: 9839.6211 - val_mae: 9840.3135\n",
      "Epoch 189/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 248.4265 - mae: 249.1171 - val_loss: 9732.9570 - val_mae: 9733.6504\n",
      "Epoch 190/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 254.6264 - mae: 255.3169 - val_loss: 9784.8926 - val_mae: 9785.5830\n",
      "Epoch 191/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 247.9981 - mae: 248.6880 - val_loss: 9661.6367 - val_mae: 9662.3291\n",
      "Epoch 192/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 243.9815 - mae: 244.6711 - val_loss: 9340.0029 - val_mae: 9340.6963\n",
      "Epoch 193/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 258.1779 - mae: 258.8669 - val_loss: 9893.1777 - val_mae: 9893.8701\n",
      "Epoch 194/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 236.8018 - mae: 237.4919 - val_loss: 9668.8535 - val_mae: 9669.5479\n",
      "Epoch 195/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 259.5125 - mae: 260.2024 - val_loss: 9661.4697 - val_mae: 9662.1611\n",
      "Epoch 196/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 249.5631 - mae: 250.2535 - val_loss: 9505.5146 - val_mae: 9506.2090\n",
      "Epoch 197/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 239.3645 - mae: 240.0532 - val_loss: 9802.4131 - val_mae: 9803.1045\n",
      "Epoch 198/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 243.9900 - mae: 244.6798 - val_loss: 9614.9795 - val_mae: 9615.6729\n",
      "Epoch 199/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 230.9214 - mae: 231.6118 - val_loss: 9958.4600 - val_mae: 9959.1523\n",
      "Epoch 200/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 244.0465 - mae: 244.7380 - val_loss: 9765.5312 - val_mae: 9766.2246\n",
      "Epoch 201/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 244.5469 - mae: 245.2369 - val_loss: 9499.6650 - val_mae: 9500.3584\n",
      "Epoch 202/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 265.0655 - mae: 265.7575 - val_loss: 9669.7910 - val_mae: 9670.4834\n",
      "Epoch 203/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 226.6454 - mae: 227.3360 - val_loss: 9893.4443 - val_mae: 9894.1357\n",
      "Epoch 204/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 241.8890 - mae: 242.5782 - val_loss: 9659.1201 - val_mae: 9659.8135\n",
      "Epoch 205/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 264.5590 - mae: 265.2489 - val_loss: 9741.3037 - val_mae: 9741.9971\n",
      "Epoch 206/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 240.4934 - mae: 241.1825 - val_loss: 9619.4609 - val_mae: 9620.1543\n",
      "Epoch 207/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 227.0938 - mae: 227.7856 - val_loss: 9752.9062 - val_mae: 9753.5996\n",
      "Epoch 208/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 227.0132 - mae: 227.7014 - val_loss: 9366.1914 - val_mae: 9366.8848\n",
      "Epoch 209/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 276.4663 - mae: 277.1578 - val_loss: 10376.7324 - val_mae: 10377.4248\n",
      "Epoch 210/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 294.4020 - mae: 295.0932 - val_loss: 9160.2842 - val_mae: 9160.9766\n",
      "Epoch 211/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 266.2535 - mae: 266.9445 - val_loss: 9791.8086 - val_mae: 9792.5010\n",
      "Epoch 212/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 238.4365 - mae: 239.1264 - val_loss: 9572.6104 - val_mae: 9573.3027\n",
      "Epoch 213/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 237.2387 - mae: 237.9274 - val_loss: 9278.6768 - val_mae: 9279.3691\n",
      "Epoch 214/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 238.6040 - mae: 239.2952 - val_loss: 9755.5771 - val_mae: 9756.2715\n",
      "Epoch 215/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 230.8095 - mae: 231.4992 - val_loss: 9671.5049 - val_mae: 9672.1982\n",
      "Epoch 216/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 223.4231 - mae: 224.1103 - val_loss: 9537.6035 - val_mae: 9538.2959\n",
      "Epoch 217/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 255.7464 - mae: 256.4364 - val_loss: 10321.8096 - val_mae: 10322.5020\n",
      "Epoch 218/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 292.5056 - mae: 293.1964 - val_loss: 9856.7158 - val_mae: 9857.4092\n",
      "Epoch 219/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 223.6587 - mae: 224.3472 - val_loss: 10053.2607 - val_mae: 10053.9541\n",
      "Epoch 220/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 247.1655 - mae: 247.8558 - val_loss: 10031.8711 - val_mae: 10032.5645\n",
      "Epoch 221/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 234.9568 - mae: 235.6480 - val_loss: 9790.6611 - val_mae: 9791.3535\n",
      "Epoch 222/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 222.4537 - mae: 223.1431 - val_loss: 10064.0811 - val_mae: 10064.7754\n",
      "Epoch 223/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 222.5963 - mae: 223.2873 - val_loss: 9498.8906 - val_mae: 9499.5830\n",
      "Epoch 224/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 221.0045 - mae: 221.6951 - val_loss: 9680.5254 - val_mae: 9681.2197\n",
      "Epoch 225/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 266.9145 - mae: 267.6035 - val_loss: 9757.5049 - val_mae: 9758.1963\n",
      "Epoch 226/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 237.6033 - mae: 238.2932 - val_loss: 9946.6797 - val_mae: 9947.3721\n",
      "Epoch 227/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 204.6157 - mae: 205.3048 - val_loss: 9659.3721 - val_mae: 9660.0645\n",
      "Epoch 228/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 224.5165 - mae: 225.2074 - val_loss: 9686.3945 - val_mae: 9687.0879\n",
      "Epoch 229/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 231.2361 - mae: 231.9264 - val_loss: 9558.4863 - val_mae: 9559.1797\n",
      "Epoch 230/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 228.5722 - mae: 229.2628 - val_loss: 9953.3906 - val_mae: 9954.0820\n",
      "Epoch 231/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 232.9212 - mae: 233.6125 - val_loss: 9762.2402 - val_mae: 9762.9316\n",
      "Epoch 232/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 247.2444 - mae: 247.9309 - val_loss: 9349.6738 - val_mae: 9350.3672\n",
      "Epoch 233/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 230.8197 - mae: 231.5101 - val_loss: 9637.2070 - val_mae: 9637.8994\n",
      "Epoch 234/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 226.3720 - mae: 227.0632 - val_loss: 9870.8281 - val_mae: 9871.5225\n",
      "Epoch 235/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 229.2299 - mae: 229.9203 - val_loss: 9765.3203 - val_mae: 9766.0137\n",
      "Epoch 236/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 229.2556 - mae: 229.9425 - val_loss: 9713.1094 - val_mae: 9713.8027\n",
      "Epoch 237/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 225.1053 - mae: 225.7947 - val_loss: 9712.7686 - val_mae: 9713.4619\n",
      "Epoch 238/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 230.6190 - mae: 231.3073 - val_loss: 10255.7256 - val_mae: 10256.4189\n",
      "Epoch 239/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 240.3349 - mae: 241.0252 - val_loss: 9632.7129 - val_mae: 9633.4062\n",
      "Epoch 240/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 201.7364 - mae: 202.4230 - val_loss: 9635.1045 - val_mae: 9635.7988\n",
      "Epoch 241/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 225.9199 - mae: 226.6109 - val_loss: 9773.8096 - val_mae: 9774.5010\n",
      "Epoch 242/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 231.4654 - mae: 232.1543 - val_loss: 9969.4023 - val_mae: 9970.0967\n",
      "Epoch 243/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 225.2321 - mae: 225.9206 - val_loss: 9805.1475 - val_mae: 9805.8408\n",
      "Epoch 244/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 211.6596 - mae: 212.3483 - val_loss: 10010.8877 - val_mae: 10011.5791\n",
      "Epoch 245/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 215.0494 - mae: 215.7373 - val_loss: 9684.6436 - val_mae: 9685.3369\n",
      "Epoch 246/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 214.2979 - mae: 214.9872 - val_loss: 10190.0693 - val_mae: 10190.7637\n",
      "Epoch 247/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 227.0095 - mae: 227.7002 - val_loss: 9865.7568 - val_mae: 9866.4492\n",
      "Epoch 248/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 216.9030 - mae: 217.5942 - val_loss: 9566.7852 - val_mae: 9567.4775\n",
      "Epoch 249/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 228.6186 - mae: 229.3093 - val_loss: 9929.3838 - val_mae: 9930.0762\n",
      "Epoch 250/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 209.4290 - mae: 210.1165 - val_loss: 9728.8652 - val_mae: 9729.5586\n",
      "Epoch 251/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 235.7429 - mae: 236.4319 - val_loss: 9670.6270 - val_mae: 9671.3203\n",
      "Epoch 252/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 226.2884 - mae: 226.9801 - val_loss: 10166.1162 - val_mae: 10166.8096\n",
      "Epoch 253/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 264.6159 - mae: 265.3075 - val_loss: 9714.8945 - val_mae: 9715.5889\n",
      "Epoch 254/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 233.8489 - mae: 234.5392 - val_loss: 9735.0361 - val_mae: 9735.7305\n",
      "Epoch 255/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 238.5877 - mae: 239.2784 - val_loss: 9999.0215 - val_mae: 9999.7148\n",
      "Epoch 256/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 232.1749 - mae: 232.8644 - val_loss: 9943.1445 - val_mae: 9943.8379\n",
      "Epoch 257/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 244.7969 - mae: 245.4855 - val_loss: 9538.4854 - val_mae: 9539.1777\n",
      "Epoch 258/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 221.4825 - mae: 222.1697 - val_loss: 9879.4766 - val_mae: 9880.1680\n",
      "Epoch 259/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 225.1539 - mae: 225.8424 - val_loss: 10004.2207 - val_mae: 10004.9141\n",
      "Epoch 260/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 218.0372 - mae: 218.7270 - val_loss: 10043.3174 - val_mae: 10044.0088\n",
      "Epoch 261/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 212.7449 - mae: 213.4327 - val_loss: 9991.0186 - val_mae: 9991.7129\n",
      "Epoch 262/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 222.0953 - mae: 222.7847 - val_loss: 9645.4600 - val_mae: 9646.1514\n",
      "Epoch 263/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 217.0362 - mae: 217.7274 - val_loss: 9814.0664 - val_mae: 9814.7598\n",
      "Epoch 264/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 210.3881 - mae: 211.0781 - val_loss: 10064.5801 - val_mae: 10065.2734\n",
      "Epoch 265/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 210.7149 - mae: 211.4052 - val_loss: 9968.3945 - val_mae: 9969.0859\n",
      "Epoch 266/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 212.1387 - mae: 212.8282 - val_loss: 9886.4102 - val_mae: 9887.1035\n",
      "Epoch 267/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 219.8644 - mae: 220.5537 - val_loss: 9543.8516 - val_mae: 9544.5449\n",
      "Epoch 268/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 231.9129 - mae: 232.5995 - val_loss: 9673.1836 - val_mae: 9673.8760\n",
      "Epoch 269/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 208.2526 - mae: 208.9437 - val_loss: 9358.1650 - val_mae: 9358.8574\n",
      "Epoch 270/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 218.5080 - mae: 219.1952 - val_loss: 9715.7705 - val_mae: 9716.4639\n",
      "Epoch 271/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 223.8032 - mae: 224.4933 - val_loss: 9815.7773 - val_mae: 9816.4697\n",
      "Epoch 272/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 215.6483 - mae: 216.3377 - val_loss: 9777.3398 - val_mae: 9778.0332\n",
      "Epoch 273/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 210.6594 - mae: 211.3460 - val_loss: 10355.9766 - val_mae: 10356.6699\n",
      "Epoch 274/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 225.6102 - mae: 226.2969 - val_loss: 9402.9424 - val_mae: 9403.6357\n",
      "Epoch 275/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 258.8954 - mae: 259.5863 - val_loss: 10117.5645 - val_mae: 10118.2568\n",
      "Epoch 276/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 245.0247 - mae: 245.7136 - val_loss: 10022.7705 - val_mae: 10023.4629\n",
      "Epoch 277/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 210.9715 - mae: 211.6594 - val_loss: 9652.8105 - val_mae: 9653.5029\n",
      "Epoch 278/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 218.7617 - mae: 219.4488 - val_loss: 9603.7129 - val_mae: 9604.4053\n",
      "Epoch 279/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 224.2297 - mae: 224.9211 - val_loss: 9736.0947 - val_mae: 9736.7871\n",
      "Epoch 280/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 201.3167 - mae: 202.0041 - val_loss: 10026.9414 - val_mae: 10027.6328\n",
      "Epoch 281/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 210.1118 - mae: 210.7997 - val_loss: 9409.4463 - val_mae: 9410.1406\n",
      "Epoch 282/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 215.9630 - mae: 216.6513 - val_loss: 9671.5488 - val_mae: 9672.2422\n",
      "Epoch 283/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 211.5920 - mae: 212.2835 - val_loss: 9798.1572 - val_mae: 9798.8496\n",
      "Epoch 284/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 197.4850 - mae: 198.1724 - val_loss: 9966.8516 - val_mae: 9967.5459\n",
      "Epoch 285/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 227.9169 - mae: 228.6071 - val_loss: 9760.6230 - val_mae: 9761.3164\n",
      "Epoch 286/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 232.0967 - mae: 232.7877 - val_loss: 9692.9580 - val_mae: 9693.6514\n",
      "Epoch 287/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 200.5860 - mae: 201.2709 - val_loss: 9650.6699 - val_mae: 9651.3623\n",
      "Epoch 288/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 221.1117 - mae: 221.8011 - val_loss: 9717.2705 - val_mae: 9717.9629\n",
      "Epoch 289/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 201.6728 - mae: 202.3596 - val_loss: 10270.1025 - val_mae: 10270.7979\n",
      "Epoch 290/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 237.4988 - mae: 238.1888 - val_loss: 9917.9863 - val_mae: 9918.6797\n",
      "Epoch 291/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 190.8367 - mae: 191.5249 - val_loss: 9583.9180 - val_mae: 9584.6123\n",
      "Epoch 292/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 238.2269 - mae: 238.9159 - val_loss: 9941.1426 - val_mae: 9941.8350\n",
      "Epoch 293/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 250.7903 - mae: 251.4814 - val_loss: 10091.7539 - val_mae: 10092.4463\n",
      "Epoch 294/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 219.9340 - mae: 220.6232 - val_loss: 9719.2783 - val_mae: 9719.9727\n",
      "Epoch 295/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 215.9918 - mae: 216.6826 - val_loss: 9653.2539 - val_mae: 9653.9492\n",
      "Epoch 296/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 222.2703 - mae: 222.9602 - val_loss: 9928.5303 - val_mae: 9929.2227\n",
      "Epoch 297/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 190.3522 - mae: 191.0413 - val_loss: 9841.2305 - val_mae: 9841.9238\n",
      "Epoch 298/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 214.8649 - mae: 215.5567 - val_loss: 9764.8770 - val_mae: 9765.5693\n",
      "Epoch 299/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 222.7197 - mae: 223.4091 - val_loss: 9739.2891 - val_mae: 9739.9824\n",
      "Epoch 300/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 205.3133 - mae: 206.0034 - val_loss: 9818.2832 - val_mae: 9818.9766\n",
      "Epoch 301/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 201.6421 - mae: 202.3279 - val_loss: 9888.1348 - val_mae: 9888.8281\n",
      "Epoch 302/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 194.6135 - mae: 195.3009 - val_loss: 10069.9385 - val_mae: 10070.6318\n",
      "Epoch 303/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 218.9901 - mae: 219.6811 - val_loss: 9964.0596 - val_mae: 9964.7520\n",
      "Epoch 304/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 230.2009 - mae: 230.8879 - val_loss: 10099.8652 - val_mae: 10100.5596\n",
      "Epoch 305/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 210.8341 - mae: 211.5254 - val_loss: 10150.5498 - val_mae: 10151.2432\n",
      "Epoch 306/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 194.9168 - mae: 195.6064 - val_loss: 9892.4287 - val_mae: 9893.1211\n",
      "Epoch 307/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 210.1999 - mae: 210.8903 - val_loss: 10185.7061 - val_mae: 10186.3994\n",
      "Epoch 308/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 221.9021 - mae: 222.5927 - val_loss: 10125.8750 - val_mae: 10126.5674\n",
      "Epoch 309/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 221.1548 - mae: 221.8439 - val_loss: 9743.5352 - val_mae: 9744.2285\n",
      "Epoch 310/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 202.4279 - mae: 203.1181 - val_loss: 9709.8223 - val_mae: 9710.5156\n",
      "Epoch 311/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 222.0627 - mae: 222.7508 - val_loss: 9371.0840 - val_mae: 9371.7773\n",
      "Epoch 312/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 191.4637 - mae: 192.1520 - val_loss: 9773.4375 - val_mae: 9774.1318\n",
      "Epoch 313/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 199.8268 - mae: 200.5176 - val_loss: 10168.9854 - val_mae: 10169.6787\n",
      "Epoch 314/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 219.3295 - mae: 220.0186 - val_loss: 10062.8232 - val_mae: 10063.5156\n",
      "Epoch 315/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 215.2715 - mae: 215.9609 - val_loss: 9974.7871 - val_mae: 9975.4795\n",
      "Epoch 316/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 233.7930 - mae: 234.4815 - val_loss: 9496.3301 - val_mae: 9497.0234\n",
      "Epoch 317/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 253.2867 - mae: 253.9754 - val_loss: 9578.2998 - val_mae: 9578.9941\n",
      "Epoch 318/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 197.2035 - mae: 197.8930 - val_loss: 9980.4092 - val_mae: 9981.1035\n",
      "Epoch 319/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 210.3871 - mae: 211.0777 - val_loss: 9954.1582 - val_mae: 9954.8496\n",
      "Epoch 320/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 213.9183 - mae: 214.6062 - val_loss: 9608.1562 - val_mae: 9608.8496\n",
      "Epoch 321/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 224.3459 - mae: 225.0368 - val_loss: 9915.5840 - val_mae: 9916.2783\n",
      "Epoch 322/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 196.3036 - mae: 196.9927 - val_loss: 9801.7705 - val_mae: 9802.4639\n",
      "Epoch 323/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 183.4085 - mae: 184.0976 - val_loss: 9718.0771 - val_mae: 9718.7705\n",
      "Epoch 324/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 187.6880 - mae: 188.3770 - val_loss: 9834.9287 - val_mae: 9835.6211\n",
      "Epoch 325/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 192.2973 - mae: 192.9885 - val_loss: 9786.7139 - val_mae: 9787.4072\n",
      "Epoch 326/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 195.3253 - mae: 196.0172 - val_loss: 10049.2539 - val_mae: 10049.9463\n",
      "Epoch 327/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 207.5599 - mae: 208.2505 - val_loss: 9894.7471 - val_mae: 9895.4395\n",
      "Epoch 328/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 182.5685 - mae: 183.2579 - val_loss: 9857.5430 - val_mae: 9858.2354\n",
      "Epoch 329/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 215.7116 - mae: 216.3986 - val_loss: 10508.5303 - val_mae: 10509.2227\n",
      "Epoch 330/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 245.9860 - mae: 246.6762 - val_loss: 9909.2803 - val_mae: 9909.9736\n",
      "Epoch 331/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 213.6482 - mae: 214.3377 - val_loss: 9324.7236 - val_mae: 9325.4160\n",
      "Epoch 332/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 214.2868 - mae: 214.9758 - val_loss: 9659.7217 - val_mae: 9660.4160\n",
      "Epoch 333/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 185.4723 - mae: 186.1599 - val_loss: 10123.0176 - val_mae: 10123.7109\n",
      "Epoch 334/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 195.0013 - mae: 195.6892 - val_loss: 9848.1074 - val_mae: 9848.7988\n",
      "Epoch 335/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 199.6861 - mae: 200.3754 - val_loss: 9538.2764 - val_mae: 9538.9697\n",
      "Epoch 336/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 223.9979 - mae: 224.6895 - val_loss: 9974.1182 - val_mae: 9974.8105\n",
      "Epoch 337/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 194.3154 - mae: 195.0050 - val_loss: 9533.6826 - val_mae: 9534.3750\n",
      "Epoch 338/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 213.1157 - mae: 213.8047 - val_loss: 9647.9199 - val_mae: 9648.6133\n",
      "Epoch 339/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 191.7045 - mae: 192.3920 - val_loss: 10099.8721 - val_mae: 10100.5635\n",
      "Epoch 340/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 234.1075 - mae: 234.7970 - val_loss: 10007.1992 - val_mae: 10007.8916\n",
      "Epoch 341/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 214.0694 - mae: 214.7589 - val_loss: 9969.3555 - val_mae: 9970.0488\n",
      "Epoch 342/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 212.7839 - mae: 213.4743 - val_loss: 9780.1797 - val_mae: 9780.8721\n",
      "Epoch 343/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 192.1721 - mae: 192.8604 - val_loss: 10147.0068 - val_mae: 10147.6982\n",
      "Epoch 344/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 200.5607 - mae: 201.2486 - val_loss: 9974.2764 - val_mae: 9974.9717\n",
      "Epoch 345/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 200.5957 - mae: 201.2846 - val_loss: 9997.9404 - val_mae: 9998.6338\n",
      "Epoch 346/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 194.0702 - mae: 194.7601 - val_loss: 10122.7520 - val_mae: 10123.4463\n",
      "Epoch 347/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 194.3807 - mae: 195.0702 - val_loss: 9809.1016 - val_mae: 9809.7949\n",
      "Epoch 348/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 204.2219 - mae: 204.9115 - val_loss: 10220.4873 - val_mae: 10221.1807\n",
      "Epoch 349/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 198.9997 - mae: 199.6887 - val_loss: 9730.5244 - val_mae: 9731.2178\n",
      "Epoch 350/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 211.8172 - mae: 212.5058 - val_loss: 9693.9482 - val_mae: 9694.6426\n",
      "Epoch 351/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 206.8356 - mae: 207.5250 - val_loss: 9876.9883 - val_mae: 9877.6816\n",
      "Epoch 352/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 204.7574 - mae: 205.4464 - val_loss: 9548.4248 - val_mae: 9549.1182\n",
      "Epoch 353/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 223.1722 - mae: 223.8644 - val_loss: 9742.0312 - val_mae: 9742.7236\n",
      "Epoch 354/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 189.3709 - mae: 190.0604 - val_loss: 9724.7295 - val_mae: 9725.4229\n",
      "Epoch 355/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 220.9805 - mae: 221.6710 - val_loss: 9888.0605 - val_mae: 9888.7529\n",
      "Epoch 356/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 199.2299 - mae: 199.9175 - val_loss: 9559.0664 - val_mae: 9559.7578\n",
      "Epoch 357/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 213.9786 - mae: 214.6678 - val_loss: 9579.8545 - val_mae: 9580.5479\n",
      "Epoch 358/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 194.8571 - mae: 195.5425 - val_loss: 10001.7578 - val_mae: 10002.4502\n",
      "Epoch 359/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 190.0801 - mae: 190.7689 - val_loss: 9691.2031 - val_mae: 9691.8965\n",
      "Epoch 360/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 208.0575 - mae: 208.7425 - val_loss: 9912.3447 - val_mae: 9913.0371\n",
      "Epoch 361/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 193.6604 - mae: 194.3499 - val_loss: 9935.7734 - val_mae: 9936.4658\n",
      "Epoch 362/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 176.2705 - mae: 176.9580 - val_loss: 9862.3799 - val_mae: 9863.0723\n",
      "Epoch 363/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 197.7828 - mae: 198.4709 - val_loss: 10088.3105 - val_mae: 10089.0039\n",
      "Epoch 364/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 201.3337 - mae: 202.0227 - val_loss: 9914.3496 - val_mae: 9915.0420\n",
      "Epoch 365/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 191.8636 - mae: 192.5511 - val_loss: 9768.3906 - val_mae: 9769.0830\n",
      "Epoch 366/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 209.0762 - mae: 209.7655 - val_loss: 10103.4199 - val_mae: 10104.1123\n",
      "Epoch 367/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 190.0847 - mae: 190.7729 - val_loss: 9667.4746 - val_mae: 9668.1680\n",
      "Epoch 368/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 206.0659 - mae: 206.7553 - val_loss: 9962.2178 - val_mae: 9962.9111\n",
      "Epoch 369/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 186.2647 - mae: 186.9547 - val_loss: 10034.0635 - val_mae: 10034.7568\n",
      "Epoch 370/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 189.4608 - mae: 190.1500 - val_loss: 9841.0127 - val_mae: 9841.7051\n",
      "Epoch 371/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 187.7874 - mae: 188.4761 - val_loss: 9392.4912 - val_mae: 9393.1846\n",
      "Epoch 372/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 213.5523 - mae: 214.2440 - val_loss: 9652.0781 - val_mae: 9652.7715\n",
      "Epoch 373/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 191.5672 - mae: 192.2568 - val_loss: 9751.2471 - val_mae: 9751.9395\n",
      "Epoch 374/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 182.1328 - mae: 182.8206 - val_loss: 10053.2412 - val_mae: 10053.9346\n",
      "Epoch 375/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 199.7568 - mae: 200.4453 - val_loss: 9929.1865 - val_mae: 9929.8799\n",
      "Epoch 376/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 183.0090 - mae: 183.7004 - val_loss: 9741.0049 - val_mae: 9741.6982\n",
      "Epoch 377/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 181.6685 - mae: 182.3550 - val_loss: 10065.6660 - val_mae: 10066.3584\n",
      "Epoch 378/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 202.2568 - mae: 202.9471 - val_loss: 9702.9414 - val_mae: 9703.6328\n",
      "Epoch 379/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 189.1428 - mae: 189.8292 - val_loss: 9845.7822 - val_mae: 9846.4756\n",
      "Epoch 380/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 173.0857 - mae: 173.7723 - val_loss: 10085.5166 - val_mae: 10086.2109\n",
      "Epoch 381/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 179.4410 - mae: 180.1287 - val_loss: 10113.3018 - val_mae: 10113.9941\n",
      "Epoch 382/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 174.0158 - mae: 174.7054 - val_loss: 10041.7490 - val_mae: 10042.4414\n",
      "Epoch 383/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 195.0332 - mae: 195.7228 - val_loss: 10077.0645 - val_mae: 10077.7578\n",
      "Epoch 384/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 194.5356 - mae: 195.2236 - val_loss: 9897.0811 - val_mae: 9897.7744\n",
      "Epoch 385/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 204.5112 - mae: 205.2009 - val_loss: 9727.0410 - val_mae: 9727.7344\n",
      "Epoch 386/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 199.4195 - mae: 200.1072 - val_loss: 9426.5186 - val_mae: 9427.2109\n",
      "Epoch 387/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 190.0663 - mae: 190.7544 - val_loss: 10140.2676 - val_mae: 10140.9590\n",
      "Epoch 388/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 201.9387 - mae: 202.6224 - val_loss: 10329.2109 - val_mae: 10329.9023\n",
      "Epoch 389/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 194.1151 - mae: 194.8053 - val_loss: 10162.4316 - val_mae: 10163.1260\n",
      "Epoch 390/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 200.6625 - mae: 201.3523 - val_loss: 10144.8301 - val_mae: 10145.5225\n",
      "Epoch 391/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 184.6883 - mae: 185.3758 - val_loss: 9799.5059 - val_mae: 9800.1982\n",
      "Epoch 392/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 182.9003 - mae: 183.5905 - val_loss: 10003.5938 - val_mae: 10004.2871\n",
      "Epoch 393/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 181.5553 - mae: 182.2439 - val_loss: 10368.0215 - val_mae: 10368.7148\n",
      "Epoch 394/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 187.4399 - mae: 188.1266 - val_loss: 9766.3428 - val_mae: 9767.0352\n",
      "Epoch 395/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 194.8000 - mae: 195.4882 - val_loss: 10019.1885 - val_mae: 10019.8799\n",
      "Epoch 396/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 184.9603 - mae: 185.6492 - val_loss: 9999.3955 - val_mae: 10000.0898\n",
      "Epoch 397/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 176.6085 - mae: 177.2986 - val_loss: 9484.3730 - val_mae: 9485.0645\n",
      "Epoch 398/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 206.8709 - mae: 207.5580 - val_loss: 9723.3965 - val_mae: 9724.0889\n",
      "Epoch 399/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 186.3758 - mae: 187.0648 - val_loss: 9997.4678 - val_mae: 9998.1602\n",
      "Epoch 400/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 183.2955 - mae: 183.9838 - val_loss: 9686.3418 - val_mae: 9687.0342\n",
      "Epoch 401/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 184.2806 - mae: 184.9693 - val_loss: 9846.2139 - val_mae: 9846.9053\n",
      "Epoch 402/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 187.4257 - mae: 188.1133 - val_loss: 9656.5195 - val_mae: 9657.2119\n",
      "Epoch 403/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 194.4188 - mae: 195.1077 - val_loss: 9502.2822 - val_mae: 9502.9756\n",
      "Epoch 404/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 193.9194 - mae: 194.6080 - val_loss: 9985.6787 - val_mae: 9986.3701\n",
      "Epoch 405/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 180.6510 - mae: 181.3398 - val_loss: 10095.6162 - val_mae: 10096.3086\n",
      "Epoch 406/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 172.1315 - mae: 172.8177 - val_loss: 10150.6670 - val_mae: 10151.3594\n",
      "Epoch 407/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 177.3859 - mae: 178.0755 - val_loss: 9841.0420 - val_mae: 9841.7344\n",
      "Epoch 408/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 189.9741 - mae: 190.6609 - val_loss: 9833.0381 - val_mae: 9833.7305\n",
      "Epoch 409/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 198.8982 - mae: 199.5864 - val_loss: 10064.4834 - val_mae: 10065.1748\n",
      "Epoch 410/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 177.1013 - mae: 177.7898 - val_loss: 9928.7305 - val_mae: 9929.4229\n",
      "Epoch 411/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 183.4580 - mae: 184.1466 - val_loss: 10027.0762 - val_mae: 10027.7686\n",
      "Epoch 412/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 180.1991 - mae: 180.8890 - val_loss: 9858.8516 - val_mae: 9859.5459\n",
      "Epoch 413/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 177.7908 - mae: 178.4775 - val_loss: 10015.1436 - val_mae: 10015.8359\n",
      "Epoch 414/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 189.8699 - mae: 190.5613 - val_loss: 9865.2295 - val_mae: 9865.9219\n",
      "Epoch 415/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 218.0206 - mae: 218.7102 - val_loss: 10150.7578 - val_mae: 10151.4512\n",
      "Epoch 416/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 210.6210 - mae: 211.3104 - val_loss: 9667.6553 - val_mae: 9668.3486\n",
      "Epoch 417/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 179.0601 - mae: 179.7490 - val_loss: 9733.7539 - val_mae: 9734.4463\n",
      "Epoch 418/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 186.5769 - mae: 187.2653 - val_loss: 9717.1387 - val_mae: 9717.8320\n",
      "Epoch 419/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 199.7441 - mae: 200.4329 - val_loss: 10147.4004 - val_mae: 10148.0938\n",
      "Epoch 420/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 208.2933 - mae: 208.9836 - val_loss: 9999.6338 - val_mae: 10000.3271\n",
      "Epoch 421/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 184.2196 - mae: 184.9083 - val_loss: 10129.9258 - val_mae: 10130.6182\n",
      "Epoch 422/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 160.6693 - mae: 161.3601 - val_loss: 10000.4336 - val_mae: 10001.1240\n",
      "Epoch 423/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 174.2546 - mae: 174.9459 - val_loss: 9940.4727 - val_mae: 9941.1641\n",
      "Epoch 424/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 175.0323 - mae: 175.7226 - val_loss: 9892.9473 - val_mae: 9893.6396\n",
      "Epoch 425/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 183.4351 - mae: 184.1230 - val_loss: 9970.1953 - val_mae: 9970.8877\n",
      "Epoch 426/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 180.8974 - mae: 181.5852 - val_loss: 10193.0859 - val_mae: 10193.7793\n",
      "Epoch 427/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 174.4451 - mae: 175.1342 - val_loss: 10097.4033 - val_mae: 10098.0957\n",
      "Epoch 428/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 186.2957 - mae: 186.9844 - val_loss: 10080.9287 - val_mae: 10081.6211\n",
      "Epoch 429/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 198.9290 - mae: 199.6198 - val_loss: 10156.4629 - val_mae: 10157.1562\n",
      "Epoch 430/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 169.8275 - mae: 170.5165 - val_loss: 10015.3428 - val_mae: 10016.0361\n",
      "Epoch 431/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 178.8437 - mae: 179.5320 - val_loss: 9848.1387 - val_mae: 9848.8311\n",
      "Epoch 432/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 174.3117 - mae: 175.0018 - val_loss: 9800.8457 - val_mae: 9801.5381\n",
      "Epoch 433/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 186.2067 - mae: 186.8931 - val_loss: 10141.0967 - val_mae: 10141.7881\n",
      "Epoch 434/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 170.9249 - mae: 171.6122 - val_loss: 9742.0820 - val_mae: 9742.7754\n",
      "Epoch 435/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 194.7090 - mae: 195.3989 - val_loss: 10274.0820 - val_mae: 10274.7754\n",
      "Epoch 436/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 174.0015 - mae: 174.6912 - val_loss: 10197.0469 - val_mae: 10197.7412\n",
      "Epoch 437/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 225.5583 - mae: 226.2492 - val_loss: 9950.5166 - val_mae: 9951.2100\n",
      "Epoch 438/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 188.0350 - mae: 188.7255 - val_loss: 9681.1885 - val_mae: 9681.8809\n",
      "Epoch 439/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 206.1217 - mae: 206.8102 - val_loss: 9921.8496 - val_mae: 9922.5439\n",
      "Epoch 440/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 168.9223 - mae: 169.6112 - val_loss: 9660.0244 - val_mae: 9660.7158\n",
      "Epoch 441/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 165.4820 - mae: 166.1705 - val_loss: 9911.1826 - val_mae: 9911.8760\n",
      "Epoch 442/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 176.1293 - mae: 176.8187 - val_loss: 9968.1660 - val_mae: 9968.8594\n",
      "Epoch 443/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 182.7593 - mae: 183.4495 - val_loss: 9916.6250 - val_mae: 9917.3184\n",
      "Epoch 444/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 203.3338 - mae: 204.0230 - val_loss: 9663.4355 - val_mae: 9664.1289\n",
      "Epoch 445/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 164.6256 - mae: 165.3112 - val_loss: 10143.1885 - val_mae: 10143.8828\n",
      "Epoch 446/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 194.4517 - mae: 195.1409 - val_loss: 9913.0791 - val_mae: 9913.7715\n",
      "Epoch 447/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 176.5586 - mae: 177.2467 - val_loss: 9821.9150 - val_mae: 9822.6074\n",
      "Epoch 448/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 185.4151 - mae: 186.1021 - val_loss: 10070.6123 - val_mae: 10071.3047\n",
      "Epoch 449/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 198.2706 - mae: 198.9623 - val_loss: 10196.7295 - val_mae: 10197.4219\n",
      "Epoch 450/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 183.7041 - mae: 184.3886 - val_loss: 9880.3408 - val_mae: 9881.0332\n",
      "Epoch 451/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 161.9151 - mae: 162.6057 - val_loss: 9527.3965 - val_mae: 9528.0898\n",
      "Epoch 452/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 187.1645 - mae: 187.8520 - val_loss: 10015.4482 - val_mae: 10016.1416\n",
      "Epoch 453/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 193.8993 - mae: 194.5876 - val_loss: 9948.5273 - val_mae: 9949.2197\n",
      "Epoch 454/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 176.9244 - mae: 177.6135 - val_loss: 9783.8945 - val_mae: 9784.5879\n",
      "Epoch 455/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 191.0482 - mae: 191.7377 - val_loss: 9509.0186 - val_mae: 9509.7119\n",
      "Epoch 456/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 184.0092 - mae: 184.6984 - val_loss: 10235.5986 - val_mae: 10236.2920\n",
      "Epoch 457/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 193.2639 - mae: 193.9548 - val_loss: 10038.2236 - val_mae: 10038.9170\n",
      "Epoch 458/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 173.5167 - mae: 174.2057 - val_loss: 9772.3760 - val_mae: 9773.0674\n",
      "Epoch 459/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 163.0604 - mae: 163.7489 - val_loss: 9782.8359 - val_mae: 9783.5293\n",
      "Epoch 460/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 191.8170 - mae: 192.5068 - val_loss: 10014.4844 - val_mae: 10015.1777\n",
      "Epoch 461/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 166.7778 - mae: 167.4655 - val_loss: 9989.6357 - val_mae: 9990.3291\n",
      "Epoch 462/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 223.1162 - mae: 223.8082 - val_loss: 9697.9512 - val_mae: 9698.6455\n",
      "Epoch 463/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 170.8357 - mae: 171.5251 - val_loss: 10014.7920 - val_mae: 10015.4844\n",
      "Epoch 464/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 170.4965 - mae: 171.1853 - val_loss: 9947.0557 - val_mae: 9947.7500\n",
      "Epoch 465/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 151.2975 - mae: 151.9881 - val_loss: 9871.9922 - val_mae: 9872.6865\n",
      "Epoch 466/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 180.2482 - mae: 180.9381 - val_loss: 10166.9355 - val_mae: 10167.6299\n",
      "Epoch 467/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 178.3580 - mae: 179.0474 - val_loss: 9900.4834 - val_mae: 9901.1758\n",
      "Epoch 468/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 177.3315 - mae: 178.0213 - val_loss: 9983.2822 - val_mae: 9983.9746\n",
      "Epoch 469/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 187.2670 - mae: 187.9579 - val_loss: 10049.4023 - val_mae: 10050.0947\n",
      "Epoch 470/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 176.0706 - mae: 176.7572 - val_loss: 9867.0244 - val_mae: 9867.7188\n",
      "Epoch 471/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 182.3669 - mae: 183.0572 - val_loss: 10075.8047 - val_mae: 10076.4971\n",
      "Epoch 472/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 196.5253 - mae: 197.2115 - val_loss: 9857.7910 - val_mae: 9858.4824\n",
      "Epoch 473/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 174.5056 - mae: 175.1952 - val_loss: 9853.4414 - val_mae: 9854.1357\n",
      "Epoch 474/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 163.2511 - mae: 163.9376 - val_loss: 10003.0996 - val_mae: 10003.7930\n",
      "Epoch 475/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 176.3837 - mae: 177.0731 - val_loss: 10402.6299 - val_mae: 10403.3213\n",
      "Epoch 476/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 187.1414 - mae: 187.8272 - val_loss: 10015.2930 - val_mae: 10015.9863\n",
      "Epoch 477/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 169.1640 - mae: 169.8529 - val_loss: 10290.7695 - val_mae: 10291.4639\n",
      "Epoch 478/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 184.1097 - mae: 184.7979 - val_loss: 9928.4980 - val_mae: 9929.1924\n",
      "Epoch 479/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 179.9978 - mae: 180.6895 - val_loss: 9891.9863 - val_mae: 9892.6787\n",
      "Epoch 480/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 153.6721 - mae: 154.3605 - val_loss: 10339.9561 - val_mae: 10340.6514\n",
      "Epoch 481/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 191.2505 - mae: 191.9414 - val_loss: 9781.9150 - val_mae: 9782.6084\n",
      "Epoch 482/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 159.2712 - mae: 159.9574 - val_loss: 10147.4424 - val_mae: 10148.1338\n",
      "Epoch 483/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 171.8406 - mae: 172.5269 - val_loss: 9949.3711 - val_mae: 9950.0645\n",
      "Epoch 484/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 186.6190 - mae: 187.3077 - val_loss: 9727.5547 - val_mae: 9728.2500\n",
      "Epoch 485/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 189.7993 - mae: 190.4899 - val_loss: 9600.7949 - val_mae: 9601.4883\n",
      "Epoch 486/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 182.3493 - mae: 183.0404 - val_loss: 9683.8613 - val_mae: 9684.5537\n",
      "Epoch 487/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 171.5334 - mae: 172.2238 - val_loss: 9507.5967 - val_mae: 9508.2891\n",
      "Epoch 488/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 171.5253 - mae: 172.2136 - val_loss: 9805.2939 - val_mae: 9805.9873\n",
      "Epoch 489/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 166.1634 - mae: 166.8535 - val_loss: 9947.7275 - val_mae: 9948.4209\n",
      "Epoch 490/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 171.4761 - mae: 172.1653 - val_loss: 9657.0039 - val_mae: 9657.6963\n",
      "Epoch 491/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 189.6645 - mae: 190.3532 - val_loss: 9933.1162 - val_mae: 9933.8076\n",
      "Epoch 492/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 166.7558 - mae: 167.4439 - val_loss: 10058.2363 - val_mae: 10058.9277\n",
      "Epoch 493/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 160.6769 - mae: 161.3637 - val_loss: 9984.1104 - val_mae: 9984.8018\n",
      "Epoch 494/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 173.7059 - mae: 174.3967 - val_loss: 9715.6689 - val_mae: 9716.3633\n",
      "Epoch 495/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 175.1158 - mae: 175.8035 - val_loss: 10114.1836 - val_mae: 10114.8770\n",
      "Epoch 496/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 182.7809 - mae: 183.4677 - val_loss: 9878.9834 - val_mae: 9879.6758\n",
      "Epoch 497/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 181.5873 - mae: 182.2762 - val_loss: 9794.0342 - val_mae: 9794.7285\n",
      "Epoch 498/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 186.2497 - mae: 186.9387 - val_loss: 9973.8213 - val_mae: 9974.5146\n",
      "Epoch 499/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 164.1470 - mae: 164.8366 - val_loss: 9889.2236 - val_mae: 9889.9170\n",
      "Epoch 500/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 167.1274 - mae: 167.8155 - val_loss: 10035.3643 - val_mae: 10036.0586\n",
      "Epoch 501/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 168.9182 - mae: 169.6075 - val_loss: 9860.7275 - val_mae: 9861.4219\n",
      "Epoch 502/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 174.6085 - mae: 175.2960 - val_loss: 9885.5566 - val_mae: 9886.2500\n",
      "Epoch 503/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 211.9220 - mae: 212.6087 - val_loss: 9941.2295 - val_mae: 9941.9229\n",
      "Epoch 504/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 170.4789 - mae: 171.1669 - val_loss: 10068.5107 - val_mae: 10069.2031\n",
      "Epoch 505/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 175.6002 - mae: 176.2913 - val_loss: 10189.2314 - val_mae: 10189.9258\n",
      "Epoch 506/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 157.9774 - mae: 158.6667 - val_loss: 10053.0664 - val_mae: 10053.7588\n",
      "Epoch 507/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 182.3042 - mae: 182.9937 - val_loss: 9729.2520 - val_mae: 9729.9453\n",
      "Epoch 508/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 185.7727 - mae: 186.4622 - val_loss: 9949.1719 - val_mae: 9949.8643\n",
      "Epoch 509/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 163.2830 - mae: 163.9723 - val_loss: 9783.5654 - val_mae: 9784.2578\n",
      "Epoch 510/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 171.3739 - mae: 172.0651 - val_loss: 9803.7041 - val_mae: 9804.3975\n",
      "Epoch 511/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 159.2253 - mae: 159.9104 - val_loss: 9887.6133 - val_mae: 9888.3076\n",
      "Epoch 512/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 171.6821 - mae: 172.3700 - val_loss: 9685.2588 - val_mae: 9685.9502\n",
      "Epoch 513/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 163.7548 - mae: 164.4455 - val_loss: 10061.1504 - val_mae: 10061.8428\n",
      "Epoch 514/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 205.8554 - mae: 206.5449 - val_loss: 9923.0469 - val_mae: 9923.7393\n",
      "Epoch 515/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 153.2913 - mae: 153.9784 - val_loss: 9966.8193 - val_mae: 9967.5127\n",
      "Epoch 516/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 155.8779 - mae: 156.5671 - val_loss: 9590.8125 - val_mae: 9591.5059\n",
      "Epoch 517/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 197.3115 - mae: 198.0008 - val_loss: 9640.5635 - val_mae: 9641.2559\n",
      "Epoch 518/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 177.4356 - mae: 178.1208 - val_loss: 9738.1602 - val_mae: 9738.8525\n",
      "Epoch 519/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 192.3973 - mae: 193.0882 - val_loss: 9978.8750 - val_mae: 9979.5703\n",
      "Epoch 520/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 164.1957 - mae: 164.8845 - val_loss: 10087.4785 - val_mae: 10088.1709\n",
      "Epoch 521/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 163.4858 - mae: 164.1766 - val_loss: 9744.3271 - val_mae: 9745.0195\n",
      "Epoch 522/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 167.2863 - mae: 167.9749 - val_loss: 9704.4756 - val_mae: 9705.1680\n",
      "Epoch 523/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 177.7850 - mae: 178.4753 - val_loss: 9980.1641 - val_mae: 9980.8594\n",
      "Epoch 524/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 194.3126 - mae: 195.0014 - val_loss: 9951.5801 - val_mae: 9952.2725\n",
      "Epoch 525/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 167.8276 - mae: 168.5151 - val_loss: 9624.4775 - val_mae: 9625.1709\n",
      "Epoch 526/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 164.2410 - mae: 164.9290 - val_loss: 9913.3125 - val_mae: 9914.0068\n",
      "Epoch 527/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 170.9517 - mae: 171.6437 - val_loss: 9763.5186 - val_mae: 9764.2109\n",
      "Epoch 528/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 157.3429 - mae: 158.0333 - val_loss: 9530.6260 - val_mae: 9531.3184\n",
      "Epoch 529/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 146.5301 - mae: 147.2179 - val_loss: 9925.3896 - val_mae: 9926.0840\n",
      "Epoch 530/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 160.8613 - mae: 161.5507 - val_loss: 9906.7080 - val_mae: 9907.4004\n",
      "Epoch 531/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 158.9856 - mae: 159.6727 - val_loss: 9847.4932 - val_mae: 9848.1855\n",
      "Epoch 532/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 155.2611 - mae: 155.9461 - val_loss: 10022.3936 - val_mae: 10023.0869\n",
      "Epoch 533/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 174.2231 - mae: 174.9133 - val_loss: 10112.0146 - val_mae: 10112.7070\n",
      "Epoch 534/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 176.0965 - mae: 176.7849 - val_loss: 10035.0479 - val_mae: 10035.7383\n",
      "Epoch 535/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 157.1286 - mae: 157.8170 - val_loss: 9810.9932 - val_mae: 9811.6865\n",
      "Epoch 536/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 166.5468 - mae: 167.2348 - val_loss: 9968.0381 - val_mae: 9968.7314\n",
      "Epoch 537/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 183.9997 - mae: 184.6874 - val_loss: 9974.2432 - val_mae: 9974.9355\n",
      "Epoch 538/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 157.2210 - mae: 157.9099 - val_loss: 10262.1445 - val_mae: 10262.8369\n",
      "Epoch 539/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 171.7909 - mae: 172.4780 - val_loss: 10045.1348 - val_mae: 10045.8281\n",
      "Epoch 540/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 163.4777 - mae: 164.1687 - val_loss: 10039.2119 - val_mae: 10039.9053\n",
      "Epoch 541/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 171.3448 - mae: 172.0325 - val_loss: 9776.8555 - val_mae: 9777.5488\n",
      "Epoch 542/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 163.9447 - mae: 164.6320 - val_loss: 9939.4512 - val_mae: 9940.1455\n",
      "Epoch 543/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 184.9305 - mae: 185.6194 - val_loss: 10033.0010 - val_mae: 10033.6924\n",
      "Epoch 544/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 153.5682 - mae: 154.2565 - val_loss: 10007.9209 - val_mae: 10008.6143\n",
      "Epoch 545/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 161.7892 - mae: 162.4769 - val_loss: 10309.6973 - val_mae: 10310.3906\n",
      "Epoch 546/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 163.2834 - mae: 163.9695 - val_loss: 10100.2744 - val_mae: 10100.9678\n",
      "Epoch 547/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 159.8443 - mae: 160.5337 - val_loss: 10009.4248 - val_mae: 10010.1182\n",
      "Epoch 548/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 168.3601 - mae: 169.0487 - val_loss: 10107.7246 - val_mae: 10108.4189\n",
      "Epoch 549/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 169.7228 - mae: 170.4137 - val_loss: 10122.0781 - val_mae: 10122.7715\n",
      "Epoch 550/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 164.5358 - mae: 165.2252 - val_loss: 9800.7861 - val_mae: 9801.4785\n",
      "Epoch 551/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 168.1160 - mae: 168.8039 - val_loss: 9717.5488 - val_mae: 9718.2422\n",
      "Epoch 552/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 180.6920 - mae: 181.3838 - val_loss: 9950.7256 - val_mae: 9951.4180\n",
      "Epoch 553/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 173.1320 - mae: 173.8220 - val_loss: 9938.2559 - val_mae: 9938.9482\n",
      "Epoch 554/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 155.8082 - mae: 156.4966 - val_loss: 9883.9043 - val_mae: 9884.5977\n",
      "Epoch 555/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 175.9935 - mae: 176.6801 - val_loss: 10052.9785 - val_mae: 10053.6729\n",
      "Epoch 556/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 160.6494 - mae: 161.3402 - val_loss: 9822.7178 - val_mae: 9823.4121\n",
      "Epoch 557/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 152.3914 - mae: 153.0766 - val_loss: 9942.8301 - val_mae: 9943.5234\n",
      "Epoch 558/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 151.1957 - mae: 151.8838 - val_loss: 9929.6338 - val_mae: 9930.3252\n",
      "Epoch 559/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 145.8751 - mae: 146.5601 - val_loss: 10153.4863 - val_mae: 10154.1777\n",
      "Epoch 560/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 154.2926 - mae: 154.9810 - val_loss: 9858.9453 - val_mae: 9859.6387\n",
      "Epoch 561/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 148.3899 - mae: 149.0761 - val_loss: 10083.9736 - val_mae: 10084.6670\n",
      "Epoch 562/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 159.9269 - mae: 160.6134 - val_loss: 9998.1455 - val_mae: 9998.8369\n",
      "Epoch 563/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 147.1347 - mae: 147.8221 - val_loss: 10155.1846 - val_mae: 10155.8779\n",
      "Epoch 564/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 146.3676 - mae: 147.0542 - val_loss: 9740.0693 - val_mae: 9740.7617\n",
      "Epoch 565/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 156.7269 - mae: 157.4160 - val_loss: 9744.5479 - val_mae: 9745.2412\n",
      "Epoch 566/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 160.6983 - mae: 161.3899 - val_loss: 9899.7021 - val_mae: 9900.3936\n",
      "Epoch 567/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 161.7039 - mae: 162.3939 - val_loss: 9894.7812 - val_mae: 9895.4746\n",
      "Epoch 568/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 161.6056 - mae: 162.2914 - val_loss: 10074.6436 - val_mae: 10075.3350\n",
      "Epoch 569/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 171.4502 - mae: 172.1389 - val_loss: 10258.5166 - val_mae: 10259.2109\n",
      "Epoch 570/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 177.6341 - mae: 178.3216 - val_loss: 10012.8242 - val_mae: 10013.5156\n",
      "Epoch 571/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 154.5742 - mae: 155.2609 - val_loss: 9877.8154 - val_mae: 9878.5088\n",
      "Epoch 572/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 162.8088 - mae: 163.4975 - val_loss: 9824.3281 - val_mae: 9825.0215\n",
      "Epoch 573/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 160.9302 - mae: 161.6209 - val_loss: 10071.2188 - val_mae: 10071.9111\n",
      "Epoch 574/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 145.8391 - mae: 146.5291 - val_loss: 10232.9092 - val_mae: 10233.6045\n",
      "Epoch 575/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 161.9574 - mae: 162.6463 - val_loss: 10046.7568 - val_mae: 10047.4502\n",
      "Epoch 576/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 148.3207 - mae: 149.0089 - val_loss: 9925.8535 - val_mae: 9926.5479\n",
      "Epoch 577/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 166.2179 - mae: 166.9068 - val_loss: 10098.6162 - val_mae: 10099.3086\n",
      "Epoch 578/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 153.6196 - mae: 154.3076 - val_loss: 9789.4590 - val_mae: 9790.1514\n",
      "Epoch 579/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 184.4630 - mae: 185.1514 - val_loss: 9913.2773 - val_mae: 9913.9688\n",
      "Epoch 580/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 166.2620 - mae: 166.9516 - val_loss: 9615.8203 - val_mae: 9616.5127\n",
      "Epoch 581/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 165.8436 - mae: 166.5295 - val_loss: 10267.5059 - val_mae: 10268.2002\n",
      "Epoch 582/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 188.6940 - mae: 189.3836 - val_loss: 9914.2549 - val_mae: 9914.9482\n",
      "Epoch 583/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 155.0134 - mae: 155.7025 - val_loss: 10143.1270 - val_mae: 10143.8203\n",
      "Epoch 584/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 156.3425 - mae: 157.0299 - val_loss: 9990.8486 - val_mae: 9991.5430\n",
      "Epoch 585/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 160.4718 - mae: 161.1581 - val_loss: 10152.4912 - val_mae: 10153.1846\n",
      "Epoch 586/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 153.9714 - mae: 154.6579 - val_loss: 9920.7227 - val_mae: 9921.4150\n",
      "Epoch 587/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 152.8913 - mae: 153.5811 - val_loss: 10116.0107 - val_mae: 10116.7031\n",
      "Epoch 588/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 163.3788 - mae: 164.0672 - val_loss: 9836.6016 - val_mae: 9837.2949\n",
      "Epoch 589/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 177.7283 - mae: 178.4177 - val_loss: 9974.0264 - val_mae: 9974.7188\n",
      "Epoch 590/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 155.5146 - mae: 156.2024 - val_loss: 10117.8389 - val_mae: 10118.5303\n",
      "Epoch 591/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 159.1160 - mae: 159.8048 - val_loss: 10116.3906 - val_mae: 10117.0850\n",
      "Epoch 592/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 152.1820 - mae: 152.8680 - val_loss: 10047.0361 - val_mae: 10047.7285\n",
      "Epoch 593/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 162.4367 - mae: 163.1261 - val_loss: 10021.6533 - val_mae: 10022.3486\n",
      "Epoch 594/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 151.4671 - mae: 152.1560 - val_loss: 10076.0068 - val_mae: 10076.7002\n",
      "Epoch 595/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 151.1670 - mae: 151.8554 - val_loss: 9943.4746 - val_mae: 9944.1689\n",
      "Epoch 596/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 158.9117 - mae: 159.6006 - val_loss: 9976.3652 - val_mae: 9977.0576\n",
      "Epoch 597/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 159.7102 - mae: 160.3953 - val_loss: 9836.4434 - val_mae: 9837.1377\n",
      "Epoch 598/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 162.7830 - mae: 163.4686 - val_loss: 10038.4785 - val_mae: 10039.1719\n",
      "Epoch 599/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 170.0403 - mae: 170.7297 - val_loss: 10234.9355 - val_mae: 10235.6279\n",
      "Epoch 600/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 189.3533 - mae: 190.0446 - val_loss: 10240.5000 - val_mae: 10241.1924\n",
      "Epoch 601/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 155.7322 - mae: 156.4213 - val_loss: 9939.0117 - val_mae: 9939.7061\n",
      "Epoch 602/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 162.4702 - mae: 163.1588 - val_loss: 9984.4746 - val_mae: 9985.1670\n",
      "Epoch 603/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 152.0526 - mae: 152.7401 - val_loss: 9916.5547 - val_mae: 9917.2480\n",
      "Epoch 604/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 147.3817 - mae: 148.0703 - val_loss: 9661.9463 - val_mae: 9662.6396\n",
      "Epoch 605/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 157.8155 - mae: 158.5071 - val_loss: 9854.4785 - val_mae: 9855.1729\n",
      "Epoch 606/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 152.5928 - mae: 153.2831 - val_loss: 9924.8203 - val_mae: 9925.5127\n",
      "Epoch 607/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 168.5608 - mae: 169.2499 - val_loss: 9952.4961 - val_mae: 9953.1885\n",
      "Epoch 608/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 158.8725 - mae: 159.5621 - val_loss: 9815.3730 - val_mae: 9816.0674\n",
      "Epoch 609/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 160.3640 - mae: 161.0526 - val_loss: 9949.5488 - val_mae: 9950.2422\n",
      "Epoch 610/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 157.2034 - mae: 157.8905 - val_loss: 9928.8574 - val_mae: 9929.5508\n",
      "Epoch 611/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 148.3207 - mae: 149.0083 - val_loss: 9981.1250 - val_mae: 9981.8174\n",
      "Epoch 612/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 150.5915 - mae: 151.2788 - val_loss: 10063.4326 - val_mae: 10064.1270\n",
      "Epoch 613/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 141.9378 - mae: 142.6253 - val_loss: 10156.3613 - val_mae: 10157.0537\n",
      "Epoch 614/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 149.0405 - mae: 149.7297 - val_loss: 9929.5039 - val_mae: 9930.1953\n",
      "Epoch 615/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 159.7646 - mae: 160.4526 - val_loss: 10075.5400 - val_mae: 10076.2324\n",
      "Epoch 616/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 160.6383 - mae: 161.3263 - val_loss: 9949.3145 - val_mae: 9950.0078\n",
      "Epoch 617/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 143.6975 - mae: 144.3848 - val_loss: 10136.0439 - val_mae: 10136.7373\n",
      "Epoch 618/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 153.7209 - mae: 154.4090 - val_loss: 9784.6074 - val_mae: 9785.2998\n",
      "Epoch 619/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 156.9876 - mae: 157.6770 - val_loss: 9957.5850 - val_mae: 9958.2783\n",
      "Epoch 620/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 153.1818 - mae: 153.8722 - val_loss: 9965.8799 - val_mae: 9966.5732\n",
      "Epoch 621/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 130.8367 - mae: 131.5239 - val_loss: 9851.7266 - val_mae: 9852.4189\n",
      "Epoch 622/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 160.1202 - mae: 160.8104 - val_loss: 9990.1221 - val_mae: 9990.8145\n",
      "Epoch 623/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 143.4377 - mae: 144.1265 - val_loss: 10073.3281 - val_mae: 10074.0205\n",
      "Epoch 624/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 166.9264 - mae: 167.6156 - val_loss: 9713.5703 - val_mae: 9714.2627\n",
      "Epoch 625/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 160.9630 - mae: 161.6484 - val_loss: 10120.2646 - val_mae: 10120.9570\n",
      "Epoch 626/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 156.2947 - mae: 156.9796 - val_loss: 10305.9688 - val_mae: 10306.6621\n",
      "Epoch 627/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 160.9196 - mae: 161.6093 - val_loss: 9656.3418 - val_mae: 9657.0361\n",
      "Epoch 628/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 159.3080 - mae: 159.9963 - val_loss: 10039.3291 - val_mae: 10040.0215\n",
      "Epoch 629/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 140.7849 - mae: 141.4723 - val_loss: 9972.5322 - val_mae: 9973.2246\n",
      "Epoch 630/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 173.1280 - mae: 173.8157 - val_loss: 9848.6357 - val_mae: 9849.3291\n",
      "Epoch 631/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 153.1052 - mae: 153.7928 - val_loss: 9869.6797 - val_mae: 9870.3750\n",
      "Epoch 632/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 152.6215 - mae: 153.3067 - val_loss: 10199.1865 - val_mae: 10199.8799\n",
      "Epoch 633/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 160.1489 - mae: 160.8376 - val_loss: 10092.0879 - val_mae: 10092.7812\n",
      "Epoch 634/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 146.5540 - mae: 147.2434 - val_loss: 10195.1025 - val_mae: 10195.7949\n",
      "Epoch 635/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 146.2731 - mae: 146.9621 - val_loss: 10026.1416 - val_mae: 10026.8350\n",
      "Epoch 636/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 186.5001 - mae: 187.1888 - val_loss: 9986.2959 - val_mae: 9986.9873\n",
      "Epoch 637/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 145.5566 - mae: 146.2428 - val_loss: 9931.8984 - val_mae: 9932.5898\n",
      "Epoch 638/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 149.7585 - mae: 150.4447 - val_loss: 10035.1680 - val_mae: 10035.8604\n",
      "Epoch 639/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 145.7215 - mae: 146.4083 - val_loss: 10077.4971 - val_mae: 10078.1895\n",
      "Epoch 640/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 152.8642 - mae: 153.5516 - val_loss: 10009.3301 - val_mae: 10010.0234\n",
      "Epoch 641/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 162.6423 - mae: 163.3314 - val_loss: 10207.2568 - val_mae: 10207.9502\n",
      "Epoch 642/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 156.4667 - mae: 157.1560 - val_loss: 10228.5625 - val_mae: 10229.2559\n",
      "Epoch 643/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 209.7441 - mae: 210.4312 - val_loss: 9668.1865 - val_mae: 9668.8789\n",
      "Epoch 644/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 159.9174 - mae: 160.6070 - val_loss: 9864.1924 - val_mae: 9864.8848\n",
      "Epoch 645/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 154.0646 - mae: 154.7544 - val_loss: 10063.3125 - val_mae: 10064.0059\n",
      "Epoch 646/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 169.9696 - mae: 170.6553 - val_loss: 9976.9863 - val_mae: 9977.6797\n",
      "Epoch 647/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 156.6502 - mae: 157.3409 - val_loss: 10335.4785 - val_mae: 10336.1719\n",
      "Epoch 648/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 162.2881 - mae: 162.9765 - val_loss: 10245.4277 - val_mae: 10246.1191\n",
      "Epoch 649/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 155.5916 - mae: 156.2800 - val_loss: 9917.8516 - val_mae: 9918.5439\n",
      "Epoch 650/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 145.0435 - mae: 145.7281 - val_loss: 9652.9619 - val_mae: 9653.6553\n",
      "Epoch 651/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 168.3265 - mae: 169.0153 - val_loss: 10217.6807 - val_mae: 10218.3740\n",
      "Epoch 652/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 173.3116 - mae: 173.9996 - val_loss: 10112.3623 - val_mae: 10113.0547\n",
      "Epoch 653/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 164.5473 - mae: 165.2377 - val_loss: 9940.4658 - val_mae: 9941.1582\n",
      "Epoch 654/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 144.8268 - mae: 145.5122 - val_loss: 10267.4297 - val_mae: 10268.1211\n",
      "Epoch 655/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 173.9348 - mae: 174.6236 - val_loss: 10314.1504 - val_mae: 10314.8447\n",
      "Epoch 656/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 189.1978 - mae: 189.8871 - val_loss: 9730.3662 - val_mae: 9731.0576\n",
      "Epoch 657/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 151.5162 - mae: 152.2051 - val_loss: 9972.2480 - val_mae: 9972.9404\n",
      "Epoch 658/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 135.8237 - mae: 136.5105 - val_loss: 9988.7441 - val_mae: 9989.4385\n",
      "Epoch 659/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 143.6941 - mae: 144.3824 - val_loss: 10152.2529 - val_mae: 10152.9482\n",
      "Epoch 660/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 146.1659 - mae: 146.8563 - val_loss: 10160.7656 - val_mae: 10161.4570\n",
      "Epoch 661/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 150.7003 - mae: 151.3872 - val_loss: 10128.1475 - val_mae: 10128.8398\n",
      "Epoch 662/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 166.0833 - mae: 166.7714 - val_loss: 10302.1973 - val_mae: 10302.8906\n",
      "Epoch 663/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 153.7291 - mae: 154.4187 - val_loss: 10069.2129 - val_mae: 10069.9043\n",
      "Epoch 664/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 142.7637 - mae: 143.4513 - val_loss: 10127.1895 - val_mae: 10127.8818\n",
      "Epoch 665/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 147.4734 - mae: 148.1610 - val_loss: 9873.5469 - val_mae: 9874.2402\n",
      "Epoch 666/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 162.2440 - mae: 162.9337 - val_loss: 9791.3125 - val_mae: 9792.0049\n",
      "Epoch 667/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 144.0731 - mae: 144.7625 - val_loss: 10013.5469 - val_mae: 10014.2402\n",
      "Epoch 668/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 151.0173 - mae: 151.7037 - val_loss: 10068.6562 - val_mae: 10069.3486\n",
      "Epoch 669/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 140.4315 - mae: 141.1188 - val_loss: 10403.0293 - val_mae: 10403.7217\n",
      "Epoch 670/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 161.6582 - mae: 162.3465 - val_loss: 9851.9482 - val_mae: 9852.6406\n",
      "Epoch 671/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 151.7909 - mae: 152.4794 - val_loss: 9989.1973 - val_mae: 9989.8896\n",
      "Epoch 672/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 158.0231 - mae: 158.7090 - val_loss: 9936.1309 - val_mae: 9936.8232\n",
      "Epoch 673/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 148.9218 - mae: 149.6048 - val_loss: 9958.2969 - val_mae: 9958.9902\n",
      "Epoch 674/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 147.6714 - mae: 148.3608 - val_loss: 10155.4141 - val_mae: 10156.1064\n",
      "Epoch 675/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 152.4626 - mae: 153.1501 - val_loss: 10039.8203 - val_mae: 10040.5137\n",
      "Epoch 676/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 131.6445 - mae: 132.3329 - val_loss: 9879.0430 - val_mae: 9879.7354\n",
      "Epoch 677/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 157.4891 - mae: 158.1798 - val_loss: 9810.7812 - val_mae: 9811.4746\n",
      "Epoch 678/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 148.8929 - mae: 149.5825 - val_loss: 9910.1572 - val_mae: 9910.8516\n",
      "Epoch 679/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 160.6304 - mae: 161.3185 - val_loss: 9997.2246 - val_mae: 9997.9170\n",
      "Epoch 680/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 144.2325 - mae: 144.9218 - val_loss: 9870.9961 - val_mae: 9871.6895\n",
      "Epoch 681/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 154.0483 - mae: 154.7373 - val_loss: 10303.1836 - val_mae: 10303.8750\n",
      "Epoch 682/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 158.2716 - mae: 158.9581 - val_loss: 9791.4727 - val_mae: 9792.1670\n",
      "Epoch 683/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 156.5568 - mae: 157.2469 - val_loss: 10002.0732 - val_mae: 10002.7676\n",
      "Epoch 684/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 151.3744 - mae: 152.0617 - val_loss: 9960.0693 - val_mae: 9960.7627\n",
      "Epoch 685/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 163.1672 - mae: 163.8546 - val_loss: 9731.4707 - val_mae: 9732.1631\n",
      "Epoch 686/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 161.8683 - mae: 162.5548 - val_loss: 9840.3271 - val_mae: 9841.0195\n",
      "Epoch 687/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 161.3830 - mae: 162.0698 - val_loss: 9790.2080 - val_mae: 9790.9004\n",
      "Epoch 688/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 145.0078 - mae: 145.6981 - val_loss: 9838.6816 - val_mae: 9839.3730\n",
      "Epoch 689/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 137.5585 - mae: 138.2460 - val_loss: 9853.3955 - val_mae: 9854.0879\n",
      "Epoch 690/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 157.5994 - mae: 158.2852 - val_loss: 9943.3730 - val_mae: 9944.0674\n",
      "Epoch 691/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 174.6961 - mae: 175.3865 - val_loss: 9923.1855 - val_mae: 9923.8779\n",
      "Epoch 692/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 155.9021 - mae: 156.5911 - val_loss: 10077.3330 - val_mae: 10078.0264\n",
      "Epoch 693/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 170.3407 - mae: 171.0297 - val_loss: 9772.0234 - val_mae: 9772.7188\n",
      "Epoch 694/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 139.3975 - mae: 140.0844 - val_loss: 9587.6650 - val_mae: 9588.3584\n",
      "Epoch 695/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 169.4622 - mae: 170.1530 - val_loss: 10255.8496 - val_mae: 10256.5430\n",
      "Epoch 696/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 144.3452 - mae: 145.0334 - val_loss: 10157.5889 - val_mae: 10158.2822\n",
      "Epoch 697/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 144.2471 - mae: 144.9346 - val_loss: 10119.8398 - val_mae: 10120.5322\n",
      "Epoch 698/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 139.2423 - mae: 139.9274 - val_loss: 9540.5381 - val_mae: 9541.2305\n",
      "Epoch 699/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 181.3961 - mae: 182.0850 - val_loss: 10122.0459 - val_mae: 10122.7393\n",
      "Epoch 700/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 169.5125 - mae: 170.2039 - val_loss: 10380.4912 - val_mae: 10381.1836\n",
      "Epoch 701/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 166.7002 - mae: 167.3903 - val_loss: 9797.2041 - val_mae: 9797.8965\n",
      "Epoch 702/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 142.1750 - mae: 142.8624 - val_loss: 9677.6357 - val_mae: 9678.3301\n",
      "Epoch 703/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 161.5929 - mae: 162.2809 - val_loss: 10005.7402 - val_mae: 10006.4336\n",
      "Epoch 704/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 143.4539 - mae: 144.1414 - val_loss: 9925.3545 - val_mae: 9926.0498\n",
      "Epoch 705/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 151.8015 - mae: 152.4900 - val_loss: 9769.9980 - val_mae: 9770.6924\n",
      "Epoch 706/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 150.5709 - mae: 151.2564 - val_loss: 10060.2979 - val_mae: 10060.9912\n",
      "Epoch 707/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 137.0776 - mae: 137.7660 - val_loss: 9985.7764 - val_mae: 9986.4688\n",
      "Epoch 708/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 137.4647 - mae: 138.1532 - val_loss: 10024.2520 - val_mae: 10024.9443\n",
      "Epoch 709/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 134.7522 - mae: 135.4415 - val_loss: 10162.1934 - val_mae: 10162.8867\n",
      "Epoch 710/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 142.0167 - mae: 142.7041 - val_loss: 9686.9141 - val_mae: 9687.6074\n",
      "Epoch 711/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 149.1987 - mae: 149.8848 - val_loss: 9871.5742 - val_mae: 9872.2666\n",
      "Epoch 712/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 147.8557 - mae: 148.5402 - val_loss: 10049.1084 - val_mae: 10049.8008\n",
      "Epoch 713/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 159.5067 - mae: 160.1944 - val_loss: 9815.9531 - val_mae: 9816.6465\n",
      "Epoch 714/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 162.4693 - mae: 163.1610 - val_loss: 10055.4844 - val_mae: 10056.1768\n",
      "Epoch 715/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 147.5514 - mae: 148.2367 - val_loss: 9860.6406 - val_mae: 9861.3340\n",
      "Epoch 716/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 148.6532 - mae: 149.3385 - val_loss: 10012.0020 - val_mae: 10012.6953\n",
      "Epoch 717/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 147.9108 - mae: 148.6005 - val_loss: 10210.1807 - val_mae: 10210.8750\n",
      "Epoch 718/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 151.9793 - mae: 152.6704 - val_loss: 9948.5898 - val_mae: 9949.2812\n",
      "Epoch 719/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 143.9627 - mae: 144.6530 - val_loss: 9760.0000 - val_mae: 9760.6943\n",
      "Epoch 720/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 148.0007 - mae: 148.6899 - val_loss: 10019.8418 - val_mae: 10020.5332\n",
      "Epoch 721/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 138.6543 - mae: 139.3422 - val_loss: 10269.6953 - val_mae: 10270.3877\n",
      "Epoch 722/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 141.6124 - mae: 142.2983 - val_loss: 10063.3623 - val_mae: 10064.0557\n",
      "Epoch 723/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 151.5767 - mae: 152.2637 - val_loss: 9968.5107 - val_mae: 9969.2041\n",
      "Epoch 724/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 130.3330 - mae: 131.0228 - val_loss: 10101.8770 - val_mae: 10102.5713\n",
      "Epoch 725/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 156.6970 - mae: 157.3866 - val_loss: 9982.4600 - val_mae: 9983.1533\n",
      "Epoch 726/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 162.4052 - mae: 163.0942 - val_loss: 9841.4209 - val_mae: 9842.1143\n",
      "Epoch 727/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 139.0260 - mae: 139.7146 - val_loss: 9837.7773 - val_mae: 9838.4697\n",
      "Epoch 728/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 140.7190 - mae: 141.4086 - val_loss: 9939.6133 - val_mae: 9940.3066\n",
      "Epoch 729/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 149.9944 - mae: 150.6815 - val_loss: 10053.0615 - val_mae: 10053.7549\n",
      "Epoch 730/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 137.8332 - mae: 138.5216 - val_loss: 10126.7812 - val_mae: 10127.4736\n",
      "Epoch 731/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 172.3008 - mae: 172.9912 - val_loss: 10054.6689 - val_mae: 10055.3623\n",
      "Epoch 732/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 151.6170 - mae: 152.3058 - val_loss: 9839.9648 - val_mae: 9840.6572\n",
      "Epoch 733/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 167.9206 - mae: 168.6110 - val_loss: 9886.4512 - val_mae: 9887.1465\n",
      "Epoch 734/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 153.4615 - mae: 154.1495 - val_loss: 9625.1777 - val_mae: 9625.8711\n",
      "Epoch 735/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 141.5945 - mae: 142.2830 - val_loss: 9946.4531 - val_mae: 9947.1484\n",
      "Epoch 736/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 147.4846 - mae: 148.1728 - val_loss: 9616.7256 - val_mae: 9617.4180\n",
      "Epoch 737/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 141.9744 - mae: 142.6636 - val_loss: 10075.4912 - val_mae: 10076.1846\n",
      "Epoch 738/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 140.9548 - mae: 141.6422 - val_loss: 10024.9287 - val_mae: 10025.6201\n",
      "Epoch 739/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 148.2375 - mae: 148.9257 - val_loss: 10116.9121 - val_mae: 10117.6045\n",
      "Epoch 740/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 132.2605 - mae: 132.9449 - val_loss: 10174.8555 - val_mae: 10175.5498\n",
      "Epoch 741/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 142.3849 - mae: 143.0711 - val_loss: 10011.3877 - val_mae: 10012.0801\n",
      "Epoch 742/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 155.7772 - mae: 156.4666 - val_loss: 9793.0957 - val_mae: 9793.7881\n",
      "Epoch 743/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 158.4316 - mae: 159.1202 - val_loss: 9991.0547 - val_mae: 9991.7500\n",
      "Epoch 744/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 136.0779 - mae: 136.7652 - val_loss: 9910.7021 - val_mae: 9911.3965\n",
      "Epoch 745/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 134.4670 - mae: 135.1545 - val_loss: 9719.7539 - val_mae: 9720.4473\n",
      "Epoch 746/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 155.6890 - mae: 156.3775 - val_loss: 10032.1436 - val_mae: 10032.8369\n",
      "Epoch 747/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 142.3267 - mae: 143.0165 - val_loss: 9931.7188 - val_mae: 9932.4111\n",
      "Epoch 748/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 146.3863 - mae: 147.0757 - val_loss: 9986.7041 - val_mae: 9987.3975\n",
      "Epoch 749/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 139.3474 - mae: 140.0371 - val_loss: 10013.2686 - val_mae: 10013.9619\n",
      "Epoch 750/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 139.6800 - mae: 140.3684 - val_loss: 9933.4531 - val_mae: 9934.1475\n",
      "Epoch 751/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 165.3390 - mae: 166.0286 - val_loss: 9768.7842 - val_mae: 9769.4775\n",
      "Epoch 752/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 142.6124 - mae: 143.3023 - val_loss: 9966.8955 - val_mae: 9967.5879\n",
      "Epoch 753/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 129.4630 - mae: 130.1474 - val_loss: 9970.4639 - val_mae: 9971.1572\n",
      "Epoch 754/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 145.8726 - mae: 146.5596 - val_loss: 9629.9941 - val_mae: 9630.6865\n",
      "Epoch 755/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 132.5475 - mae: 133.2306 - val_loss: 9563.3730 - val_mae: 9564.0664\n",
      "Epoch 756/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 158.5905 - mae: 159.2804 - val_loss: 9707.3701 - val_mae: 9708.0625\n",
      "Epoch 757/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 169.1281 - mae: 169.8160 - val_loss: 9750.3633 - val_mae: 9751.0586\n",
      "Epoch 758/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 145.1140 - mae: 145.8033 - val_loss: 9886.6094 - val_mae: 9887.3037\n",
      "Epoch 759/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 156.9887 - mae: 157.6764 - val_loss: 10094.4541 - val_mae: 10095.1475\n",
      "Epoch 760/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 145.2828 - mae: 145.9711 - val_loss: 10085.9502 - val_mae: 10086.6406\n",
      "Epoch 761/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 137.7962 - mae: 138.4834 - val_loss: 10038.5967 - val_mae: 10039.2900\n",
      "Epoch 762/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 126.0076 - mae: 126.6905 - val_loss: 10088.8271 - val_mae: 10089.5205\n",
      "Epoch 763/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 127.8887 - mae: 128.5750 - val_loss: 10043.1699 - val_mae: 10043.8623\n",
      "Epoch 764/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 141.9177 - mae: 142.6073 - val_loss: 9592.5977 - val_mae: 9593.2891\n",
      "Epoch 765/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 147.8032 - mae: 148.4925 - val_loss: 9808.2305 - val_mae: 9808.9229\n",
      "Epoch 766/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 179.2646 - mae: 179.9561 - val_loss: 10015.9844 - val_mae: 10016.6768\n",
      "Epoch 767/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 149.9131 - mae: 150.5979 - val_loss: 9871.9512 - val_mae: 9872.6445\n",
      "Epoch 768/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 143.2973 - mae: 143.9857 - val_loss: 9933.8701 - val_mae: 9934.5625\n",
      "Epoch 769/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 125.5366 - mae: 126.2226 - val_loss: 9884.3682 - val_mae: 9885.0625\n",
      "Epoch 770/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 141.2280 - mae: 141.9156 - val_loss: 9958.3037 - val_mae: 9958.9961\n",
      "Epoch 771/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 149.1018 - mae: 149.7876 - val_loss: 9833.0547 - val_mae: 9833.7480\n",
      "Epoch 772/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 128.8981 - mae: 129.5875 - val_loss: 9931.6260 - val_mae: 9932.3193\n",
      "Epoch 773/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 151.1308 - mae: 151.8194 - val_loss: 10144.4062 - val_mae: 10145.0996\n",
      "Epoch 774/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 137.1943 - mae: 137.8843 - val_loss: 9759.2539 - val_mae: 9759.9492\n",
      "Epoch 775/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 142.5341 - mae: 143.2194 - val_loss: 9705.1025 - val_mae: 9705.7949\n",
      "Epoch 776/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 128.2623 - mae: 128.9503 - val_loss: 9935.6709 - val_mae: 9936.3643\n",
      "Epoch 777/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 139.7856 - mae: 140.4739 - val_loss: 9625.8057 - val_mae: 9626.5000\n",
      "Epoch 778/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 146.2072 - mae: 146.8968 - val_loss: 10075.8311 - val_mae: 10076.5244\n",
      "Epoch 779/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 133.5708 - mae: 134.2605 - val_loss: 10025.8662 - val_mae: 10026.5576\n",
      "Epoch 780/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 153.9196 - mae: 154.6088 - val_loss: 9869.3633 - val_mae: 9870.0576\n",
      "Epoch 781/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 151.7916 - mae: 152.4795 - val_loss: 9605.4082 - val_mae: 9606.1006\n",
      "Epoch 782/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 143.1964 - mae: 143.8858 - val_loss: 9572.5947 - val_mae: 9573.2871\n",
      "Epoch 783/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 132.7805 - mae: 133.4694 - val_loss: 9859.9424 - val_mae: 9860.6348\n",
      "Epoch 784/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 151.9608 - mae: 152.6502 - val_loss: 9791.3799 - val_mae: 9792.0752\n",
      "Epoch 785/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 152.0292 - mae: 152.7173 - val_loss: 9797.9980 - val_mae: 9798.6924\n",
      "Epoch 786/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 125.7339 - mae: 126.4211 - val_loss: 10198.4609 - val_mae: 10199.1533\n",
      "Epoch 787/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 142.0311 - mae: 142.7194 - val_loss: 9876.4795 - val_mae: 9877.1729\n",
      "Epoch 788/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 152.8062 - mae: 153.4933 - val_loss: 9715.7422 - val_mae: 9716.4346\n",
      "Epoch 789/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 150.4587 - mae: 151.1457 - val_loss: 9944.7578 - val_mae: 9945.4502\n",
      "Epoch 790/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 144.1203 - mae: 144.8083 - val_loss: 9756.2598 - val_mae: 9756.9531\n",
      "Epoch 791/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 128.8776 - mae: 129.5616 - val_loss: 9941.8057 - val_mae: 9942.5000\n",
      "Epoch 792/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 143.0846 - mae: 143.7744 - val_loss: 10020.8496 - val_mae: 10021.5439\n",
      "Epoch 793/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 132.3290 - mae: 133.0178 - val_loss: 9858.1123 - val_mae: 9858.8066\n",
      "Epoch 794/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 162.7510 - mae: 163.4416 - val_loss: 9863.1367 - val_mae: 9863.8301\n",
      "Epoch 795/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 149.8875 - mae: 150.5746 - val_loss: 10216.8311 - val_mae: 10217.5234\n",
      "Epoch 796/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 140.0856 - mae: 140.7741 - val_loss: 9810.6895 - val_mae: 9811.3838\n",
      "Epoch 797/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 138.4340 - mae: 139.1196 - val_loss: 10176.1895 - val_mae: 10176.8809\n",
      "Epoch 798/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 137.8260 - mae: 138.5153 - val_loss: 9718.8545 - val_mae: 9719.5469\n",
      "Epoch 799/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 139.6539 - mae: 140.3425 - val_loss: 10187.2441 - val_mae: 10187.9365\n",
      "Epoch 800/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 133.1348 - mae: 133.8232 - val_loss: 9857.4863 - val_mae: 9858.1787\n",
      "Epoch 801/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 138.2633 - mae: 138.9529 - val_loss: 9966.6338 - val_mae: 9967.3262\n",
      "Epoch 802/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 142.4535 - mae: 143.1448 - val_loss: 9769.9551 - val_mae: 9770.6484\n",
      "Epoch 803/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 156.8974 - mae: 157.5862 - val_loss: 9870.3740 - val_mae: 9871.0674\n",
      "Epoch 804/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 139.0361 - mae: 139.7251 - val_loss: 10037.0869 - val_mae: 10037.7793\n",
      "Epoch 805/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 148.1467 - mae: 148.8341 - val_loss: 9895.3857 - val_mae: 9896.0791\n",
      "Epoch 806/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 152.8314 - mae: 153.5193 - val_loss: 9427.7246 - val_mae: 9428.4180\n",
      "Epoch 807/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 155.5747 - mae: 156.2655 - val_loss: 9748.7480 - val_mae: 9749.4424\n",
      "Epoch 808/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 134.4354 - mae: 135.1242 - val_loss: 9860.5791 - val_mae: 9861.2725\n",
      "Epoch 809/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 125.6574 - mae: 126.3452 - val_loss: 10006.5547 - val_mae: 10007.2500\n",
      "Epoch 810/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 128.6038 - mae: 129.2893 - val_loss: 10036.2861 - val_mae: 10036.9775\n",
      "Epoch 811/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 132.3219 - mae: 133.0070 - val_loss: 9815.3789 - val_mae: 9816.0713\n",
      "Epoch 812/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 148.4844 - mae: 149.1680 - val_loss: 9963.5635 - val_mae: 9964.2549\n",
      "Epoch 813/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 133.6639 - mae: 134.3520 - val_loss: 10048.9512 - val_mae: 10049.6445\n",
      "Epoch 814/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 152.7603 - mae: 153.4480 - val_loss: 10198.5889 - val_mae: 10199.2822\n",
      "Epoch 815/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 146.4443 - mae: 147.1302 - val_loss: 9744.6719 - val_mae: 9745.3643\n",
      "Epoch 816/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 154.4851 - mae: 155.1716 - val_loss: 9697.5137 - val_mae: 9698.2051\n",
      "Epoch 817/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 141.1236 - mae: 141.8095 - val_loss: 9746.3760 - val_mae: 9747.0693\n",
      "Epoch 818/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 137.4138 - mae: 138.1030 - val_loss: 9768.4414 - val_mae: 9769.1338\n",
      "Epoch 819/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 146.8954 - mae: 147.5821 - val_loss: 9977.8652 - val_mae: 9978.5586\n",
      "Epoch 820/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 132.6049 - mae: 133.2909 - val_loss: 9619.4141 - val_mae: 9620.1074\n",
      "Epoch 821/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 131.5169 - mae: 132.2054 - val_loss: 10008.1123 - val_mae: 10008.8037\n",
      "Epoch 822/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 159.3746 - mae: 160.0605 - val_loss: 10015.6426 - val_mae: 10016.3350\n",
      "Epoch 823/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 175.1587 - mae: 175.8462 - val_loss: 10014.5518 - val_mae: 10015.2441\n",
      "Epoch 824/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 143.1910 - mae: 143.8787 - val_loss: 9799.4180 - val_mae: 9800.1104\n",
      "Epoch 825/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 159.9471 - mae: 160.6379 - val_loss: 9860.5137 - val_mae: 9861.2070\n",
      "Epoch 826/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 131.4044 - mae: 132.0931 - val_loss: 9634.6035 - val_mae: 9635.2959\n",
      "Epoch 827/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 149.5962 - mae: 150.2834 - val_loss: 9724.5654 - val_mae: 9725.2578\n",
      "Epoch 828/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 135.7873 - mae: 136.4770 - val_loss: 9966.0459 - val_mae: 9966.7383\n",
      "Epoch 829/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 121.2564 - mae: 121.9464 - val_loss: 9930.1709 - val_mae: 9930.8643\n",
      "Epoch 830/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 133.9898 - mae: 134.6783 - val_loss: 9944.1670 - val_mae: 9944.8594\n",
      "Epoch 831/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 142.5720 - mae: 143.2592 - val_loss: 9859.0586 - val_mae: 9859.7520\n",
      "Epoch 832/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 133.6900 - mae: 134.3776 - val_loss: 9949.3721 - val_mae: 9950.0654\n",
      "Epoch 833/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 145.3786 - mae: 146.0667 - val_loss: 9808.9541 - val_mae: 9809.6504\n",
      "Epoch 834/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 120.8307 - mae: 121.5143 - val_loss: 9601.2773 - val_mae: 9601.9707\n",
      "Epoch 835/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 133.7700 - mae: 134.4536 - val_loss: 9783.6406 - val_mae: 9784.3350\n",
      "Epoch 836/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 134.4721 - mae: 135.1563 - val_loss: 9615.9648 - val_mae: 9616.6582\n",
      "Epoch 837/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 139.0369 - mae: 139.7232 - val_loss: 9737.8779 - val_mae: 9738.5723\n",
      "Epoch 838/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 132.4524 - mae: 133.1403 - val_loss: 9883.6162 - val_mae: 9884.3086\n",
      "Epoch 839/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 134.3426 - mae: 135.0281 - val_loss: 9846.5107 - val_mae: 9847.2021\n",
      "Epoch 840/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 136.0928 - mae: 136.7774 - val_loss: 9678.3848 - val_mae: 9679.0791\n",
      "Epoch 841/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 131.2335 - mae: 131.9210 - val_loss: 9887.7285 - val_mae: 9888.4209\n",
      "Epoch 842/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 141.3058 - mae: 141.9948 - val_loss: 9492.6729 - val_mae: 9493.3652\n",
      "Epoch 843/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 142.3202 - mae: 143.0094 - val_loss: 9803.2168 - val_mae: 9803.9092\n",
      "Epoch 844/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 147.1235 - mae: 147.8102 - val_loss: 9523.1094 - val_mae: 9523.8018\n",
      "Epoch 845/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 146.4559 - mae: 147.1436 - val_loss: 9833.9561 - val_mae: 9834.6504\n",
      "Epoch 846/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 142.8624 - mae: 143.5473 - val_loss: 9866.4219 - val_mae: 9867.1152\n",
      "Epoch 847/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 135.3792 - mae: 136.0659 - val_loss: 9637.5664 - val_mae: 9638.2578\n",
      "Epoch 848/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 131.6938 - mae: 132.3838 - val_loss: 9841.3193 - val_mae: 9842.0146\n",
      "Epoch 849/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 126.7099 - mae: 127.3977 - val_loss: 9489.2686 - val_mae: 9489.9619\n",
      "Epoch 850/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 135.4269 - mae: 136.1158 - val_loss: 9628.6699 - val_mae: 9629.3633\n",
      "Epoch 851/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 127.7235 - mae: 128.4086 - val_loss: 9608.7910 - val_mae: 9609.4844\n",
      "Epoch 852/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 147.0029 - mae: 147.6918 - val_loss: 9722.3311 - val_mae: 9723.0244\n",
      "Epoch 853/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 142.3942 - mae: 143.0845 - val_loss: 9807.2314 - val_mae: 9807.9248\n",
      "Epoch 854/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 147.1347 - mae: 147.8258 - val_loss: 9517.6914 - val_mae: 9518.3838\n",
      "Epoch 855/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 141.2853 - mae: 141.9712 - val_loss: 9820.1494 - val_mae: 9820.8428\n",
      "Epoch 856/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 145.9849 - mae: 146.6745 - val_loss: 9856.1211 - val_mae: 9856.8154\n",
      "Epoch 857/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 137.7423 - mae: 138.4308 - val_loss: 9796.7676 - val_mae: 9797.4619\n",
      "Epoch 858/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 129.7170 - mae: 130.4053 - val_loss: 9681.3301 - val_mae: 9682.0234\n",
      "Epoch 859/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 135.6216 - mae: 136.3116 - val_loss: 9901.7500 - val_mae: 9902.4424\n",
      "Epoch 860/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 133.3302 - mae: 134.0171 - val_loss: 9900.8174 - val_mae: 9901.5098\n",
      "Epoch 861/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 146.4998 - mae: 147.1865 - val_loss: 9755.0840 - val_mae: 9755.7773\n",
      "Epoch 862/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 135.5230 - mae: 136.2108 - val_loss: 9808.8555 - val_mae: 9809.5498\n",
      "Epoch 863/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 131.4723 - mae: 132.1585 - val_loss: 9886.8135 - val_mae: 9887.5068\n",
      "Epoch 864/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 134.3137 - mae: 135.0002 - val_loss: 9930.1738 - val_mae: 9930.8662\n",
      "Epoch 865/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 157.3950 - mae: 158.0822 - val_loss: 9767.3203 - val_mae: 9768.0137\n",
      "Epoch 866/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 136.3830 - mae: 137.0733 - val_loss: 9577.0869 - val_mae: 9577.7803\n",
      "Epoch 867/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 140.0980 - mae: 140.7885 - val_loss: 9920.5273 - val_mae: 9921.2207\n",
      "Epoch 868/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 142.8046 - mae: 143.4941 - val_loss: 9491.8574 - val_mae: 9492.5498\n",
      "Epoch 869/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 142.8511 - mae: 143.5426 - val_loss: 9844.3340 - val_mae: 9845.0254\n",
      "Epoch 870/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 135.9095 - mae: 136.5958 - val_loss: 9782.0605 - val_mae: 9782.7529\n",
      "Epoch 871/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 126.8761 - mae: 127.5635 - val_loss: 9504.0859 - val_mae: 9504.7783\n",
      "Epoch 872/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 125.9236 - mae: 126.6149 - val_loss: 9571.8184 - val_mae: 9572.5117\n",
      "Epoch 873/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 124.4498 - mae: 125.1326 - val_loss: 9915.5664 - val_mae: 9916.2578\n",
      "Epoch 874/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 132.5277 - mae: 133.2149 - val_loss: 9802.5088 - val_mae: 9803.2012\n",
      "Epoch 875/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 140.4274 - mae: 141.1128 - val_loss: 9831.7773 - val_mae: 9832.4707\n",
      "Epoch 876/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 125.5293 - mae: 126.2187 - val_loss: 9812.0303 - val_mae: 9812.7227\n",
      "Epoch 877/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 117.0593 - mae: 117.7458 - val_loss: 9654.3633 - val_mae: 9655.0566\n",
      "Epoch 878/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 138.2148 - mae: 138.9024 - val_loss: 9732.2324 - val_mae: 9732.9258\n",
      "Epoch 879/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 125.4577 - mae: 126.1420 - val_loss: 9551.0615 - val_mae: 9551.7539\n",
      "Epoch 880/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 150.0666 - mae: 150.7534 - val_loss: 9752.0176 - val_mae: 9752.7100\n",
      "Epoch 881/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 133.7617 - mae: 134.4501 - val_loss: 9644.1797 - val_mae: 9644.8721\n",
      "Epoch 882/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 147.0806 - mae: 147.7691 - val_loss: 9769.4199 - val_mae: 9770.1123\n",
      "Epoch 883/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 130.2104 - mae: 130.8988 - val_loss: 9940.4678 - val_mae: 9941.1621\n",
      "Epoch 884/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 119.5396 - mae: 120.2231 - val_loss: 9979.8135 - val_mae: 9980.5059\n",
      "Epoch 885/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 147.1663 - mae: 147.8554 - val_loss: 9805.0020 - val_mae: 9805.6953\n",
      "Epoch 886/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 128.4963 - mae: 129.1866 - val_loss: 10014.8721 - val_mae: 10015.5654\n",
      "Epoch 887/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 126.9784 - mae: 127.6663 - val_loss: 9696.9570 - val_mae: 9697.6504\n",
      "Epoch 888/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 129.7137 - mae: 130.3997 - val_loss: 9631.7188 - val_mae: 9632.4111\n",
      "Epoch 889/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 135.1002 - mae: 135.7880 - val_loss: 9742.7695 - val_mae: 9743.4629\n",
      "Epoch 890/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 130.9489 - mae: 131.6341 - val_loss: 9903.0908 - val_mae: 9903.7842\n",
      "Epoch 891/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 132.0523 - mae: 132.7390 - val_loss: 9565.4648 - val_mae: 9566.1572\n",
      "Epoch 892/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 151.9659 - mae: 152.6531 - val_loss: 9832.0879 - val_mae: 9832.7803\n",
      "Epoch 893/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 122.2131 - mae: 122.9026 - val_loss: 9735.7451 - val_mae: 9736.4395\n",
      "Epoch 894/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 140.5177 - mae: 141.2038 - val_loss: 9419.4170 - val_mae: 9420.1113\n",
      "Epoch 895/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 137.4708 - mae: 138.1613 - val_loss: 9875.9150 - val_mae: 9876.6084\n",
      "Epoch 896/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 142.4177 - mae: 143.1048 - val_loss: 10008.4590 - val_mae: 10009.1514\n",
      "Epoch 897/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 136.4545 - mae: 137.1407 - val_loss: 9647.3691 - val_mae: 9648.0635\n",
      "Epoch 898/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 147.7761 - mae: 148.4647 - val_loss: 9774.3281 - val_mae: 9775.0225\n",
      "Epoch 899/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 131.1093 - mae: 131.7960 - val_loss: 9967.1553 - val_mae: 9967.8496\n",
      "Epoch 900/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 136.9223 - mae: 137.6087 - val_loss: 9829.3096 - val_mae: 9830.0029\n",
      "Epoch 901/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 128.0590 - mae: 128.7474 - val_loss: 9697.7227 - val_mae: 9698.4160\n",
      "Epoch 902/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 123.1584 - mae: 123.8444 - val_loss: 10104.1338 - val_mae: 10104.8262\n",
      "Epoch 903/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 135.0971 - mae: 135.7854 - val_loss: 9489.3701 - val_mae: 9490.0645\n",
      "Epoch 904/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 133.2435 - mae: 133.9302 - val_loss: 9747.3086 - val_mae: 9748.0000\n",
      "Epoch 905/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 130.2590 - mae: 130.9481 - val_loss: 10236.6465 - val_mae: 10237.3398\n",
      "Epoch 906/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 149.0963 - mae: 149.7848 - val_loss: 9596.2666 - val_mae: 9596.9590\n",
      "Epoch 907/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 143.3371 - mae: 144.0264 - val_loss: 9575.3652 - val_mae: 9576.0576\n",
      "Epoch 908/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 123.6419 - mae: 124.3295 - val_loss: 9644.1885 - val_mae: 9644.8818\n",
      "Epoch 909/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 123.8337 - mae: 124.5205 - val_loss: 10047.1602 - val_mae: 10047.8535\n",
      "Epoch 910/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 136.8680 - mae: 137.5579 - val_loss: 9537.5840 - val_mae: 9538.2773\n",
      "Epoch 911/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 124.0272 - mae: 124.7135 - val_loss: 9600.0947 - val_mae: 9600.7871\n",
      "Epoch 912/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 128.8056 - mae: 129.4923 - val_loss: 9612.2305 - val_mae: 9612.9238\n",
      "Epoch 913/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 128.2627 - mae: 128.9512 - val_loss: 9811.9619 - val_mae: 9812.6553\n",
      "Epoch 914/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 134.4048 - mae: 135.0938 - val_loss: 9860.8613 - val_mae: 9861.5537\n",
      "Epoch 915/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 136.5587 - mae: 137.2481 - val_loss: 9685.5908 - val_mae: 9686.2852\n",
      "Epoch 916/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 132.1025 - mae: 132.7908 - val_loss: 9633.4717 - val_mae: 9634.1641\n",
      "Epoch 917/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 125.6208 - mae: 126.3078 - val_loss: 9707.9570 - val_mae: 9708.6504\n",
      "Epoch 918/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 118.5425 - mae: 119.2278 - val_loss: 9749.9551 - val_mae: 9750.6494\n",
      "Epoch 919/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 122.5452 - mae: 123.2320 - val_loss: 9864.7871 - val_mae: 9865.4814\n",
      "Epoch 920/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 144.7042 - mae: 145.3926 - val_loss: 10071.6133 - val_mae: 10072.3076\n",
      "Epoch 921/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 135.1963 - mae: 135.8825 - val_loss: 9900.5537 - val_mae: 9901.2461\n",
      "Epoch 922/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 133.0037 - mae: 133.6917 - val_loss: 9847.2715 - val_mae: 9847.9639\n",
      "Epoch 923/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 120.6481 - mae: 121.3327 - val_loss: 9575.1387 - val_mae: 9575.8291\n",
      "Epoch 924/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 117.4368 - mae: 118.1232 - val_loss: 9403.0986 - val_mae: 9403.7920\n",
      "Epoch 925/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 142.8005 - mae: 143.4910 - val_loss: 9944.0732 - val_mae: 9944.7656\n",
      "Epoch 926/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 157.3675 - mae: 158.0554 - val_loss: 9415.7080 - val_mae: 9416.4023\n",
      "Epoch 927/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 135.3581 - mae: 136.0411 - val_loss: 9712.9287 - val_mae: 9713.6221\n",
      "Epoch 928/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 135.7080 - mae: 136.3970 - val_loss: 9650.3037 - val_mae: 9650.9951\n",
      "Epoch 929/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 123.9822 - mae: 124.6690 - val_loss: 9522.3262 - val_mae: 9523.0186\n",
      "Epoch 930/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 133.6056 - mae: 134.2916 - val_loss: 9720.1982 - val_mae: 9720.8916\n",
      "Epoch 931/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 128.5304 - mae: 129.2186 - val_loss: 9674.3477 - val_mae: 9675.0410\n",
      "Epoch 932/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 136.8806 - mae: 137.5691 - val_loss: 9637.9570 - val_mae: 9638.6494\n",
      "Epoch 933/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 126.5638 - mae: 127.2515 - val_loss: 9745.3682 - val_mae: 9746.0615\n",
      "Epoch 934/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 130.5619 - mae: 131.2503 - val_loss: 9596.7930 - val_mae: 9597.4863\n",
      "Epoch 935/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 120.3471 - mae: 121.0318 - val_loss: 9735.2686 - val_mae: 9735.9619\n",
      "Epoch 936/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 134.4590 - mae: 135.1465 - val_loss: 9699.9141 - val_mae: 9700.6064\n",
      "Epoch 937/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 124.6141 - mae: 125.3029 - val_loss: 9783.1660 - val_mae: 9783.8584\n",
      "Epoch 938/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 132.8913 - mae: 133.5808 - val_loss: 9561.5625 - val_mae: 9562.2568\n",
      "Epoch 939/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 128.5333 - mae: 129.2234 - val_loss: 10014.4277 - val_mae: 10015.1211\n",
      "Epoch 940/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 126.6499 - mae: 127.3335 - val_loss: 9731.0029 - val_mae: 9731.6973\n",
      "Epoch 941/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 139.6910 - mae: 140.3785 - val_loss: 9727.6377 - val_mae: 9728.3320\n",
      "Epoch 942/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 133.3299 - mae: 134.0198 - val_loss: 9833.9766 - val_mae: 9834.6699\n",
      "Epoch 943/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 152.3433 - mae: 153.0329 - val_loss: 9938.6191 - val_mae: 9939.3125\n",
      "Epoch 944/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 146.2652 - mae: 146.9530 - val_loss: 9853.9629 - val_mae: 9854.6562\n",
      "Epoch 945/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 138.0881 - mae: 138.7748 - val_loss: 9822.3770 - val_mae: 9823.0713\n",
      "Epoch 946/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 140.6860 - mae: 141.3781 - val_loss: 9433.7764 - val_mae: 9434.4678\n",
      "Epoch 947/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 122.4831 - mae: 123.1722 - val_loss: 9853.3359 - val_mae: 9854.0283\n",
      "Epoch 948/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 143.9225 - mae: 144.6109 - val_loss: 9534.0459 - val_mae: 9534.7393\n",
      "Epoch 949/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 131.0362 - mae: 131.7264 - val_loss: 9650.1094 - val_mae: 9650.8027\n",
      "Epoch 950/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 115.3329 - mae: 116.0184 - val_loss: 9609.6045 - val_mae: 9610.2969\n",
      "Epoch 951/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 120.0694 - mae: 120.7565 - val_loss: 9654.6787 - val_mae: 9655.3711\n",
      "Epoch 952/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 115.2889 - mae: 115.9761 - val_loss: 9765.5635 - val_mae: 9766.2578\n",
      "Epoch 953/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 131.5279 - mae: 132.2181 - val_loss: 9551.5293 - val_mae: 9552.2217\n",
      "Epoch 954/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 119.6418 - mae: 120.3285 - val_loss: 9907.4932 - val_mae: 9908.1865\n",
      "Epoch 955/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 138.4315 - mae: 139.1207 - val_loss: 9907.0605 - val_mae: 9907.7539\n",
      "Epoch 956/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 131.1173 - mae: 131.8053 - val_loss: 9737.2168 - val_mae: 9737.9111\n",
      "Epoch 957/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 123.6280 - mae: 124.3172 - val_loss: 9725.7441 - val_mae: 9726.4365\n",
      "Epoch 958/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 127.2729 - mae: 127.9638 - val_loss: 9826.8467 - val_mae: 9827.5381\n",
      "Epoch 959/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 136.7906 - mae: 137.4764 - val_loss: 9594.9785 - val_mae: 9595.6719\n",
      "Epoch 960/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 130.0282 - mae: 130.7158 - val_loss: 9458.5791 - val_mae: 9459.2734\n",
      "Epoch 961/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 156.5657 - mae: 157.2535 - val_loss: 9661.3154 - val_mae: 9662.0078\n",
      "Epoch 962/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 141.4383 - mae: 142.1279 - val_loss: 9730.9092 - val_mae: 9731.6045\n",
      "Epoch 963/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 130.6253 - mae: 131.3128 - val_loss: 9413.8320 - val_mae: 9414.5234\n",
      "Epoch 964/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 123.7605 - mae: 124.4476 - val_loss: 9522.7031 - val_mae: 9523.3955\n",
      "Epoch 965/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 133.9091 - mae: 134.5959 - val_loss: 9452.7129 - val_mae: 9453.4062\n",
      "Epoch 966/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 123.3787 - mae: 124.0621 - val_loss: 9822.8799 - val_mae: 9823.5742\n",
      "Epoch 967/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 134.0548 - mae: 134.7425 - val_loss: 9489.7500 - val_mae: 9490.4424\n",
      "Epoch 968/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 135.6055 - mae: 136.2952 - val_loss: 9637.3682 - val_mae: 9638.0605\n",
      "Epoch 969/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 154.4094 - mae: 155.0979 - val_loss: 9679.6055 - val_mae: 9680.2979\n",
      "Epoch 970/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 124.6989 - mae: 125.3873 - val_loss: 9668.8916 - val_mae: 9669.5830\n",
      "Epoch 971/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 124.2487 - mae: 124.9357 - val_loss: 9548.1289 - val_mae: 9548.8223\n",
      "Epoch 972/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 157.3058 - mae: 157.9955 - val_loss: 9709.5732 - val_mae: 9710.2656\n",
      "Epoch 973/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 164.0587 - mae: 164.7482 - val_loss: 9936.4580 - val_mae: 9937.1514\n",
      "Epoch 974/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 125.8976 - mae: 126.5852 - val_loss: 9486.6787 - val_mae: 9487.3711\n",
      "Epoch 975/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 128.8624 - mae: 129.5523 - val_loss: 9498.3301 - val_mae: 9499.0234\n",
      "Epoch 976/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 131.0675 - mae: 131.7542 - val_loss: 9693.8525 - val_mae: 9694.5469\n",
      "Epoch 977/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 130.0395 - mae: 130.7277 - val_loss: 9911.8018 - val_mae: 9912.4941\n",
      "Epoch 978/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 117.9043 - mae: 118.5914 - val_loss: 9379.8721 - val_mae: 9380.5645\n",
      "Epoch 979/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 132.2604 - mae: 132.9487 - val_loss: 9667.1553 - val_mae: 9667.8496\n",
      "Epoch 980/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 129.4878 - mae: 130.1775 - val_loss: 9747.0791 - val_mae: 9747.7715\n",
      "Epoch 981/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 125.4824 - mae: 126.1707 - val_loss: 9759.3926 - val_mae: 9760.0850\n",
      "Epoch 982/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 124.5951 - mae: 125.2818 - val_loss: 9763.1523 - val_mae: 9763.8467\n",
      "Epoch 983/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 147.5702 - mae: 148.2588 - val_loss: 9791.1475 - val_mae: 9791.8408\n",
      "Epoch 984/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 114.4999 - mae: 115.1891 - val_loss: 9724.3369 - val_mae: 9725.0293\n",
      "Epoch 985/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 113.0944 - mae: 113.7835 - val_loss: 9813.9443 - val_mae: 9814.6377\n",
      "Epoch 986/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 119.9382 - mae: 120.6276 - val_loss: 9723.6777 - val_mae: 9724.3701\n",
      "Epoch 987/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 132.5687 - mae: 133.2568 - val_loss: 9853.1562 - val_mae: 9853.8506\n",
      "Epoch 988/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 138.2472 - mae: 138.9319 - val_loss: 9896.9238 - val_mae: 9897.6162\n",
      "Epoch 989/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 119.4788 - mae: 120.1677 - val_loss: 9667.0381 - val_mae: 9667.7314\n",
      "Epoch 990/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 130.7095 - mae: 131.3949 - val_loss: 9525.0322 - val_mae: 9525.7256\n",
      "Epoch 991/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 140.7962 - mae: 141.4843 - val_loss: 9509.4092 - val_mae: 9510.1016\n",
      "Epoch 992/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 122.6287 - mae: 123.3176 - val_loss: 9697.2646 - val_mae: 9697.9590\n",
      "Epoch 993/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 125.7717 - mae: 126.4607 - val_loss: 9863.5420 - val_mae: 9864.2354\n",
      "Epoch 994/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 120.8422 - mae: 121.5291 - val_loss: 9479.3115 - val_mae: 9480.0049\n",
      "Epoch 995/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 109.4820 - mae: 110.1659 - val_loss: 9712.7832 - val_mae: 9713.4766\n",
      "Epoch 996/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 122.0766 - mae: 122.7619 - val_loss: 9361.4971 - val_mae: 9362.1895\n",
      "Epoch 997/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 127.8400 - mae: 128.5253 - val_loss: 9424.3555 - val_mae: 9425.0488\n",
      "Epoch 998/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 118.0294 - mae: 118.7186 - val_loss: 9115.5059 - val_mae: 9116.1992\n",
      "Epoch 999/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 171.8794 - mae: 172.5691 - val_loss: 9533.1768 - val_mae: 9533.8701\n",
      "Epoch 1000/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 125.5837 - mae: 126.2696 - val_loss: 9816.4307 - val_mae: 9817.1250\n",
      "Epoch 1001/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 134.4226 - mae: 135.1071 - val_loss: 9395.3545 - val_mae: 9396.0488\n",
      "Epoch 1002/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 136.1728 - mae: 136.8601 - val_loss: 9659.0947 - val_mae: 9659.7871\n",
      "Epoch 1003/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 122.4657 - mae: 123.1496 - val_loss: 9601.0391 - val_mae: 9601.7314\n",
      "Epoch 1004/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 130.2730 - mae: 130.9629 - val_loss: 9714.1855 - val_mae: 9714.8789\n",
      "Epoch 1005/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 156.6322 - mae: 157.3222 - val_loss: 9536.8936 - val_mae: 9537.5869\n",
      "Epoch 1006/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 130.0421 - mae: 130.7306 - val_loss: 9476.6709 - val_mae: 9477.3643\n",
      "Epoch 1007/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 146.5851 - mae: 147.2762 - val_loss: 9722.8467 - val_mae: 9723.5400\n",
      "Epoch 1008/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 116.4921 - mae: 117.1785 - val_loss: 9616.5566 - val_mae: 9617.2500\n",
      "Epoch 1009/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 116.7243 - mae: 117.4119 - val_loss: 9537.7549 - val_mae: 9538.4482\n",
      "Epoch 1010/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 120.9769 - mae: 121.6621 - val_loss: 9463.2217 - val_mae: 9463.9160\n",
      "Epoch 1011/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 129.8915 - mae: 130.5785 - val_loss: 9436.5059 - val_mae: 9437.1982\n",
      "Epoch 1012/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 124.5405 - mae: 125.2264 - val_loss: 9446.9814 - val_mae: 9447.6719\n",
      "Epoch 1013/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 127.0540 - mae: 127.7418 - val_loss: 9502.0605 - val_mae: 9502.7539\n",
      "Epoch 1014/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 122.0333 - mae: 122.7211 - val_loss: 9729.2598 - val_mae: 9729.9531\n",
      "Epoch 1015/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 126.4878 - mae: 127.1779 - val_loss: 9647.6523 - val_mae: 9648.3457\n",
      "Epoch 1016/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 135.0236 - mae: 135.7089 - val_loss: 9501.1094 - val_mae: 9501.8018\n",
      "Epoch 1017/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 126.3985 - mae: 127.0863 - val_loss: 9592.1768 - val_mae: 9592.8691\n",
      "Epoch 1018/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 115.0310 - mae: 115.7180 - val_loss: 9740.2988 - val_mae: 9740.9922\n",
      "Epoch 1019/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 109.0335 - mae: 109.7149 - val_loss: 9801.2627 - val_mae: 9801.9541\n",
      "Epoch 1020/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 146.2441 - mae: 146.9312 - val_loss: 9400.4082 - val_mae: 9401.1006\n",
      "Epoch 1021/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 120.0668 - mae: 120.7518 - val_loss: 9389.5098 - val_mae: 9390.2021\n",
      "Epoch 1022/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 134.8994 - mae: 135.5883 - val_loss: 9386.4824 - val_mae: 9387.1748\n",
      "Epoch 1023/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 140.9863 - mae: 141.6704 - val_loss: 9590.8965 - val_mae: 9591.5908\n",
      "Epoch 1024/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 113.4061 - mae: 114.0949 - val_loss: 9450.9043 - val_mae: 9451.5977\n",
      "Epoch 1025/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 107.4986 - mae: 108.1827 - val_loss: 9753.0215 - val_mae: 9753.7168\n",
      "Epoch 1026/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 128.9192 - mae: 129.6071 - val_loss: 9873.0859 - val_mae: 9873.7803\n",
      "Epoch 1027/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 134.6600 - mae: 135.3460 - val_loss: 9441.1963 - val_mae: 9441.8896\n",
      "Epoch 1028/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 128.9031 - mae: 129.5923 - val_loss: 9459.4072 - val_mae: 9460.1006\n",
      "Epoch 1029/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 122.0753 - mae: 122.7612 - val_loss: 9729.4971 - val_mae: 9730.1895\n",
      "Epoch 1030/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 118.3906 - mae: 119.0798 - val_loss: 9431.2080 - val_mae: 9431.9023\n",
      "Epoch 1031/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 113.9790 - mae: 114.6650 - val_loss: 9515.6406 - val_mae: 9516.3340\n",
      "Epoch 1032/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 130.9924 - mae: 131.6799 - val_loss: 9551.1367 - val_mae: 9551.8291\n",
      "Epoch 1033/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 120.4823 - mae: 121.1694 - val_loss: 9706.6680 - val_mae: 9707.3594\n",
      "Epoch 1034/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 119.6485 - mae: 120.3350 - val_loss: 9706.0957 - val_mae: 9706.7881\n",
      "Epoch 1035/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 130.8742 - mae: 131.5615 - val_loss: 9795.8398 - val_mae: 9796.5312\n",
      "Epoch 1036/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 134.2322 - mae: 134.9206 - val_loss: 9678.3408 - val_mae: 9679.0342\n",
      "Epoch 1037/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 135.3195 - mae: 136.0065 - val_loss: 9372.3057 - val_mae: 9372.9980\n",
      "Epoch 1038/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 127.5315 - mae: 128.2210 - val_loss: 9636.3125 - val_mae: 9637.0049\n",
      "Epoch 1039/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 127.4839 - mae: 128.1728 - val_loss: 9483.6768 - val_mae: 9484.3701\n",
      "Epoch 1040/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 121.9955 - mae: 122.6806 - val_loss: 9643.3887 - val_mae: 9644.0820\n",
      "Epoch 1041/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 130.3847 - mae: 131.0719 - val_loss: 9505.6797 - val_mae: 9506.3740\n",
      "Epoch 1042/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 110.5834 - mae: 111.2666 - val_loss: 9345.3438 - val_mae: 9346.0361\n",
      "Epoch 1043/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 123.9334 - mae: 124.6221 - val_loss: 9500.5469 - val_mae: 9501.2393\n",
      "Epoch 1044/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 129.4798 - mae: 130.1670 - val_loss: 9053.4316 - val_mae: 9054.1250\n",
      "Epoch 1045/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 145.1086 - mae: 145.7954 - val_loss: 9555.8066 - val_mae: 9556.5000\n",
      "Epoch 1046/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 142.8329 - mae: 143.5201 - val_loss: 9722.2178 - val_mae: 9722.9121\n",
      "Epoch 1047/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 124.6985 - mae: 125.3804 - val_loss: 9699.8555 - val_mae: 9700.5488\n",
      "Epoch 1048/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 131.3055 - mae: 131.9937 - val_loss: 9396.1172 - val_mae: 9396.8105\n",
      "Epoch 1049/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 135.6119 - mae: 136.2993 - val_loss: 9699.4639 - val_mae: 9700.1582\n",
      "Epoch 1050/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 119.2369 - mae: 119.9239 - val_loss: 9868.6758 - val_mae: 9869.3701\n",
      "Epoch 1051/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 126.7223 - mae: 127.4061 - val_loss: 9233.6025 - val_mae: 9234.2969\n",
      "Epoch 1052/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 139.0777 - mae: 139.7657 - val_loss: 9517.7266 - val_mae: 9518.4189\n",
      "Epoch 1053/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 114.2992 - mae: 114.9880 - val_loss: 9585.8828 - val_mae: 9586.5752\n",
      "Epoch 1054/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 121.0156 - mae: 121.7035 - val_loss: 9718.0244 - val_mae: 9718.7168\n",
      "Epoch 1055/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 117.3650 - mae: 118.0521 - val_loss: 9545.7979 - val_mae: 9546.4902\n",
      "Epoch 1056/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 119.5968 - mae: 120.2851 - val_loss: 9529.7842 - val_mae: 9530.4775\n",
      "Epoch 1057/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 117.7297 - mae: 118.4152 - val_loss: 9581.5010 - val_mae: 9582.1924\n",
      "Epoch 1058/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 121.5292 - mae: 122.2144 - val_loss: 9718.5566 - val_mae: 9719.2500\n",
      "Epoch 1059/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 132.1153 - mae: 132.8032 - val_loss: 9654.4189 - val_mae: 9655.1113\n",
      "Epoch 1060/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 118.2810 - mae: 118.9696 - val_loss: 9479.6387 - val_mae: 9480.3311\n",
      "Epoch 1061/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 124.1424 - mae: 124.8331 - val_loss: 9485.9971 - val_mae: 9486.6904\n",
      "Epoch 1062/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 132.0953 - mae: 132.7822 - val_loss: 9532.4570 - val_mae: 9533.1504\n",
      "Epoch 1063/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 129.2410 - mae: 129.9305 - val_loss: 9389.6260 - val_mae: 9390.3193\n",
      "Epoch 1064/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 126.6195 - mae: 127.3012 - val_loss: 9676.9336 - val_mae: 9677.6279\n",
      "Epoch 1065/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 129.0770 - mae: 129.7593 - val_loss: 9733.7939 - val_mae: 9734.4863\n",
      "Epoch 1066/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 140.7414 - mae: 141.4249 - val_loss: 9584.7822 - val_mae: 9585.4756\n",
      "Epoch 1067/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 119.2675 - mae: 119.9538 - val_loss: 9572.2773 - val_mae: 9572.9707\n",
      "Epoch 1068/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 118.8963 - mae: 119.5827 - val_loss: 9538.4990 - val_mae: 9539.1914\n",
      "Epoch 1069/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 144.6527 - mae: 145.3394 - val_loss: 9756.0898 - val_mae: 9756.7832\n",
      "Epoch 1070/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 105.3579 - mae: 106.0425 - val_loss: 9654.5488 - val_mae: 9655.2412\n",
      "Epoch 1071/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 116.8198 - mae: 117.5039 - val_loss: 9704.2939 - val_mae: 9704.9873\n",
      "Epoch 1072/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 109.2028 - mae: 109.8894 - val_loss: 9431.2627 - val_mae: 9431.9561\n",
      "Epoch 1073/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 116.1685 - mae: 116.8580 - val_loss: 9561.6152 - val_mae: 9562.3086\n",
      "Epoch 1074/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 114.6263 - mae: 115.3143 - val_loss: 9849.6582 - val_mae: 9850.3516\n",
      "Epoch 1075/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 125.1232 - mae: 125.8077 - val_loss: 9830.0498 - val_mae: 9830.7441\n",
      "Epoch 1076/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 128.0075 - mae: 128.6921 - val_loss: 9634.9365 - val_mae: 9635.6309\n",
      "Epoch 1077/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 117.8962 - mae: 118.5830 - val_loss: 9537.4951 - val_mae: 9538.1875\n",
      "Epoch 1078/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 118.7132 - mae: 119.4036 - val_loss: 9575.8496 - val_mae: 9576.5430\n",
      "Epoch 1079/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 127.4275 - mae: 128.1162 - val_loss: 9701.4209 - val_mae: 9702.1133\n",
      "Epoch 1080/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 121.0317 - mae: 121.7193 - val_loss: 9521.7920 - val_mae: 9522.4854\n",
      "Epoch 1081/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 130.6088 - mae: 131.2961 - val_loss: 9608.1738 - val_mae: 9608.8652\n",
      "Epoch 1082/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 124.3872 - mae: 125.0753 - val_loss: 9739.0039 - val_mae: 9739.6963\n",
      "Epoch 1083/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 119.2743 - mae: 119.9605 - val_loss: 9674.9707 - val_mae: 9675.6631\n",
      "Epoch 1084/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 117.0581 - mae: 117.7481 - val_loss: 9497.5566 - val_mae: 9498.2500\n",
      "Epoch 1085/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 120.9984 - mae: 121.6849 - val_loss: 9749.5693 - val_mae: 9750.2617\n",
      "Epoch 1086/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 98.2294 - mae: 98.9135 - val_loss: 9720.4590 - val_mae: 9721.1523\n",
      "Epoch 1087/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 115.1772 - mae: 115.8641 - val_loss: 9296.8438 - val_mae: 9297.5361\n",
      "Epoch 1088/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 122.7014 - mae: 123.3902 - val_loss: 9374.9648 - val_mae: 9375.6592\n",
      "Epoch 1089/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 132.7939 - mae: 133.4832 - val_loss: 9509.7256 - val_mae: 9510.4180\n",
      "Epoch 1090/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 111.3433 - mae: 112.0282 - val_loss: 9561.1084 - val_mae: 9561.8027\n",
      "Epoch 1091/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 125.4950 - mae: 126.1852 - val_loss: 9809.5107 - val_mae: 9810.2031\n",
      "Epoch 1092/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 137.7300 - mae: 138.4159 - val_loss: 9351.0459 - val_mae: 9351.7383\n",
      "Epoch 1093/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 123.3744 - mae: 124.0641 - val_loss: 9499.1270 - val_mae: 9499.8213\n",
      "Epoch 1094/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 129.6527 - mae: 130.3404 - val_loss: 9749.0645 - val_mae: 9749.7568\n",
      "Epoch 1095/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 112.9465 - mae: 113.6319 - val_loss: 9537.2559 - val_mae: 9537.9502\n",
      "Epoch 1096/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 118.2259 - mae: 118.9152 - val_loss: 9561.7705 - val_mae: 9562.4619\n",
      "Epoch 1097/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 128.2700 - mae: 128.9608 - val_loss: 9735.3623 - val_mae: 9736.0557\n",
      "Epoch 1098/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 124.2670 - mae: 124.9554 - val_loss: 9697.8350 - val_mae: 9698.5283\n",
      "Epoch 1099/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 128.2798 - mae: 128.9657 - val_loss: 9498.9258 - val_mae: 9499.6182\n",
      "Epoch 1100/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 114.9325 - mae: 115.6185 - val_loss: 9504.7998 - val_mae: 9505.4941\n",
      "Epoch 1101/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 118.2346 - mae: 118.9222 - val_loss: 9692.4062 - val_mae: 9693.0996\n",
      "Epoch 1102/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 125.9458 - mae: 126.6339 - val_loss: 9604.1680 - val_mae: 9604.8604\n",
      "Epoch 1103/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 117.9738 - mae: 118.6592 - val_loss: 9380.9805 - val_mae: 9381.6738\n",
      "Epoch 1104/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 112.0326 - mae: 112.7219 - val_loss: 9749.9121 - val_mae: 9750.6055\n",
      "Epoch 1105/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 123.2045 - mae: 123.8919 - val_loss: 9746.7314 - val_mae: 9747.4238\n",
      "Epoch 1106/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 140.1176 - mae: 140.8069 - val_loss: 9445.2314 - val_mae: 9445.9248\n",
      "Epoch 1107/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 103.1425 - mae: 103.8307 - val_loss: 9577.1826 - val_mae: 9577.8740\n",
      "Epoch 1108/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 131.0444 - mae: 131.7318 - val_loss: 9475.9834 - val_mae: 9476.6768\n",
      "Epoch 1109/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 131.3733 - mae: 132.0575 - val_loss: 9598.8672 - val_mae: 9599.5615\n",
      "Epoch 1110/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 128.9144 - mae: 129.6039 - val_loss: 9298.3896 - val_mae: 9299.0830\n",
      "Epoch 1111/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 129.5070 - mae: 130.1973 - val_loss: 9672.1592 - val_mae: 9672.8525\n",
      "Epoch 1112/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 129.6870 - mae: 130.3713 - val_loss: 9320.2246 - val_mae: 9320.9170\n",
      "Epoch 1113/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 116.8204 - mae: 117.5108 - val_loss: 9485.3311 - val_mae: 9486.0244\n",
      "Epoch 1114/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 114.9014 - mae: 115.5912 - val_loss: 9859.0957 - val_mae: 9859.7900\n",
      "Epoch 1115/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 116.5526 - mae: 117.2383 - val_loss: 9531.3506 - val_mae: 9532.0449\n",
      "Epoch 1116/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 123.2379 - mae: 123.9258 - val_loss: 9714.2949 - val_mae: 9714.9883\n",
      "Epoch 1117/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 122.1075 - mae: 122.7914 - val_loss: 9509.9014 - val_mae: 9510.5938\n",
      "Epoch 1118/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 116.7383 - mae: 117.4279 - val_loss: 9484.9434 - val_mae: 9485.6357\n",
      "Epoch 1119/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 120.5544 - mae: 121.2398 - val_loss: 9674.7910 - val_mae: 9675.4844\n",
      "Epoch 1120/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 122.9856 - mae: 123.6729 - val_loss: 9498.6475 - val_mae: 9499.3418\n",
      "Epoch 1121/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 118.2666 - mae: 118.9499 - val_loss: 9371.3545 - val_mae: 9372.0488\n",
      "Epoch 1122/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 115.0221 - mae: 115.7104 - val_loss: 9477.0967 - val_mae: 9477.7881\n",
      "Epoch 1123/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 124.5247 - mae: 125.2121 - val_loss: 9528.4668 - val_mae: 9529.1602\n",
      "Epoch 1124/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 117.0475 - mae: 117.7348 - val_loss: 9699.7500 - val_mae: 9700.4424\n",
      "Epoch 1125/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 121.7172 - mae: 122.4025 - val_loss: 9867.1113 - val_mae: 9867.8037\n",
      "Epoch 1126/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 143.9748 - mae: 144.6638 - val_loss: 9505.1377 - val_mae: 9505.8320\n",
      "Epoch 1127/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 110.8508 - mae: 111.5375 - val_loss: 9641.1826 - val_mae: 9641.8760\n",
      "Epoch 1128/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 120.6398 - mae: 121.3285 - val_loss: 9517.6045 - val_mae: 9518.2979\n",
      "Epoch 1129/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 131.1430 - mae: 131.8307 - val_loss: 9486.6035 - val_mae: 9487.2959\n",
      "Epoch 1130/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 110.5576 - mae: 111.2458 - val_loss: 9552.8535 - val_mae: 9553.5479\n",
      "Epoch 1131/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 112.3964 - mae: 113.0850 - val_loss: 9504.9502 - val_mae: 9505.6445\n",
      "Epoch 1132/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 110.9215 - mae: 111.6102 - val_loss: 9662.2246 - val_mae: 9662.9170\n",
      "Epoch 1133/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 114.2885 - mae: 114.9767 - val_loss: 9610.6113 - val_mae: 9611.3037\n",
      "Epoch 1134/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 120.8757 - mae: 121.5637 - val_loss: 9600.9199 - val_mae: 9601.6133\n",
      "Epoch 1135/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 112.3473 - mae: 113.0316 - val_loss: 9629.6758 - val_mae: 9630.3701\n",
      "Epoch 1136/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 111.0574 - mae: 111.7423 - val_loss: 9395.2266 - val_mae: 9395.9189\n",
      "Epoch 1137/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 112.9267 - mae: 113.6134 - val_loss: 9690.1279 - val_mae: 9690.8213\n",
      "Epoch 1138/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 139.2507 - mae: 139.9382 - val_loss: 9699.4844 - val_mae: 9700.1787\n",
      "Epoch 1139/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 135.1090 - mae: 135.7955 - val_loss: 9482.8965 - val_mae: 9483.5898\n",
      "Epoch 1140/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 117.1272 - mae: 117.8123 - val_loss: 9318.2314 - val_mae: 9318.9238\n",
      "Epoch 1141/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 114.2418 - mae: 114.9270 - val_loss: 9487.4580 - val_mae: 9488.1504\n",
      "Epoch 1142/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 120.4240 - mae: 121.1108 - val_loss: 9643.8984 - val_mae: 9644.5928\n",
      "Epoch 1143/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 117.2407 - mae: 117.9279 - val_loss: 9524.7725 - val_mae: 9525.4658\n",
      "Epoch 1144/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 121.2028 - mae: 121.8905 - val_loss: 9705.2070 - val_mae: 9705.9004\n",
      "Epoch 1145/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 112.3513 - mae: 113.0408 - val_loss: 9497.9248 - val_mae: 9498.6191\n",
      "Epoch 1146/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 129.7372 - mae: 130.4256 - val_loss: 9447.2549 - val_mae: 9447.9492\n",
      "Epoch 1147/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 110.0923 - mae: 110.7807 - val_loss: 9565.4414 - val_mae: 9566.1338\n",
      "Epoch 1148/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 124.7107 - mae: 125.3971 - val_loss: 9461.3008 - val_mae: 9461.9941\n",
      "Epoch 1149/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 127.7004 - mae: 128.3876 - val_loss: 9585.9805 - val_mae: 9586.6719\n",
      "Epoch 1150/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 139.8410 - mae: 140.5278 - val_loss: 9462.8652 - val_mae: 9463.5576\n",
      "Epoch 1151/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 135.7213 - mae: 136.4082 - val_loss: 9554.2676 - val_mae: 9554.9600\n",
      "Epoch 1152/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 108.1731 - mae: 108.8603 - val_loss: 9394.8779 - val_mae: 9395.5713\n",
      "Epoch 1153/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 124.7870 - mae: 125.4758 - val_loss: 9460.2500 - val_mae: 9460.9434\n",
      "Epoch 1154/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 111.2118 - mae: 111.8948 - val_loss: 9571.5400 - val_mae: 9572.2334\n",
      "Epoch 1155/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 125.4042 - mae: 126.0895 - val_loss: 9680.7041 - val_mae: 9681.3965\n",
      "Epoch 1156/5000\n",
      "46/46 [==============================] - 2s 40ms/step - loss: 119.2419 - mae: 119.9289 - val_loss: 9571.1475 - val_mae: 9571.8418\n",
      "Epoch 1157/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 142.7327 - mae: 143.4226 - val_loss: 9525.4229 - val_mae: 9526.1182\n",
      "Epoch 1158/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 122.3710 - mae: 123.0552 - val_loss: 9592.7510 - val_mae: 9593.4443\n",
      "Epoch 1159/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 109.5357 - mae: 110.2249 - val_loss: 9165.3848 - val_mae: 9166.0781\n",
      "Epoch 1160/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 141.9424 - mae: 142.6281 - val_loss: 9653.6533 - val_mae: 9654.3486\n",
      "Epoch 1161/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 123.1296 - mae: 123.8148 - val_loss: 9684.6172 - val_mae: 9685.3096\n",
      "Epoch 1162/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 123.3760 - mae: 124.0595 - val_loss: 9627.4990 - val_mae: 9628.1924\n",
      "Epoch 1163/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 127.2325 - mae: 127.9191 - val_loss: 9934.6357 - val_mae: 9935.3301\n",
      "Epoch 1164/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 125.2456 - mae: 125.9283 - val_loss: 9438.6055 - val_mae: 9439.2969\n",
      "Epoch 1165/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 115.9692 - mae: 116.6588 - val_loss: 9427.2129 - val_mae: 9427.9072\n",
      "Epoch 1166/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 119.3135 - mae: 120.0022 - val_loss: 9836.4736 - val_mae: 9837.1660\n",
      "Epoch 1167/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 119.5154 - mae: 120.2044 - val_loss: 9659.3818 - val_mae: 9660.0752\n",
      "Epoch 1168/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 109.8758 - mae: 110.5630 - val_loss: 9541.7432 - val_mae: 9542.4346\n",
      "Epoch 1169/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 116.4051 - mae: 117.0927 - val_loss: 9633.2598 - val_mae: 9633.9531\n",
      "Epoch 1170/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 108.3359 - mae: 109.0214 - val_loss: 9608.2246 - val_mae: 9608.9180\n",
      "Epoch 1171/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 116.6166 - mae: 117.3054 - val_loss: 9370.6143 - val_mae: 9371.3076\n",
      "Epoch 1172/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 142.3894 - mae: 143.0767 - val_loss: 9762.9336 - val_mae: 9763.6260\n",
      "Epoch 1173/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 159.3668 - mae: 160.0556 - val_loss: 9667.1787 - val_mae: 9667.8721\n",
      "Epoch 1174/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 132.0616 - mae: 132.7511 - val_loss: 9631.1758 - val_mae: 9631.8691\n",
      "Epoch 1175/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 108.9971 - mae: 109.6846 - val_loss: 9492.5947 - val_mae: 9493.2891\n",
      "Epoch 1176/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 120.9021 - mae: 121.5876 - val_loss: 9497.4922 - val_mae: 9498.1846\n",
      "Epoch 1177/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 107.0023 - mae: 107.6873 - val_loss: 9270.4199 - val_mae: 9271.1133\n",
      "Epoch 1178/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 120.3518 - mae: 121.0403 - val_loss: 9392.5967 - val_mae: 9393.2891\n",
      "Epoch 1179/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 102.4340 - mae: 103.1218 - val_loss: 9495.0820 - val_mae: 9495.7754\n",
      "Epoch 1180/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 124.7652 - mae: 125.4539 - val_loss: 9459.1113 - val_mae: 9459.8057\n",
      "Epoch 1181/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 106.9469 - mae: 107.6303 - val_loss: 9311.1113 - val_mae: 9311.8037\n",
      "Epoch 1182/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 126.0506 - mae: 126.7378 - val_loss: 9588.9150 - val_mae: 9589.6074\n",
      "Epoch 1183/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 117.9054 - mae: 118.5919 - val_loss: 9593.2861 - val_mae: 9593.9785\n",
      "Epoch 1184/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 105.0568 - mae: 105.7404 - val_loss: 9584.1729 - val_mae: 9584.8672\n",
      "Epoch 1185/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 138.8058 - mae: 139.4938 - val_loss: 9511.4277 - val_mae: 9512.1221\n",
      "Epoch 1186/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 124.2217 - mae: 124.9104 - val_loss: 9751.1191 - val_mae: 9751.8125\n",
      "Epoch 1187/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 124.4261 - mae: 125.1130 - val_loss: 9800.0996 - val_mae: 9800.7920\n",
      "Epoch 1188/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 131.2140 - mae: 131.9050 - val_loss: 9479.9951 - val_mae: 9480.6895\n",
      "Epoch 1189/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 120.2320 - mae: 120.9179 - val_loss: 9263.2559 - val_mae: 9263.9482\n",
      "Epoch 1190/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 129.0945 - mae: 129.7838 - val_loss: 9546.5713 - val_mae: 9547.2646\n",
      "Epoch 1191/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 126.7425 - mae: 127.4308 - val_loss: 9622.6494 - val_mae: 9623.3428\n",
      "Epoch 1192/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 115.1038 - mae: 115.7933 - val_loss: 9589.2422 - val_mae: 9589.9346\n",
      "Epoch 1193/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 106.9702 - mae: 107.6571 - val_loss: 9441.7812 - val_mae: 9442.4746\n",
      "Epoch 1194/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 138.4745 - mae: 139.1625 - val_loss: 9410.1416 - val_mae: 9410.8340\n",
      "Epoch 1195/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 139.0660 - mae: 139.7536 - val_loss: 9548.7393 - val_mae: 9549.4326\n",
      "Epoch 1196/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 117.2079 - mae: 117.8949 - val_loss: 9778.0244 - val_mae: 9778.7178\n",
      "Epoch 1197/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 114.3435 - mae: 115.0322 - val_loss: 9650.0078 - val_mae: 9650.7012\n",
      "Epoch 1198/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 129.3761 - mae: 130.0651 - val_loss: 9308.2119 - val_mae: 9308.9053\n",
      "Epoch 1199/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 116.0033 - mae: 116.6919 - val_loss: 9563.3164 - val_mae: 9564.0098\n",
      "Epoch 1200/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 113.9804 - mae: 114.6683 - val_loss: 9795.1377 - val_mae: 9795.8320\n",
      "Epoch 1201/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 133.3895 - mae: 134.0785 - val_loss: 9678.6934 - val_mae: 9679.3877\n",
      "Epoch 1202/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 110.3935 - mae: 111.0804 - val_loss: 9700.6455 - val_mae: 9701.3389\n",
      "Epoch 1203/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 106.9315 - mae: 107.6197 - val_loss: 9676.7305 - val_mae: 9677.4229\n",
      "Epoch 1204/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 121.4761 - mae: 122.1651 - val_loss: 9397.1680 - val_mae: 9397.8613\n",
      "Epoch 1205/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 136.3800 - mae: 137.0696 - val_loss: 9325.3789 - val_mae: 9326.0713\n",
      "Epoch 1206/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 122.9094 - mae: 123.5960 - val_loss: 9460.0088 - val_mae: 9460.7021\n",
      "Epoch 1207/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 112.4371 - mae: 113.1188 - val_loss: 9462.2754 - val_mae: 9462.9678\n",
      "Epoch 1208/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 118.0625 - mae: 118.7521 - val_loss: 9509.1328 - val_mae: 9509.8262\n",
      "Epoch 1209/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 117.1151 - mae: 117.8025 - val_loss: 9049.5518 - val_mae: 9050.2461\n",
      "Epoch 1210/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 140.5849 - mae: 141.2722 - val_loss: 9550.7373 - val_mae: 9551.4297\n",
      "Epoch 1211/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 119.4246 - mae: 120.1120 - val_loss: 9357.3496 - val_mae: 9358.0410\n",
      "Epoch 1212/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 106.3780 - mae: 107.0615 - val_loss: 9286.5391 - val_mae: 9287.2334\n",
      "Epoch 1213/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 139.1940 - mae: 139.8836 - val_loss: 9635.1484 - val_mae: 9635.8418\n",
      "Epoch 1214/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 122.2096 - mae: 122.8981 - val_loss: 9564.4014 - val_mae: 9565.0947\n",
      "Epoch 1215/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 121.1801 - mae: 121.8642 - val_loss: 9450.6816 - val_mae: 9451.3730\n",
      "Epoch 1216/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 111.3024 - mae: 111.9875 - val_loss: 9497.4609 - val_mae: 9498.1543\n",
      "Epoch 1217/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 119.5060 - mae: 120.1945 - val_loss: 9196.2705 - val_mae: 9196.9619\n",
      "Epoch 1218/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 136.9078 - mae: 137.5979 - val_loss: 9337.1631 - val_mae: 9337.8564\n",
      "Epoch 1219/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 117.4436 - mae: 118.1335 - val_loss: 9654.2510 - val_mae: 9654.9443\n",
      "Epoch 1220/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 104.5537 - mae: 105.2430 - val_loss: 9345.8525 - val_mae: 9346.5479\n",
      "Epoch 1221/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 116.5487 - mae: 117.2359 - val_loss: 9493.8838 - val_mae: 9494.5762\n",
      "Epoch 1222/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 115.1129 - mae: 115.8028 - val_loss: 9537.5615 - val_mae: 9538.2529\n",
      "Epoch 1223/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 127.1022 - mae: 127.7868 - val_loss: 9373.0420 - val_mae: 9373.7344\n",
      "Epoch 1224/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 113.3112 - mae: 113.9986 - val_loss: 9275.0000 - val_mae: 9275.6934\n",
      "Epoch 1225/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 126.7107 - mae: 127.3959 - val_loss: 9785.7051 - val_mae: 9786.3994\n",
      "Epoch 1226/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 124.6824 - mae: 125.3704 - val_loss: 9195.2607 - val_mae: 9195.9551\n",
      "Epoch 1227/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 109.9893 - mae: 110.6754 - val_loss: 9642.6943 - val_mae: 9643.3857\n",
      "Epoch 1228/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 121.2483 - mae: 121.9357 - val_loss: 9348.3203 - val_mae: 9349.0127\n",
      "Epoch 1229/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 114.1119 - mae: 114.7977 - val_loss: 9460.8867 - val_mae: 9461.5791\n",
      "Epoch 1230/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 115.2690 - mae: 115.9544 - val_loss: 9477.5469 - val_mae: 9478.2402\n",
      "Epoch 1231/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 115.9568 - mae: 116.6434 - val_loss: 9585.5977 - val_mae: 9586.2900\n",
      "Epoch 1232/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 133.8837 - mae: 134.5709 - val_loss: 9262.8242 - val_mae: 9263.5176\n",
      "Epoch 1233/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 131.5443 - mae: 132.2334 - val_loss: 9439.2842 - val_mae: 9439.9775\n",
      "Epoch 1234/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 103.4682 - mae: 104.1544 - val_loss: 9782.3486 - val_mae: 9783.0439\n",
      "Epoch 1235/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 128.0281 - mae: 128.7154 - val_loss: 9763.8799 - val_mae: 9764.5723\n",
      "Epoch 1236/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 114.1697 - mae: 114.8575 - val_loss: 9809.3232 - val_mae: 9810.0166\n",
      "Epoch 1237/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 118.9649 - mae: 119.6509 - val_loss: 9740.2520 - val_mae: 9740.9443\n",
      "Epoch 1238/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 106.5155 - mae: 107.2036 - val_loss: 9512.0146 - val_mae: 9512.7061\n",
      "Epoch 1239/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 119.2389 - mae: 119.9262 - val_loss: 9474.1201 - val_mae: 9474.8125\n",
      "Epoch 1240/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 106.2912 - mae: 106.9784 - val_loss: 9587.7949 - val_mae: 9588.4883\n",
      "Epoch 1241/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 122.4218 - mae: 123.1097 - val_loss: 9603.1338 - val_mae: 9603.8281\n",
      "Epoch 1242/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 122.6908 - mae: 123.3764 - val_loss: 9514.6592 - val_mae: 9515.3525\n",
      "Epoch 1243/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 132.4728 - mae: 133.1612 - val_loss: 9740.6182 - val_mae: 9741.3115\n",
      "Epoch 1244/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 118.5063 - mae: 119.1945 - val_loss: 9485.4014 - val_mae: 9486.0938\n",
      "Epoch 1245/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 108.6340 - mae: 109.3212 - val_loss: 9587.0820 - val_mae: 9587.7754\n",
      "Epoch 1246/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 111.4838 - mae: 112.1700 - val_loss: 9354.3652 - val_mae: 9355.0576\n",
      "Epoch 1247/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 118.2889 - mae: 118.9730 - val_loss: 9529.2910 - val_mae: 9529.9844\n",
      "Epoch 1248/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 113.1263 - mae: 113.8130 - val_loss: 9533.6191 - val_mae: 9534.3115\n",
      "Epoch 1249/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 124.8506 - mae: 125.5384 - val_loss: 9305.6016 - val_mae: 9306.2949\n",
      "Epoch 1250/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 118.4057 - mae: 119.0928 - val_loss: 9634.8604 - val_mae: 9635.5547\n",
      "Epoch 1251/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 112.2442 - mae: 112.9295 - val_loss: 9595.4180 - val_mae: 9596.1104\n",
      "Epoch 1252/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 112.9705 - mae: 113.6577 - val_loss: 9338.3789 - val_mae: 9339.0703\n",
      "Epoch 1253/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 115.6795 - mae: 116.3674 - val_loss: 9465.2031 - val_mae: 9465.8955\n",
      "Epoch 1254/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 113.2590 - mae: 113.9453 - val_loss: 9455.0977 - val_mae: 9455.7910\n",
      "Epoch 1255/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 121.9830 - mae: 122.6690 - val_loss: 9654.9609 - val_mae: 9655.6533\n",
      "Epoch 1256/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 124.4723 - mae: 125.1603 - val_loss: 9402.0498 - val_mae: 9402.7461\n",
      "Epoch 1257/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 118.4891 - mae: 119.1760 - val_loss: 9578.3047 - val_mae: 9578.9980\n",
      "Epoch 1258/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 116.7231 - mae: 117.4125 - val_loss: 9598.2627 - val_mae: 9598.9561\n",
      "Epoch 1259/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 105.0501 - mae: 105.7348 - val_loss: 9555.7910 - val_mae: 9556.4854\n",
      "Epoch 1260/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 113.5463 - mae: 114.2361 - val_loss: 9434.0547 - val_mae: 9434.7471\n",
      "Epoch 1261/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 104.8325 - mae: 105.5188 - val_loss: 9485.6338 - val_mae: 9486.3271\n",
      "Epoch 1262/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 118.2969 - mae: 118.9802 - val_loss: 9735.3662 - val_mae: 9736.0586\n",
      "Epoch 1263/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 111.2582 - mae: 111.9425 - val_loss: 9557.5703 - val_mae: 9558.2627\n",
      "Epoch 1264/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 108.3890 - mae: 109.0772 - val_loss: 9694.5781 - val_mae: 9695.2695\n",
      "Epoch 1265/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 124.5054 - mae: 125.1902 - val_loss: 9448.1504 - val_mae: 9448.8428\n",
      "Epoch 1266/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 112.0606 - mae: 112.7459 - val_loss: 9610.1865 - val_mae: 9610.8809\n",
      "Epoch 1267/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 119.2569 - mae: 119.9470 - val_loss: 9306.1113 - val_mae: 9306.8047\n",
      "Epoch 1268/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 112.3272 - mae: 113.0155 - val_loss: 9277.6826 - val_mae: 9278.3779\n",
      "Epoch 1269/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 119.4758 - mae: 120.1665 - val_loss: 9427.5723 - val_mae: 9428.2666\n",
      "Epoch 1270/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 137.1554 - mae: 137.8449 - val_loss: 9456.2861 - val_mae: 9456.9785\n",
      "Epoch 1271/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 116.6341 - mae: 117.3205 - val_loss: 9621.3291 - val_mae: 9622.0215\n",
      "Epoch 1272/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 110.4072 - mae: 111.0941 - val_loss: 9523.9824 - val_mae: 9524.6748\n",
      "Epoch 1273/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 122.7009 - mae: 123.3870 - val_loss: 9506.8682 - val_mae: 9507.5615\n",
      "Epoch 1274/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 116.4499 - mae: 117.1374 - val_loss: 9549.8018 - val_mae: 9550.4961\n",
      "Epoch 1275/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 128.8260 - mae: 129.5107 - val_loss: 9509.9141 - val_mae: 9510.6064\n",
      "Epoch 1276/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 118.8028 - mae: 119.4916 - val_loss: 9391.6250 - val_mae: 9392.3184\n",
      "Epoch 1277/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 114.4101 - mae: 115.0920 - val_loss: 9255.9424 - val_mae: 9256.6357\n",
      "Epoch 1278/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 104.4506 - mae: 105.1327 - val_loss: 9665.1904 - val_mae: 9665.8828\n",
      "Epoch 1279/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 120.2855 - mae: 120.9735 - val_loss: 9402.7168 - val_mae: 9403.4111\n",
      "Epoch 1280/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 116.9745 - mae: 117.6597 - val_loss: 9357.2686 - val_mae: 9357.9609\n",
      "Epoch 1281/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 123.5588 - mae: 124.2459 - val_loss: 9719.6719 - val_mae: 9720.3643\n",
      "Epoch 1282/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 129.7452 - mae: 130.4307 - val_loss: 9324.3418 - val_mae: 9325.0352\n",
      "Epoch 1283/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 112.9301 - mae: 113.6179 - val_loss: 9635.6221 - val_mae: 9636.3154\n",
      "Epoch 1284/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 109.7185 - mae: 110.4044 - val_loss: 9532.6396 - val_mae: 9533.3320\n",
      "Epoch 1285/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 119.2408 - mae: 119.9285 - val_loss: 9339.7812 - val_mae: 9340.4736\n",
      "Epoch 1286/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 129.1352 - mae: 129.8217 - val_loss: 9516.5674 - val_mae: 9517.2607\n",
      "Epoch 1287/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 123.0958 - mae: 123.7810 - val_loss: 9520.8701 - val_mae: 9521.5645\n",
      "Epoch 1288/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 100.3048 - mae: 100.9910 - val_loss: 9641.0908 - val_mae: 9641.7822\n",
      "Epoch 1289/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 108.6662 - mae: 109.3488 - val_loss: 9147.3779 - val_mae: 9148.0684\n",
      "Epoch 1290/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 131.4581 - mae: 132.1472 - val_loss: 9459.4004 - val_mae: 9460.0918\n",
      "Epoch 1291/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 109.0913 - mae: 109.7758 - val_loss: 9429.0332 - val_mae: 9429.7266\n",
      "Epoch 1292/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 108.1378 - mae: 108.8258 - val_loss: 9557.9072 - val_mae: 9558.5986\n",
      "Epoch 1293/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 106.5232 - mae: 107.2100 - val_loss: 9448.4727 - val_mae: 9449.1660\n",
      "Epoch 1294/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 110.1061 - mae: 110.7925 - val_loss: 9406.4551 - val_mae: 9407.1494\n",
      "Epoch 1295/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 118.5345 - mae: 119.2216 - val_loss: 9742.0518 - val_mae: 9742.7461\n",
      "Epoch 1296/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 115.9451 - mae: 116.6298 - val_loss: 9548.7988 - val_mae: 9549.4932\n",
      "Epoch 1297/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 107.1569 - mae: 107.8454 - val_loss: 9388.2432 - val_mae: 9388.9365\n",
      "Epoch 1298/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 105.9307 - mae: 106.6112 - val_loss: 9379.9033 - val_mae: 9380.5967\n",
      "Epoch 1299/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 130.0786 - mae: 130.7684 - val_loss: 9421.4014 - val_mae: 9422.0947\n",
      "Epoch 1300/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 115.2324 - mae: 115.9201 - val_loss: 9621.2627 - val_mae: 9621.9551\n",
      "Epoch 1301/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 111.3288 - mae: 112.0138 - val_loss: 9536.5049 - val_mae: 9537.1973\n",
      "Epoch 1302/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 103.0139 - mae: 103.7018 - val_loss: 9513.2881 - val_mae: 9513.9814\n",
      "Epoch 1303/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 101.9005 - mae: 102.5840 - val_loss: 9525.8398 - val_mae: 9526.5332\n",
      "Epoch 1304/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 114.5370 - mae: 115.2222 - val_loss: 9347.0820 - val_mae: 9347.7754\n",
      "Epoch 1305/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 120.9998 - mae: 121.6842 - val_loss: 9419.3350 - val_mae: 9420.0283\n",
      "Epoch 1306/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 113.6904 - mae: 114.3770 - val_loss: 9429.0830 - val_mae: 9429.7744\n",
      "Epoch 1307/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 116.7599 - mae: 117.4452 - val_loss: 9722.2266 - val_mae: 9722.9189\n",
      "Epoch 1308/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 120.7756 - mae: 121.4604 - val_loss: 9631.3262 - val_mae: 9632.0205\n",
      "Epoch 1309/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 113.3302 - mae: 114.0189 - val_loss: 9457.9258 - val_mae: 9458.6191\n",
      "Epoch 1310/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 105.0073 - mae: 105.6895 - val_loss: 9457.2041 - val_mae: 9457.8975\n",
      "Epoch 1311/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 102.9653 - mae: 103.6532 - val_loss: 9558.4287 - val_mae: 9559.1211\n",
      "Epoch 1312/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 110.7692 - mae: 111.4538 - val_loss: 9689.7295 - val_mae: 9690.4229\n",
      "Epoch 1313/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 112.9839 - mae: 113.6724 - val_loss: 9391.6641 - val_mae: 9392.3574\n",
      "Epoch 1314/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 115.8734 - mae: 116.5590 - val_loss: 9401.4932 - val_mae: 9402.1855\n",
      "Epoch 1315/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 129.4831 - mae: 130.1689 - val_loss: 9503.9648 - val_mae: 9504.6582\n",
      "Epoch 1316/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 131.3004 - mae: 131.9871 - val_loss: 9511.2393 - val_mae: 9511.9336\n",
      "Epoch 1317/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 110.6483 - mae: 111.3380 - val_loss: 9730.5107 - val_mae: 9731.2031\n",
      "Epoch 1318/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 104.2523 - mae: 104.9379 - val_loss: 9293.6025 - val_mae: 9294.2949\n",
      "Epoch 1319/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 106.1384 - mae: 106.8241 - val_loss: 9538.7812 - val_mae: 9539.4736\n",
      "Epoch 1320/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 115.0678 - mae: 115.7525 - val_loss: 9496.9346 - val_mae: 9497.6279\n",
      "Epoch 1321/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 125.2711 - mae: 125.9603 - val_loss: 9421.1836 - val_mae: 9421.8760\n",
      "Epoch 1322/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 105.0751 - mae: 105.7628 - val_loss: 9300.3975 - val_mae: 9301.0898\n",
      "Epoch 1323/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 100.2878 - mae: 100.9700 - val_loss: 9529.1572 - val_mae: 9529.8506\n",
      "Epoch 1324/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 115.3355 - mae: 116.0237 - val_loss: 9263.1406 - val_mae: 9263.8330\n",
      "Epoch 1325/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 116.5171 - mae: 117.2074 - val_loss: 9708.1826 - val_mae: 9708.8760\n",
      "Epoch 1326/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 110.7329 - mae: 111.4194 - val_loss: 9488.5576 - val_mae: 9489.2500\n",
      "Epoch 1327/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 102.4874 - mae: 103.1722 - val_loss: 9234.5049 - val_mae: 9235.1992\n",
      "Epoch 1328/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 120.8871 - mae: 121.5754 - val_loss: 9448.2178 - val_mae: 9448.9111\n",
      "Epoch 1329/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 111.6887 - mae: 112.3751 - val_loss: 9230.9521 - val_mae: 9231.6455\n",
      "Epoch 1330/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 122.9542 - mae: 123.6422 - val_loss: 9798.0459 - val_mae: 9798.7402\n",
      "Epoch 1331/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 113.8069 - mae: 114.4949 - val_loss: 9471.9043 - val_mae: 9472.5977\n",
      "Epoch 1332/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 114.0915 - mae: 114.7772 - val_loss: 9480.9336 - val_mae: 9481.6260\n",
      "Epoch 1333/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 107.1538 - mae: 107.8432 - val_loss: 9416.9268 - val_mae: 9417.6191\n",
      "Epoch 1334/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 111.0643 - mae: 111.7526 - val_loss: 9215.8750 - val_mae: 9216.5674\n",
      "Epoch 1335/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 113.7982 - mae: 114.4848 - val_loss: 9596.4355 - val_mae: 9597.1289\n",
      "Epoch 1336/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 108.1876 - mae: 108.8768 - val_loss: 9479.1309 - val_mae: 9479.8242\n",
      "Epoch 1337/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 102.4048 - mae: 103.0890 - val_loss: 9887.7090 - val_mae: 9888.4014\n",
      "Epoch 1338/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 129.7356 - mae: 130.4222 - val_loss: 9181.4316 - val_mae: 9182.1230\n",
      "Epoch 1339/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 120.7674 - mae: 121.4529 - val_loss: 9366.0762 - val_mae: 9366.7686\n",
      "Epoch 1340/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 102.7561 - mae: 103.4433 - val_loss: 9712.9502 - val_mae: 9713.6445\n",
      "Epoch 1341/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 120.6423 - mae: 121.3298 - val_loss: 9357.9941 - val_mae: 9358.6865\n",
      "Epoch 1342/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 103.7350 - mae: 104.4225 - val_loss: 9337.3809 - val_mae: 9338.0752\n",
      "Epoch 1343/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 110.9995 - mae: 111.6859 - val_loss: 9705.5293 - val_mae: 9706.2227\n",
      "Epoch 1344/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 139.9676 - mae: 140.6556 - val_loss: 9497.0449 - val_mae: 9497.7383\n",
      "Epoch 1345/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 107.7013 - mae: 108.3869 - val_loss: 9652.8330 - val_mae: 9653.5264\n",
      "Epoch 1346/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 101.1162 - mae: 101.8023 - val_loss: 9495.7627 - val_mae: 9496.4551\n",
      "Epoch 1347/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 114.4388 - mae: 115.1239 - val_loss: 9291.9629 - val_mae: 9292.6562\n",
      "Epoch 1348/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 110.4938 - mae: 111.1817 - val_loss: 9805.5938 - val_mae: 9806.2861\n",
      "Epoch 1349/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 103.1821 - mae: 103.8688 - val_loss: 9602.3135 - val_mae: 9603.0059\n",
      "Epoch 1350/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 104.5948 - mae: 105.2816 - val_loss: 9440.0850 - val_mae: 9440.7783\n",
      "Epoch 1351/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 107.2202 - mae: 107.9047 - val_loss: 9685.2041 - val_mae: 9685.8965\n",
      "Epoch 1352/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 126.8674 - mae: 127.5551 - val_loss: 9446.5703 - val_mae: 9447.2627\n",
      "Epoch 1353/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 111.7667 - mae: 112.4529 - val_loss: 9542.2295 - val_mae: 9542.9219\n",
      "Epoch 1354/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 107.0784 - mae: 107.7656 - val_loss: 9435.3838 - val_mae: 9436.0781\n",
      "Epoch 1355/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 120.5521 - mae: 121.2371 - val_loss: 9395.6758 - val_mae: 9396.3701\n",
      "Epoch 1356/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 115.2896 - mae: 115.9745 - val_loss: 9474.2227 - val_mae: 9474.9150\n",
      "Epoch 1357/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 105.4867 - mae: 106.1707 - val_loss: 9418.1250 - val_mae: 9418.8164\n",
      "Epoch 1358/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 122.8070 - mae: 123.4924 - val_loss: 9632.1338 - val_mae: 9632.8262\n",
      "Epoch 1359/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 114.4251 - mae: 115.1131 - val_loss: 9372.0166 - val_mae: 9372.7100\n",
      "Epoch 1360/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 109.1667 - mae: 109.8559 - val_loss: 9550.5322 - val_mae: 9551.2256\n",
      "Epoch 1361/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 100.4466 - mae: 101.1347 - val_loss: 9583.8652 - val_mae: 9584.5576\n",
      "Epoch 1362/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 104.0299 - mae: 104.7179 - val_loss: 9538.1152 - val_mae: 9538.8086\n",
      "Epoch 1363/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 106.9759 - mae: 107.6635 - val_loss: 9411.9482 - val_mae: 9412.6416\n",
      "Epoch 1364/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 101.0032 - mae: 101.6909 - val_loss: 9633.9756 - val_mae: 9634.6699\n",
      "Epoch 1365/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 115.0900 - mae: 115.7772 - val_loss: 9253.9863 - val_mae: 9254.6797\n",
      "Epoch 1366/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 108.3066 - mae: 108.9932 - val_loss: 9472.3066 - val_mae: 9473.0000\n",
      "Epoch 1367/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 105.9053 - mae: 106.5929 - val_loss: 9320.4512 - val_mae: 9321.1455\n",
      "Epoch 1368/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 120.8254 - mae: 121.5111 - val_loss: 9471.4668 - val_mae: 9472.1592\n",
      "Epoch 1369/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 115.1851 - mae: 115.8704 - val_loss: 9265.8496 - val_mae: 9266.5439\n",
      "Epoch 1370/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 113.4649 - mae: 114.1515 - val_loss: 9245.7646 - val_mae: 9246.4570\n",
      "Epoch 1371/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 103.5539 - mae: 104.2401 - val_loss: 9317.6318 - val_mae: 9318.3232\n",
      "Epoch 1372/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 98.6294 - mae: 99.3137 - val_loss: 9384.9580 - val_mae: 9385.6514\n",
      "Epoch 1373/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 103.8834 - mae: 104.5718 - val_loss: 9304.2178 - val_mae: 9304.9102\n",
      "Epoch 1374/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 96.1235 - mae: 96.8097 - val_loss: 9457.8271 - val_mae: 9458.5195\n",
      "Epoch 1375/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 90.5773 - mae: 91.2615 - val_loss: 9449.2246 - val_mae: 9449.9180\n",
      "Epoch 1376/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 110.9231 - mae: 111.6088 - val_loss: 9102.2217 - val_mae: 9102.9150\n",
      "Epoch 1377/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 104.7807 - mae: 105.4694 - val_loss: 9651.4043 - val_mae: 9652.0977\n",
      "Epoch 1378/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 109.8006 - mae: 110.4874 - val_loss: 9376.7080 - val_mae: 9377.3994\n",
      "Epoch 1379/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 116.4598 - mae: 117.1480 - val_loss: 9438.5107 - val_mae: 9439.2041\n",
      "Epoch 1380/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 97.4753 - mae: 98.1652 - val_loss: 9457.0742 - val_mae: 9457.7686\n",
      "Epoch 1381/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 103.1782 - mae: 103.8681 - val_loss: 9633.1934 - val_mae: 9633.8867\n",
      "Epoch 1382/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 111.1246 - mae: 111.8107 - val_loss: 9383.7617 - val_mae: 9384.4541\n",
      "Epoch 1383/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 101.1749 - mae: 101.8610 - val_loss: 9066.5742 - val_mae: 9067.2666\n",
      "Epoch 1384/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 115.1418 - mae: 115.8255 - val_loss: 9375.1582 - val_mae: 9375.8506\n",
      "Epoch 1385/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 113.2675 - mae: 113.9539 - val_loss: 9615.7861 - val_mae: 9616.4795\n",
      "Epoch 1386/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 106.7590 - mae: 107.4478 - val_loss: 9636.2637 - val_mae: 9636.9570\n",
      "Epoch 1387/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 105.4590 - mae: 106.1474 - val_loss: 9695.8770 - val_mae: 9696.5703\n",
      "Epoch 1388/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 105.8287 - mae: 106.5129 - val_loss: 9621.5449 - val_mae: 9622.2373\n",
      "Epoch 1389/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 111.8226 - mae: 112.5084 - val_loss: 9574.1016 - val_mae: 9574.7949\n",
      "Epoch 1390/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 98.9424 - mae: 99.6284 - val_loss: 9437.8047 - val_mae: 9438.4990\n",
      "Epoch 1391/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 117.3402 - mae: 118.0254 - val_loss: 9346.8760 - val_mae: 9347.5693\n",
      "Epoch 1392/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 141.5606 - mae: 142.2501 - val_loss: 9383.2285 - val_mae: 9383.9199\n",
      "Epoch 1393/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 122.0790 - mae: 122.7663 - val_loss: 9261.7432 - val_mae: 9262.4365\n",
      "Epoch 1394/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 98.6480 - mae: 99.3345 - val_loss: 9459.9414 - val_mae: 9460.6338\n",
      "Epoch 1395/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 98.2171 - mae: 98.9007 - val_loss: 9411.8574 - val_mae: 9412.5508\n",
      "Epoch 1396/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 110.5974 - mae: 111.2840 - val_loss: 9401.6914 - val_mae: 9402.3848\n",
      "Epoch 1397/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 117.4333 - mae: 118.1193 - val_loss: 9618.4062 - val_mae: 9619.1006\n",
      "Epoch 1398/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 104.5900 - mae: 105.2771 - val_loss: 9588.0820 - val_mae: 9588.7744\n",
      "Epoch 1399/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 105.3016 - mae: 105.9877 - val_loss: 9389.4619 - val_mae: 9390.1562\n",
      "Epoch 1400/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 103.9710 - mae: 104.6564 - val_loss: 9530.9941 - val_mae: 9531.6865\n",
      "Epoch 1401/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 107.9574 - mae: 108.6450 - val_loss: 9624.3818 - val_mae: 9625.0752\n",
      "Epoch 1402/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 117.9940 - mae: 118.6781 - val_loss: 9722.1562 - val_mae: 9722.8496\n",
      "Epoch 1403/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 105.5650 - mae: 106.2520 - val_loss: 9550.3750 - val_mae: 9551.0674\n",
      "Epoch 1404/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 128.0561 - mae: 128.7460 - val_loss: 9456.7949 - val_mae: 9457.4863\n",
      "Epoch 1405/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 96.2076 - mae: 96.8934 - val_loss: 9729.0889 - val_mae: 9729.7822\n",
      "Epoch 1406/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 124.7724 - mae: 125.4601 - val_loss: 9673.9951 - val_mae: 9674.6885\n",
      "Epoch 1407/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 110.5807 - mae: 111.2667 - val_loss: 9470.7910 - val_mae: 9471.4844\n",
      "Epoch 1408/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 115.0665 - mae: 115.7546 - val_loss: 9673.3838 - val_mae: 9674.0771\n",
      "Epoch 1409/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 96.0701 - mae: 96.7583 - val_loss: 9435.3594 - val_mae: 9436.0527\n",
      "Epoch 1410/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 116.2709 - mae: 116.9539 - val_loss: 9382.8105 - val_mae: 9383.5049\n",
      "Epoch 1411/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 117.0964 - mae: 117.7849 - val_loss: 9389.8447 - val_mae: 9390.5391\n",
      "Epoch 1412/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 112.3740 - mae: 113.0616 - val_loss: 9736.5254 - val_mae: 9737.2178\n",
      "Epoch 1413/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 119.0473 - mae: 119.7363 - val_loss: 9604.8428 - val_mae: 9605.5352\n",
      "Epoch 1414/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 111.0681 - mae: 111.7543 - val_loss: 9565.9932 - val_mae: 9566.6846\n",
      "Epoch 1415/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 115.0770 - mae: 115.7660 - val_loss: 9355.4404 - val_mae: 9356.1338\n",
      "Epoch 1416/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 122.3084 - mae: 122.9949 - val_loss: 9388.8740 - val_mae: 9389.5674\n",
      "Epoch 1417/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 103.1726 - mae: 103.8611 - val_loss: 9450.7646 - val_mae: 9451.4561\n",
      "Epoch 1418/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 118.6714 - mae: 119.3604 - val_loss: 9799.6270 - val_mae: 9800.3184\n",
      "Epoch 1419/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 119.4777 - mae: 120.1679 - val_loss: 9707.1943 - val_mae: 9707.8877\n",
      "Epoch 1420/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 104.6729 - mae: 105.3587 - val_loss: 9669.1895 - val_mae: 9669.8809\n",
      "Epoch 1421/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 101.8109 - mae: 102.4976 - val_loss: 9326.2705 - val_mae: 9326.9648\n",
      "Epoch 1422/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 118.2953 - mae: 118.9813 - val_loss: 9664.3545 - val_mae: 9665.0479\n",
      "Epoch 1423/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 124.3419 - mae: 125.0304 - val_loss: 9566.1309 - val_mae: 9566.8242\n",
      "Epoch 1424/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 101.2393 - mae: 101.9204 - val_loss: 9303.8242 - val_mae: 9304.5186\n",
      "Epoch 1425/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 104.4396 - mae: 105.1267 - val_loss: 9349.5957 - val_mae: 9350.2881\n",
      "Epoch 1426/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 101.4066 - mae: 102.0929 - val_loss: 9521.2002 - val_mae: 9521.8936\n",
      "Epoch 1427/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 110.9885 - mae: 111.6735 - val_loss: 9542.9346 - val_mae: 9543.6270\n",
      "Epoch 1428/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 101.0993 - mae: 101.7877 - val_loss: 9445.7500 - val_mae: 9446.4443\n",
      "Epoch 1429/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 97.1030 - mae: 97.7874 - val_loss: 9322.3828 - val_mae: 9323.0742\n",
      "Epoch 1430/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 98.5553 - mae: 99.2442 - val_loss: 9569.1436 - val_mae: 9569.8369\n",
      "Epoch 1431/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 120.5332 - mae: 121.2208 - val_loss: 9439.1797 - val_mae: 9439.8730\n",
      "Epoch 1432/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 99.1133 - mae: 99.8001 - val_loss: 9847.1152 - val_mae: 9847.8086\n",
      "Epoch 1433/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 113.7905 - mae: 114.4791 - val_loss: 9304.3779 - val_mae: 9305.0723\n",
      "Epoch 1434/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 109.6823 - mae: 110.3672 - val_loss: 9440.5557 - val_mae: 9441.2490\n",
      "Epoch 1435/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 119.7787 - mae: 120.4663 - val_loss: 9275.5625 - val_mae: 9276.2549\n",
      "Epoch 1436/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 127.4918 - mae: 128.1795 - val_loss: 9361.9336 - val_mae: 9362.6250\n",
      "Epoch 1437/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 109.8634 - mae: 110.5462 - val_loss: 9515.8438 - val_mae: 9516.5352\n",
      "Epoch 1438/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 109.3731 - mae: 110.0616 - val_loss: 9425.7832 - val_mae: 9426.4756\n",
      "Epoch 1439/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 103.1822 - mae: 103.8664 - val_loss: 9514.1104 - val_mae: 9514.8047\n",
      "Epoch 1440/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 102.7424 - mae: 103.4289 - val_loss: 9802.3838 - val_mae: 9803.0771\n",
      "Epoch 1441/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 121.6518 - mae: 122.3392 - val_loss: 9254.1396 - val_mae: 9254.8320\n",
      "Epoch 1442/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 122.6993 - mae: 123.3854 - val_loss: 9528.7100 - val_mae: 9529.4023\n",
      "Epoch 1443/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 105.1706 - mae: 105.8569 - val_loss: 9309.0723 - val_mae: 9309.7666\n",
      "Epoch 1444/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 101.2085 - mae: 101.8948 - val_loss: 9366.1680 - val_mae: 9366.8623\n",
      "Epoch 1445/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 96.6401 - mae: 97.3267 - val_loss: 9355.0859 - val_mae: 9355.7803\n",
      "Epoch 1446/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 129.8538 - mae: 130.5424 - val_loss: 9394.0557 - val_mae: 9394.7500\n",
      "Epoch 1447/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 97.6964 - mae: 98.3769 - val_loss: 9372.7363 - val_mae: 9373.4287\n",
      "Epoch 1448/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 109.9417 - mae: 110.6292 - val_loss: 9503.8008 - val_mae: 9504.4932\n",
      "Epoch 1449/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 106.8128 - mae: 107.4992 - val_loss: 9235.3799 - val_mae: 9236.0732\n",
      "Epoch 1450/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 112.1970 - mae: 112.8815 - val_loss: 9505.1045 - val_mae: 9505.7979\n",
      "Epoch 1451/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 104.4032 - mae: 105.0868 - val_loss: 9275.7500 - val_mae: 9276.4443\n",
      "Epoch 1452/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 120.8821 - mae: 121.5713 - val_loss: 9339.8955 - val_mae: 9340.5889\n",
      "Epoch 1453/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 116.5743 - mae: 117.2620 - val_loss: 9495.4111 - val_mae: 9496.1035\n",
      "Epoch 1454/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 109.4438 - mae: 110.1293 - val_loss: 8892.9863 - val_mae: 8893.6797\n",
      "Epoch 1455/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 138.4354 - mae: 139.1250 - val_loss: 9282.9287 - val_mae: 9283.6221\n",
      "Epoch 1456/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 105.4355 - mae: 106.1214 - val_loss: 9462.8691 - val_mae: 9463.5635\n",
      "Epoch 1457/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 104.2856 - mae: 104.9713 - val_loss: 9535.5850 - val_mae: 9536.2773\n",
      "Epoch 1458/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 104.1314 - mae: 104.8180 - val_loss: 9493.0791 - val_mae: 9493.7725\n",
      "Epoch 1459/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 116.4204 - mae: 117.1085 - val_loss: 9889.2295 - val_mae: 9889.9238\n",
      "Epoch 1460/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 129.9520 - mae: 130.6407 - val_loss: 9383.6289 - val_mae: 9384.3232\n",
      "Epoch 1461/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 114.1264 - mae: 114.8154 - val_loss: 9685.7021 - val_mae: 9686.3945\n",
      "Epoch 1462/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 108.3064 - mae: 108.9900 - val_loss: 9598.5557 - val_mae: 9599.2500\n",
      "Epoch 1463/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 101.9559 - mae: 102.6432 - val_loss: 9571.6504 - val_mae: 9572.3438\n",
      "Epoch 1464/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 96.1045 - mae: 96.7922 - val_loss: 9600.5742 - val_mae: 9601.2666\n",
      "Epoch 1465/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 125.4606 - mae: 126.1470 - val_loss: 9293.9668 - val_mae: 9294.6602\n",
      "Epoch 1466/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 122.5170 - mae: 123.2076 - val_loss: 9530.4619 - val_mae: 9531.1562\n",
      "Epoch 1467/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 128.1794 - mae: 128.8688 - val_loss: 9505.6660 - val_mae: 9506.3604\n",
      "Epoch 1468/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 99.2617 - mae: 99.9462 - val_loss: 9535.5117 - val_mae: 9536.2051\n",
      "Epoch 1469/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 116.5440 - mae: 117.2316 - val_loss: 9221.8516 - val_mae: 9222.5459\n",
      "Epoch 1470/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 115.5426 - mae: 116.2282 - val_loss: 9447.3965 - val_mae: 9448.0889\n",
      "Epoch 1471/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 110.7292 - mae: 111.4146 - val_loss: 9197.1836 - val_mae: 9197.8779\n",
      "Epoch 1472/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 112.9570 - mae: 113.6377 - val_loss: 9149.0830 - val_mae: 9149.7754\n",
      "Epoch 1473/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 113.7799 - mae: 114.4661 - val_loss: 9119.7178 - val_mae: 9120.4111\n",
      "Epoch 1474/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 129.2900 - mae: 129.9757 - val_loss: 9383.7236 - val_mae: 9384.4170\n",
      "Epoch 1475/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 102.1523 - mae: 102.8358 - val_loss: 9672.9404 - val_mae: 9673.6318\n",
      "Epoch 1476/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 124.1387 - mae: 124.8273 - val_loss: 9559.9131 - val_mae: 9560.6055\n",
      "Epoch 1477/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 91.8680 - mae: 92.5526 - val_loss: 9913.5000 - val_mae: 9914.1924\n",
      "Epoch 1478/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 119.7830 - mae: 120.4702 - val_loss: 9546.9189 - val_mae: 9547.6133\n",
      "Epoch 1479/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 118.9989 - mae: 119.6857 - val_loss: 9315.5732 - val_mae: 9316.2656\n",
      "Epoch 1480/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 113.3958 - mae: 114.0804 - val_loss: 9407.7549 - val_mae: 9408.4502\n",
      "Epoch 1481/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 98.7852 - mae: 99.4676 - val_loss: 9588.6904 - val_mae: 9589.3828\n",
      "Epoch 1482/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 112.6681 - mae: 113.3559 - val_loss: 9489.0537 - val_mae: 9489.7480\n",
      "Epoch 1483/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 99.6484 - mae: 100.3331 - val_loss: 9371.8066 - val_mae: 9372.4980\n",
      "Epoch 1484/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 105.7400 - mae: 106.4266 - val_loss: 9281.9180 - val_mae: 9282.6104\n",
      "Epoch 1485/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 111.2475 - mae: 111.9329 - val_loss: 9833.0498 - val_mae: 9833.7441\n",
      "Epoch 1486/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 136.9829 - mae: 137.6722 - val_loss: 9527.9961 - val_mae: 9528.6895\n",
      "Epoch 1487/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 106.8291 - mae: 107.5163 - val_loss: 9445.9980 - val_mae: 9446.6895\n",
      "Epoch 1488/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 97.3186 - mae: 98.0055 - val_loss: 9482.9893 - val_mae: 9483.6826\n",
      "Epoch 1489/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 109.9651 - mae: 110.6542 - val_loss: 9467.2617 - val_mae: 9467.9551\n",
      "Epoch 1490/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 105.3048 - mae: 105.9874 - val_loss: 9162.9287 - val_mae: 9163.6230\n",
      "Epoch 1491/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 107.0945 - mae: 107.7800 - val_loss: 9478.2158 - val_mae: 9478.9082\n",
      "Epoch 1492/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 99.9288 - mae: 100.6107 - val_loss: 9397.5771 - val_mae: 9398.2715\n",
      "Epoch 1493/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 103.7099 - mae: 104.3928 - val_loss: 9566.5322 - val_mae: 9567.2256\n",
      "Epoch 1494/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 116.9560 - mae: 117.6395 - val_loss: 9454.1484 - val_mae: 9454.8418\n",
      "Epoch 1495/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 110.1211 - mae: 110.8100 - val_loss: 9377.9756 - val_mae: 9378.6699\n",
      "Epoch 1496/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 117.1735 - mae: 117.8623 - val_loss: 9531.1924 - val_mae: 9531.8857\n",
      "Epoch 1497/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 127.3965 - mae: 128.0820 - val_loss: 8943.5898 - val_mae: 8944.2822\n",
      "Epoch 1498/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 129.8621 - mae: 130.5504 - val_loss: 9233.5889 - val_mae: 9234.2822\n",
      "Epoch 1499/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 101.6214 - mae: 102.3082 - val_loss: 9519.1631 - val_mae: 9519.8555\n",
      "Epoch 1500/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 107.5776 - mae: 108.2619 - val_loss: 9428.4375 - val_mae: 9429.1299\n",
      "Epoch 1501/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 119.7683 - mae: 120.4561 - val_loss: 9266.1201 - val_mae: 9266.8135\n",
      "Epoch 1502/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 102.4394 - mae: 103.1282 - val_loss: 9403.5420 - val_mae: 9404.2334\n",
      "Epoch 1503/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 98.2613 - mae: 98.9446 - val_loss: 9473.7529 - val_mae: 9474.4473\n",
      "Epoch 1504/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 105.9658 - mae: 106.6534 - val_loss: 9732.7041 - val_mae: 9733.3955\n",
      "Epoch 1505/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 108.8328 - mae: 109.5207 - val_loss: 9556.3047 - val_mae: 9556.9980\n",
      "Epoch 1506/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 96.7634 - mae: 97.4484 - val_loss: 9511.7246 - val_mae: 9512.4189\n",
      "Epoch 1507/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 97.0437 - mae: 97.7261 - val_loss: 9478.8066 - val_mae: 9479.4990\n",
      "Epoch 1508/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 97.5801 - mae: 98.2661 - val_loss: 9599.0762 - val_mae: 9599.7686\n",
      "Epoch 1509/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 106.0328 - mae: 106.7224 - val_loss: 9477.6006 - val_mae: 9478.2939\n",
      "Epoch 1510/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 99.5409 - mae: 100.2267 - val_loss: 9371.6064 - val_mae: 9372.2998\n",
      "Epoch 1511/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 101.2164 - mae: 101.9025 - val_loss: 9436.5703 - val_mae: 9437.2637\n",
      "Epoch 1512/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 94.5670 - mae: 95.2537 - val_loss: 9633.4160 - val_mae: 9634.1094\n",
      "Epoch 1513/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 112.4107 - mae: 113.0939 - val_loss: 9784.8203 - val_mae: 9785.5146\n",
      "Epoch 1514/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 125.0426 - mae: 125.7305 - val_loss: 9191.2803 - val_mae: 9191.9736\n",
      "Epoch 1515/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 129.7550 - mae: 130.4456 - val_loss: 9331.1279 - val_mae: 9331.8223\n",
      "Epoch 1516/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 107.2050 - mae: 107.8908 - val_loss: 9210.6182 - val_mae: 9211.3115\n",
      "Epoch 1517/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 107.6948 - mae: 108.3831 - val_loss: 9543.3301 - val_mae: 9544.0244\n",
      "Epoch 1518/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 100.4668 - mae: 101.1484 - val_loss: 9360.0742 - val_mae: 9360.7666\n",
      "Epoch 1519/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 115.5301 - mae: 116.2152 - val_loss: 9355.5791 - val_mae: 9356.2725\n",
      "Epoch 1520/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 104.5254 - mae: 105.2087 - val_loss: 9401.9629 - val_mae: 9402.6562\n",
      "Epoch 1521/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 108.2888 - mae: 108.9762 - val_loss: 9362.3975 - val_mae: 9363.0898\n",
      "Epoch 1522/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 119.3044 - mae: 119.9917 - val_loss: 9416.8955 - val_mae: 9417.5889\n",
      "Epoch 1523/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 111.3947 - mae: 112.0841 - val_loss: 9297.0400 - val_mae: 9297.7324\n",
      "Epoch 1524/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 127.1997 - mae: 127.8872 - val_loss: 9556.6113 - val_mae: 9557.3047\n",
      "Epoch 1525/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 120.9687 - mae: 121.6516 - val_loss: 9623.2227 - val_mae: 9623.9150\n",
      "Epoch 1526/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 116.7620 - mae: 117.4496 - val_loss: 9382.6562 - val_mae: 9383.3486\n",
      "Epoch 1527/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 112.9911 - mae: 113.6796 - val_loss: 9455.0977 - val_mae: 9455.7891\n",
      "Epoch 1528/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 98.6255 - mae: 99.3151 - val_loss: 9598.0293 - val_mae: 9598.7217\n",
      "Epoch 1529/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 124.3846 - mae: 125.0688 - val_loss: 9349.5830 - val_mae: 9350.2754\n",
      "Epoch 1530/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 118.0467 - mae: 118.7339 - val_loss: 9634.7500 - val_mae: 9635.4434\n",
      "Epoch 1531/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 106.6615 - mae: 107.3466 - val_loss: 9420.7480 - val_mae: 9421.4414\n",
      "Epoch 1532/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 100.3848 - mae: 101.0701 - val_loss: 9253.9902 - val_mae: 9254.6826\n",
      "Epoch 1533/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 114.3674 - mae: 115.0525 - val_loss: 9325.8730 - val_mae: 9326.5654\n",
      "Epoch 1534/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 105.9314 - mae: 106.6184 - val_loss: 9700.7373 - val_mae: 9701.4297\n",
      "Epoch 1535/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 123.7432 - mae: 124.4329 - val_loss: 9368.4824 - val_mae: 9369.1748\n",
      "Epoch 1536/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 102.5922 - mae: 103.2802 - val_loss: 9542.6465 - val_mae: 9543.3398\n",
      "Epoch 1537/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 112.3759 - mae: 113.0602 - val_loss: 9200.7998 - val_mae: 9201.4922\n",
      "Epoch 1538/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 106.8512 - mae: 107.5389 - val_loss: 9323.5264 - val_mae: 9324.2217\n",
      "Epoch 1539/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 101.5120 - mae: 102.1970 - val_loss: 9577.2236 - val_mae: 9577.9170\n",
      "Epoch 1540/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 120.5430 - mae: 121.2281 - val_loss: 9284.9893 - val_mae: 9285.6826\n",
      "Epoch 1541/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 122.2296 - mae: 122.9175 - val_loss: 9390.1504 - val_mae: 9390.8438\n",
      "Epoch 1542/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 100.3079 - mae: 100.9940 - val_loss: 9677.1787 - val_mae: 9677.8721\n",
      "Epoch 1543/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 105.8732 - mae: 106.5604 - val_loss: 9500.5371 - val_mae: 9501.2295\n",
      "Epoch 1544/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 109.2233 - mae: 109.9090 - val_loss: 9430.5195 - val_mae: 9431.2129\n",
      "Epoch 1545/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 124.2651 - mae: 124.9507 - val_loss: 9305.0752 - val_mae: 9305.7676\n",
      "Epoch 1546/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 117.2955 - mae: 117.9844 - val_loss: 9479.6611 - val_mae: 9480.3535\n",
      "Epoch 1547/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 113.7086 - mae: 114.3978 - val_loss: 9307.8740 - val_mae: 9308.5674\n",
      "Epoch 1548/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 128.8920 - mae: 129.5784 - val_loss: 9377.1494 - val_mae: 9377.8428\n",
      "Epoch 1549/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 105.0860 - mae: 105.7679 - val_loss: 9381.0059 - val_mae: 9381.6992\n",
      "Epoch 1550/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 91.6245 - mae: 92.3107 - val_loss: 9478.8057 - val_mae: 9479.4990\n",
      "Epoch 1551/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 114.7004 - mae: 115.3868 - val_loss: 9432.2529 - val_mae: 9432.9482\n",
      "Epoch 1552/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 92.6654 - mae: 93.3524 - val_loss: 9426.0830 - val_mae: 9426.7773\n",
      "Epoch 1553/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 109.6010 - mae: 110.2899 - val_loss: 9452.6436 - val_mae: 9453.3369\n",
      "Epoch 1554/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 116.8921 - mae: 117.5745 - val_loss: 9451.4551 - val_mae: 9452.1494\n",
      "Epoch 1555/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 102.4194 - mae: 103.1045 - val_loss: 9245.3887 - val_mae: 9246.0830\n",
      "Epoch 1556/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 108.6967 - mae: 109.3861 - val_loss: 9155.5410 - val_mae: 9156.2334\n",
      "Epoch 1557/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 124.4969 - mae: 125.1844 - val_loss: 9147.1318 - val_mae: 9147.8242\n",
      "Epoch 1558/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 108.5040 - mae: 109.1886 - val_loss: 9482.4082 - val_mae: 9483.1025\n",
      "Epoch 1559/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 107.0462 - mae: 107.7323 - val_loss: 9285.9775 - val_mae: 9286.6699\n",
      "Epoch 1560/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 104.3237 - mae: 105.0108 - val_loss: 9381.8525 - val_mae: 9382.5459\n",
      "Epoch 1561/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 94.6810 - mae: 95.3681 - val_loss: 9595.9502 - val_mae: 9596.6436\n",
      "Epoch 1562/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 119.9569 - mae: 120.6428 - val_loss: 9489.9922 - val_mae: 9490.6846\n",
      "Epoch 1563/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 104.9902 - mae: 105.6767 - val_loss: 9540.5010 - val_mae: 9541.1943\n",
      "Epoch 1564/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 117.3610 - mae: 118.0502 - val_loss: 9526.8652 - val_mae: 9527.5586\n",
      "Epoch 1565/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 97.9910 - mae: 98.6799 - val_loss: 9367.1406 - val_mae: 9367.8340\n",
      "Epoch 1566/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 93.1746 - mae: 93.8602 - val_loss: 9556.9629 - val_mae: 9557.6553\n",
      "Epoch 1567/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 103.7981 - mae: 104.4860 - val_loss: 9364.5332 - val_mae: 9365.2275\n",
      "Epoch 1568/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 120.7414 - mae: 121.4283 - val_loss: 9534.7881 - val_mae: 9535.4805\n",
      "Epoch 1569/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 108.6243 - mae: 109.3129 - val_loss: 9252.3135 - val_mae: 9253.0059\n",
      "Epoch 1570/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 117.8469 - mae: 118.5350 - val_loss: 9281.2061 - val_mae: 9281.8984\n",
      "Epoch 1571/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 97.3461 - mae: 98.0310 - val_loss: 9518.7734 - val_mae: 9519.4658\n",
      "Epoch 1572/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 99.5436 - mae: 100.2295 - val_loss: 9557.6143 - val_mae: 9558.3066\n",
      "Epoch 1573/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 101.1477 - mae: 101.8331 - val_loss: 9395.0762 - val_mae: 9395.7695\n",
      "Epoch 1574/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 114.4167 - mae: 115.1044 - val_loss: 9366.9824 - val_mae: 9367.6758\n",
      "Epoch 1575/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 117.2229 - mae: 117.9099 - val_loss: 9456.3105 - val_mae: 9457.0039\n",
      "Epoch 1576/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 98.1901 - mae: 98.8766 - val_loss: 9579.0371 - val_mae: 9579.7314\n",
      "Epoch 1577/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 93.9464 - mae: 94.6337 - val_loss: 9520.4004 - val_mae: 9521.0918\n",
      "Epoch 1578/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 105.8804 - mae: 106.5669 - val_loss: 9432.9248 - val_mae: 9433.6182\n",
      "Epoch 1579/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 122.9507 - mae: 123.6393 - val_loss: 9388.4053 - val_mae: 9389.0967\n",
      "Epoch 1580/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 102.0452 - mae: 102.7319 - val_loss: 9157.2275 - val_mae: 9157.9209\n",
      "Epoch 1581/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 117.5281 - mae: 118.2145 - val_loss: 9525.2637 - val_mae: 9525.9570\n",
      "Epoch 1582/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 97.9830 - mae: 98.6719 - val_loss: 9252.7734 - val_mae: 9253.4678\n",
      "Epoch 1583/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 117.2838 - mae: 117.9696 - val_loss: 9457.1514 - val_mae: 9457.8447\n",
      "Epoch 1584/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 95.0140 - mae: 95.6987 - val_loss: 9258.9922 - val_mae: 9259.6855\n",
      "Epoch 1585/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 104.9609 - mae: 105.6463 - val_loss: 9258.4209 - val_mae: 9259.1152\n",
      "Epoch 1586/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 118.6337 - mae: 119.3230 - val_loss: 9444.5732 - val_mae: 9445.2656\n",
      "Epoch 1587/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 109.0112 - mae: 109.6996 - val_loss: 9395.9902 - val_mae: 9396.6836\n",
      "Epoch 1588/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 93.1426 - mae: 93.8286 - val_loss: 9417.0713 - val_mae: 9417.7646\n",
      "Epoch 1589/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 100.4516 - mae: 101.1367 - val_loss: 9371.6406 - val_mae: 9372.3350\n",
      "Epoch 1590/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 102.0122 - mae: 102.6962 - val_loss: 9596.2900 - val_mae: 9596.9824\n",
      "Epoch 1591/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 103.3001 - mae: 103.9901 - val_loss: 9318.0879 - val_mae: 9318.7803\n",
      "Epoch 1592/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 101.0312 - mae: 101.7130 - val_loss: 9255.7822 - val_mae: 9256.4746\n",
      "Epoch 1593/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 123.9705 - mae: 124.6571 - val_loss: 9181.2031 - val_mae: 9181.8965\n",
      "Epoch 1594/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 110.9310 - mae: 111.6208 - val_loss: 9707.9180 - val_mae: 9708.6113\n",
      "Epoch 1595/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 108.2957 - mae: 108.9809 - val_loss: 9438.1338 - val_mae: 9438.8281\n",
      "Epoch 1596/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 91.5132 - mae: 92.1987 - val_loss: 9414.8760 - val_mae: 9415.5693\n",
      "Epoch 1597/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 101.8378 - mae: 102.5248 - val_loss: 9406.2100 - val_mae: 9406.9023\n",
      "Epoch 1598/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 97.9076 - mae: 98.5953 - val_loss: 9427.6494 - val_mae: 9428.3438\n",
      "Epoch 1599/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 111.9814 - mae: 112.6694 - val_loss: 9181.8066 - val_mae: 9182.5010\n",
      "Epoch 1600/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 115.9610 - mae: 116.6496 - val_loss: 9234.9199 - val_mae: 9235.6123\n",
      "Epoch 1601/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 103.9005 - mae: 104.5878 - val_loss: 9642.7988 - val_mae: 9643.4902\n",
      "Epoch 1602/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 110.7293 - mae: 111.4160 - val_loss: 9387.0967 - val_mae: 9387.7891\n",
      "Epoch 1603/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 103.7057 - mae: 104.3901 - val_loss: 9488.0576 - val_mae: 9488.7510\n",
      "Epoch 1604/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 124.4433 - mae: 125.1325 - val_loss: 9578.4180 - val_mae: 9579.1104\n",
      "Epoch 1605/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 112.6401 - mae: 113.3259 - val_loss: 9513.3564 - val_mae: 9514.0488\n",
      "Epoch 1606/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 98.3862 - mae: 99.0697 - val_loss: 9471.3848 - val_mae: 9472.0771\n",
      "Epoch 1607/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 132.9198 - mae: 133.6081 - val_loss: 9357.1309 - val_mae: 9357.8232\n",
      "Epoch 1608/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 106.2135 - mae: 106.8988 - val_loss: 9441.9668 - val_mae: 9442.6611\n",
      "Epoch 1609/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 112.0486 - mae: 112.7385 - val_loss: 9282.0293 - val_mae: 9282.7227\n",
      "Epoch 1610/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 91.4344 - mae: 92.1187 - val_loss: 9401.8135 - val_mae: 9402.5059\n",
      "Epoch 1611/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 101.0099 - mae: 101.6964 - val_loss: 9296.2666 - val_mae: 9296.9600\n",
      "Epoch 1612/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 109.1640 - mae: 109.8526 - val_loss: 9345.9512 - val_mae: 9346.6465\n",
      "Epoch 1613/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 105.6714 - mae: 106.3596 - val_loss: 9194.3242 - val_mae: 9195.0166\n",
      "Epoch 1614/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 110.7203 - mae: 111.4088 - val_loss: 9365.3125 - val_mae: 9366.0059\n",
      "Epoch 1615/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 101.4612 - mae: 102.1457 - val_loss: 9317.3398 - val_mae: 9318.0322\n",
      "Epoch 1616/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 112.2508 - mae: 112.9393 - val_loss: 9297.9395 - val_mae: 9298.6318\n",
      "Epoch 1617/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 95.0514 - mae: 95.7367 - val_loss: 9477.8271 - val_mae: 9478.5205\n",
      "Epoch 1618/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 106.8246 - mae: 107.5143 - val_loss: 9560.8213 - val_mae: 9561.5137\n",
      "Epoch 1619/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 113.4996 - mae: 114.1849 - val_loss: 9348.7012 - val_mae: 9349.3945\n",
      "Epoch 1620/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 98.5112 - mae: 99.1972 - val_loss: 9332.3105 - val_mae: 9333.0049\n",
      "Epoch 1621/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 103.3225 - mae: 104.0082 - val_loss: 9401.5352 - val_mae: 9402.2266\n",
      "Epoch 1622/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 97.9621 - mae: 98.6469 - val_loss: 9403.3809 - val_mae: 9404.0732\n",
      "Epoch 1623/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 93.7457 - mae: 94.4339 - val_loss: 9409.4248 - val_mae: 9410.1172\n",
      "Epoch 1624/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 105.2182 - mae: 105.9007 - val_loss: 9502.9756 - val_mae: 9503.6670\n",
      "Epoch 1625/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 114.2133 - mae: 114.9006 - val_loss: 9371.6494 - val_mae: 9372.3418\n",
      "Epoch 1626/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 106.0659 - mae: 106.7522 - val_loss: 9458.1328 - val_mae: 9458.8252\n",
      "Epoch 1627/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 115.0317 - mae: 115.7214 - val_loss: 9434.9951 - val_mae: 9435.6885\n",
      "Epoch 1628/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 100.0321 - mae: 100.7197 - val_loss: 9477.8008 - val_mae: 9478.4932\n",
      "Epoch 1629/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 112.8631 - mae: 113.5436 - val_loss: 9476.1064 - val_mae: 9476.7998\n",
      "Epoch 1630/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 101.3957 - mae: 102.0841 - val_loss: 9413.4824 - val_mae: 9414.1768\n",
      "Epoch 1631/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 99.8485 - mae: 100.5349 - val_loss: 9142.8936 - val_mae: 9143.5879\n",
      "Epoch 1632/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 114.7328 - mae: 115.4188 - val_loss: 9491.0576 - val_mae: 9491.7520\n",
      "Epoch 1633/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 107.6914 - mae: 108.3767 - val_loss: 9463.1641 - val_mae: 9463.8574\n",
      "Epoch 1634/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 110.0452 - mae: 110.7329 - val_loss: 9488.9238 - val_mae: 9489.6152\n",
      "Epoch 1635/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 109.7376 - mae: 110.4249 - val_loss: 9242.9980 - val_mae: 9243.6904\n",
      "Epoch 1636/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 105.9563 - mae: 106.6409 - val_loss: 9410.7754 - val_mae: 9411.4678\n",
      "Epoch 1637/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 104.5656 - mae: 105.2512 - val_loss: 9568.5938 - val_mae: 9569.2891\n",
      "Epoch 1638/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 98.9101 - mae: 99.5928 - val_loss: 9484.1602 - val_mae: 9484.8535\n",
      "Epoch 1639/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 102.7429 - mae: 103.4273 - val_loss: 9198.3154 - val_mae: 9199.0098\n",
      "Epoch 1640/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 106.2339 - mae: 106.9182 - val_loss: 9232.9990 - val_mae: 9233.6914\n",
      "Epoch 1641/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 103.1498 - mae: 103.8354 - val_loss: 9476.1953 - val_mae: 9476.8877\n",
      "Epoch 1642/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 102.2312 - mae: 102.9191 - val_loss: 9338.3643 - val_mae: 9339.0576\n",
      "Epoch 1643/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 107.8093 - mae: 108.4976 - val_loss: 9385.1162 - val_mae: 9385.8096\n",
      "Epoch 1644/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 104.6329 - mae: 105.3210 - val_loss: 9394.7119 - val_mae: 9395.4043\n",
      "Epoch 1645/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 95.9709 - mae: 96.6559 - val_loss: 9199.0498 - val_mae: 9199.7441\n",
      "Epoch 1646/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 96.0825 - mae: 96.7689 - val_loss: 9067.4785 - val_mae: 9068.1719\n",
      "Epoch 1647/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 103.3951 - mae: 104.0776 - val_loss: 9317.4512 - val_mae: 9318.1455\n",
      "Epoch 1648/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 120.5680 - mae: 121.2562 - val_loss: 9345.5195 - val_mae: 9346.2129\n",
      "Epoch 1649/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 107.0282 - mae: 107.7134 - val_loss: 9644.2148 - val_mae: 9644.9072\n",
      "Epoch 1650/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 110.5962 - mae: 111.2824 - val_loss: 9510.0303 - val_mae: 9510.7236\n",
      "Epoch 1651/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 113.3870 - mae: 114.0695 - val_loss: 9531.8838 - val_mae: 9532.5771\n",
      "Epoch 1652/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 106.9468 - mae: 107.6338 - val_loss: 9555.3936 - val_mae: 9556.0869\n",
      "Epoch 1653/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 110.4951 - mae: 111.1807 - val_loss: 9394.2061 - val_mae: 9394.8975\n",
      "Epoch 1654/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 98.5912 - mae: 99.2802 - val_loss: 9406.8096 - val_mae: 9407.5029\n",
      "Epoch 1655/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 109.7805 - mae: 110.4623 - val_loss: 9336.2852 - val_mae: 9336.9766\n",
      "Epoch 1656/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 98.7886 - mae: 99.4774 - val_loss: 9173.4883 - val_mae: 9174.1816\n",
      "Epoch 1657/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 105.7268 - mae: 106.4136 - val_loss: 9349.6211 - val_mae: 9350.3145\n",
      "Epoch 1658/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 96.6189 - mae: 97.3037 - val_loss: 9529.7764 - val_mae: 9530.4678\n",
      "Epoch 1659/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 105.3565 - mae: 106.0440 - val_loss: 9323.1387 - val_mae: 9323.8311\n",
      "Epoch 1660/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 95.0889 - mae: 95.7740 - val_loss: 9516.8223 - val_mae: 9517.5156\n",
      "Epoch 1661/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 99.4620 - mae: 100.1491 - val_loss: 9338.5977 - val_mae: 9339.2910\n",
      "Epoch 1662/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 100.3743 - mae: 101.0587 - val_loss: 9646.1816 - val_mae: 9646.8750\n",
      "Epoch 1663/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 111.2471 - mae: 111.9357 - val_loss: 9414.0518 - val_mae: 9414.7441\n",
      "Epoch 1664/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 100.1372 - mae: 100.8269 - val_loss: 9160.4492 - val_mae: 9161.1426\n",
      "Epoch 1665/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 107.9178 - mae: 108.6049 - val_loss: 9469.5586 - val_mae: 9470.2520\n",
      "Epoch 1666/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 100.1190 - mae: 100.8053 - val_loss: 9471.5391 - val_mae: 9472.2314\n",
      "Epoch 1667/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 92.6884 - mae: 93.3676 - val_loss: 9322.1045 - val_mae: 9322.7988\n",
      "Epoch 1668/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 94.5835 - mae: 95.2705 - val_loss: 9287.7217 - val_mae: 9288.4141\n",
      "Epoch 1669/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 98.6552 - mae: 99.3398 - val_loss: 9403.2109 - val_mae: 9403.9043\n",
      "Epoch 1670/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 103.8046 - mae: 104.4903 - val_loss: 9363.2803 - val_mae: 9363.9727\n",
      "Epoch 1671/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 104.7590 - mae: 105.4464 - val_loss: 9461.6523 - val_mae: 9462.3467\n",
      "Epoch 1672/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 105.0032 - mae: 105.6884 - val_loss: 9228.6104 - val_mae: 9229.3027\n",
      "Epoch 1673/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 115.9781 - mae: 116.6661 - val_loss: 9388.7139 - val_mae: 9389.4072\n",
      "Epoch 1674/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 96.8594 - mae: 97.5448 - val_loss: 9513.7129 - val_mae: 9514.4053\n",
      "Epoch 1675/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 99.7056 - mae: 100.3929 - val_loss: 9472.2324 - val_mae: 9472.9258\n",
      "Epoch 1676/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 112.2776 - mae: 112.9637 - val_loss: 9321.4580 - val_mae: 9322.1504\n",
      "Epoch 1677/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 107.2663 - mae: 107.9558 - val_loss: 9520.1924 - val_mae: 9520.8857\n",
      "Epoch 1678/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 103.9293 - mae: 104.6153 - val_loss: 9398.5225 - val_mae: 9399.2158\n",
      "Epoch 1679/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 90.6983 - mae: 91.3839 - val_loss: 9326.8115 - val_mae: 9327.5068\n",
      "Epoch 1680/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 87.8804 - mae: 88.5650 - val_loss: 9225.5928 - val_mae: 9226.2852\n",
      "Epoch 1681/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 102.3846 - mae: 103.0693 - val_loss: 9632.2314 - val_mae: 9632.9238\n",
      "Epoch 1682/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 108.8328 - mae: 109.5183 - val_loss: 9453.5186 - val_mae: 9454.2129\n",
      "Epoch 1683/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 95.3193 - mae: 96.0015 - val_loss: 9340.8545 - val_mae: 9341.5488\n",
      "Epoch 1684/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 93.9646 - mae: 94.6532 - val_loss: 9470.0000 - val_mae: 9470.6924\n",
      "Epoch 1685/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 101.7941 - mae: 102.4811 - val_loss: 9464.2051 - val_mae: 9464.8994\n",
      "Epoch 1686/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 85.2332 - mae: 85.9172 - val_loss: 9347.8223 - val_mae: 9348.5156\n",
      "Epoch 1687/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 95.0143 - mae: 95.7000 - val_loss: 9400.7510 - val_mae: 9401.4443\n",
      "Epoch 1688/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 100.3616 - mae: 101.0470 - val_loss: 9464.9199 - val_mae: 9465.6133\n",
      "Epoch 1689/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 93.9346 - mae: 94.6219 - val_loss: 9476.7959 - val_mae: 9477.4893\n",
      "Epoch 1690/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 99.8985 - mae: 100.5806 - val_loss: 9465.0732 - val_mae: 9465.7656\n",
      "Epoch 1691/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 99.1901 - mae: 99.8753 - val_loss: 9207.1191 - val_mae: 9207.8115\n",
      "Epoch 1692/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 119.1522 - mae: 119.8392 - val_loss: 9461.2051 - val_mae: 9461.8994\n",
      "Epoch 1693/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 96.2644 - mae: 96.9488 - val_loss: 9439.6963 - val_mae: 9440.3896\n",
      "Epoch 1694/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 104.8631 - mae: 105.5519 - val_loss: 9607.3535 - val_mae: 9608.0479\n",
      "Epoch 1695/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 111.0957 - mae: 111.7843 - val_loss: 9536.1318 - val_mae: 9536.8252\n",
      "Epoch 1696/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 123.4653 - mae: 124.1509 - val_loss: 9163.3936 - val_mae: 9164.0869\n",
      "Epoch 1697/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 109.7197 - mae: 110.4084 - val_loss: 9262.4570 - val_mae: 9263.1494\n",
      "Epoch 1698/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 99.6646 - mae: 100.3513 - val_loss: 9481.7197 - val_mae: 9482.4131\n",
      "Epoch 1699/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 106.5305 - mae: 107.2207 - val_loss: 9244.9482 - val_mae: 9245.6406\n",
      "Epoch 1700/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 93.0716 - mae: 93.7604 - val_loss: 9428.4707 - val_mae: 9429.1631\n",
      "Epoch 1701/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 91.6828 - mae: 92.3686 - val_loss: 9468.6611 - val_mae: 9469.3535\n",
      "Epoch 1702/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 106.8381 - mae: 107.5243 - val_loss: 9564.8721 - val_mae: 9565.5654\n",
      "Epoch 1703/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 112.7286 - mae: 113.4142 - val_loss: 9277.5420 - val_mae: 9278.2344\n",
      "Epoch 1704/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 94.4012 - mae: 95.0865 - val_loss: 9283.7402 - val_mae: 9284.4326\n",
      "Epoch 1705/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 95.1202 - mae: 95.8048 - val_loss: 9388.9102 - val_mae: 9389.6035\n",
      "Epoch 1706/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 106.2794 - mae: 106.9683 - val_loss: 9409.7949 - val_mae: 9410.4873\n",
      "Epoch 1707/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 92.0667 - mae: 92.7530 - val_loss: 9357.6416 - val_mae: 9358.3340\n",
      "Epoch 1708/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 93.3041 - mae: 93.9873 - val_loss: 9412.8535 - val_mae: 9413.5469\n",
      "Epoch 1709/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 99.2067 - mae: 99.8943 - val_loss: 9343.5205 - val_mae: 9344.2139\n",
      "Epoch 1710/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 89.7713 - mae: 90.4531 - val_loss: 9591.5059 - val_mae: 9592.1973\n",
      "Epoch 1711/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 121.1731 - mae: 121.8623 - val_loss: 9583.2959 - val_mae: 9583.9893\n",
      "Epoch 1712/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 101.4470 - mae: 102.1309 - val_loss: 9283.6328 - val_mae: 9284.3262\n",
      "Epoch 1713/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 94.4850 - mae: 95.1728 - val_loss: 9376.1807 - val_mae: 9376.8740\n",
      "Epoch 1714/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 96.1239 - mae: 96.8088 - val_loss: 9415.9785 - val_mae: 9416.6719\n",
      "Epoch 1715/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 122.3984 - mae: 123.0864 - val_loss: 9251.6592 - val_mae: 9252.3516\n",
      "Epoch 1716/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 108.3738 - mae: 109.0592 - val_loss: 9554.2529 - val_mae: 9554.9463\n",
      "Epoch 1717/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 109.1761 - mae: 109.8606 - val_loss: 9405.5049 - val_mae: 9406.1982\n",
      "Epoch 1718/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 106.2887 - mae: 106.9757 - val_loss: 9679.7930 - val_mae: 9680.4844\n",
      "Epoch 1719/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 102.3941 - mae: 103.0822 - val_loss: 9298.8027 - val_mae: 9299.4951\n",
      "Epoch 1720/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 105.9063 - mae: 106.5958 - val_loss: 9242.2627 - val_mae: 9242.9541\n",
      "Epoch 1721/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 108.7324 - mae: 109.4199 - val_loss: 9261.1787 - val_mae: 9261.8691\n",
      "Epoch 1722/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 98.9451 - mae: 99.6294 - val_loss: 9391.5420 - val_mae: 9392.2344\n",
      "Epoch 1723/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 99.1644 - mae: 99.8504 - val_loss: 9254.8838 - val_mae: 9255.5762\n",
      "Epoch 1724/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 106.3011 - mae: 106.9879 - val_loss: 9387.0176 - val_mae: 9387.7109\n",
      "Epoch 1725/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 101.1397 - mae: 101.8262 - val_loss: 9514.5410 - val_mae: 9515.2354\n",
      "Epoch 1726/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 111.9773 - mae: 112.6636 - val_loss: 9090.4727 - val_mae: 9091.1670\n",
      "Epoch 1727/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 113.3338 - mae: 114.0234 - val_loss: 9395.5615 - val_mae: 9396.2549\n",
      "Epoch 1728/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 95.6279 - mae: 96.3151 - val_loss: 9449.2861 - val_mae: 9449.9795\n",
      "Epoch 1729/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 92.0074 - mae: 92.6910 - val_loss: 9343.8525 - val_mae: 9344.5469\n",
      "Epoch 1730/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 106.5445 - mae: 107.2315 - val_loss: 9653.9004 - val_mae: 9654.5918\n",
      "Epoch 1731/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 116.2368 - mae: 116.9226 - val_loss: 9375.3164 - val_mae: 9376.0098\n",
      "Epoch 1732/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 95.7455 - mae: 96.4308 - val_loss: 9402.8496 - val_mae: 9403.5430\n",
      "Epoch 1733/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 103.1831 - mae: 103.8705 - val_loss: 9682.0342 - val_mae: 9682.7275\n",
      "Epoch 1734/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 98.8201 - mae: 99.5020 - val_loss: 9407.6621 - val_mae: 9408.3555\n",
      "Epoch 1735/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 96.9937 - mae: 97.6802 - val_loss: 9493.6572 - val_mae: 9494.3496\n",
      "Epoch 1736/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 98.8518 - mae: 99.5353 - val_loss: 9564.4912 - val_mae: 9565.1846\n",
      "Epoch 1737/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 103.9993 - mae: 104.6875 - val_loss: 9433.6016 - val_mae: 9434.2930\n",
      "Epoch 1738/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 87.5963 - mae: 88.2796 - val_loss: 9455.4844 - val_mae: 9456.1787\n",
      "Epoch 1739/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 105.6594 - mae: 106.3443 - val_loss: 9597.7939 - val_mae: 9598.4854\n",
      "Epoch 1740/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 100.3002 - mae: 100.9846 - val_loss: 9321.0771 - val_mae: 9321.7695\n",
      "Epoch 1741/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 100.7404 - mae: 101.4264 - val_loss: 9573.8428 - val_mae: 9574.5352\n",
      "Epoch 1742/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 99.8167 - mae: 100.5023 - val_loss: 9290.4307 - val_mae: 9291.1240\n",
      "Epoch 1743/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 95.5195 - mae: 96.2026 - val_loss: 9297.5430 - val_mae: 9298.2344\n",
      "Epoch 1744/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 109.9609 - mae: 110.6481 - val_loss: 9254.0000 - val_mae: 9254.6924\n",
      "Epoch 1745/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 107.7776 - mae: 108.4658 - val_loss: 9425.6338 - val_mae: 9426.3252\n",
      "Epoch 1746/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 103.0979 - mae: 103.7841 - val_loss: 9584.7598 - val_mae: 9585.4521\n",
      "Epoch 1747/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 101.2169 - mae: 101.9035 - val_loss: 9491.4492 - val_mae: 9492.1426\n",
      "Epoch 1748/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 99.1731 - mae: 99.8604 - val_loss: 9646.9277 - val_mae: 9647.6201\n",
      "Epoch 1749/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 100.9360 - mae: 101.6191 - val_loss: 9376.4385 - val_mae: 9377.1328\n",
      "Epoch 1750/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 103.5672 - mae: 104.2552 - val_loss: 9358.8066 - val_mae: 9359.4990\n",
      "Epoch 1751/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 103.7969 - mae: 104.4846 - val_loss: 9518.0088 - val_mae: 9518.7012\n",
      "Epoch 1752/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 96.8893 - mae: 97.5735 - val_loss: 9388.1436 - val_mae: 9388.8369\n",
      "Epoch 1753/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 90.7336 - mae: 91.4172 - val_loss: 9429.7412 - val_mae: 9430.4346\n",
      "Epoch 1754/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 86.6102 - mae: 87.2989 - val_loss: 9279.1289 - val_mae: 9279.8232\n",
      "Epoch 1755/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 102.3932 - mae: 103.0796 - val_loss: 9306.3730 - val_mae: 9307.0654\n",
      "Epoch 1756/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 103.7531 - mae: 104.4414 - val_loss: 9313.2705 - val_mae: 9313.9639\n",
      "Epoch 1757/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 92.2836 - mae: 92.9706 - val_loss: 9348.7930 - val_mae: 9349.4863\n",
      "Epoch 1758/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 93.5420 - mae: 94.2263 - val_loss: 9385.8496 - val_mae: 9386.5449\n",
      "Epoch 1759/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 92.2937 - mae: 92.9812 - val_loss: 9398.7041 - val_mae: 9399.3975\n",
      "Epoch 1760/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 102.7531 - mae: 103.4388 - val_loss: 9372.3105 - val_mae: 9373.0029\n",
      "Epoch 1761/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 97.6076 - mae: 98.2929 - val_loss: 9070.9131 - val_mae: 9071.6064\n",
      "Epoch 1762/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 120.7098 - mae: 121.3929 - val_loss: 9234.3438 - val_mae: 9235.0381\n",
      "Epoch 1763/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 95.2427 - mae: 95.9286 - val_loss: 9599.0430 - val_mae: 9599.7354\n",
      "Epoch 1764/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 110.1286 - mae: 110.8151 - val_loss: 9569.2432 - val_mae: 9569.9375\n",
      "Epoch 1765/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 102.2866 - mae: 102.9737 - val_loss: 9517.8887 - val_mae: 9518.5811\n",
      "Epoch 1766/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 93.7013 - mae: 94.3841 - val_loss: 9204.3467 - val_mae: 9205.0400\n",
      "Epoch 1767/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 99.5283 - mae: 100.2106 - val_loss: 9468.7275 - val_mae: 9469.4199\n",
      "Epoch 1768/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 103.3850 - mae: 104.0685 - val_loss: 9455.2578 - val_mae: 9455.9521\n",
      "Epoch 1769/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 91.2123 - mae: 91.8955 - val_loss: 9331.1660 - val_mae: 9331.8594\n",
      "Epoch 1770/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 96.4713 - mae: 97.1544 - val_loss: 9402.9697 - val_mae: 9403.6621\n",
      "Epoch 1771/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 86.1607 - mae: 86.8446 - val_loss: 9227.9590 - val_mae: 9228.6514\n",
      "Epoch 1772/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 101.3199 - mae: 102.0063 - val_loss: 9412.2432 - val_mae: 9412.9365\n",
      "Epoch 1773/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 96.3964 - mae: 97.0813 - val_loss: 9450.4971 - val_mae: 9451.1895\n",
      "Epoch 1774/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 98.2463 - mae: 98.9312 - val_loss: 9498.5273 - val_mae: 9499.2197\n",
      "Epoch 1775/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 103.9418 - mae: 104.6262 - val_loss: 9407.1865 - val_mae: 9407.8799\n",
      "Epoch 1776/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 113.2475 - mae: 113.9345 - val_loss: 9363.9531 - val_mae: 9364.6475\n",
      "Epoch 1777/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 87.8195 - mae: 88.5025 - val_loss: 9369.5020 - val_mae: 9370.1953\n",
      "Epoch 1778/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 89.6267 - mae: 90.3136 - val_loss: 9560.1191 - val_mae: 9560.8125\n",
      "Epoch 1779/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 93.8358 - mae: 94.5218 - val_loss: 9298.0488 - val_mae: 9298.7412\n",
      "Epoch 1780/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 116.1931 - mae: 116.8793 - val_loss: 9504.2002 - val_mae: 9504.8926\n",
      "Epoch 1781/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 110.2754 - mae: 110.9629 - val_loss: 9634.8779 - val_mae: 9635.5703\n",
      "Epoch 1782/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 103.6699 - mae: 104.3556 - val_loss: 9162.5078 - val_mae: 9163.2002\n",
      "Epoch 1783/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 99.3941 - mae: 100.0804 - val_loss: 9265.1465 - val_mae: 9265.8408\n",
      "Epoch 1784/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 97.4384 - mae: 98.1251 - val_loss: 9348.0635 - val_mae: 9348.7568\n",
      "Epoch 1785/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 85.6786 - mae: 86.3629 - val_loss: 9370.3506 - val_mae: 9371.0449\n",
      "Epoch 1786/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 96.9679 - mae: 97.6557 - val_loss: 9220.3662 - val_mae: 9221.0596\n",
      "Epoch 1787/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 88.7698 - mae: 89.4551 - val_loss: 9410.0371 - val_mae: 9410.7305\n",
      "Epoch 1788/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 95.6960 - mae: 96.3819 - val_loss: 9408.1816 - val_mae: 9408.8750\n",
      "Epoch 1789/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 102.6101 - mae: 103.2939 - val_loss: 9439.4648 - val_mae: 9440.1582\n",
      "Epoch 1790/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 93.6764 - mae: 94.3609 - val_loss: 9388.2236 - val_mae: 9388.9170\n",
      "Epoch 1791/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 95.7101 - mae: 96.3946 - val_loss: 9367.0449 - val_mae: 9367.7383\n",
      "Epoch 1792/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 100.9187 - mae: 101.6052 - val_loss: 9255.8271 - val_mae: 9256.5215\n",
      "Epoch 1793/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 87.3849 - mae: 88.0688 - val_loss: 9468.8721 - val_mae: 9469.5654\n",
      "Epoch 1794/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 100.5148 - mae: 101.2031 - val_loss: 9543.3838 - val_mae: 9544.0781\n",
      "Epoch 1795/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 122.7552 - mae: 123.4401 - val_loss: 9067.0684 - val_mae: 9067.7607\n",
      "Epoch 1796/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 110.4859 - mae: 111.1719 - val_loss: 9257.3604 - val_mae: 9258.0537\n",
      "Epoch 1797/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 98.5448 - mae: 99.2340 - val_loss: 9297.9238 - val_mae: 9298.6172\n",
      "Epoch 1798/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 96.3998 - mae: 97.0893 - val_loss: 9178.5635 - val_mae: 9179.2568\n",
      "Epoch 1799/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 97.4432 - mae: 98.1291 - val_loss: 9363.5488 - val_mae: 9364.2432\n",
      "Epoch 1800/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 100.6735 - mae: 101.3623 - val_loss: 9494.6904 - val_mae: 9495.3848\n",
      "Epoch 1801/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 84.9531 - mae: 85.6383 - val_loss: 9476.8711 - val_mae: 9477.5645\n",
      "Epoch 1802/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 92.9242 - mae: 93.6123 - val_loss: 9528.5625 - val_mae: 9529.2549\n",
      "Epoch 1803/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 106.4711 - mae: 107.1558 - val_loss: 9150.1592 - val_mae: 9150.8535\n",
      "Epoch 1804/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 94.4063 - mae: 95.0914 - val_loss: 9339.6025 - val_mae: 9340.2959\n",
      "Epoch 1805/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 95.4696 - mae: 96.1559 - val_loss: 9286.2441 - val_mae: 9286.9385\n",
      "Epoch 1806/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 94.0144 - mae: 94.7017 - val_loss: 9310.7275 - val_mae: 9311.4209\n",
      "Epoch 1807/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 92.1779 - mae: 92.8607 - val_loss: 9461.0488 - val_mae: 9461.7422\n",
      "Epoch 1808/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 89.2903 - mae: 89.9805 - val_loss: 9451.7012 - val_mae: 9452.3926\n",
      "Epoch 1809/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 96.2438 - mae: 96.9274 - val_loss: 9367.5068 - val_mae: 9368.1992\n",
      "Epoch 1810/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 93.1086 - mae: 93.7919 - val_loss: 9384.6973 - val_mae: 9385.3906\n",
      "Epoch 1811/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 105.1599 - mae: 105.8455 - val_loss: 9245.9248 - val_mae: 9246.6182\n",
      "Epoch 1812/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 97.5693 - mae: 98.2520 - val_loss: 9610.0967 - val_mae: 9610.7900\n",
      "Epoch 1813/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 96.6914 - mae: 97.3740 - val_loss: 9311.7520 - val_mae: 9312.4473\n",
      "Epoch 1814/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 103.4858 - mae: 104.1749 - val_loss: 9315.9775 - val_mae: 9316.6699\n",
      "Epoch 1815/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 97.6482 - mae: 98.3364 - val_loss: 9444.4473 - val_mae: 9445.1396\n",
      "Epoch 1816/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 104.2674 - mae: 104.9544 - val_loss: 9396.4561 - val_mae: 9397.1494\n",
      "Epoch 1817/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 89.3425 - mae: 90.0257 - val_loss: 9389.8994 - val_mae: 9390.5918\n",
      "Epoch 1818/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 94.0776 - mae: 94.7620 - val_loss: 9324.1045 - val_mae: 9324.7969\n",
      "Epoch 1819/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 101.6418 - mae: 102.3292 - val_loss: 9369.3652 - val_mae: 9370.0586\n",
      "Epoch 1820/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 107.0790 - mae: 107.7657 - val_loss: 9480.6338 - val_mae: 9481.3262\n",
      "Epoch 1821/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 89.9790 - mae: 90.6662 - val_loss: 9326.5283 - val_mae: 9327.2217\n",
      "Epoch 1822/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 99.2571 - mae: 99.9444 - val_loss: 9560.6348 - val_mae: 9561.3271\n",
      "Epoch 1823/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 98.3953 - mae: 99.0798 - val_loss: 9404.7725 - val_mae: 9405.4648\n",
      "Epoch 1824/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 95.4801 - mae: 96.1683 - val_loss: 9171.4805 - val_mae: 9172.1738\n",
      "Epoch 1825/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 108.7274 - mae: 109.4150 - val_loss: 9466.0215 - val_mae: 9466.7139\n",
      "Epoch 1826/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 102.9905 - mae: 103.6770 - val_loss: 9484.6777 - val_mae: 9485.3691\n",
      "Epoch 1827/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 99.0315 - mae: 99.7194 - val_loss: 9333.0479 - val_mae: 9333.7402\n",
      "Epoch 1828/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 104.7330 - mae: 105.4220 - val_loss: 9386.5273 - val_mae: 9387.2197\n",
      "Epoch 1829/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 88.3328 - mae: 89.0167 - val_loss: 9316.0137 - val_mae: 9316.7061\n",
      "Epoch 1830/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 99.9842 - mae: 100.6711 - val_loss: 9471.0742 - val_mae: 9471.7666\n",
      "Epoch 1831/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 100.7914 - mae: 101.4790 - val_loss: 9494.2910 - val_mae: 9494.9834\n",
      "Epoch 1832/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 102.0626 - mae: 102.7462 - val_loss: 9347.1230 - val_mae: 9347.8164\n",
      "Epoch 1833/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 100.5425 - mae: 101.2298 - val_loss: 9549.4834 - val_mae: 9550.1738\n",
      "Epoch 1834/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 101.0333 - mae: 101.7158 - val_loss: 9522.7793 - val_mae: 9523.4717\n",
      "Epoch 1835/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 106.6051 - mae: 107.2878 - val_loss: 9337.9326 - val_mae: 9338.6270\n",
      "Epoch 1836/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 105.4789 - mae: 106.1692 - val_loss: 9404.1582 - val_mae: 9404.8525\n",
      "Epoch 1837/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 92.0012 - mae: 92.6869 - val_loss: 9181.3496 - val_mae: 9182.0439\n",
      "Epoch 1838/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 93.9597 - mae: 94.6461 - val_loss: 9405.1504 - val_mae: 9405.8447\n",
      "Epoch 1839/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 93.9307 - mae: 94.6145 - val_loss: 9249.8975 - val_mae: 9250.5908\n",
      "Epoch 1840/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 86.3903 - mae: 87.0728 - val_loss: 9559.5332 - val_mae: 9560.2266\n",
      "Epoch 1841/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 110.2706 - mae: 110.9559 - val_loss: 9412.4121 - val_mae: 9413.1035\n",
      "Epoch 1842/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 96.7694 - mae: 97.4543 - val_loss: 9336.5479 - val_mae: 9337.2422\n",
      "Epoch 1843/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 96.7519 - mae: 97.4397 - val_loss: 9448.1953 - val_mae: 9448.8896\n",
      "Epoch 1844/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 111.5078 - mae: 112.1946 - val_loss: 9450.3350 - val_mae: 9451.0273\n",
      "Epoch 1845/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 109.5222 - mae: 110.2087 - val_loss: 9377.6465 - val_mae: 9378.3389\n",
      "Epoch 1846/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 90.6613 - mae: 91.3453 - val_loss: 9379.2480 - val_mae: 9379.9414\n",
      "Epoch 1847/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 101.4671 - mae: 102.1550 - val_loss: 9113.6719 - val_mae: 9114.3643\n",
      "Epoch 1848/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 114.6787 - mae: 115.3627 - val_loss: 9434.2568 - val_mae: 9434.9502\n",
      "Epoch 1849/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 99.7283 - mae: 100.4161 - val_loss: 9463.8252 - val_mae: 9464.5176\n",
      "Epoch 1850/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 99.2540 - mae: 99.9392 - val_loss: 9680.9971 - val_mae: 9681.6914\n",
      "Epoch 1851/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 94.1065 - mae: 94.7927 - val_loss: 9607.1338 - val_mae: 9607.8271\n",
      "Epoch 1852/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 97.1039 - mae: 97.7896 - val_loss: 9356.0312 - val_mae: 9356.7236\n",
      "Epoch 1853/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 106.4912 - mae: 107.1744 - val_loss: 9460.2930 - val_mae: 9460.9863\n",
      "Epoch 1854/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 92.4340 - mae: 93.1195 - val_loss: 9400.3262 - val_mae: 9401.0186\n",
      "Epoch 1855/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 81.9999 - mae: 82.6822 - val_loss: 9468.7061 - val_mae: 9469.3984\n",
      "Epoch 1856/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 91.8797 - mae: 92.5645 - val_loss: 9288.3574 - val_mae: 9289.0508\n",
      "Epoch 1857/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 123.5049 - mae: 124.1958 - val_loss: 9451.3369 - val_mae: 9452.0303\n",
      "Epoch 1858/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 101.3455 - mae: 102.0288 - val_loss: 9227.1318 - val_mae: 9227.8252\n",
      "Epoch 1859/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 98.4437 - mae: 99.1298 - val_loss: 9185.1982 - val_mae: 9185.8906\n",
      "Epoch 1860/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 99.9489 - mae: 100.6317 - val_loss: 9487.4883 - val_mae: 9488.1807\n",
      "Epoch 1861/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 101.9810 - mae: 102.6671 - val_loss: 9307.1846 - val_mae: 9307.8770\n",
      "Epoch 1862/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 89.2334 - mae: 89.9212 - val_loss: 9594.6416 - val_mae: 9595.3359\n",
      "Epoch 1863/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 94.7927 - mae: 95.4755 - val_loss: 9252.8643 - val_mae: 9253.5566\n",
      "Epoch 1864/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 92.6068 - mae: 93.2942 - val_loss: 9561.2373 - val_mae: 9561.9277\n",
      "Epoch 1865/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 99.3119 - mae: 99.9981 - val_loss: 9543.0264 - val_mae: 9543.7207\n",
      "Epoch 1866/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 106.2098 - mae: 106.8970 - val_loss: 9463.0557 - val_mae: 9463.7500\n",
      "Epoch 1867/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 93.8443 - mae: 94.5299 - val_loss: 9420.1543 - val_mae: 9420.8477\n",
      "Epoch 1868/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 94.9496 - mae: 95.6342 - val_loss: 9272.3301 - val_mae: 9273.0244\n",
      "Epoch 1869/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 98.3829 - mae: 99.0693 - val_loss: 9466.9609 - val_mae: 9467.6533\n",
      "Epoch 1870/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 102.4476 - mae: 103.1324 - val_loss: 9450.3408 - val_mae: 9451.0332\n",
      "Epoch 1871/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 96.0948 - mae: 96.7811 - val_loss: 9251.4297 - val_mae: 9252.1230\n",
      "Epoch 1872/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 105.1949 - mae: 105.8839 - val_loss: 9335.5830 - val_mae: 9336.2764\n",
      "Epoch 1873/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 105.5803 - mae: 106.2628 - val_loss: 9186.8584 - val_mae: 9187.5518\n",
      "Epoch 1874/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 120.9921 - mae: 121.6806 - val_loss: 9548.0391 - val_mae: 9548.7334\n",
      "Epoch 1875/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 101.2323 - mae: 101.9205 - val_loss: 9434.9395 - val_mae: 9435.6328\n",
      "Epoch 1876/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 92.4666 - mae: 93.1538 - val_loss: 9254.9424 - val_mae: 9255.6357\n",
      "Epoch 1877/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 102.5129 - mae: 103.1991 - val_loss: 9155.3066 - val_mae: 9156.0000\n",
      "Epoch 1878/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 102.9791 - mae: 103.6638 - val_loss: 9304.0645 - val_mae: 9304.7578\n",
      "Epoch 1879/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 90.1297 - mae: 90.8149 - val_loss: 9492.9375 - val_mae: 9493.6309\n",
      "Epoch 1880/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 98.4896 - mae: 99.1733 - val_loss: 9407.1582 - val_mae: 9407.8506\n",
      "Epoch 1881/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 86.8529 - mae: 87.5357 - val_loss: 9409.1416 - val_mae: 9409.8340\n",
      "Epoch 1882/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 98.5966 - mae: 99.2747 - val_loss: 9272.8281 - val_mae: 9273.5205\n",
      "Epoch 1883/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 112.4701 - mae: 113.1554 - val_loss: 9304.6543 - val_mae: 9305.3467\n",
      "Epoch 1884/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 98.2862 - mae: 98.9703 - val_loss: 9365.6504 - val_mae: 9366.3438\n",
      "Epoch 1885/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 84.3615 - mae: 85.0460 - val_loss: 9345.6826 - val_mae: 9346.3760\n",
      "Epoch 1886/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 100.2232 - mae: 100.9112 - val_loss: 9632.1543 - val_mae: 9632.8496\n",
      "Epoch 1887/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 93.8775 - mae: 94.5641 - val_loss: 9366.9199 - val_mae: 9367.6133\n",
      "Epoch 1888/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 92.5411 - mae: 93.2265 - val_loss: 9314.2568 - val_mae: 9314.9502\n",
      "Epoch 1889/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 99.1213 - mae: 99.8081 - val_loss: 9338.6641 - val_mae: 9339.3574\n",
      "Epoch 1890/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 113.2224 - mae: 113.9116 - val_loss: 9483.8623 - val_mae: 9484.5557\n",
      "Epoch 1891/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 106.0437 - mae: 106.7279 - val_loss: 9375.0420 - val_mae: 9375.7363\n",
      "Epoch 1892/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 77.6744 - mae: 78.3558 - val_loss: 9300.1680 - val_mae: 9300.8604\n",
      "Epoch 1893/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 125.1504 - mae: 125.8398 - val_loss: 9336.2402 - val_mae: 9336.9326\n",
      "Epoch 1894/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 88.0427 - mae: 88.7303 - val_loss: 9376.1396 - val_mae: 9376.8320\n",
      "Epoch 1895/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 98.7152 - mae: 99.4029 - val_loss: 9240.8047 - val_mae: 9241.4971\n",
      "Epoch 1896/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 94.0103 - mae: 94.6947 - val_loss: 9109.1123 - val_mae: 9109.8047\n",
      "Epoch 1897/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 95.8049 - mae: 96.4924 - val_loss: 9377.4453 - val_mae: 9378.1387\n",
      "Epoch 1898/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 105.3742 - mae: 106.0606 - val_loss: 9329.3877 - val_mae: 9330.0801\n",
      "Epoch 1899/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 89.7796 - mae: 90.4607 - val_loss: 9366.6445 - val_mae: 9367.3379\n",
      "Epoch 1900/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 92.1340 - mae: 92.8164 - val_loss: 9323.9990 - val_mae: 9324.6924\n",
      "Epoch 1901/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 103.7835 - mae: 104.4693 - val_loss: 9166.0820 - val_mae: 9166.7754\n",
      "Epoch 1902/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 91.0327 - mae: 91.7169 - val_loss: 9339.8496 - val_mae: 9340.5430\n",
      "Epoch 1903/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 84.9470 - mae: 85.6332 - val_loss: 9080.7217 - val_mae: 9081.4141\n",
      "Epoch 1904/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 92.6140 - mae: 93.2986 - val_loss: 9151.4746 - val_mae: 9152.1689\n",
      "Epoch 1905/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 99.8717 - mae: 100.5582 - val_loss: 9315.4512 - val_mae: 9316.1455\n",
      "Epoch 1906/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 90.1216 - mae: 90.8066 - val_loss: 9355.4668 - val_mae: 9356.1602\n",
      "Epoch 1907/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 91.1545 - mae: 91.8348 - val_loss: 9188.5459 - val_mae: 9189.2373\n",
      "Epoch 1908/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 90.1252 - mae: 90.8142 - val_loss: 9516.5977 - val_mae: 9517.2910\n",
      "Epoch 1909/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 100.8467 - mae: 101.5344 - val_loss: 9598.5029 - val_mae: 9599.1943\n",
      "Epoch 1910/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 108.1087 - mae: 108.7970 - val_loss: 9229.1562 - val_mae: 9229.8496\n",
      "Epoch 1911/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 97.0665 - mae: 97.7511 - val_loss: 9334.7588 - val_mae: 9335.4531\n",
      "Epoch 1912/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 106.3684 - mae: 107.0543 - val_loss: 9377.4346 - val_mae: 9378.1279\n",
      "Epoch 1913/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 98.9812 - mae: 99.6684 - val_loss: 9359.7090 - val_mae: 9360.4014\n",
      "Epoch 1914/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 115.0949 - mae: 115.7813 - val_loss: 9387.4375 - val_mae: 9388.1299\n",
      "Epoch 1915/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 101.1043 - mae: 101.7925 - val_loss: 9213.3506 - val_mae: 9214.0430\n",
      "Epoch 1916/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 89.3044 - mae: 89.9918 - val_loss: 9397.8184 - val_mae: 9398.5127\n",
      "Epoch 1917/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 104.3767 - mae: 105.0636 - val_loss: 9338.1914 - val_mae: 9338.8838\n",
      "Epoch 1918/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 93.4083 - mae: 94.0936 - val_loss: 9126.7217 - val_mae: 9127.4160\n",
      "Epoch 1919/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 103.2333 - mae: 103.9191 - val_loss: 9306.5117 - val_mae: 9307.2051\n",
      "Epoch 1920/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 91.2999 - mae: 91.9891 - val_loss: 9423.0898 - val_mae: 9423.7822\n",
      "Epoch 1921/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 90.7561 - mae: 91.4411 - val_loss: 9264.8066 - val_mae: 9265.5000\n",
      "Epoch 1922/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 92.7256 - mae: 93.4098 - val_loss: 9394.8994 - val_mae: 9395.5918\n",
      "Epoch 1923/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 89.6290 - mae: 90.3130 - val_loss: 9300.7207 - val_mae: 9301.4131\n",
      "Epoch 1924/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 95.3917 - mae: 96.0757 - val_loss: 9127.7705 - val_mae: 9128.4629\n",
      "Epoch 1925/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 125.0487 - mae: 125.7317 - val_loss: 9309.2725 - val_mae: 9309.9658\n",
      "Epoch 1926/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 91.2139 - mae: 91.8998 - val_loss: 9428.2422 - val_mae: 9428.9346\n",
      "Epoch 1927/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 91.0047 - mae: 91.6863 - val_loss: 9324.7930 - val_mae: 9325.4863\n",
      "Epoch 1928/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 90.1276 - mae: 90.8089 - val_loss: 9410.1729 - val_mae: 9410.8662\n",
      "Epoch 1929/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 101.4784 - mae: 102.1645 - val_loss: 9491.4258 - val_mae: 9492.1182\n",
      "Epoch 1930/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 89.6765 - mae: 90.3604 - val_loss: 9246.1387 - val_mae: 9246.8320\n",
      "Epoch 1931/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 111.9588 - mae: 112.6423 - val_loss: 9558.0996 - val_mae: 9558.7920\n",
      "Epoch 1932/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 103.9919 - mae: 104.6772 - val_loss: 9313.3555 - val_mae: 9314.0498\n",
      "Epoch 1933/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 93.0264 - mae: 93.7146 - val_loss: 9315.5107 - val_mae: 9316.2041\n",
      "Epoch 1934/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 104.0970 - mae: 104.7838 - val_loss: 9343.1504 - val_mae: 9343.8447\n",
      "Epoch 1935/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 94.9140 - mae: 95.5996 - val_loss: 9197.8750 - val_mae: 9198.5674\n",
      "Epoch 1936/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 109.9194 - mae: 110.6082 - val_loss: 9295.9932 - val_mae: 9296.6855\n",
      "Epoch 1937/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 102.0387 - mae: 102.7244 - val_loss: 9216.0996 - val_mae: 9216.7920\n",
      "Epoch 1938/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 103.2409 - mae: 103.9285 - val_loss: 9294.9492 - val_mae: 9295.6416\n",
      "Epoch 1939/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 97.9257 - mae: 98.6126 - val_loss: 9151.6133 - val_mae: 9152.3066\n",
      "Epoch 1940/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 91.3089 - mae: 91.9926 - val_loss: 9542.0811 - val_mae: 9542.7744\n",
      "Epoch 1941/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 99.7151 - mae: 100.4034 - val_loss: 9386.0986 - val_mae: 9386.7910\n",
      "Epoch 1942/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 86.9511 - mae: 87.6323 - val_loss: 9454.1895 - val_mae: 9454.8809\n",
      "Epoch 1943/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 95.4555 - mae: 96.1444 - val_loss: 9174.3945 - val_mae: 9175.0879\n",
      "Epoch 1944/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 95.9792 - mae: 96.6650 - val_loss: 9338.3779 - val_mae: 9339.0703\n",
      "Epoch 1945/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 90.9829 - mae: 91.6672 - val_loss: 9444.7529 - val_mae: 9445.4463\n",
      "Epoch 1946/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 94.7436 - mae: 95.4300 - val_loss: 9154.4873 - val_mae: 9155.1797\n",
      "Epoch 1947/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 96.5286 - mae: 97.2159 - val_loss: 9354.4434 - val_mae: 9355.1357\n",
      "Epoch 1948/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 96.4241 - mae: 97.1112 - val_loss: 9463.8828 - val_mae: 9464.5762\n",
      "Epoch 1949/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 92.3816 - mae: 93.0662 - val_loss: 9295.1338 - val_mae: 9295.8262\n",
      "Epoch 1950/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 99.6945 - mae: 100.3783 - val_loss: 9345.3311 - val_mae: 9346.0244\n",
      "Epoch 1951/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 103.6891 - mae: 104.3786 - val_loss: 9282.3975 - val_mae: 9283.0908\n",
      "Epoch 1952/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 105.9817 - mae: 106.6683 - val_loss: 9360.0312 - val_mae: 9360.7236\n",
      "Epoch 1953/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 87.9523 - mae: 88.6411 - val_loss: 9471.6846 - val_mae: 9472.3770\n",
      "Epoch 1954/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 99.1240 - mae: 99.8090 - val_loss: 9339.7061 - val_mae: 9340.3994\n",
      "Epoch 1955/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 91.4686 - mae: 92.1548 - val_loss: 9453.1719 - val_mae: 9453.8643\n",
      "Epoch 1956/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 86.8192 - mae: 87.5079 - val_loss: 9330.8535 - val_mae: 9331.5459\n",
      "Epoch 1957/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 93.7088 - mae: 94.3965 - val_loss: 9514.3594 - val_mae: 9515.0518\n",
      "Epoch 1958/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 98.4837 - mae: 99.1692 - val_loss: 9525.1602 - val_mae: 9525.8525\n",
      "Epoch 1959/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 92.5516 - mae: 93.2377 - val_loss: 9522.0215 - val_mae: 9522.7148\n",
      "Epoch 1960/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 99.2444 - mae: 99.9293 - val_loss: 9352.6699 - val_mae: 9353.3643\n",
      "Epoch 1961/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 100.6763 - mae: 101.3601 - val_loss: 9281.9814 - val_mae: 9282.6738\n",
      "Epoch 1962/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 105.8865 - mae: 106.5737 - val_loss: 9155.6494 - val_mae: 9156.3428\n",
      "Epoch 1963/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 90.2698 - mae: 90.9542 - val_loss: 9322.4531 - val_mae: 9323.1494\n",
      "Epoch 1964/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 85.6356 - mae: 86.3222 - val_loss: 9291.2334 - val_mae: 9291.9268\n",
      "Epoch 1965/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 101.5324 - mae: 102.2161 - val_loss: 9430.5205 - val_mae: 9431.2139\n",
      "Epoch 1966/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 93.8203 - mae: 94.5053 - val_loss: 9422.6602 - val_mae: 9423.3525\n",
      "Epoch 1967/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 99.5995 - mae: 100.2861 - val_loss: 9403.0293 - val_mae: 9403.7207\n",
      "Epoch 1968/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 106.0258 - mae: 106.7112 - val_loss: 9393.1309 - val_mae: 9393.8232\n",
      "Epoch 1969/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 89.2135 - mae: 89.8984 - val_loss: 9281.6787 - val_mae: 9282.3711\n",
      "Epoch 1970/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 96.0803 - mae: 96.7646 - val_loss: 9162.9541 - val_mae: 9163.6494\n",
      "Epoch 1971/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 99.3703 - mae: 100.0601 - val_loss: 9405.4053 - val_mae: 9406.0996\n",
      "Epoch 1972/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 88.0257 - mae: 88.7105 - val_loss: 9297.8271 - val_mae: 9298.5205\n",
      "Epoch 1973/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 86.3249 - mae: 87.0086 - val_loss: 9271.2471 - val_mae: 9271.9414\n",
      "Epoch 1974/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 91.2295 - mae: 91.9123 - val_loss: 9439.2100 - val_mae: 9439.9053\n",
      "Epoch 1975/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 110.7115 - mae: 111.3984 - val_loss: 9218.9297 - val_mae: 9219.6230\n",
      "Epoch 1976/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 90.2240 - mae: 90.9137 - val_loss: 9492.8701 - val_mae: 9493.5654\n",
      "Epoch 1977/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 96.2102 - mae: 96.8968 - val_loss: 9194.1904 - val_mae: 9194.8838\n",
      "Epoch 1978/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 86.7031 - mae: 87.3876 - val_loss: 9166.6738 - val_mae: 9167.3672\n",
      "Epoch 1979/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 89.4547 - mae: 90.1405 - val_loss: 9445.3887 - val_mae: 9446.0811\n",
      "Epoch 1980/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 106.5245 - mae: 107.2139 - val_loss: 9617.7734 - val_mae: 9618.4668\n",
      "Epoch 1981/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 89.2647 - mae: 89.9510 - val_loss: 9626.8096 - val_mae: 9627.5029\n",
      "Epoch 1982/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 105.3360 - mae: 106.0206 - val_loss: 9358.2109 - val_mae: 9358.9014\n",
      "Epoch 1983/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 86.1500 - mae: 86.8362 - val_loss: 9324.4736 - val_mae: 9325.1670\n",
      "Epoch 1984/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 85.5967 - mae: 86.2831 - val_loss: 9425.7891 - val_mae: 9426.4814\n",
      "Epoch 1985/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 81.1769 - mae: 81.8611 - val_loss: 9524.3750 - val_mae: 9525.0684\n",
      "Epoch 1986/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 96.2269 - mae: 96.9152 - val_loss: 9441.9980 - val_mae: 9442.6904\n",
      "Epoch 1987/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 99.2823 - mae: 99.9671 - val_loss: 9538.2842 - val_mae: 9538.9785\n",
      "Epoch 1988/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 88.6216 - mae: 89.3091 - val_loss: 9422.8291 - val_mae: 9423.5225\n",
      "Epoch 1989/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 103.4890 - mae: 104.1705 - val_loss: 9321.6455 - val_mae: 9322.3389\n",
      "Epoch 1990/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 98.4136 - mae: 99.1019 - val_loss: 9419.9229 - val_mae: 9420.6182\n",
      "Epoch 1991/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 95.9082 - mae: 96.5932 - val_loss: 9456.1191 - val_mae: 9456.8125\n",
      "Epoch 1992/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 84.0614 - mae: 84.7456 - val_loss: 9294.0654 - val_mae: 9294.7598\n",
      "Epoch 1993/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 95.0400 - mae: 95.7265 - val_loss: 9120.4902 - val_mae: 9121.1836\n",
      "Epoch 1994/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 99.6628 - mae: 100.3500 - val_loss: 9334.9521 - val_mae: 9335.6475\n",
      "Epoch 1995/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 114.6748 - mae: 115.3617 - val_loss: 9482.3506 - val_mae: 9483.0449\n",
      "Epoch 1996/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 89.3933 - mae: 90.0768 - val_loss: 9584.1318 - val_mae: 9584.8252\n",
      "Epoch 1997/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 97.6754 - mae: 98.3624 - val_loss: 9440.6221 - val_mae: 9441.3154\n",
      "Epoch 1998/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 88.5709 - mae: 89.2507 - val_loss: 9624.1924 - val_mae: 9624.8857\n",
      "Epoch 1999/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 91.1971 - mae: 91.8853 - val_loss: 9500.6328 - val_mae: 9501.3252\n",
      "Epoch 2000/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 86.1640 - mae: 86.8484 - val_loss: 9325.3916 - val_mae: 9326.0850\n",
      "Epoch 2001/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 102.3279 - mae: 103.0150 - val_loss: 9456.0830 - val_mae: 9456.7764\n",
      "Epoch 2002/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 89.0208 - mae: 89.7032 - val_loss: 9723.8672 - val_mae: 9724.5586\n",
      "Epoch 2003/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 97.5961 - mae: 98.2822 - val_loss: 9268.2734 - val_mae: 9268.9668\n",
      "Epoch 2004/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 99.4091 - mae: 100.0961 - val_loss: 9577.1582 - val_mae: 9577.8506\n",
      "Epoch 2005/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 106.6243 - mae: 107.3124 - val_loss: 9516.4766 - val_mae: 9517.1689\n",
      "Epoch 2006/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 100.1812 - mae: 100.8662 - val_loss: 9321.1367 - val_mae: 9321.8311\n",
      "Epoch 2007/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 94.7558 - mae: 95.4460 - val_loss: 9342.0049 - val_mae: 9342.6982\n",
      "Epoch 2008/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 87.0322 - mae: 87.7140 - val_loss: 9455.0605 - val_mae: 9455.7529\n",
      "Epoch 2009/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 98.3373 - mae: 99.0208 - val_loss: 9345.6318 - val_mae: 9346.3262\n",
      "Epoch 2010/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 96.3259 - mae: 97.0088 - val_loss: 9392.7480 - val_mae: 9393.4414\n",
      "Epoch 2011/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 90.5492 - mae: 91.2352 - val_loss: 9364.3047 - val_mae: 9364.9971\n",
      "Epoch 2012/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 91.6089 - mae: 92.2934 - val_loss: 9568.5928 - val_mae: 9569.2861\n",
      "Epoch 2013/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 87.9620 - mae: 88.6496 - val_loss: 9409.8594 - val_mae: 9410.5518\n",
      "Epoch 2014/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 88.2183 - mae: 88.9017 - val_loss: 9423.3877 - val_mae: 9424.0791\n",
      "Epoch 2015/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 98.7712 - mae: 99.4578 - val_loss: 9367.2734 - val_mae: 9367.9668\n",
      "Epoch 2016/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 99.2257 - mae: 99.9088 - val_loss: 9385.5732 - val_mae: 9386.2666\n",
      "Epoch 2017/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 97.2715 - mae: 97.9581 - val_loss: 9518.4316 - val_mae: 9519.1240\n",
      "Epoch 2018/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 97.8773 - mae: 98.5625 - val_loss: 9251.7773 - val_mae: 9252.4697\n",
      "Epoch 2019/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 87.8425 - mae: 88.5253 - val_loss: 9148.9404 - val_mae: 9149.6309\n",
      "Epoch 2020/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 106.5278 - mae: 107.2129 - val_loss: 9258.8594 - val_mae: 9259.5518\n",
      "Epoch 2021/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 108.2940 - mae: 108.9758 - val_loss: 9115.8682 - val_mae: 9116.5605\n",
      "Epoch 2022/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 102.6294 - mae: 103.3120 - val_loss: 9549.5234 - val_mae: 9550.2158\n",
      "Epoch 2023/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 93.6274 - mae: 94.3124 - val_loss: 9257.5020 - val_mae: 9258.1963\n",
      "Epoch 2024/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 90.1714 - mae: 90.8561 - val_loss: 9447.1953 - val_mae: 9447.8887\n",
      "Epoch 2025/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 99.2417 - mae: 99.9273 - val_loss: 9283.0996 - val_mae: 9283.7930\n",
      "Epoch 2026/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 113.5269 - mae: 114.2130 - val_loss: 9301.3340 - val_mae: 9302.0273\n",
      "Epoch 2027/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 100.7113 - mae: 101.3973 - val_loss: 9612.6709 - val_mae: 9613.3633\n",
      "Epoch 2028/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 92.8955 - mae: 93.5789 - val_loss: 9414.3008 - val_mae: 9414.9932\n",
      "Epoch 2029/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 90.1788 - mae: 90.8634 - val_loss: 9284.1182 - val_mae: 9284.8115\n",
      "Epoch 2030/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 97.8613 - mae: 98.5483 - val_loss: 9370.8838 - val_mae: 9371.5771\n",
      "Epoch 2031/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 88.9144 - mae: 89.5998 - val_loss: 9286.1553 - val_mae: 9286.8486\n",
      "Epoch 2032/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 93.6150 - mae: 94.2996 - val_loss: 9249.6934 - val_mae: 9250.3857\n",
      "Epoch 2033/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 105.0988 - mae: 105.7875 - val_loss: 9309.8428 - val_mae: 9310.5352\n",
      "Epoch 2034/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 98.3675 - mae: 99.0533 - val_loss: 9295.1396 - val_mae: 9295.8340\n",
      "Epoch 2035/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 88.4045 - mae: 89.0865 - val_loss: 9337.2041 - val_mae: 9337.8984\n",
      "Epoch 2036/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 101.9792 - mae: 102.6674 - val_loss: 9592.2139 - val_mae: 9592.9082\n",
      "Epoch 2037/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 97.6194 - mae: 98.3055 - val_loss: 9407.0107 - val_mae: 9407.7051\n",
      "Epoch 2038/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 86.9148 - mae: 87.6009 - val_loss: 9275.7471 - val_mae: 9276.4404\n",
      "Epoch 2039/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 88.3821 - mae: 89.0675 - val_loss: 9306.0137 - val_mae: 9306.7061\n",
      "Epoch 2040/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 92.5685 - mae: 93.2534 - val_loss: 9583.8047 - val_mae: 9584.4961\n",
      "Epoch 2041/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 86.3687 - mae: 87.0558 - val_loss: 9469.2744 - val_mae: 9469.9678\n",
      "Epoch 2042/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 86.7820 - mae: 87.4659 - val_loss: 9127.0830 - val_mae: 9127.7764\n",
      "Epoch 2043/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 88.4417 - mae: 89.1259 - val_loss: 9283.1211 - val_mae: 9283.8135\n",
      "Epoch 2044/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 86.9979 - mae: 87.6869 - val_loss: 9304.6162 - val_mae: 9305.3086\n",
      "Epoch 2045/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 99.8325 - mae: 100.5187 - val_loss: 9451.6123 - val_mae: 9452.3047\n",
      "Epoch 2046/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 84.6841 - mae: 85.3719 - val_loss: 9356.7568 - val_mae: 9357.4512\n",
      "Epoch 2047/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 94.1788 - mae: 94.8640 - val_loss: 9294.8975 - val_mae: 9295.5898\n",
      "Epoch 2048/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 89.5933 - mae: 90.2797 - val_loss: 9420.5859 - val_mae: 9421.2803\n",
      "Epoch 2049/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 110.1268 - mae: 110.8150 - val_loss: 9150.8457 - val_mae: 9151.5381\n",
      "Epoch 2050/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 98.4096 - mae: 99.0972 - val_loss: 9384.8975 - val_mae: 9385.5908\n",
      "Epoch 2051/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 98.2870 - mae: 98.9686 - val_loss: 9138.2930 - val_mae: 9138.9863\n",
      "Epoch 2052/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 115.8813 - mae: 116.5708 - val_loss: 9461.2568 - val_mae: 9461.9512\n",
      "Epoch 2053/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 82.1622 - mae: 82.8471 - val_loss: 9089.5615 - val_mae: 9090.2539\n",
      "Epoch 2054/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 101.2180 - mae: 101.9020 - val_loss: 9462.4443 - val_mae: 9463.1387\n",
      "Epoch 2055/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 90.4238 - mae: 91.1085 - val_loss: 9412.6553 - val_mae: 9413.3486\n",
      "Epoch 2056/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 94.9708 - mae: 95.6528 - val_loss: 9581.7422 - val_mae: 9582.4346\n",
      "Epoch 2057/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 100.5471 - mae: 101.2338 - val_loss: 9278.1270 - val_mae: 9278.8213\n",
      "Epoch 2058/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 90.6581 - mae: 91.3449 - val_loss: 9410.1562 - val_mae: 9410.8496\n",
      "Epoch 2059/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 96.1729 - mae: 96.8565 - val_loss: 9303.1787 - val_mae: 9303.8711\n",
      "Epoch 2060/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 87.9344 - mae: 88.6190 - val_loss: 9331.5586 - val_mae: 9332.2510\n",
      "Epoch 2061/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 92.0323 - mae: 92.7169 - val_loss: 9340.1133 - val_mae: 9340.8047\n",
      "Epoch 2062/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 91.7306 - mae: 92.4178 - val_loss: 9412.6855 - val_mae: 9413.3789\n",
      "Epoch 2063/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 80.7137 - mae: 81.3996 - val_loss: 9486.9434 - val_mae: 9487.6377\n",
      "Epoch 2064/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 91.6724 - mae: 92.3597 - val_loss: 9226.3945 - val_mae: 9227.0889\n",
      "Epoch 2065/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 85.5463 - mae: 86.2333 - val_loss: 9504.7900 - val_mae: 9505.4844\n",
      "Epoch 2066/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 90.3846 - mae: 91.0722 - val_loss: 9319.9443 - val_mae: 9320.6377\n",
      "Epoch 2067/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 100.5456 - mae: 101.2317 - val_loss: 9568.5029 - val_mae: 9569.1963\n",
      "Epoch 2068/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 101.2799 - mae: 101.9666 - val_loss: 9359.5107 - val_mae: 9360.2041\n",
      "Epoch 2069/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 82.9865 - mae: 83.6706 - val_loss: 9253.4521 - val_mae: 9254.1465\n",
      "Epoch 2070/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 88.9373 - mae: 89.6226 - val_loss: 9267.7246 - val_mae: 9268.4180\n",
      "Epoch 2071/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 92.5635 - mae: 93.2476 - val_loss: 9395.8818 - val_mae: 9396.5752\n",
      "Epoch 2072/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 100.7296 - mae: 101.4178 - val_loss: 9137.2666 - val_mae: 9137.9600\n",
      "Epoch 2073/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 99.2883 - mae: 99.9754 - val_loss: 9451.9336 - val_mae: 9452.6260\n",
      "Epoch 2074/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 95.3568 - mae: 96.0416 - val_loss: 9286.1064 - val_mae: 9286.8008\n",
      "Epoch 2075/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 81.2093 - mae: 81.8935 - val_loss: 9281.9082 - val_mae: 9282.6006\n",
      "Epoch 2076/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 81.2702 - mae: 81.9564 - val_loss: 9180.2070 - val_mae: 9180.8994\n",
      "Epoch 2077/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 95.5208 - mae: 96.2091 - val_loss: 9224.1895 - val_mae: 9224.8828\n",
      "Epoch 2078/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 91.8435 - mae: 92.5245 - val_loss: 9331.3887 - val_mae: 9332.0820\n",
      "Epoch 2079/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 88.6599 - mae: 89.3456 - val_loss: 9504.4961 - val_mae: 9505.1875\n",
      "Epoch 2080/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 99.3179 - mae: 100.0055 - val_loss: 9191.8506 - val_mae: 9192.5449\n",
      "Epoch 2081/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 94.8358 - mae: 95.5188 - val_loss: 9106.1387 - val_mae: 9106.8311\n",
      "Epoch 2082/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 93.3363 - mae: 94.0234 - val_loss: 9084.4043 - val_mae: 9085.0967\n",
      "Epoch 2083/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 98.1490 - mae: 98.8344 - val_loss: 9477.6387 - val_mae: 9478.3311\n",
      "Epoch 2084/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 103.8654 - mae: 104.5491 - val_loss: 9261.1494 - val_mae: 9261.8408\n",
      "Epoch 2085/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 95.3266 - mae: 96.0118 - val_loss: 9313.4980 - val_mae: 9314.1914\n",
      "Epoch 2086/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 106.0083 - mae: 106.6949 - val_loss: 9051.5439 - val_mae: 9052.2363\n",
      "Epoch 2087/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 100.2860 - mae: 100.9713 - val_loss: 9321.5498 - val_mae: 9322.2441\n",
      "Epoch 2088/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 83.7991 - mae: 84.4849 - val_loss: 9550.2666 - val_mae: 9550.9600\n",
      "Epoch 2089/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 89.5459 - mae: 90.2321 - val_loss: 9351.4980 - val_mae: 9352.1924\n",
      "Epoch 2090/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 89.8881 - mae: 90.5757 - val_loss: 9312.8262 - val_mae: 9313.5186\n",
      "Epoch 2091/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 86.9383 - mae: 87.6257 - val_loss: 9431.6748 - val_mae: 9432.3682\n",
      "Epoch 2092/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 88.9309 - mae: 89.6156 - val_loss: 9095.7334 - val_mae: 9096.4258\n",
      "Epoch 2093/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 117.9426 - mae: 118.6285 - val_loss: 9278.3662 - val_mae: 9279.0605\n",
      "Epoch 2094/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 82.2706 - mae: 82.9538 - val_loss: 9471.0586 - val_mae: 9471.7529\n",
      "Epoch 2095/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 103.5400 - mae: 104.2254 - val_loss: 9421.4600 - val_mae: 9422.1523\n",
      "Epoch 2096/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 89.3437 - mae: 90.0244 - val_loss: 9158.9941 - val_mae: 9159.6855\n",
      "Epoch 2097/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 101.5604 - mae: 102.2474 - val_loss: 9345.2412 - val_mae: 9345.9346\n",
      "Epoch 2098/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 94.6116 - mae: 95.2974 - val_loss: 9397.1074 - val_mae: 9397.8018\n",
      "Epoch 2099/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 91.2861 - mae: 91.9724 - val_loss: 9275.4902 - val_mae: 9276.1836\n",
      "Epoch 2100/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 94.7703 - mae: 95.4578 - val_loss: 9305.1748 - val_mae: 9305.8682\n",
      "Epoch 2101/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 80.0089 - mae: 80.6948 - val_loss: 9483.1631 - val_mae: 9483.8545\n",
      "Epoch 2102/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 88.8537 - mae: 89.5360 - val_loss: 9223.2256 - val_mae: 9223.9180\n",
      "Epoch 2103/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 88.7557 - mae: 89.4386 - val_loss: 9317.9502 - val_mae: 9318.6436\n",
      "Epoch 2104/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 85.7170 - mae: 86.4018 - val_loss: 9294.5498 - val_mae: 9295.2432\n",
      "Epoch 2105/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 83.1532 - mae: 83.8360 - val_loss: 9459.1055 - val_mae: 9459.7969\n",
      "Epoch 2106/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 82.0312 - mae: 82.7164 - val_loss: 9440.6201 - val_mae: 9441.3125\n",
      "Epoch 2107/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 91.2065 - mae: 91.8940 - val_loss: 9176.9668 - val_mae: 9177.6582\n",
      "Epoch 2108/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 92.4209 - mae: 93.1030 - val_loss: 9353.1650 - val_mae: 9353.8574\n",
      "Epoch 2109/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 79.5849 - mae: 80.2672 - val_loss: 9172.7295 - val_mae: 9173.4229\n",
      "Epoch 2110/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 96.6882 - mae: 97.3742 - val_loss: 9343.0928 - val_mae: 9343.7861\n",
      "Epoch 2111/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 92.0968 - mae: 92.7794 - val_loss: 9429.2578 - val_mae: 9429.9512\n",
      "Epoch 2112/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 93.1263 - mae: 93.8116 - val_loss: 9347.8779 - val_mae: 9348.5723\n",
      "Epoch 2113/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 94.2575 - mae: 94.9434 - val_loss: 9272.0967 - val_mae: 9272.7891\n",
      "Epoch 2114/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 91.7234 - mae: 92.4096 - val_loss: 9210.5059 - val_mae: 9211.1992\n",
      "Epoch 2115/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 81.3390 - mae: 82.0228 - val_loss: 9379.7168 - val_mae: 9380.4102\n",
      "Epoch 2116/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 90.9763 - mae: 91.6588 - val_loss: 9256.8447 - val_mae: 9257.5381\n",
      "Epoch 2117/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 90.2099 - mae: 90.8946 - val_loss: 9353.6396 - val_mae: 9354.3330\n",
      "Epoch 2118/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 85.7851 - mae: 86.4748 - val_loss: 9284.8926 - val_mae: 9285.5869\n",
      "Epoch 2119/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 82.8565 - mae: 83.5433 - val_loss: 9304.5703 - val_mae: 9305.2637\n",
      "Epoch 2120/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 72.8165 - mae: 73.5018 - val_loss: 9340.1660 - val_mae: 9340.8604\n",
      "Epoch 2121/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 83.5085 - mae: 84.1918 - val_loss: 9341.1660 - val_mae: 9341.8594\n",
      "Epoch 2122/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 87.2480 - mae: 87.9304 - val_loss: 9212.2080 - val_mae: 9212.9023\n",
      "Epoch 2123/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 99.8085 - mae: 100.4947 - val_loss: 9302.7637 - val_mae: 9303.4570\n",
      "Epoch 2124/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 91.6085 - mae: 92.2954 - val_loss: 9280.1543 - val_mae: 9280.8486\n",
      "Epoch 2125/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 89.1215 - mae: 89.8027 - val_loss: 9229.3232 - val_mae: 9230.0166\n",
      "Epoch 2126/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 88.2273 - mae: 88.9109 - val_loss: 9285.7168 - val_mae: 9286.4102\n",
      "Epoch 2127/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 93.6781 - mae: 94.3615 - val_loss: 9349.5322 - val_mae: 9350.2256\n",
      "Epoch 2128/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 89.5991 - mae: 90.2871 - val_loss: 9411.6807 - val_mae: 9412.3730\n",
      "Epoch 2129/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 90.3815 - mae: 91.0709 - val_loss: 9247.6123 - val_mae: 9248.3057\n",
      "Epoch 2130/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 91.1055 - mae: 91.7893 - val_loss: 9367.3066 - val_mae: 9368.0000\n",
      "Epoch 2131/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 87.2878 - mae: 87.9758 - val_loss: 9490.3984 - val_mae: 9491.0908\n",
      "Epoch 2132/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 87.7704 - mae: 88.4548 - val_loss: 9503.2256 - val_mae: 9503.9180\n",
      "Epoch 2133/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 83.1618 - mae: 83.8429 - val_loss: 9267.6377 - val_mae: 9268.3301\n",
      "Epoch 2134/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 89.7468 - mae: 90.4324 - val_loss: 9270.7598 - val_mae: 9271.4541\n",
      "Epoch 2135/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 87.9303 - mae: 88.6161 - val_loss: 9586.1963 - val_mae: 9586.8896\n",
      "Epoch 2136/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 95.6792 - mae: 96.3658 - val_loss: 9319.3682 - val_mae: 9320.0615\n",
      "Epoch 2137/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 87.2178 - mae: 87.9011 - val_loss: 9387.3848 - val_mae: 9388.0771\n",
      "Epoch 2138/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 93.9757 - mae: 94.6594 - val_loss: 9461.4893 - val_mae: 9462.1816\n",
      "Epoch 2139/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 102.5198 - mae: 103.2026 - val_loss: 9474.3447 - val_mae: 9475.0371\n",
      "Epoch 2140/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 87.5414 - mae: 88.2265 - val_loss: 9293.0332 - val_mae: 9293.7266\n",
      "Epoch 2141/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 88.6214 - mae: 89.3074 - val_loss: 9438.0488 - val_mae: 9438.7412\n",
      "Epoch 2142/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 88.8633 - mae: 89.5504 - val_loss: 9496.1660 - val_mae: 9496.8584\n",
      "Epoch 2143/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 96.3952 - mae: 97.0797 - val_loss: 9614.0303 - val_mae: 9614.7236\n",
      "Epoch 2144/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 94.5157 - mae: 95.2031 - val_loss: 9365.1357 - val_mae: 9365.8281\n",
      "Epoch 2145/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 88.2031 - mae: 88.8852 - val_loss: 9455.1084 - val_mae: 9455.8008\n",
      "Epoch 2146/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 92.7514 - mae: 93.4370 - val_loss: 9138.7695 - val_mae: 9139.4619\n",
      "Epoch 2147/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 97.7934 - mae: 98.4805 - val_loss: 9182.8418 - val_mae: 9183.5342\n",
      "Epoch 2148/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 92.8742 - mae: 93.5571 - val_loss: 9340.8799 - val_mae: 9341.5732\n",
      "Epoch 2149/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 102.5616 - mae: 103.2449 - val_loss: 9145.5547 - val_mae: 9146.2490\n",
      "Epoch 2150/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 88.3865 - mae: 89.0726 - val_loss: 9227.3584 - val_mae: 9228.0518\n",
      "Epoch 2151/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 94.5388 - mae: 95.2264 - val_loss: 9359.3799 - val_mae: 9360.0732\n",
      "Epoch 2152/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 93.1820 - mae: 93.8686 - val_loss: 9337.8184 - val_mae: 9338.5117\n",
      "Epoch 2153/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 82.1840 - mae: 82.8671 - val_loss: 9526.7559 - val_mae: 9527.4502\n",
      "Epoch 2154/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 86.3465 - mae: 87.0299 - val_loss: 9520.0674 - val_mae: 9520.7607\n",
      "Epoch 2155/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 100.5738 - mae: 101.2575 - val_loss: 9394.7705 - val_mae: 9395.4648\n",
      "Epoch 2156/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 88.5947 - mae: 89.2824 - val_loss: 9336.0625 - val_mae: 9336.7559\n",
      "Epoch 2157/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 86.7239 - mae: 87.4095 - val_loss: 9292.7881 - val_mae: 9293.4805\n",
      "Epoch 2158/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 79.9212 - mae: 80.6041 - val_loss: 9288.6299 - val_mae: 9289.3223\n",
      "Epoch 2159/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 87.7774 - mae: 88.4618 - val_loss: 9217.6592 - val_mae: 9218.3516\n",
      "Epoch 2160/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 88.4572 - mae: 89.1447 - val_loss: 9284.1562 - val_mae: 9284.8496\n",
      "Epoch 2161/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 98.0339 - mae: 98.7201 - val_loss: 9176.1699 - val_mae: 9176.8633\n",
      "Epoch 2162/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 91.8972 - mae: 92.5842 - val_loss: 9334.3457 - val_mae: 9335.0381\n",
      "Epoch 2163/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 86.2971 - mae: 86.9820 - val_loss: 9304.7686 - val_mae: 9305.4609\n",
      "Epoch 2164/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 86.8347 - mae: 87.5225 - val_loss: 9305.1963 - val_mae: 9305.8906\n",
      "Epoch 2165/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 82.3000 - mae: 82.9856 - val_loss: 9434.1025 - val_mae: 9434.7949\n",
      "Epoch 2166/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 84.0896 - mae: 84.7770 - val_loss: 9280.8691 - val_mae: 9281.5625\n",
      "Epoch 2167/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 111.3785 - mae: 112.0680 - val_loss: 9127.2402 - val_mae: 9127.9326\n",
      "Epoch 2168/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 100.5297 - mae: 101.2150 - val_loss: 9278.4424 - val_mae: 9279.1357\n",
      "Epoch 2169/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 103.0298 - mae: 103.7167 - val_loss: 9324.7529 - val_mae: 9325.4482\n",
      "Epoch 2170/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 97.9483 - mae: 98.6343 - val_loss: 9369.1084 - val_mae: 9369.8008\n",
      "Epoch 2171/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 89.9758 - mae: 90.6565 - val_loss: 9395.0527 - val_mae: 9395.7471\n",
      "Epoch 2172/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 87.2872 - mae: 87.9715 - val_loss: 9187.4043 - val_mae: 9188.0967\n",
      "Epoch 2173/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 99.3193 - mae: 100.0057 - val_loss: 9422.4736 - val_mae: 9423.1670\n",
      "Epoch 2174/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 78.0838 - mae: 78.7670 - val_loss: 9381.5049 - val_mae: 9382.1973\n",
      "Epoch 2175/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 93.6133 - mae: 94.2970 - val_loss: 9241.6709 - val_mae: 9242.3643\n",
      "Epoch 2176/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 89.8077 - mae: 90.4940 - val_loss: 9235.8359 - val_mae: 9236.5283\n",
      "Epoch 2177/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 81.0336 - mae: 81.7185 - val_loss: 9353.6250 - val_mae: 9354.3184\n",
      "Epoch 2178/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 88.0732 - mae: 88.7589 - val_loss: 9525.3115 - val_mae: 9526.0039\n",
      "Epoch 2179/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 92.7476 - mae: 93.4311 - val_loss: 9314.8955 - val_mae: 9315.5889\n",
      "Epoch 2180/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 93.1516 - mae: 93.8362 - val_loss: 9343.0498 - val_mae: 9343.7432\n",
      "Epoch 2181/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 112.1634 - mae: 112.8510 - val_loss: 9282.9346 - val_mae: 9283.6270\n",
      "Epoch 2182/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 83.5164 - mae: 84.2058 - val_loss: 9390.5537 - val_mae: 9391.2480\n",
      "Epoch 2183/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 88.0422 - mae: 88.7276 - val_loss: 9295.7861 - val_mae: 9296.4775\n",
      "Epoch 2184/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 86.2709 - mae: 86.9581 - val_loss: 9272.6514 - val_mae: 9273.3457\n",
      "Epoch 2185/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 106.4433 - mae: 107.1314 - val_loss: 9193.0146 - val_mae: 9193.7080\n",
      "Epoch 2186/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 75.1427 - mae: 75.8265 - val_loss: 9449.2744 - val_mae: 9449.9688\n",
      "Epoch 2187/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 89.6497 - mae: 90.3318 - val_loss: 9179.7754 - val_mae: 9180.4678\n",
      "Epoch 2188/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 81.2627 - mae: 81.9487 - val_loss: 9339.4395 - val_mae: 9340.1328\n",
      "Epoch 2189/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 87.2162 - mae: 87.8988 - val_loss: 9264.3682 - val_mae: 9265.0615\n",
      "Epoch 2190/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 90.2680 - mae: 90.9476 - val_loss: 9261.0381 - val_mae: 9261.7305\n",
      "Epoch 2191/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 90.0592 - mae: 90.7463 - val_loss: 9300.7285 - val_mae: 9301.4219\n",
      "Epoch 2192/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 79.3412 - mae: 80.0273 - val_loss: 9470.3857 - val_mae: 9471.0781\n",
      "Epoch 2193/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 90.9194 - mae: 91.5994 - val_loss: 9438.6240 - val_mae: 9439.3184\n",
      "Epoch 2194/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 86.4522 - mae: 87.1374 - val_loss: 9341.2627 - val_mae: 9341.9570\n",
      "Epoch 2195/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 84.5076 - mae: 85.1926 - val_loss: 9362.5430 - val_mae: 9363.2373\n",
      "Epoch 2196/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 83.5714 - mae: 84.2560 - val_loss: 9107.6582 - val_mae: 9108.3506\n",
      "Epoch 2197/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 91.7854 - mae: 92.4716 - val_loss: 9417.0996 - val_mae: 9417.7939\n",
      "Epoch 2198/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 86.5310 - mae: 87.2155 - val_loss: 9283.2539 - val_mae: 9283.9473\n",
      "Epoch 2199/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 104.9069 - mae: 105.5930 - val_loss: 9398.4326 - val_mae: 9399.1260\n",
      "Epoch 2200/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 82.9758 - mae: 83.6609 - val_loss: 9327.7139 - val_mae: 9328.4062\n",
      "Epoch 2201/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 85.0116 - mae: 85.6985 - val_loss: 9420.4727 - val_mae: 9421.1660\n",
      "Epoch 2202/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 100.2861 - mae: 100.9741 - val_loss: 9246.6670 - val_mae: 9247.3604\n",
      "Epoch 2203/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 110.5554 - mae: 111.2404 - val_loss: 9238.6748 - val_mae: 9239.3691\n",
      "Epoch 2204/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 95.5677 - mae: 96.2496 - val_loss: 9246.3857 - val_mae: 9247.0791\n",
      "Epoch 2205/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 90.8332 - mae: 91.5164 - val_loss: 9396.2031 - val_mae: 9396.8965\n",
      "Epoch 2206/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 86.4804 - mae: 87.1607 - val_loss: 9203.4219 - val_mae: 9204.1162\n",
      "Epoch 2207/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 91.9997 - mae: 92.6830 - val_loss: 9472.5635 - val_mae: 9473.2549\n",
      "Epoch 2208/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 92.4990 - mae: 93.1884 - val_loss: 9319.4912 - val_mae: 9320.1846\n",
      "Epoch 2209/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 94.2959 - mae: 94.9839 - val_loss: 9475.0586 - val_mae: 9475.7520\n",
      "Epoch 2210/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 88.7161 - mae: 89.4006 - val_loss: 9185.7178 - val_mae: 9186.4102\n",
      "Epoch 2211/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 93.5626 - mae: 94.2476 - val_loss: 9549.5625 - val_mae: 9550.2549\n",
      "Epoch 2212/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 99.3543 - mae: 100.0408 - val_loss: 9146.1719 - val_mae: 9146.8643\n",
      "Epoch 2213/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 79.6512 - mae: 80.3380 - val_loss: 9506.6934 - val_mae: 9507.3867\n",
      "Epoch 2214/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 83.1909 - mae: 83.8731 - val_loss: 9272.3643 - val_mae: 9273.0557\n",
      "Epoch 2215/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 82.8819 - mae: 83.5665 - val_loss: 9352.6504 - val_mae: 9353.3438\n",
      "Epoch 2216/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 92.8207 - mae: 93.5055 - val_loss: 9595.1299 - val_mae: 9595.8242\n",
      "Epoch 2217/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 92.3070 - mae: 92.9910 - val_loss: 9276.7715 - val_mae: 9277.4648\n",
      "Epoch 2218/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 78.2798 - mae: 78.9636 - val_loss: 9250.5469 - val_mae: 9251.2402\n",
      "Epoch 2219/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 90.9186 - mae: 91.6056 - val_loss: 9188.7676 - val_mae: 9189.4609\n",
      "Epoch 2220/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 100.6245 - mae: 101.3100 - val_loss: 9224.3252 - val_mae: 9225.0176\n",
      "Epoch 2221/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 80.4390 - mae: 81.1229 - val_loss: 9407.1016 - val_mae: 9407.7949\n",
      "Epoch 2222/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 95.0481 - mae: 95.7367 - val_loss: 9470.7686 - val_mae: 9471.4629\n",
      "Epoch 2223/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 104.3765 - mae: 105.0625 - val_loss: 9187.2334 - val_mae: 9187.9277\n",
      "Epoch 2224/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 93.4325 - mae: 94.1198 - val_loss: 9325.0518 - val_mae: 9325.7461\n",
      "Epoch 2225/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 100.8038 - mae: 101.4836 - val_loss: 9571.6328 - val_mae: 9572.3271\n",
      "Epoch 2226/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 105.3522 - mae: 106.0336 - val_loss: 9177.5977 - val_mae: 9178.2900\n",
      "Epoch 2227/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 93.1482 - mae: 93.8294 - val_loss: 9405.5098 - val_mae: 9406.2031\n",
      "Epoch 2228/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 79.9118 - mae: 80.5980 - val_loss: 9500.0400 - val_mae: 9500.7334\n",
      "Epoch 2229/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 121.3741 - mae: 122.0619 - val_loss: 9501.9609 - val_mae: 9502.6543\n",
      "Epoch 2230/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 77.7979 - mae: 78.4817 - val_loss: 9358.4580 - val_mae: 9359.1523\n",
      "Epoch 2231/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 88.3781 - mae: 89.0661 - val_loss: 9239.9746 - val_mae: 9240.6680\n",
      "Epoch 2232/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 82.6619 - mae: 83.3410 - val_loss: 9487.7354 - val_mae: 9488.4277\n",
      "Epoch 2233/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 86.6378 - mae: 87.3239 - val_loss: 9292.3730 - val_mae: 9293.0654\n",
      "Epoch 2234/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 89.9021 - mae: 90.5876 - val_loss: 9505.4980 - val_mae: 9506.1914\n",
      "Epoch 2235/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 85.0408 - mae: 85.7240 - val_loss: 9368.5527 - val_mae: 9369.2480\n",
      "Epoch 2236/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 76.4195 - mae: 77.1080 - val_loss: 9344.2910 - val_mae: 9344.9844\n",
      "Epoch 2237/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 79.5076 - mae: 80.1891 - val_loss: 9425.2607 - val_mae: 9425.9531\n",
      "Epoch 2238/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 95.8887 - mae: 96.5740 - val_loss: 9301.2480 - val_mae: 9301.9424\n",
      "Epoch 2239/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 94.1189 - mae: 94.7983 - val_loss: 9345.2412 - val_mae: 9345.9346\n",
      "Epoch 2240/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 99.6734 - mae: 100.3605 - val_loss: 9286.2090 - val_mae: 9286.9023\n",
      "Epoch 2241/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 95.5136 - mae: 96.2001 - val_loss: 9303.8262 - val_mae: 9304.5195\n",
      "Epoch 2242/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 87.9963 - mae: 88.6788 - val_loss: 9261.6309 - val_mae: 9262.3242\n",
      "Epoch 2243/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 90.6457 - mae: 91.3306 - val_loss: 9363.7676 - val_mae: 9364.4600\n",
      "Epoch 2244/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 86.8970 - mae: 87.5782 - val_loss: 9536.0020 - val_mae: 9536.6943\n",
      "Epoch 2245/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 85.6844 - mae: 86.3703 - val_loss: 9311.4395 - val_mae: 9312.1338\n",
      "Epoch 2246/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 89.2836 - mae: 89.9669 - val_loss: 9490.9336 - val_mae: 9491.6260\n",
      "Epoch 2247/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 108.4024 - mae: 109.0834 - val_loss: 9206.1826 - val_mae: 9206.8760\n",
      "Epoch 2248/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 83.2213 - mae: 83.9066 - val_loss: 9340.5537 - val_mae: 9341.2471\n",
      "Epoch 2249/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 92.3197 - mae: 93.0047 - val_loss: 9349.4893 - val_mae: 9350.1826\n",
      "Epoch 2250/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 101.0922 - mae: 101.7800 - val_loss: 9230.7090 - val_mae: 9231.4023\n",
      "Epoch 2251/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 106.4794 - mae: 107.1670 - val_loss: 9295.0088 - val_mae: 9295.7021\n",
      "Epoch 2252/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 91.6879 - mae: 92.3756 - val_loss: 9111.5674 - val_mae: 9112.2607\n",
      "Epoch 2253/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 86.0566 - mae: 86.7377 - val_loss: 9117.2588 - val_mae: 9117.9521\n",
      "Epoch 2254/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 90.2136 - mae: 90.9001 - val_loss: 9227.6807 - val_mae: 9228.3730\n",
      "Epoch 2255/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 81.8838 - mae: 82.5652 - val_loss: 9233.1846 - val_mae: 9233.8770\n",
      "Epoch 2256/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 85.8519 - mae: 86.5323 - val_loss: 9221.8018 - val_mae: 9222.4951\n",
      "Epoch 2257/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 75.2134 - mae: 75.8978 - val_loss: 9107.8799 - val_mae: 9108.5732\n",
      "Epoch 2258/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 83.4643 - mae: 84.1486 - val_loss: 9304.3447 - val_mae: 9305.0381\n",
      "Epoch 2259/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 80.0564 - mae: 80.7389 - val_loss: 9265.3809 - val_mae: 9266.0732\n",
      "Epoch 2260/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 77.4800 - mae: 78.1606 - val_loss: 9231.2568 - val_mae: 9231.9502\n",
      "Epoch 2261/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 86.5365 - mae: 87.2192 - val_loss: 9338.6553 - val_mae: 9339.3496\n",
      "Epoch 2262/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 90.7955 - mae: 91.4811 - val_loss: 9294.8262 - val_mae: 9295.5195\n",
      "Epoch 2263/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 81.3632 - mae: 82.0488 - val_loss: 9242.5215 - val_mae: 9243.2139\n",
      "Epoch 2264/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 76.4734 - mae: 77.1600 - val_loss: 9197.5186 - val_mae: 9198.2109\n",
      "Epoch 2265/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 86.7652 - mae: 87.4513 - val_loss: 9157.6143 - val_mae: 9158.3076\n",
      "Epoch 2266/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 95.7260 - mae: 96.4115 - val_loss: 9233.2705 - val_mae: 9233.9648\n",
      "Epoch 2267/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 84.3176 - mae: 85.0017 - val_loss: 9147.2656 - val_mae: 9147.9590\n",
      "Epoch 2268/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 83.0742 - mae: 83.7597 - val_loss: 9265.3643 - val_mae: 9266.0566\n",
      "Epoch 2269/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 88.9596 - mae: 89.6454 - val_loss: 9263.0918 - val_mae: 9263.7852\n",
      "Epoch 2270/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 86.7865 - mae: 87.4722 - val_loss: 9082.7441 - val_mae: 9083.4375\n",
      "Epoch 2271/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 96.5166 - mae: 97.2016 - val_loss: 9324.3613 - val_mae: 9325.0537\n",
      "Epoch 2272/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 88.7329 - mae: 89.4173 - val_loss: 9254.1885 - val_mae: 9254.8799\n",
      "Epoch 2273/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 84.1673 - mae: 84.8542 - val_loss: 9000.7354 - val_mae: 9001.4287\n",
      "Epoch 2274/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 110.1803 - mae: 110.8661 - val_loss: 9142.9658 - val_mae: 9143.6592\n",
      "Epoch 2275/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 82.2826 - mae: 82.9684 - val_loss: 9336.1182 - val_mae: 9336.8115\n",
      "Epoch 2276/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 94.4100 - mae: 95.0937 - val_loss: 9318.4580 - val_mae: 9319.1523\n",
      "Epoch 2277/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 81.6747 - mae: 82.3557 - val_loss: 9176.3486 - val_mae: 9177.0410\n",
      "Epoch 2278/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 86.8651 - mae: 87.5475 - val_loss: 9352.3105 - val_mae: 9353.0029\n",
      "Epoch 2279/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 89.3122 - mae: 89.9981 - val_loss: 9540.0723 - val_mae: 9540.7676\n",
      "Epoch 2280/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 88.3396 - mae: 89.0272 - val_loss: 9343.5186 - val_mae: 9344.2119\n",
      "Epoch 2281/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 83.9879 - mae: 84.6755 - val_loss: 9235.8643 - val_mae: 9236.5566\n",
      "Epoch 2282/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 100.6339 - mae: 101.3142 - val_loss: 9379.5186 - val_mae: 9380.2109\n",
      "Epoch 2283/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 89.2120 - mae: 89.8999 - val_loss: 9124.5273 - val_mae: 9125.2197\n",
      "Epoch 2284/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 90.0883 - mae: 90.7769 - val_loss: 9356.5225 - val_mae: 9357.2158\n",
      "Epoch 2285/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 87.2615 - mae: 87.9458 - val_loss: 9239.1973 - val_mae: 9239.8896\n",
      "Epoch 2286/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 96.1259 - mae: 96.8141 - val_loss: 9359.0361 - val_mae: 9359.7295\n",
      "Epoch 2287/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 80.8061 - mae: 81.4939 - val_loss: 9403.6611 - val_mae: 9404.3535\n",
      "Epoch 2288/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 95.0452 - mae: 95.7256 - val_loss: 9243.3916 - val_mae: 9244.0850\n",
      "Epoch 2289/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 103.3370 - mae: 104.0244 - val_loss: 9384.1104 - val_mae: 9384.8037\n",
      "Epoch 2290/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 99.9687 - mae: 100.6554 - val_loss: 9086.9541 - val_mae: 9087.6475\n",
      "Epoch 2291/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 115.7108 - mae: 116.3952 - val_loss: 9211.6943 - val_mae: 9212.3877\n",
      "Epoch 2292/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 80.3438 - mae: 81.0274 - val_loss: 9392.2656 - val_mae: 9392.9590\n",
      "Epoch 2293/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 78.5863 - mae: 79.2711 - val_loss: 9443.2041 - val_mae: 9443.8965\n",
      "Epoch 2294/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 87.0373 - mae: 87.7244 - val_loss: 9161.7344 - val_mae: 9162.4268\n",
      "Epoch 2295/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 90.5165 - mae: 91.2024 - val_loss: 9252.2334 - val_mae: 9252.9248\n",
      "Epoch 2296/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 92.6654 - mae: 93.3564 - val_loss: 9581.6045 - val_mae: 9582.2969\n",
      "Epoch 2297/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 92.0532 - mae: 92.7376 - val_loss: 9294.3105 - val_mae: 9295.0039\n",
      "Epoch 2298/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 108.5629 - mae: 109.2515 - val_loss: 9551.8223 - val_mae: 9552.5146\n",
      "Epoch 2299/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 106.9147 - mae: 107.6004 - val_loss: 9282.5039 - val_mae: 9283.1973\n",
      "Epoch 2300/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 93.2928 - mae: 93.9811 - val_loss: 9092.4961 - val_mae: 9093.1895\n",
      "Epoch 2301/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 102.2531 - mae: 102.9421 - val_loss: 9172.9365 - val_mae: 9173.6309\n",
      "Epoch 2302/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 98.4571 - mae: 99.1461 - val_loss: 9488.0322 - val_mae: 9488.7256\n",
      "Epoch 2303/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 103.7713 - mae: 104.4558 - val_loss: 9282.1582 - val_mae: 9282.8516\n",
      "Epoch 2304/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 94.3990 - mae: 95.0858 - val_loss: 9297.7334 - val_mae: 9298.4258\n",
      "Epoch 2305/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 76.0776 - mae: 76.7618 - val_loss: 9332.8350 - val_mae: 9333.5283\n",
      "Epoch 2306/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 97.7284 - mae: 98.4152 - val_loss: 9102.3066 - val_mae: 9103.0000\n",
      "Epoch 2307/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 92.6722 - mae: 93.3588 - val_loss: 9070.0996 - val_mae: 9070.7939\n",
      "Epoch 2308/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 94.1208 - mae: 94.8018 - val_loss: 9245.4814 - val_mae: 9246.1748\n",
      "Epoch 2309/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 85.6497 - mae: 86.3368 - val_loss: 9434.1016 - val_mae: 9434.7949\n",
      "Epoch 2310/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 99.7243 - mae: 100.4118 - val_loss: 9395.4824 - val_mae: 9396.1748\n",
      "Epoch 2311/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 89.3235 - mae: 90.0077 - val_loss: 9405.9736 - val_mae: 9406.6660\n",
      "Epoch 2312/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 83.5336 - mae: 84.2216 - val_loss: 9503.9043 - val_mae: 9504.5986\n",
      "Epoch 2313/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 72.2550 - mae: 72.9375 - val_loss: 9427.1562 - val_mae: 9427.8496\n",
      "Epoch 2314/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 90.1667 - mae: 90.8536 - val_loss: 9419.6377 - val_mae: 9420.3301\n",
      "Epoch 2315/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 84.9653 - mae: 85.6523 - val_loss: 9207.2021 - val_mae: 9207.8955\n",
      "Epoch 2316/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 108.0783 - mae: 108.7666 - val_loss: 9365.0576 - val_mae: 9365.7510\n",
      "Epoch 2317/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 89.5877 - mae: 90.2754 - val_loss: 9263.7588 - val_mae: 9264.4521\n",
      "Epoch 2318/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 105.6541 - mae: 106.3399 - val_loss: 9230.2705 - val_mae: 9230.9629\n",
      "Epoch 2319/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 80.6589 - mae: 81.3399 - val_loss: 9390.9570 - val_mae: 9391.6504\n",
      "Epoch 2320/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 87.5621 - mae: 88.2465 - val_loss: 9285.2539 - val_mae: 9285.9482\n",
      "Epoch 2321/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 82.6254 - mae: 83.3090 - val_loss: 9089.1582 - val_mae: 9089.8516\n",
      "Epoch 2322/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 103.6298 - mae: 104.3169 - val_loss: 9207.7783 - val_mae: 9208.4717\n",
      "Epoch 2323/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 90.4233 - mae: 91.1078 - val_loss: 9388.5068 - val_mae: 9389.2002\n",
      "Epoch 2324/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 95.4415 - mae: 96.1283 - val_loss: 9207.0381 - val_mae: 9207.7305\n",
      "Epoch 2325/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 88.2742 - mae: 88.9619 - val_loss: 9404.5117 - val_mae: 9405.2051\n",
      "Epoch 2326/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 88.9370 - mae: 89.6202 - val_loss: 9205.8516 - val_mae: 9206.5449\n",
      "Epoch 2327/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 95.0510 - mae: 95.7344 - val_loss: 9329.6143 - val_mae: 9330.3076\n",
      "Epoch 2328/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 82.2514 - mae: 82.9347 - val_loss: 9064.9717 - val_mae: 9065.6660\n",
      "Epoch 2329/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 92.5254 - mae: 93.2135 - val_loss: 9292.0576 - val_mae: 9292.7500\n",
      "Epoch 2330/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 87.7317 - mae: 88.4145 - val_loss: 9341.4297 - val_mae: 9342.1230\n",
      "Epoch 2331/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 76.7811 - mae: 77.4634 - val_loss: 9275.1777 - val_mae: 9275.8701\n",
      "Epoch 2332/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 81.8537 - mae: 82.5394 - val_loss: 9439.8311 - val_mae: 9440.5244\n",
      "Epoch 2333/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 90.7268 - mae: 91.4081 - val_loss: 9321.9639 - val_mae: 9322.6582\n",
      "Epoch 2334/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 103.2666 - mae: 103.9527 - val_loss: 9412.9941 - val_mae: 9413.6885\n",
      "Epoch 2335/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 90.4160 - mae: 91.1006 - val_loss: 9402.0117 - val_mae: 9402.7051\n",
      "Epoch 2336/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 95.9964 - mae: 96.6791 - val_loss: 9202.0234 - val_mae: 9202.7168\n",
      "Epoch 2337/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 85.0360 - mae: 85.7204 - val_loss: 9201.2783 - val_mae: 9201.9717\n",
      "Epoch 2338/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 96.3584 - mae: 97.0446 - val_loss: 9277.2021 - val_mae: 9277.8945\n",
      "Epoch 2339/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 87.6003 - mae: 88.2853 - val_loss: 9142.2842 - val_mae: 9142.9785\n",
      "Epoch 2340/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 95.3691 - mae: 96.0529 - val_loss: 9425.4883 - val_mae: 9426.1807\n",
      "Epoch 2341/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 105.2848 - mae: 105.9732 - val_loss: 9228.7646 - val_mae: 9229.4590\n",
      "Epoch 2342/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 80.1369 - mae: 80.8235 - val_loss: 9136.3896 - val_mae: 9137.0811\n",
      "Epoch 2343/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 80.7592 - mae: 81.4411 - val_loss: 9146.6377 - val_mae: 9147.3301\n",
      "Epoch 2344/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 88.4437 - mae: 89.1255 - val_loss: 9382.0195 - val_mae: 9382.7129\n",
      "Epoch 2345/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 81.5373 - mae: 82.2215 - val_loss: 9284.5303 - val_mae: 9285.2256\n",
      "Epoch 2346/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 86.1307 - mae: 86.8131 - val_loss: 9462.0068 - val_mae: 9462.7002\n",
      "Epoch 2347/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 96.1295 - mae: 96.8146 - val_loss: 9177.3584 - val_mae: 9178.0508\n",
      "Epoch 2348/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 82.8904 - mae: 83.5730 - val_loss: 9238.3271 - val_mae: 9239.0186\n",
      "Epoch 2349/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 83.5472 - mae: 84.2333 - val_loss: 9140.1191 - val_mae: 9140.8105\n",
      "Epoch 2350/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 90.7962 - mae: 91.4804 - val_loss: 9120.3135 - val_mae: 9121.0068\n",
      "Epoch 2351/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 70.9770 - mae: 71.6586 - val_loss: 9265.5869 - val_mae: 9266.2793\n",
      "Epoch 2352/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 79.6696 - mae: 80.3526 - val_loss: 9293.9863 - val_mae: 9294.6787\n",
      "Epoch 2353/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 92.3000 - mae: 92.9814 - val_loss: 9257.1602 - val_mae: 9257.8535\n",
      "Epoch 2354/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 91.1763 - mae: 91.8619 - val_loss: 9203.0098 - val_mae: 9203.7021\n",
      "Epoch 2355/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 87.6361 - mae: 88.3208 - val_loss: 9402.9053 - val_mae: 9403.5977\n",
      "Epoch 2356/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 81.9785 - mae: 82.6670 - val_loss: 9460.1934 - val_mae: 9460.8857\n",
      "Epoch 2357/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 82.3926 - mae: 83.0781 - val_loss: 9324.4238 - val_mae: 9325.1172\n",
      "Epoch 2358/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 84.9275 - mae: 85.6130 - val_loss: 9214.9297 - val_mae: 9215.6221\n",
      "Epoch 2359/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 79.8404 - mae: 80.5201 - val_loss: 9331.8760 - val_mae: 9332.5693\n",
      "Epoch 2360/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 80.4617 - mae: 81.1487 - val_loss: 9234.8633 - val_mae: 9235.5547\n",
      "Epoch 2361/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 81.8685 - mae: 82.5564 - val_loss: 9267.9980 - val_mae: 9268.6924\n",
      "Epoch 2362/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 88.4152 - mae: 89.1003 - val_loss: 9327.0771 - val_mae: 9327.7695\n",
      "Epoch 2363/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 73.6894 - mae: 74.3693 - val_loss: 9171.6445 - val_mae: 9172.3369\n",
      "Epoch 2364/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 92.4094 - mae: 93.0963 - val_loss: 9297.2041 - val_mae: 9297.8975\n",
      "Epoch 2365/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 93.9501 - mae: 94.6395 - val_loss: 9349.0840 - val_mae: 9349.7783\n",
      "Epoch 2366/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 81.9471 - mae: 82.6347 - val_loss: 9476.4375 - val_mae: 9477.1318\n",
      "Epoch 2367/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 91.6224 - mae: 92.3043 - val_loss: 9310.9434 - val_mae: 9311.6357\n",
      "Epoch 2368/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 85.2167 - mae: 85.9026 - val_loss: 9334.3955 - val_mae: 9335.0879\n",
      "Epoch 2369/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 106.3671 - mae: 107.0550 - val_loss: 9037.6992 - val_mae: 9038.3926\n",
      "Epoch 2370/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 112.8842 - mae: 113.5706 - val_loss: 9380.6133 - val_mae: 9381.3066\n",
      "Epoch 2371/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 75.9328 - mae: 76.6180 - val_loss: 9192.9102 - val_mae: 9193.6035\n",
      "Epoch 2372/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 87.5996 - mae: 88.2866 - val_loss: 9328.8438 - val_mae: 9329.5371\n",
      "Epoch 2373/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 84.4003 - mae: 85.0871 - val_loss: 9315.4043 - val_mae: 9316.0986\n",
      "Epoch 2374/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 83.4127 - mae: 84.0990 - val_loss: 9315.9365 - val_mae: 9316.6289\n",
      "Epoch 2375/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 88.9792 - mae: 89.6620 - val_loss: 9101.7871 - val_mae: 9102.4824\n",
      "Epoch 2376/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 87.7106 - mae: 88.3976 - val_loss: 9259.0811 - val_mae: 9259.7744\n",
      "Epoch 2377/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 89.6087 - mae: 90.2922 - val_loss: 9323.8623 - val_mae: 9324.5566\n",
      "Epoch 2378/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 80.8531 - mae: 81.5359 - val_loss: 9399.3613 - val_mae: 9400.0537\n",
      "Epoch 2379/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 95.1976 - mae: 95.8824 - val_loss: 9287.8477 - val_mae: 9288.5420\n",
      "Epoch 2380/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 98.7714 - mae: 99.4583 - val_loss: 9140.4590 - val_mae: 9141.1533\n",
      "Epoch 2381/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 78.8499 - mae: 79.5352 - val_loss: 9412.4570 - val_mae: 9413.1494\n",
      "Epoch 2382/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 83.9597 - mae: 84.6466 - val_loss: 9357.9189 - val_mae: 9358.6133\n",
      "Epoch 2383/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 100.5145 - mae: 101.1997 - val_loss: 9307.6631 - val_mae: 9308.3555\n",
      "Epoch 2384/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 89.6415 - mae: 90.3289 - val_loss: 9352.0234 - val_mae: 9352.7158\n",
      "Epoch 2385/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 93.4682 - mae: 94.1570 - val_loss: 9150.9248 - val_mae: 9151.6182\n",
      "Epoch 2386/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 91.5750 - mae: 92.2618 - val_loss: 9244.4678 - val_mae: 9245.1602\n",
      "Epoch 2387/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 87.7748 - mae: 88.4626 - val_loss: 9312.1143 - val_mae: 9312.8076\n",
      "Epoch 2388/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 86.2491 - mae: 86.9359 - val_loss: 9330.6807 - val_mae: 9331.3721\n",
      "Epoch 2389/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 84.7671 - mae: 85.4510 - val_loss: 9223.0381 - val_mae: 9223.7314\n",
      "Epoch 2390/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 70.4735 - mae: 71.1525 - val_loss: 9390.9600 - val_mae: 9391.6523\n",
      "Epoch 2391/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 93.4808 - mae: 94.1659 - val_loss: 9305.3232 - val_mae: 9306.0166\n",
      "Epoch 2392/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 82.4064 - mae: 83.0909 - val_loss: 9331.1494 - val_mae: 9331.8428\n",
      "Epoch 2393/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 80.8096 - mae: 81.4933 - val_loss: 9239.5195 - val_mae: 9240.2119\n",
      "Epoch 2394/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 77.8491 - mae: 78.5371 - val_loss: 9451.8945 - val_mae: 9452.5869\n",
      "Epoch 2395/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 81.1755 - mae: 81.8635 - val_loss: 9281.5459 - val_mae: 9282.2383\n",
      "Epoch 2396/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 83.7049 - mae: 84.3891 - val_loss: 9404.9893 - val_mae: 9405.6826\n",
      "Epoch 2397/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 107.3755 - mae: 108.0616 - val_loss: 9163.8682 - val_mae: 9164.5625\n",
      "Epoch 2398/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 87.4654 - mae: 88.1515 - val_loss: 9066.2578 - val_mae: 9066.9502\n",
      "Epoch 2399/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 82.8030 - mae: 83.4894 - val_loss: 9329.7949 - val_mae: 9330.4873\n",
      "Epoch 2400/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 81.1921 - mae: 81.8791 - val_loss: 9261.4385 - val_mae: 9262.1328\n",
      "Epoch 2401/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 98.1552 - mae: 98.8440 - val_loss: 9403.2285 - val_mae: 9403.9209\n",
      "Epoch 2402/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 76.3896 - mae: 77.0789 - val_loss: 9399.1758 - val_mae: 9399.8691\n",
      "Epoch 2403/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 89.8936 - mae: 90.5790 - val_loss: 9287.9678 - val_mae: 9288.6602\n",
      "Epoch 2404/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 75.8372 - mae: 76.5198 - val_loss: 9350.4111 - val_mae: 9351.1055\n",
      "Epoch 2405/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 75.8083 - mae: 76.4937 - val_loss: 9378.4434 - val_mae: 9379.1348\n",
      "Epoch 2406/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 83.6582 - mae: 84.3401 - val_loss: 9266.7285 - val_mae: 9267.4219\n",
      "Epoch 2407/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 90.4218 - mae: 91.1053 - val_loss: 9331.5088 - val_mae: 9332.2031\n",
      "Epoch 2408/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 82.3017 - mae: 82.9850 - val_loss: 9066.6768 - val_mae: 9067.3691\n",
      "Epoch 2409/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 92.6942 - mae: 93.3794 - val_loss: 9394.3398 - val_mae: 9395.0322\n",
      "Epoch 2410/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 84.0502 - mae: 84.7294 - val_loss: 9408.9619 - val_mae: 9409.6543\n",
      "Epoch 2411/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 86.8251 - mae: 87.5132 - val_loss: 9204.7578 - val_mae: 9205.4512\n",
      "Epoch 2412/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 87.4366 - mae: 88.1202 - val_loss: 9245.0830 - val_mae: 9245.7744\n",
      "Epoch 2413/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 80.6401 - mae: 81.3273 - val_loss: 9185.9424 - val_mae: 9186.6348\n",
      "Epoch 2414/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 113.9443 - mae: 114.6300 - val_loss: 9516.4619 - val_mae: 9517.1562\n",
      "Epoch 2415/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 83.8873 - mae: 84.5698 - val_loss: 9408.1992 - val_mae: 9408.8936\n",
      "Epoch 2416/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 79.9291 - mae: 80.6103 - val_loss: 9247.0234 - val_mae: 9247.7168\n",
      "Epoch 2417/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 89.8456 - mae: 90.5330 - val_loss: 9329.3447 - val_mae: 9330.0391\n",
      "Epoch 2418/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 82.8389 - mae: 83.5252 - val_loss: 9221.4941 - val_mae: 9222.1885\n",
      "Epoch 2419/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 79.1881 - mae: 79.8754 - val_loss: 9360.8838 - val_mae: 9361.5762\n",
      "Epoch 2420/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 89.2397 - mae: 89.9254 - val_loss: 9474.1035 - val_mae: 9474.7969\n",
      "Epoch 2421/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 83.2589 - mae: 83.9418 - val_loss: 9364.4424 - val_mae: 9365.1357\n",
      "Epoch 2422/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 81.0768 - mae: 81.7637 - val_loss: 9304.7412 - val_mae: 9305.4346\n",
      "Epoch 2423/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 80.4767 - mae: 81.1613 - val_loss: 9610.0361 - val_mae: 9610.7305\n",
      "Epoch 2424/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 94.2074 - mae: 94.8944 - val_loss: 9362.4248 - val_mae: 9363.1182\n",
      "Epoch 2425/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 104.3727 - mae: 105.0582 - val_loss: 9418.9463 - val_mae: 9419.6396\n",
      "Epoch 2426/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 87.4453 - mae: 88.1312 - val_loss: 9277.9600 - val_mae: 9278.6533\n",
      "Epoch 2427/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 88.1156 - mae: 88.8011 - val_loss: 9450.7305 - val_mae: 9451.4229\n",
      "Epoch 2428/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 76.0822 - mae: 76.7700 - val_loss: 9248.7471 - val_mae: 9249.4414\n",
      "Epoch 2429/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 75.5592 - mae: 76.2434 - val_loss: 9412.9561 - val_mae: 9413.6504\n",
      "Epoch 2430/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 82.5463 - mae: 83.2344 - val_loss: 9156.2764 - val_mae: 9156.9688\n",
      "Epoch 2431/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 95.1381 - mae: 95.8277 - val_loss: 9281.2236 - val_mae: 9281.9170\n",
      "Epoch 2432/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 77.9064 - mae: 78.5911 - val_loss: 9171.4150 - val_mae: 9172.1074\n",
      "Epoch 2433/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 90.8116 - mae: 91.5004 - val_loss: 9337.7686 - val_mae: 9338.4609\n",
      "Epoch 2434/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 94.0807 - mae: 94.7656 - val_loss: 9409.5889 - val_mae: 9410.2812\n",
      "Epoch 2435/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 84.1178 - mae: 84.8045 - val_loss: 9318.8105 - val_mae: 9319.5029\n",
      "Epoch 2436/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 77.7196 - mae: 78.4033 - val_loss: 9474.4990 - val_mae: 9475.1924\n",
      "Epoch 2437/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 86.5395 - mae: 87.2204 - val_loss: 9197.2822 - val_mae: 9197.9746\n",
      "Epoch 2438/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 73.9876 - mae: 74.6720 - val_loss: 9242.9014 - val_mae: 9243.5947\n",
      "Epoch 2439/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 86.3238 - mae: 87.0079 - val_loss: 9211.5801 - val_mae: 9212.2744\n",
      "Epoch 2440/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 90.4079 - mae: 91.0968 - val_loss: 9332.6533 - val_mae: 9333.3486\n",
      "Epoch 2441/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 89.9623 - mae: 90.6478 - val_loss: 9295.2363 - val_mae: 9295.9297\n",
      "Epoch 2442/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 81.6841 - mae: 82.3669 - val_loss: 9386.3740 - val_mae: 9387.0664\n",
      "Epoch 2443/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 82.5400 - mae: 83.2241 - val_loss: 9077.7109 - val_mae: 9078.4033\n",
      "Epoch 2444/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 89.9172 - mae: 90.6028 - val_loss: 9310.3584 - val_mae: 9311.0508\n",
      "Epoch 2445/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 86.1644 - mae: 86.8468 - val_loss: 9497.0840 - val_mae: 9497.7764\n",
      "Epoch 2446/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 94.8014 - mae: 95.4899 - val_loss: 9219.8877 - val_mae: 9220.5801\n",
      "Epoch 2447/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 77.7894 - mae: 78.4706 - val_loss: 9451.8496 - val_mae: 9452.5430\n",
      "Epoch 2448/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 87.9730 - mae: 88.6570 - val_loss: 9275.3770 - val_mae: 9276.0703\n",
      "Epoch 2449/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 90.1395 - mae: 90.8265 - val_loss: 9353.8193 - val_mae: 9354.5117\n",
      "Epoch 2450/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 69.1340 - mae: 69.8157 - val_loss: 9425.5762 - val_mae: 9426.2695\n",
      "Epoch 2451/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 76.9662 - mae: 77.6534 - val_loss: 9182.5205 - val_mae: 9183.2139\n",
      "Epoch 2452/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 80.6721 - mae: 81.3553 - val_loss: 9137.9473 - val_mae: 9138.6387\n",
      "Epoch 2453/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 83.9075 - mae: 84.5942 - val_loss: 9378.9727 - val_mae: 9379.6660\n",
      "Epoch 2454/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 88.9059 - mae: 89.5933 - val_loss: 9111.6611 - val_mae: 9112.3535\n",
      "Epoch 2455/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 88.3664 - mae: 89.0509 - val_loss: 9308.4717 - val_mae: 9309.1650\n",
      "Epoch 2456/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 101.3874 - mae: 102.0748 - val_loss: 9250.0459 - val_mae: 9250.7383\n",
      "Epoch 2457/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 88.0234 - mae: 88.7100 - val_loss: 9222.3623 - val_mae: 9223.0557\n",
      "Epoch 2458/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 86.5327 - mae: 87.2181 - val_loss: 9062.6992 - val_mae: 9063.3916\n",
      "Epoch 2459/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 80.2406 - mae: 80.9212 - val_loss: 9192.5811 - val_mae: 9193.2764\n",
      "Epoch 2460/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 93.4038 - mae: 94.0910 - val_loss: 9389.8994 - val_mae: 9390.5918\n",
      "Epoch 2461/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 80.6273 - mae: 81.3146 - val_loss: 9173.2393 - val_mae: 9173.9307\n",
      "Epoch 2462/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 89.7818 - mae: 90.4664 - val_loss: 9370.6035 - val_mae: 9371.2969\n",
      "Epoch 2463/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 79.9572 - mae: 80.6405 - val_loss: 9376.4297 - val_mae: 9377.1230\n",
      "Epoch 2464/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 77.3642 - mae: 78.0484 - val_loss: 9553.2510 - val_mae: 9553.9443\n",
      "Epoch 2465/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 81.6923 - mae: 82.3761 - val_loss: 9340.3271 - val_mae: 9341.0195\n",
      "Epoch 2466/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 80.8597 - mae: 81.5400 - val_loss: 9653.1387 - val_mae: 9653.8330\n",
      "Epoch 2467/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 102.6552 - mae: 103.3425 - val_loss: 9226.6494 - val_mae: 9227.3428\n",
      "Epoch 2468/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 89.8741 - mae: 90.5619 - val_loss: 9331.1494 - val_mae: 9331.8408\n",
      "Epoch 2469/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 94.9263 - mae: 95.6134 - val_loss: 9281.4619 - val_mae: 9282.1543\n",
      "Epoch 2470/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 82.5417 - mae: 83.2291 - val_loss: 9274.8750 - val_mae: 9275.5684\n",
      "Epoch 2471/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 88.2642 - mae: 88.9474 - val_loss: 9507.4873 - val_mae: 9508.1797\n",
      "Epoch 2472/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 79.5176 - mae: 80.1992 - val_loss: 9304.3135 - val_mae: 9305.0078\n",
      "Epoch 2473/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 72.1688 - mae: 72.8477 - val_loss: 9240.7178 - val_mae: 9241.4111\n",
      "Epoch 2474/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 80.5297 - mae: 81.2114 - val_loss: 9310.6094 - val_mae: 9311.3018\n",
      "Epoch 2475/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 99.2626 - mae: 99.9514 - val_loss: 9267.0713 - val_mae: 9267.7637\n",
      "Epoch 2476/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 75.3200 - mae: 76.0041 - val_loss: 9410.9043 - val_mae: 9411.5967\n",
      "Epoch 2477/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 81.1040 - mae: 81.7922 - val_loss: 9395.2334 - val_mae: 9395.9268\n",
      "Epoch 2478/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 97.8922 - mae: 98.5787 - val_loss: 9461.5635 - val_mae: 9462.2568\n",
      "Epoch 2479/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 102.4063 - mae: 103.0896 - val_loss: 9239.6064 - val_mae: 9240.2998\n",
      "Epoch 2480/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 79.4910 - mae: 80.1714 - val_loss: 9273.9404 - val_mae: 9274.6338\n",
      "Epoch 2481/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 87.3519 - mae: 88.0359 - val_loss: 8930.8516 - val_mae: 8931.5459\n",
      "Epoch 2482/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 94.4935 - mae: 95.1797 - val_loss: 9309.7295 - val_mae: 9310.4219\n",
      "Epoch 2483/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 74.9293 - mae: 75.6167 - val_loss: 9044.2412 - val_mae: 9044.9316\n",
      "Epoch 2484/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 94.3488 - mae: 95.0314 - val_loss: 9180.5879 - val_mae: 9181.2822\n",
      "Epoch 2485/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 88.8307 - mae: 89.5164 - val_loss: 9199.3242 - val_mae: 9200.0186\n",
      "Epoch 2486/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 86.2820 - mae: 86.9689 - val_loss: 9364.9883 - val_mae: 9365.6826\n",
      "Epoch 2487/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 86.3879 - mae: 87.0723 - val_loss: 9297.8936 - val_mae: 9298.5869\n",
      "Epoch 2488/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 94.4558 - mae: 95.1430 - val_loss: 9386.3066 - val_mae: 9387.0010\n",
      "Epoch 2489/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 90.5972 - mae: 91.2839 - val_loss: 9282.5518 - val_mae: 9283.2471\n",
      "Epoch 2490/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 99.9076 - mae: 100.5928 - val_loss: 9274.7227 - val_mae: 9275.4170\n",
      "Epoch 2491/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 92.8317 - mae: 93.5197 - val_loss: 9370.0156 - val_mae: 9370.7090\n",
      "Epoch 2492/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 86.2870 - mae: 86.9733 - val_loss: 9244.6748 - val_mae: 9245.3672\n",
      "Epoch 2493/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 97.2110 - mae: 97.9004 - val_loss: 9243.0107 - val_mae: 9243.7041\n",
      "Epoch 2494/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 85.4764 - mae: 86.1641 - val_loss: 9244.5098 - val_mae: 9245.2021\n",
      "Epoch 2495/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 78.3548 - mae: 79.0363 - val_loss: 9422.3994 - val_mae: 9423.0918\n",
      "Epoch 2496/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 104.2168 - mae: 104.9014 - val_loss: 9317.4834 - val_mae: 9318.1758\n",
      "Epoch 2497/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 86.4943 - mae: 87.1839 - val_loss: 9278.3496 - val_mae: 9279.0430\n",
      "Epoch 2498/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 91.5568 - mae: 92.2437 - val_loss: 9152.2539 - val_mae: 9152.9482\n",
      "Epoch 2499/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 97.9707 - mae: 98.6598 - val_loss: 9348.6699 - val_mae: 9349.3633\n",
      "Epoch 2500/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 94.2311 - mae: 94.9178 - val_loss: 9090.0635 - val_mae: 9090.7559\n",
      "Epoch 2501/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 94.3106 - mae: 94.9961 - val_loss: 9179.6162 - val_mae: 9180.3096\n",
      "Epoch 2502/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 91.6991 - mae: 92.3846 - val_loss: 9265.0957 - val_mae: 9265.7891\n",
      "Epoch 2503/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 89.6313 - mae: 90.3177 - val_loss: 9371.3086 - val_mae: 9372.0010\n",
      "Epoch 2504/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 89.8546 - mae: 90.5395 - val_loss: 9263.7773 - val_mae: 9264.4717\n",
      "Epoch 2505/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 100.0040 - mae: 100.6888 - val_loss: 9133.3164 - val_mae: 9134.0098\n",
      "Epoch 2506/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 84.1893 - mae: 84.8740 - val_loss: 9224.0068 - val_mae: 9224.7002\n",
      "Epoch 2507/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 77.9842 - mae: 78.6676 - val_loss: 9164.7129 - val_mae: 9165.4062\n",
      "Epoch 2508/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 92.6519 - mae: 93.3402 - val_loss: 9396.9883 - val_mae: 9397.6826\n",
      "Epoch 2509/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 84.9195 - mae: 85.6039 - val_loss: 9225.5479 - val_mae: 9226.2422\n",
      "Epoch 2510/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 82.1470 - mae: 82.8323 - val_loss: 9238.6758 - val_mae: 9239.3691\n",
      "Epoch 2511/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 71.3319 - mae: 72.0167 - val_loss: 9231.3105 - val_mae: 9232.0029\n",
      "Epoch 2512/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 77.5880 - mae: 78.2704 - val_loss: 9129.5225 - val_mae: 9130.2148\n",
      "Epoch 2513/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 79.6440 - mae: 80.3246 - val_loss: 9192.0586 - val_mae: 9192.7510\n",
      "Epoch 2514/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 76.6644 - mae: 77.3523 - val_loss: 9206.9541 - val_mae: 9207.6484\n",
      "Epoch 2515/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 84.2592 - mae: 84.9458 - val_loss: 9293.4883 - val_mae: 9294.1816\n",
      "Epoch 2516/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 89.1950 - mae: 89.8819 - val_loss: 9258.9609 - val_mae: 9259.6553\n",
      "Epoch 2517/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 80.1166 - mae: 80.8001 - val_loss: 9314.4561 - val_mae: 9315.1514\n",
      "Epoch 2518/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 88.4875 - mae: 89.1706 - val_loss: 9140.5830 - val_mae: 9141.2764\n",
      "Epoch 2519/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 71.0116 - mae: 71.6994 - val_loss: 9290.2314 - val_mae: 9290.9258\n",
      "Epoch 2520/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 81.7660 - mae: 82.4515 - val_loss: 9302.9717 - val_mae: 9303.6650\n",
      "Epoch 2521/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 88.6709 - mae: 89.3579 - val_loss: 9266.8467 - val_mae: 9267.5391\n",
      "Epoch 2522/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 81.1001 - mae: 81.7836 - val_loss: 9243.6475 - val_mae: 9244.3389\n",
      "Epoch 2523/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 84.6964 - mae: 85.3808 - val_loss: 9279.6885 - val_mae: 9280.3809\n",
      "Epoch 2524/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 87.6182 - mae: 88.3007 - val_loss: 9275.2666 - val_mae: 9275.9600\n",
      "Epoch 2525/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 91.2326 - mae: 91.9159 - val_loss: 9357.5498 - val_mae: 9358.2412\n",
      "Epoch 2526/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 88.6144 - mae: 89.2989 - val_loss: 9042.1689 - val_mae: 9042.8604\n",
      "Epoch 2527/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 91.3640 - mae: 92.0472 - val_loss: 9227.2012 - val_mae: 9227.8945\n",
      "Epoch 2528/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 83.6014 - mae: 84.2885 - val_loss: 9236.3486 - val_mae: 9237.0420\n",
      "Epoch 2529/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 85.7842 - mae: 86.4696 - val_loss: 9282.8496 - val_mae: 9283.5449\n",
      "Epoch 2530/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 79.7406 - mae: 80.4250 - val_loss: 9320.5928 - val_mae: 9321.2852\n",
      "Epoch 2531/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 82.3547 - mae: 83.0411 - val_loss: 9304.7861 - val_mae: 9305.4795\n",
      "Epoch 2532/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 77.1477 - mae: 77.8292 - val_loss: 9316.0723 - val_mae: 9316.7656\n",
      "Epoch 2533/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 81.1977 - mae: 81.8858 - val_loss: 9358.8486 - val_mae: 9359.5420\n",
      "Epoch 2534/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 90.9854 - mae: 91.6715 - val_loss: 9315.2617 - val_mae: 9315.9551\n",
      "Epoch 2535/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 76.1210 - mae: 76.8050 - val_loss: 9167.1504 - val_mae: 9167.8428\n",
      "Epoch 2536/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 76.5994 - mae: 77.2867 - val_loss: 9426.4238 - val_mae: 9427.1182\n",
      "Epoch 2537/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 74.6792 - mae: 75.3619 - val_loss: 9188.9844 - val_mae: 9189.6758\n",
      "Epoch 2538/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 87.1025 - mae: 87.7872 - val_loss: 9388.7490 - val_mae: 9389.4424\n",
      "Epoch 2539/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 83.9669 - mae: 84.6501 - val_loss: 9388.2510 - val_mae: 9388.9463\n",
      "Epoch 2540/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 85.3701 - mae: 86.0577 - val_loss: 9371.8779 - val_mae: 9372.5703\n",
      "Epoch 2541/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 95.7621 - mae: 96.4424 - val_loss: 9421.0420 - val_mae: 9421.7363\n",
      "Epoch 2542/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 96.2114 - mae: 96.8981 - val_loss: 9275.0205 - val_mae: 9275.7129\n",
      "Epoch 2543/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 89.5079 - mae: 90.1934 - val_loss: 9402.8525 - val_mae: 9403.5469\n",
      "Epoch 2544/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 85.9282 - mae: 86.6137 - val_loss: 9326.8525 - val_mae: 9327.5449\n",
      "Epoch 2545/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 71.8933 - mae: 72.5770 - val_loss: 9105.8359 - val_mae: 9106.5283\n",
      "Epoch 2546/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 80.4865 - mae: 81.1717 - val_loss: 9287.1934 - val_mae: 9287.8867\n",
      "Epoch 2547/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 76.0889 - mae: 76.7736 - val_loss: 9356.7812 - val_mae: 9357.4746\n",
      "Epoch 2548/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 78.2274 - mae: 78.9107 - val_loss: 9306.9287 - val_mae: 9307.6201\n",
      "Epoch 2549/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 90.3864 - mae: 91.0698 - val_loss: 9371.5312 - val_mae: 9372.2246\n",
      "Epoch 2550/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 94.3965 - mae: 95.0802 - val_loss: 9262.6602 - val_mae: 9263.3525\n",
      "Epoch 2551/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 80.9196 - mae: 81.5978 - val_loss: 9299.0684 - val_mae: 9299.7617\n",
      "Epoch 2552/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 91.3069 - mae: 91.9942 - val_loss: 9469.3906 - val_mae: 9470.0840\n",
      "Epoch 2553/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 80.6759 - mae: 81.3648 - val_loss: 9326.5625 - val_mae: 9327.2539\n",
      "Epoch 2554/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 69.3280 - mae: 70.0113 - val_loss: 9246.3379 - val_mae: 9247.0303\n",
      "Epoch 2555/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 79.7298 - mae: 80.4105 - val_loss: 9434.2891 - val_mae: 9434.9824\n",
      "Epoch 2556/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 77.8674 - mae: 78.5484 - val_loss: 9188.6641 - val_mae: 9189.3584\n",
      "Epoch 2557/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 84.8774 - mae: 85.5603 - val_loss: 9209.3545 - val_mae: 9210.0488\n",
      "Epoch 2558/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 75.5500 - mae: 76.2340 - val_loss: 9182.8779 - val_mae: 9183.5713\n",
      "Epoch 2559/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 81.3469 - mae: 82.0256 - val_loss: 9537.7520 - val_mae: 9538.4463\n",
      "Epoch 2560/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 92.0396 - mae: 92.7251 - val_loss: 9108.0312 - val_mae: 9108.7227\n",
      "Epoch 2561/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 91.7098 - mae: 92.3943 - val_loss: 9372.9531 - val_mae: 9373.6475\n",
      "Epoch 2562/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 76.5942 - mae: 77.2773 - val_loss: 9228.7656 - val_mae: 9229.4580\n",
      "Epoch 2563/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 97.4456 - mae: 98.1348 - val_loss: 9317.7725 - val_mae: 9318.4658\n",
      "Epoch 2564/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 86.5076 - mae: 87.1936 - val_loss: 9253.7822 - val_mae: 9254.4756\n",
      "Epoch 2565/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 88.0469 - mae: 88.7316 - val_loss: 8992.4590 - val_mae: 8993.1523\n",
      "Epoch 2566/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 81.1811 - mae: 81.8658 - val_loss: 9209.1777 - val_mae: 9209.8711\n",
      "Epoch 2567/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 77.5501 - mae: 78.2309 - val_loss: 9470.9023 - val_mae: 9471.5938\n",
      "Epoch 2568/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 82.1720 - mae: 82.8593 - val_loss: 9361.6973 - val_mae: 9362.3896\n",
      "Epoch 2569/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 86.9536 - mae: 87.6402 - val_loss: 9015.9424 - val_mae: 9016.6357\n",
      "Epoch 2570/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 85.9249 - mae: 86.6115 - val_loss: 9147.9014 - val_mae: 9148.5928\n",
      "Epoch 2571/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 89.8960 - mae: 90.5769 - val_loss: 9216.7695 - val_mae: 9217.4629\n",
      "Epoch 2572/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 79.1835 - mae: 79.8707 - val_loss: 9275.0488 - val_mae: 9275.7412\n",
      "Epoch 2573/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 80.6087 - mae: 81.2915 - val_loss: 9214.4443 - val_mae: 9215.1377\n",
      "Epoch 2574/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 104.3158 - mae: 105.0032 - val_loss: 9348.0918 - val_mae: 9348.7852\n",
      "Epoch 2575/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 85.5771 - mae: 86.2651 - val_loss: 9342.5352 - val_mae: 9343.2295\n",
      "Epoch 2576/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 70.8105 - mae: 71.4914 - val_loss: 9149.6357 - val_mae: 9150.3301\n",
      "Epoch 2577/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 70.8147 - mae: 71.4996 - val_loss: 9393.0830 - val_mae: 9393.7764\n",
      "Epoch 2578/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 78.5667 - mae: 79.2542 - val_loss: 9388.1514 - val_mae: 9388.8447\n",
      "Epoch 2579/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 78.0053 - mae: 78.6874 - val_loss: 9249.6455 - val_mae: 9250.3379\n",
      "Epoch 2580/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 94.4603 - mae: 95.1475 - val_loss: 9233.1162 - val_mae: 9233.8086\n",
      "Epoch 2581/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 93.8020 - mae: 94.4897 - val_loss: 9432.7627 - val_mae: 9433.4561\n",
      "Epoch 2582/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 81.1166 - mae: 81.7995 - val_loss: 9255.6309 - val_mae: 9256.3242\n",
      "Epoch 2583/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 83.1592 - mae: 83.8449 - val_loss: 9428.0986 - val_mae: 9428.7910\n",
      "Epoch 2584/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 90.1772 - mae: 90.8597 - val_loss: 9291.7344 - val_mae: 9292.4287\n",
      "Epoch 2585/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 80.1585 - mae: 80.8430 - val_loss: 9277.6113 - val_mae: 9278.3037\n",
      "Epoch 2586/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 73.0220 - mae: 73.7034 - val_loss: 9259.4766 - val_mae: 9260.1689\n",
      "Epoch 2587/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 81.5444 - mae: 82.2273 - val_loss: 9171.9697 - val_mae: 9172.6621\n",
      "Epoch 2588/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 81.4397 - mae: 82.1233 - val_loss: 9115.5977 - val_mae: 9116.2900\n",
      "Epoch 2589/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 93.1787 - mae: 93.8638 - val_loss: 9276.4609 - val_mae: 9277.1543\n",
      "Epoch 2590/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 90.1219 - mae: 90.8068 - val_loss: 9254.5781 - val_mae: 9255.2725\n",
      "Epoch 2591/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 95.1615 - mae: 95.8464 - val_loss: 9172.5029 - val_mae: 9173.1963\n",
      "Epoch 2592/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 73.6640 - mae: 74.3468 - val_loss: 9233.8809 - val_mae: 9234.5742\n",
      "Epoch 2593/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 86.2860 - mae: 86.9724 - val_loss: 9325.3145 - val_mae: 9326.0078\n",
      "Epoch 2594/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 75.8361 - mae: 76.5128 - val_loss: 9291.2881 - val_mae: 9291.9805\n",
      "Epoch 2595/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 77.1644 - mae: 77.8499 - val_loss: 9184.8203 - val_mae: 9185.5127\n",
      "Epoch 2596/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 77.5886 - mae: 78.2702 - val_loss: 9047.9160 - val_mae: 9048.6074\n",
      "Epoch 2597/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 79.4102 - mae: 80.0970 - val_loss: 9248.6240 - val_mae: 9249.3174\n",
      "Epoch 2598/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 76.4151 - mae: 77.1009 - val_loss: 9229.5732 - val_mae: 9230.2656\n",
      "Epoch 2599/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 93.1632 - mae: 93.8458 - val_loss: 9139.9414 - val_mae: 9140.6338\n",
      "Epoch 2600/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 96.3683 - mae: 97.0566 - val_loss: 9081.8223 - val_mae: 9082.5156\n",
      "Epoch 2601/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 82.4080 - mae: 83.0921 - val_loss: 9030.2246 - val_mae: 9030.9170\n",
      "Epoch 2602/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 90.9279 - mae: 91.6145 - val_loss: 9342.7080 - val_mae: 9343.4023\n",
      "Epoch 2603/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 91.2315 - mae: 91.9190 - val_loss: 9498.7725 - val_mae: 9499.4668\n",
      "Epoch 2604/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 91.0893 - mae: 91.7760 - val_loss: 9369.7227 - val_mae: 9370.4160\n",
      "Epoch 2605/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 74.4128 - mae: 75.0961 - val_loss: 9270.3330 - val_mae: 9271.0254\n",
      "Epoch 2606/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 78.2070 - mae: 78.8903 - val_loss: 9214.1807 - val_mae: 9214.8730\n",
      "Epoch 2607/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 82.5060 - mae: 83.1879 - val_loss: 9287.0371 - val_mae: 9287.7305\n",
      "Epoch 2608/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 91.6925 - mae: 92.3777 - val_loss: 9167.1602 - val_mae: 9167.8525\n",
      "Epoch 2609/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 99.7902 - mae: 100.4752 - val_loss: 9083.3662 - val_mae: 9084.0605\n",
      "Epoch 2610/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 90.1717 - mae: 90.8512 - val_loss: 9359.0928 - val_mae: 9359.7861\n",
      "Epoch 2611/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 81.8517 - mae: 82.5359 - val_loss: 9261.9932 - val_mae: 9262.6865\n",
      "Epoch 2612/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 78.0891 - mae: 78.7738 - val_loss: 9280.3662 - val_mae: 9281.0586\n",
      "Epoch 2613/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 96.1074 - mae: 96.7978 - val_loss: 9214.5654 - val_mae: 9215.2588\n",
      "Epoch 2614/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 103.7772 - mae: 104.4625 - val_loss: 9294.5205 - val_mae: 9295.2129\n",
      "Epoch 2615/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 76.6351 - mae: 77.3168 - val_loss: 9232.7871 - val_mae: 9233.4795\n",
      "Epoch 2616/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 72.5980 - mae: 73.2838 - val_loss: 9051.8857 - val_mae: 9052.5781\n",
      "Epoch 2617/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 91.6244 - mae: 92.3078 - val_loss: 9173.9131 - val_mae: 9174.6055\n",
      "Epoch 2618/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 78.2110 - mae: 78.8970 - val_loss: 9334.1455 - val_mae: 9334.8379\n",
      "Epoch 2619/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 73.5811 - mae: 74.2683 - val_loss: 9248.1211 - val_mae: 9248.8135\n",
      "Epoch 2620/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 83.3292 - mae: 84.0150 - val_loss: 9323.2500 - val_mae: 9323.9424\n",
      "Epoch 2621/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 96.0382 - mae: 96.7230 - val_loss: 9101.7861 - val_mae: 9102.4785\n",
      "Epoch 2622/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 83.4260 - mae: 84.1084 - val_loss: 9112.4854 - val_mae: 9113.1787\n",
      "Epoch 2623/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 80.0119 - mae: 80.6914 - val_loss: 9123.8721 - val_mae: 9124.5654\n",
      "Epoch 2624/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 84.5413 - mae: 85.2232 - val_loss: 9197.5996 - val_mae: 9198.2930\n",
      "Epoch 2625/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 85.9311 - mae: 86.6151 - val_loss: 9417.0996 - val_mae: 9417.7930\n",
      "Epoch 2626/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 80.3467 - mae: 81.0305 - val_loss: 9306.9531 - val_mae: 9307.6475\n",
      "Epoch 2627/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 104.4583 - mae: 105.1464 - val_loss: 9351.8525 - val_mae: 9352.5449\n",
      "Epoch 2628/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 88.5309 - mae: 89.2147 - val_loss: 9199.4697 - val_mae: 9200.1631\n",
      "Epoch 2629/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 84.5776 - mae: 85.2643 - val_loss: 9104.7861 - val_mae: 9105.4795\n",
      "Epoch 2630/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 84.7097 - mae: 85.3981 - val_loss: 9159.5273 - val_mae: 9160.2207\n",
      "Epoch 2631/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 88.9832 - mae: 89.6706 - val_loss: 9280.4561 - val_mae: 9281.1494\n",
      "Epoch 2632/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 82.8429 - mae: 83.5283 - val_loss: 9254.3711 - val_mae: 9255.0645\n",
      "Epoch 2633/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 74.3945 - mae: 75.0743 - val_loss: 9124.1514 - val_mae: 9124.8447\n",
      "Epoch 2634/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 80.0281 - mae: 80.7151 - val_loss: 9187.4043 - val_mae: 9188.0967\n",
      "Epoch 2635/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 80.3970 - mae: 81.0819 - val_loss: 9251.7100 - val_mae: 9252.4043\n",
      "Epoch 2636/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 80.2828 - mae: 80.9702 - val_loss: 9041.8359 - val_mae: 9042.5273\n",
      "Epoch 2637/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 101.6538 - mae: 102.3402 - val_loss: 9347.3340 - val_mae: 9348.0264\n",
      "Epoch 2638/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 85.5190 - mae: 86.2082 - val_loss: 9298.8779 - val_mae: 9299.5713\n",
      "Epoch 2639/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 87.2128 - mae: 87.8999 - val_loss: 9275.7441 - val_mae: 9276.4365\n",
      "Epoch 2640/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 81.0787 - mae: 81.7672 - val_loss: 9208.0498 - val_mae: 9208.7441\n",
      "Epoch 2641/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 73.9987 - mae: 74.6834 - val_loss: 9423.0400 - val_mae: 9423.7324\n",
      "Epoch 2642/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 85.0980 - mae: 85.7841 - val_loss: 9172.4922 - val_mae: 9173.1846\n",
      "Epoch 2643/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 72.5128 - mae: 73.1963 - val_loss: 9086.6113 - val_mae: 9087.3057\n",
      "Epoch 2644/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 83.3224 - mae: 84.0106 - val_loss: 9266.2930 - val_mae: 9266.9863\n",
      "Epoch 2645/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 102.2324 - mae: 102.9196 - val_loss: 9104.9590 - val_mae: 9105.6533\n",
      "Epoch 2646/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 95.6741 - mae: 96.3605 - val_loss: 9159.2275 - val_mae: 9159.9209\n",
      "Epoch 2647/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 77.7642 - mae: 78.4478 - val_loss: 9484.0156 - val_mae: 9484.7100\n",
      "Epoch 2648/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 83.2082 - mae: 83.8891 - val_loss: 9312.8389 - val_mae: 9313.5322\n",
      "Epoch 2649/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 72.5386 - mae: 73.2250 - val_loss: 9227.8496 - val_mae: 9228.5430\n",
      "Epoch 2650/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 79.1277 - mae: 79.8144 - val_loss: 9239.7041 - val_mae: 9240.3945\n",
      "Epoch 2651/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 70.7672 - mae: 71.4492 - val_loss: 9261.2031 - val_mae: 9261.8955\n",
      "Epoch 2652/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 72.6935 - mae: 73.3739 - val_loss: 9110.6445 - val_mae: 9111.3369\n",
      "Epoch 2653/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 79.9138 - mae: 80.6004 - val_loss: 9321.4746 - val_mae: 9322.1680\n",
      "Epoch 2654/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 73.8537 - mae: 74.5361 - val_loss: 9332.8906 - val_mae: 9333.5840\n",
      "Epoch 2655/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 78.6622 - mae: 79.3474 - val_loss: 9165.3721 - val_mae: 9166.0645\n",
      "Epoch 2656/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 87.6631 - mae: 88.3521 - val_loss: 9426.7188 - val_mae: 9427.4111\n",
      "Epoch 2657/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 90.1694 - mae: 90.8533 - val_loss: 9356.9834 - val_mae: 9357.6768\n",
      "Epoch 2658/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 82.8199 - mae: 83.5032 - val_loss: 9295.7236 - val_mae: 9296.4160\n",
      "Epoch 2659/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 83.5530 - mae: 84.2364 - val_loss: 9259.5498 - val_mae: 9260.2441\n",
      "Epoch 2660/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 79.9782 - mae: 80.6626 - val_loss: 9278.0830 - val_mae: 9278.7764\n",
      "Epoch 2661/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 90.9553 - mae: 91.6403 - val_loss: 9290.3359 - val_mae: 9291.0293\n",
      "Epoch 2662/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 65.6572 - mae: 66.3403 - val_loss: 9240.4502 - val_mae: 9241.1436\n",
      "Epoch 2663/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 78.3978 - mae: 79.0819 - val_loss: 9217.8174 - val_mae: 9218.5098\n",
      "Epoch 2664/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 76.6477 - mae: 77.3326 - val_loss: 9142.6172 - val_mae: 9143.3096\n",
      "Epoch 2665/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 86.2707 - mae: 86.9572 - val_loss: 9298.0088 - val_mae: 9298.7021\n",
      "Epoch 2666/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 84.0425 - mae: 84.7263 - val_loss: 9245.1914 - val_mae: 9245.8838\n",
      "Epoch 2667/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 80.1984 - mae: 80.8845 - val_loss: 9195.8008 - val_mae: 9196.4932\n",
      "Epoch 2668/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 73.5989 - mae: 74.2776 - val_loss: 9134.1074 - val_mae: 9134.7998\n",
      "Epoch 2669/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 86.3382 - mae: 87.0264 - val_loss: 9296.5342 - val_mae: 9297.2266\n",
      "Epoch 2670/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 81.9821 - mae: 82.6626 - val_loss: 9115.5225 - val_mae: 9116.2139\n",
      "Epoch 2671/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 85.7249 - mae: 86.4092 - val_loss: 9235.1963 - val_mae: 9235.8887\n",
      "Epoch 2672/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 84.3121 - mae: 85.0011 - val_loss: 9165.4092 - val_mae: 9166.1025\n",
      "Epoch 2673/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 83.4229 - mae: 84.1047 - val_loss: 9221.2842 - val_mae: 9221.9775\n",
      "Epoch 2674/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 76.7821 - mae: 77.4647 - val_loss: 8941.9443 - val_mae: 8942.6357\n",
      "Epoch 2675/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 84.8285 - mae: 85.5094 - val_loss: 9172.5771 - val_mae: 9173.2705\n",
      "Epoch 2676/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 68.4600 - mae: 69.1410 - val_loss: 9258.0312 - val_mae: 9258.7246\n",
      "Epoch 2677/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 74.1014 - mae: 74.7812 - val_loss: 9130.9482 - val_mae: 9131.6436\n",
      "Epoch 2678/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 87.8374 - mae: 88.5208 - val_loss: 9266.3604 - val_mae: 9267.0527\n",
      "Epoch 2679/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 84.4423 - mae: 85.1257 - val_loss: 9091.6426 - val_mae: 9092.3359\n",
      "Epoch 2680/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 83.7012 - mae: 84.3875 - val_loss: 9058.5596 - val_mae: 9059.2539\n",
      "Epoch 2681/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 86.1514 - mae: 86.8361 - val_loss: 9094.9404 - val_mae: 9095.6338\n",
      "Epoch 2682/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 71.3725 - mae: 72.0574 - val_loss: 9273.2539 - val_mae: 9273.9482\n",
      "Epoch 2683/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 83.2516 - mae: 83.9345 - val_loss: 9507.9326 - val_mae: 9508.6260\n",
      "Epoch 2684/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 106.3402 - mae: 107.0250 - val_loss: 9125.1924 - val_mae: 9125.8857\n",
      "Epoch 2685/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 78.7367 - mae: 79.4215 - val_loss: 9212.5439 - val_mae: 9213.2373\n",
      "Epoch 2686/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 77.6418 - mae: 78.3305 - val_loss: 8871.2959 - val_mae: 8871.9893\n",
      "Epoch 2687/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 105.3183 - mae: 106.0034 - val_loss: 9224.8379 - val_mae: 9225.5322\n",
      "Epoch 2688/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 79.7226 - mae: 80.4078 - val_loss: 9403.3711 - val_mae: 9404.0635\n",
      "Epoch 2689/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 81.6989 - mae: 82.3834 - val_loss: 9187.4609 - val_mae: 9188.1543\n",
      "Epoch 2690/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 76.6728 - mae: 77.3549 - val_loss: 9404.8145 - val_mae: 9405.5068\n",
      "Epoch 2691/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 94.8698 - mae: 95.5547 - val_loss: 9155.5762 - val_mae: 9156.2686\n",
      "Epoch 2692/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 76.2920 - mae: 76.9794 - val_loss: 9294.2842 - val_mae: 9294.9775\n",
      "Epoch 2693/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 79.0352 - mae: 79.7191 - val_loss: 9245.6152 - val_mae: 9246.3076\n",
      "Epoch 2694/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 80.2344 - mae: 80.9201 - val_loss: 9058.2461 - val_mae: 9058.9385\n",
      "Epoch 2695/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 88.7528 - mae: 89.4389 - val_loss: 9297.8301 - val_mae: 9298.5234\n",
      "Epoch 2696/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 85.2927 - mae: 85.9784 - val_loss: 9372.0166 - val_mae: 9372.7100\n",
      "Epoch 2697/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 78.2331 - mae: 78.9195 - val_loss: 9252.1875 - val_mae: 9252.8809\n",
      "Epoch 2698/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 69.0119 - mae: 69.6987 - val_loss: 9176.4854 - val_mae: 9177.1758\n",
      "Epoch 2699/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 91.0542 - mae: 91.7422 - val_loss: 9248.3027 - val_mae: 9248.9941\n",
      "Epoch 2700/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 78.4084 - mae: 79.0918 - val_loss: 9255.1143 - val_mae: 9255.8047\n",
      "Epoch 2701/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 79.0201 - mae: 79.7018 - val_loss: 9244.3945 - val_mae: 9245.0889\n",
      "Epoch 2702/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 84.9377 - mae: 85.6230 - val_loss: 9187.3760 - val_mae: 9188.0693\n",
      "Epoch 2703/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 88.3202 - mae: 89.0073 - val_loss: 9060.1348 - val_mae: 9060.8271\n",
      "Epoch 2704/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 84.3376 - mae: 85.0251 - val_loss: 8999.2383 - val_mae: 8999.9307\n",
      "Epoch 2705/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 89.7772 - mae: 90.4628 - val_loss: 9210.3623 - val_mae: 9211.0566\n",
      "Epoch 2706/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 72.9830 - mae: 73.6678 - val_loss: 9228.8438 - val_mae: 9229.5371\n",
      "Epoch 2707/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 79.0365 - mae: 79.7202 - val_loss: 9260.1719 - val_mae: 9260.8643\n",
      "Epoch 2708/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 81.5459 - mae: 82.2317 - val_loss: 9087.2822 - val_mae: 9087.9766\n",
      "Epoch 2709/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 81.2578 - mae: 81.9445 - val_loss: 9255.2920 - val_mae: 9255.9854\n",
      "Epoch 2710/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 74.9900 - mae: 75.6712 - val_loss: 9183.0137 - val_mae: 9183.7070\n",
      "Epoch 2711/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 90.0934 - mae: 90.7831 - val_loss: 9087.4209 - val_mae: 9088.1133\n",
      "Epoch 2712/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 78.8302 - mae: 79.5140 - val_loss: 9166.3525 - val_mae: 9167.0469\n",
      "Epoch 2713/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 91.8336 - mae: 92.5164 - val_loss: 9499.1172 - val_mae: 9499.8096\n",
      "Epoch 2714/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 85.5061 - mae: 86.1915 - val_loss: 9357.2842 - val_mae: 9357.9775\n",
      "Epoch 2715/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 83.1107 - mae: 83.7904 - val_loss: 9495.9082 - val_mae: 9496.6006\n",
      "Epoch 2716/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 93.8813 - mae: 94.5628 - val_loss: 9205.5986 - val_mae: 9206.2939\n",
      "Epoch 2717/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 74.2989 - mae: 74.9801 - val_loss: 9315.5176 - val_mae: 9316.2090\n",
      "Epoch 2718/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 71.3727 - mae: 72.0591 - val_loss: 9180.2588 - val_mae: 9180.9512\n",
      "Epoch 2719/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 82.5432 - mae: 83.2296 - val_loss: 9258.8955 - val_mae: 9259.5889\n",
      "Epoch 2720/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 79.6209 - mae: 80.3052 - val_loss: 9303.4844 - val_mae: 9304.1777\n",
      "Epoch 2721/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 96.6421 - mae: 97.3287 - val_loss: 9230.1533 - val_mae: 9230.8467\n",
      "Epoch 2722/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 82.3437 - mae: 83.0277 - val_loss: 9228.1035 - val_mae: 9228.7969\n",
      "Epoch 2723/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 113.0142 - mae: 113.7029 - val_loss: 9211.0742 - val_mae: 9211.7656\n",
      "Epoch 2724/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 82.8903 - mae: 83.5742 - val_loss: 9175.5576 - val_mae: 9176.2510\n",
      "Epoch 2725/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 75.0956 - mae: 75.7776 - val_loss: 9214.7256 - val_mae: 9215.4189\n",
      "Epoch 2726/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 87.7986 - mae: 88.4777 - val_loss: 9123.8994 - val_mae: 9124.5918\n",
      "Epoch 2727/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 87.5620 - mae: 88.2503 - val_loss: 9317.7734 - val_mae: 9318.4658\n",
      "Epoch 2728/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 77.9615 - mae: 78.6460 - val_loss: 9260.5449 - val_mae: 9261.2383\n",
      "Epoch 2729/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 84.1773 - mae: 84.8643 - val_loss: 9171.6875 - val_mae: 9172.3799\n",
      "Epoch 2730/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 85.4428 - mae: 86.1275 - val_loss: 8985.2842 - val_mae: 8985.9766\n",
      "Epoch 2731/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 103.4798 - mae: 104.1672 - val_loss: 9301.4922 - val_mae: 9302.1855\n",
      "Epoch 2732/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 75.8949 - mae: 76.5795 - val_loss: 9239.0078 - val_mae: 9239.7012\n",
      "Epoch 2733/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 93.8935 - mae: 94.5756 - val_loss: 9226.7305 - val_mae: 9227.4229\n",
      "Epoch 2734/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 80.1268 - mae: 80.8085 - val_loss: 9301.8096 - val_mae: 9302.5020\n",
      "Epoch 2735/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 78.7865 - mae: 79.4722 - val_loss: 9161.9072 - val_mae: 9162.6016\n",
      "Epoch 2736/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 87.6502 - mae: 88.3320 - val_loss: 9193.3828 - val_mae: 9194.0752\n",
      "Epoch 2737/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 76.2009 - mae: 76.8856 - val_loss: 9259.6709 - val_mae: 9260.3633\n",
      "Epoch 2738/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 76.5930 - mae: 77.2742 - val_loss: 9162.0381 - val_mae: 9162.7314\n",
      "Epoch 2739/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 91.1145 - mae: 91.8002 - val_loss: 9091.0049 - val_mae: 9091.6992\n",
      "Epoch 2740/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 87.6066 - mae: 88.2900 - val_loss: 9146.9180 - val_mae: 9147.6123\n",
      "Epoch 2741/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 90.1176 - mae: 90.8039 - val_loss: 9252.0059 - val_mae: 9252.6992\n",
      "Epoch 2742/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 77.9031 - mae: 78.5898 - val_loss: 9209.7402 - val_mae: 9210.4326\n",
      "Epoch 2743/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 79.7003 - mae: 80.3808 - val_loss: 9330.9189 - val_mae: 9331.6123\n",
      "Epoch 2744/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 78.3916 - mae: 79.0771 - val_loss: 9176.2500 - val_mae: 9176.9434\n",
      "Epoch 2745/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 90.9486 - mae: 91.6322 - val_loss: 9161.9414 - val_mae: 9162.6338\n",
      "Epoch 2746/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 74.4571 - mae: 75.1393 - val_loss: 9292.8135 - val_mae: 9293.5068\n",
      "Epoch 2747/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 71.0497 - mae: 71.7358 - val_loss: 9255.6768 - val_mae: 9256.3701\n",
      "Epoch 2748/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 72.5911 - mae: 73.2744 - val_loss: 9156.7197 - val_mae: 9157.4121\n",
      "Epoch 2749/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 76.7155 - mae: 77.4033 - val_loss: 9241.1631 - val_mae: 9241.8574\n",
      "Epoch 2750/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 77.8696 - mae: 78.5551 - val_loss: 9384.7451 - val_mae: 9385.4365\n",
      "Epoch 2751/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 78.2776 - mae: 78.9653 - val_loss: 9251.1182 - val_mae: 9251.8115\n",
      "Epoch 2752/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 84.3234 - mae: 85.0100 - val_loss: 9102.3984 - val_mae: 9103.0898\n",
      "Epoch 2753/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 91.2904 - mae: 91.9756 - val_loss: 9100.6709 - val_mae: 9101.3633\n",
      "Epoch 2754/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 83.8647 - mae: 84.5505 - val_loss: 9122.5439 - val_mae: 9123.2363\n",
      "Epoch 2755/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 81.5606 - mae: 82.2457 - val_loss: 9261.9941 - val_mae: 9262.6865\n",
      "Epoch 2756/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 82.8061 - mae: 83.4914 - val_loss: 9229.9502 - val_mae: 9230.6445\n",
      "Epoch 2757/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 75.1887 - mae: 75.8714 - val_loss: 9135.9922 - val_mae: 9136.6855\n",
      "Epoch 2758/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 83.8472 - mae: 84.5309 - val_loss: 9132.0615 - val_mae: 9132.7539\n",
      "Epoch 2759/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 88.7148 - mae: 89.4018 - val_loss: 9218.9033 - val_mae: 9219.5967\n",
      "Epoch 2760/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 73.3396 - mae: 74.0270 - val_loss: 9279.8799 - val_mae: 9280.5732\n",
      "Epoch 2761/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 69.9683 - mae: 70.6515 - val_loss: 9377.7988 - val_mae: 9378.4922\n",
      "Epoch 2762/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 87.1355 - mae: 87.8205 - val_loss: 9283.4072 - val_mae: 9284.1006\n",
      "Epoch 2763/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 83.4615 - mae: 84.1495 - val_loss: 9418.9873 - val_mae: 9419.6807\n",
      "Epoch 2764/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 87.0866 - mae: 87.7720 - val_loss: 9192.5732 - val_mae: 9193.2666\n",
      "Epoch 2765/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 77.6124 - mae: 78.2978 - val_loss: 9254.5000 - val_mae: 9255.1934\n",
      "Epoch 2766/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 86.6255 - mae: 87.3122 - val_loss: 9369.6445 - val_mae: 9370.3379\n",
      "Epoch 2767/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 74.0701 - mae: 74.7554 - val_loss: 9188.1016 - val_mae: 9188.7939\n",
      "Epoch 2768/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 81.5043 - mae: 82.1902 - val_loss: 9044.4336 - val_mae: 9045.1279\n",
      "Epoch 2769/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 104.2717 - mae: 104.9599 - val_loss: 9111.2412 - val_mae: 9111.9326\n",
      "Epoch 2770/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 86.3374 - mae: 87.0230 - val_loss: 9048.6494 - val_mae: 9049.3428\n",
      "Epoch 2771/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 86.2779 - mae: 86.9604 - val_loss: 9193.7783 - val_mae: 9194.4717\n",
      "Epoch 2772/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 78.3331 - mae: 79.0149 - val_loss: 9436.4297 - val_mae: 9437.1211\n",
      "Epoch 2773/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 90.0706 - mae: 90.7580 - val_loss: 9393.4277 - val_mae: 9394.1201\n",
      "Epoch 2774/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 71.3605 - mae: 72.0425 - val_loss: 9176.2598 - val_mae: 9176.9531\n",
      "Epoch 2775/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 80.1143 - mae: 80.7997 - val_loss: 9278.1504 - val_mae: 9278.8447\n",
      "Epoch 2776/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 99.2518 - mae: 99.9397 - val_loss: 9115.3311 - val_mae: 9116.0244\n",
      "Epoch 2777/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 84.2202 - mae: 84.9015 - val_loss: 9266.9941 - val_mae: 9267.6865\n",
      "Epoch 2778/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 82.4384 - mae: 83.1246 - val_loss: 9395.0322 - val_mae: 9395.7256\n",
      "Epoch 2779/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 74.3483 - mae: 75.0344 - val_loss: 9146.2402 - val_mae: 9146.9316\n",
      "Epoch 2780/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 89.1749 - mae: 89.8606 - val_loss: 9182.9688 - val_mae: 9183.6611\n",
      "Epoch 2781/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 76.4017 - mae: 77.0833 - val_loss: 9248.7793 - val_mae: 9249.4736\n",
      "Epoch 2782/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 78.6254 - mae: 79.3081 - val_loss: 9224.7480 - val_mae: 9225.4424\n",
      "Epoch 2783/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 78.7049 - mae: 79.3926 - val_loss: 9247.2334 - val_mae: 9247.9268\n",
      "Epoch 2784/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 72.6538 - mae: 73.3312 - val_loss: 9289.2412 - val_mae: 9289.9336\n",
      "Epoch 2785/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 72.2880 - mae: 72.9692 - val_loss: 9278.4619 - val_mae: 9279.1543\n",
      "Epoch 2786/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 78.1935 - mae: 78.8753 - val_loss: 9430.2598 - val_mae: 9430.9531\n",
      "Epoch 2787/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 82.7513 - mae: 83.4354 - val_loss: 9249.7490 - val_mae: 9250.4414\n",
      "Epoch 2788/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 77.3762 - mae: 78.0592 - val_loss: 9048.1426 - val_mae: 9048.8359\n",
      "Epoch 2789/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 82.3624 - mae: 83.0422 - val_loss: 9211.9121 - val_mae: 9212.6055\n",
      "Epoch 2790/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 79.0946 - mae: 79.7807 - val_loss: 9143.9365 - val_mae: 9144.6309\n",
      "Epoch 2791/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 88.1336 - mae: 88.8217 - val_loss: 9214.4785 - val_mae: 9215.1729\n",
      "Epoch 2792/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 79.3688 - mae: 80.0563 - val_loss: 9074.6367 - val_mae: 9075.3291\n",
      "Epoch 2793/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 74.6375 - mae: 75.3240 - val_loss: 9220.1045 - val_mae: 9220.7979\n",
      "Epoch 2794/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 95.1152 - mae: 95.7979 - val_loss: 9102.6455 - val_mae: 9103.3398\n",
      "Epoch 2795/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 78.2880 - mae: 78.9709 - val_loss: 9331.7295 - val_mae: 9332.4229\n",
      "Epoch 2796/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 85.8423 - mae: 86.5309 - val_loss: 9212.3828 - val_mae: 9213.0762\n",
      "Epoch 2797/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 88.1761 - mae: 88.8622 - val_loss: 9121.3418 - val_mae: 9122.0342\n",
      "Epoch 2798/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 72.1552 - mae: 72.8349 - val_loss: 9007.5049 - val_mae: 9008.1973\n",
      "Epoch 2799/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 75.0008 - mae: 75.6868 - val_loss: 8888.1611 - val_mae: 8888.8535\n",
      "Epoch 2800/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 88.1037 - mae: 88.7910 - val_loss: 9093.3457 - val_mae: 9094.0381\n",
      "Epoch 2801/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 71.8570 - mae: 72.5365 - val_loss: 9108.1299 - val_mae: 9108.8223\n",
      "Epoch 2802/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 83.5662 - mae: 84.2498 - val_loss: 9209.8682 - val_mae: 9210.5605\n",
      "Epoch 2803/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 95.1955 - mae: 95.8805 - val_loss: 8950.1660 - val_mae: 8950.8584\n",
      "Epoch 2804/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 98.4836 - mae: 99.1672 - val_loss: 9091.7773 - val_mae: 9092.4697\n",
      "Epoch 2805/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 76.4481 - mae: 77.1293 - val_loss: 9221.0889 - val_mae: 9221.7822\n",
      "Epoch 2806/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 86.1187 - mae: 86.7978 - val_loss: 9077.3467 - val_mae: 9078.0400\n",
      "Epoch 2807/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 81.7848 - mae: 82.4707 - val_loss: 9120.8076 - val_mae: 9121.5020\n",
      "Epoch 2808/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 79.6792 - mae: 80.3609 - val_loss: 9237.7422 - val_mae: 9238.4365\n",
      "Epoch 2809/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 80.7930 - mae: 81.4778 - val_loss: 9212.8057 - val_mae: 9213.4990\n",
      "Epoch 2810/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 77.1643 - mae: 77.8492 - val_loss: 9163.5771 - val_mae: 9164.2705\n",
      "Epoch 2811/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 88.4603 - mae: 89.1473 - val_loss: 9180.0273 - val_mae: 9180.7197\n",
      "Epoch 2812/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 81.2743 - mae: 81.9607 - val_loss: 8992.6934 - val_mae: 8993.3857\n",
      "Epoch 2813/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 75.0370 - mae: 75.7181 - val_loss: 9131.1611 - val_mae: 9131.8535\n",
      "Epoch 2814/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 83.0831 - mae: 83.7663 - val_loss: 9210.8330 - val_mae: 9211.5254\n",
      "Epoch 2815/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 72.2467 - mae: 72.9286 - val_loss: 9243.7754 - val_mae: 9244.4668\n",
      "Epoch 2816/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 75.0783 - mae: 75.7669 - val_loss: 9261.2998 - val_mae: 9261.9932\n",
      "Epoch 2817/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 81.4922 - mae: 82.1717 - val_loss: 9287.1064 - val_mae: 9287.8018\n",
      "Epoch 2818/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 76.4937 - mae: 77.1812 - val_loss: 9258.2227 - val_mae: 9258.9150\n",
      "Epoch 2819/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 77.2633 - mae: 77.9475 - val_loss: 9147.3564 - val_mae: 9148.0508\n",
      "Epoch 2820/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 89.7742 - mae: 90.4575 - val_loss: 9197.0146 - val_mae: 9197.7070\n",
      "Epoch 2821/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 75.7067 - mae: 76.3940 - val_loss: 9123.9043 - val_mae: 9124.5977\n",
      "Epoch 2822/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 80.9219 - mae: 81.6072 - val_loss: 9239.4639 - val_mae: 9240.1582\n",
      "Epoch 2823/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 78.4126 - mae: 79.1002 - val_loss: 9290.7217 - val_mae: 9291.4141\n",
      "Epoch 2824/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 83.2147 - mae: 83.9016 - val_loss: 9217.8467 - val_mae: 9218.5410\n",
      "Epoch 2825/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 76.0726 - mae: 76.7565 - val_loss: 9116.6504 - val_mae: 9117.3457\n",
      "Epoch 2826/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 67.7033 - mae: 68.3896 - val_loss: 9116.2383 - val_mae: 9116.9307\n",
      "Epoch 2827/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 81.1636 - mae: 81.8452 - val_loss: 9024.2559 - val_mae: 9024.9502\n",
      "Epoch 2828/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 82.5062 - mae: 83.1919 - val_loss: 9183.4131 - val_mae: 9184.1045\n",
      "Epoch 2829/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 67.6219 - mae: 68.3071 - val_loss: 9068.4297 - val_mae: 9069.1240\n",
      "Epoch 2830/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 76.8324 - mae: 77.5227 - val_loss: 9349.6875 - val_mae: 9350.3818\n",
      "Epoch 2831/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 86.6381 - mae: 87.3229 - val_loss: 9159.3145 - val_mae: 9160.0068\n",
      "Epoch 2832/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 93.6204 - mae: 94.3089 - val_loss: 9337.1855 - val_mae: 9337.8789\n",
      "Epoch 2833/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 94.7977 - mae: 95.4868 - val_loss: 9156.0527 - val_mae: 9156.7471\n",
      "Epoch 2834/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 89.7427 - mae: 90.4276 - val_loss: 9318.9346 - val_mae: 9319.6260\n",
      "Epoch 2835/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 92.1387 - mae: 92.8252 - val_loss: 9167.4863 - val_mae: 9168.1797\n",
      "Epoch 2836/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 83.5470 - mae: 84.2329 - val_loss: 9148.3242 - val_mae: 9149.0176\n",
      "Epoch 2837/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 74.9454 - mae: 75.6300 - val_loss: 9040.9551 - val_mae: 9041.6475\n",
      "Epoch 2838/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 77.5032 - mae: 78.1876 - val_loss: 9236.7754 - val_mae: 9237.4678\n",
      "Epoch 2839/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 95.6455 - mae: 96.3307 - val_loss: 9030.1543 - val_mae: 9030.8477\n",
      "Epoch 2840/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 80.0571 - mae: 80.7424 - val_loss: 9165.7188 - val_mae: 9166.4111\n",
      "Epoch 2841/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 71.1493 - mae: 71.8334 - val_loss: 9168.5547 - val_mae: 9169.2490\n",
      "Epoch 2842/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 76.4371 - mae: 77.1234 - val_loss: 9159.7959 - val_mae: 9160.4893\n",
      "Epoch 2843/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 75.0880 - mae: 75.7676 - val_loss: 9272.9834 - val_mae: 9273.6768\n",
      "Epoch 2844/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 77.7061 - mae: 78.3941 - val_loss: 9182.8086 - val_mae: 9183.5020\n",
      "Epoch 2845/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 85.0376 - mae: 85.7207 - val_loss: 9296.1367 - val_mae: 9296.8291\n",
      "Epoch 2846/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 80.3104 - mae: 80.9983 - val_loss: 9222.2500 - val_mae: 9222.9434\n",
      "Epoch 2847/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 78.0978 - mae: 78.7854 - val_loss: 9050.4482 - val_mae: 9051.1406\n",
      "Epoch 2848/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 82.8322 - mae: 83.5199 - val_loss: 9198.2461 - val_mae: 9198.9395\n",
      "Epoch 2849/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 86.5432 - mae: 87.2292 - val_loss: 9140.0459 - val_mae: 9140.7393\n",
      "Epoch 2850/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 70.8010 - mae: 71.4882 - val_loss: 9149.4502 - val_mae: 9150.1436\n",
      "Epoch 2851/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 77.8373 - mae: 78.5207 - val_loss: 9244.3359 - val_mae: 9245.0293\n",
      "Epoch 2852/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 86.5649 - mae: 87.2521 - val_loss: 9088.1631 - val_mae: 9088.8555\n",
      "Epoch 2853/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 78.6389 - mae: 79.3196 - val_loss: 9126.7939 - val_mae: 9127.4863\n",
      "Epoch 2854/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 81.5551 - mae: 82.2383 - val_loss: 9043.2793 - val_mae: 9043.9736\n",
      "Epoch 2855/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 83.3174 - mae: 83.9983 - val_loss: 9147.3154 - val_mae: 9148.0088\n",
      "Epoch 2856/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 89.0128 - mae: 89.7004 - val_loss: 9189.7461 - val_mae: 9190.4395\n",
      "Epoch 2857/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 78.8045 - mae: 79.4856 - val_loss: 9192.5928 - val_mae: 9193.2852\n",
      "Epoch 2858/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 79.2604 - mae: 79.9440 - val_loss: 9177.6357 - val_mae: 9178.3291\n",
      "Epoch 2859/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 79.3195 - mae: 80.0060 - val_loss: 9208.4277 - val_mae: 9209.1221\n",
      "Epoch 2860/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 75.1406 - mae: 75.8234 - val_loss: 9108.1523 - val_mae: 9108.8457\n",
      "Epoch 2861/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 89.1144 - mae: 89.8019 - val_loss: 9176.2373 - val_mae: 9176.9307\n",
      "Epoch 2862/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 76.4846 - mae: 77.1671 - val_loss: 9403.2969 - val_mae: 9403.9893\n",
      "Epoch 2863/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 89.8741 - mae: 90.5611 - val_loss: 8998.0322 - val_mae: 8998.7246\n",
      "Epoch 2864/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 77.8281 - mae: 78.5147 - val_loss: 9087.7314 - val_mae: 9088.4238\n",
      "Epoch 2865/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 80.6062 - mae: 81.2893 - val_loss: 9053.5703 - val_mae: 9054.2627\n",
      "Epoch 2866/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 72.7689 - mae: 73.4545 - val_loss: 9093.4902 - val_mae: 9094.1836\n",
      "Epoch 2867/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 70.5156 - mae: 71.2002 - val_loss: 9239.7354 - val_mae: 9240.4277\n",
      "Epoch 2868/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 83.4167 - mae: 84.0997 - val_loss: 9182.8096 - val_mae: 9183.5020\n",
      "Epoch 2869/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 64.6159 - mae: 65.2997 - val_loss: 9097.5234 - val_mae: 9098.2158\n",
      "Epoch 2870/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 77.2096 - mae: 77.8964 - val_loss: 9328.9219 - val_mae: 9329.6152\n",
      "Epoch 2871/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 89.7960 - mae: 90.4831 - val_loss: 9020.9814 - val_mae: 9021.6748\n",
      "Epoch 2872/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 85.3142 - mae: 85.9985 - val_loss: 9329.0420 - val_mae: 9329.7354\n",
      "Epoch 2873/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 76.7463 - mae: 77.4289 - val_loss: 9288.4404 - val_mae: 9289.1338\n",
      "Epoch 2874/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 91.8384 - mae: 92.5252 - val_loss: 9147.9541 - val_mae: 9148.6484\n",
      "Epoch 2875/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 97.3273 - mae: 98.0110 - val_loss: 9201.2754 - val_mae: 9201.9678\n",
      "Epoch 2876/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 78.2990 - mae: 78.9839 - val_loss: 9111.8652 - val_mae: 9112.5586\n",
      "Epoch 2877/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 87.3961 - mae: 88.0796 - val_loss: 9142.7900 - val_mae: 9143.4834\n",
      "Epoch 2878/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 70.5699 - mae: 71.2559 - val_loss: 9200.9463 - val_mae: 9201.6387\n",
      "Epoch 2879/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 80.9600 - mae: 81.6468 - val_loss: 9064.3311 - val_mae: 9065.0244\n",
      "Epoch 2880/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 83.3209 - mae: 84.0072 - val_loss: 9224.9033 - val_mae: 9225.5957\n",
      "Epoch 2881/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 74.1461 - mae: 74.8283 - val_loss: 9198.4443 - val_mae: 9199.1367\n",
      "Epoch 2882/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 80.6809 - mae: 81.3659 - val_loss: 9050.2334 - val_mae: 9050.9277\n",
      "Epoch 2883/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 76.3551 - mae: 77.0360 - val_loss: 9072.9170 - val_mae: 9073.6113\n",
      "Epoch 2884/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 85.1043 - mae: 85.7935 - val_loss: 9038.8613 - val_mae: 9039.5547\n",
      "Epoch 2885/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 91.3941 - mae: 92.0801 - val_loss: 9252.1465 - val_mae: 9252.8398\n",
      "Epoch 2886/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 70.3664 - mae: 71.0511 - val_loss: 9296.3301 - val_mae: 9297.0225\n",
      "Epoch 2887/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 84.3411 - mae: 85.0273 - val_loss: 9261.4951 - val_mae: 9262.1895\n",
      "Epoch 2888/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 83.1398 - mae: 83.8262 - val_loss: 9233.4121 - val_mae: 9234.1064\n",
      "Epoch 2889/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 82.9334 - mae: 83.6180 - val_loss: 9213.5791 - val_mae: 9214.2725\n",
      "Epoch 2890/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 93.9057 - mae: 94.5923 - val_loss: 9296.2500 - val_mae: 9296.9424\n",
      "Epoch 2891/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 82.2454 - mae: 82.9291 - val_loss: 9268.6611 - val_mae: 9269.3535\n",
      "Epoch 2892/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 69.3829 - mae: 70.0638 - val_loss: 9213.2773 - val_mae: 9213.9707\n",
      "Epoch 2893/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 74.1505 - mae: 74.8357 - val_loss: 9256.6045 - val_mae: 9257.2988\n",
      "Epoch 2894/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 82.4922 - mae: 83.1779 - val_loss: 9452.6084 - val_mae: 9453.3018\n",
      "Epoch 2895/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 80.2065 - mae: 80.8938 - val_loss: 9121.7354 - val_mae: 9122.4287\n",
      "Epoch 2896/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 82.5095 - mae: 83.1940 - val_loss: 9164.3594 - val_mae: 9165.0537\n",
      "Epoch 2897/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 77.0751 - mae: 77.7586 - val_loss: 9290.7568 - val_mae: 9291.4502\n",
      "Epoch 2898/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 99.8919 - mae: 100.5759 - val_loss: 9224.3428 - val_mae: 9225.0361\n",
      "Epoch 2899/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 82.8284 - mae: 83.5157 - val_loss: 9188.4023 - val_mae: 9189.0957\n",
      "Epoch 2900/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 76.6336 - mae: 77.3197 - val_loss: 9139.2168 - val_mae: 9139.9092\n",
      "Epoch 2901/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 79.5307 - mae: 80.2186 - val_loss: 9120.2764 - val_mae: 9120.9707\n",
      "Epoch 2902/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 95.8867 - mae: 96.5712 - val_loss: 9224.8779 - val_mae: 9225.5713\n",
      "Epoch 2903/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 93.6491 - mae: 94.3360 - val_loss: 9244.3408 - val_mae: 9245.0332\n",
      "Epoch 2904/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 80.4216 - mae: 81.1100 - val_loss: 9303.3594 - val_mae: 9304.0518\n",
      "Epoch 2905/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 72.2122 - mae: 72.8939 - val_loss: 9223.6973 - val_mae: 9224.3906\n",
      "Epoch 2906/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 74.8976 - mae: 75.5803 - val_loss: 9290.0107 - val_mae: 9290.7021\n",
      "Epoch 2907/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 80.3578 - mae: 81.0428 - val_loss: 9276.5703 - val_mae: 9277.2627\n",
      "Epoch 2908/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 76.2099 - mae: 76.8946 - val_loss: 9126.1729 - val_mae: 9126.8652\n",
      "Epoch 2909/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 94.3520 - mae: 95.0360 - val_loss: 9131.8496 - val_mae: 9132.5420\n",
      "Epoch 2910/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 78.0595 - mae: 78.7459 - val_loss: 9256.2393 - val_mae: 9256.9336\n",
      "Epoch 2911/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 77.4524 - mae: 78.1337 - val_loss: 9111.7715 - val_mae: 9112.4639\n",
      "Epoch 2912/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 81.8979 - mae: 82.5856 - val_loss: 9083.3984 - val_mae: 9084.0918\n",
      "Epoch 2913/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 85.5303 - mae: 86.2140 - val_loss: 9041.8291 - val_mae: 9042.5215\n",
      "Epoch 2914/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 82.6694 - mae: 83.3532 - val_loss: 9161.1963 - val_mae: 9161.8896\n",
      "Epoch 2915/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 65.8670 - mae: 66.5493 - val_loss: 9144.9385 - val_mae: 9145.6318\n",
      "Epoch 2916/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 68.2563 - mae: 68.9411 - val_loss: 9147.8623 - val_mae: 9148.5547\n",
      "Epoch 2917/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 75.8377 - mae: 76.5232 - val_loss: 9294.2324 - val_mae: 9294.9248\n",
      "Epoch 2918/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 68.9854 - mae: 69.6703 - val_loss: 9239.4180 - val_mae: 9240.1113\n",
      "Epoch 2919/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 75.4550 - mae: 76.1390 - val_loss: 9194.2002 - val_mae: 9194.8936\n",
      "Epoch 2920/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 87.1625 - mae: 87.8500 - val_loss: 9120.3184 - val_mae: 9121.0127\n",
      "Epoch 2921/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 74.5462 - mae: 75.2310 - val_loss: 9222.5225 - val_mae: 9223.2158\n",
      "Epoch 2922/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 70.2057 - mae: 70.8864 - val_loss: 9118.0371 - val_mae: 9118.7314\n",
      "Epoch 2923/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 81.0521 - mae: 81.7392 - val_loss: 9344.9688 - val_mae: 9345.6611\n",
      "Epoch 2924/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 81.5219 - mae: 82.2039 - val_loss: 9088.3320 - val_mae: 9089.0234\n",
      "Epoch 2925/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 81.8020 - mae: 82.4876 - val_loss: 9331.1279 - val_mae: 9331.8203\n",
      "Epoch 2926/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 93.6149 - mae: 94.3019 - val_loss: 9362.7051 - val_mae: 9363.3984\n",
      "Epoch 2927/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 98.3351 - mae: 99.0213 - val_loss: 9106.8193 - val_mae: 9107.5117\n",
      "Epoch 2928/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 93.3992 - mae: 94.0852 - val_loss: 9232.5947 - val_mae: 9233.2871\n",
      "Epoch 2929/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 75.5606 - mae: 76.2420 - val_loss: 9081.7773 - val_mae: 9082.4707\n",
      "Epoch 2930/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 81.7635 - mae: 82.4435 - val_loss: 9112.7236 - val_mae: 9113.4170\n",
      "Epoch 2931/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 72.4919 - mae: 73.1743 - val_loss: 9173.8496 - val_mae: 9174.5430\n",
      "Epoch 2932/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 89.3649 - mae: 90.0472 - val_loss: 9229.1553 - val_mae: 9229.8496\n",
      "Epoch 2933/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 74.1761 - mae: 74.8566 - val_loss: 9137.2832 - val_mae: 9137.9756\n",
      "Epoch 2934/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 80.8162 - mae: 81.5019 - val_loss: 9222.5693 - val_mae: 9223.2617\n",
      "Epoch 2935/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 66.6596 - mae: 67.3402 - val_loss: 9226.8125 - val_mae: 9227.5049\n",
      "Epoch 2936/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 70.6451 - mae: 71.3273 - val_loss: 9338.1777 - val_mae: 9338.8701\n",
      "Epoch 2937/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 76.9848 - mae: 77.6686 - val_loss: 9242.5352 - val_mae: 9243.2295\n",
      "Epoch 2938/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 84.1647 - mae: 84.8509 - val_loss: 9250.6621 - val_mae: 9251.3545\n",
      "Epoch 2939/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 75.4674 - mae: 76.1483 - val_loss: 9162.4043 - val_mae: 9163.0967\n",
      "Epoch 2940/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 74.4457 - mae: 75.1226 - val_loss: 9205.7441 - val_mae: 9206.4365\n",
      "Epoch 2941/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 86.2258 - mae: 86.9113 - val_loss: 9144.8594 - val_mae: 9145.5518\n",
      "Epoch 2942/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 70.8173 - mae: 71.4993 - val_loss: 9284.8496 - val_mae: 9285.5430\n",
      "Epoch 2943/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 82.0913 - mae: 82.7727 - val_loss: 9110.1016 - val_mae: 9110.7949\n",
      "Epoch 2944/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 93.7774 - mae: 94.4641 - val_loss: 9310.2031 - val_mae: 9310.8955\n",
      "Epoch 2945/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 76.2150 - mae: 76.9024 - val_loss: 9191.1416 - val_mae: 9191.8330\n",
      "Epoch 2946/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 86.7740 - mae: 87.4594 - val_loss: 9265.7617 - val_mae: 9266.4541\n",
      "Epoch 2947/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 67.9603 - mae: 68.6485 - val_loss: 9276.1973 - val_mae: 9276.8896\n",
      "Epoch 2948/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 74.3193 - mae: 75.0074 - val_loss: 9157.1514 - val_mae: 9157.8457\n",
      "Epoch 2949/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 82.5988 - mae: 83.2838 - val_loss: 9257.2344 - val_mae: 9257.9287\n",
      "Epoch 2950/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 75.9977 - mae: 76.6806 - val_loss: 8973.4414 - val_mae: 8974.1348\n",
      "Epoch 2951/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 108.4797 - mae: 109.1663 - val_loss: 9316.8291 - val_mae: 9317.5225\n",
      "Epoch 2952/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 78.8720 - mae: 79.5572 - val_loss: 9359.9980 - val_mae: 9360.6904\n",
      "Epoch 2953/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 77.3282 - mae: 78.0110 - val_loss: 9222.0596 - val_mae: 9222.7520\n",
      "Epoch 2954/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 90.7240 - mae: 91.4075 - val_loss: 9319.1533 - val_mae: 9319.8477\n",
      "Epoch 2955/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 77.0207 - mae: 77.7092 - val_loss: 9158.3711 - val_mae: 9159.0635\n",
      "Epoch 2956/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 72.2041 - mae: 72.8874 - val_loss: 9287.9414 - val_mae: 9288.6357\n",
      "Epoch 2957/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 90.8505 - mae: 91.5384 - val_loss: 9144.8408 - val_mae: 9145.5352\n",
      "Epoch 2958/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 74.5863 - mae: 75.2709 - val_loss: 9131.6191 - val_mae: 9132.3135\n",
      "Epoch 2959/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 79.3426 - mae: 80.0273 - val_loss: 9280.8789 - val_mae: 9281.5723\n",
      "Epoch 2960/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 83.6020 - mae: 84.2874 - val_loss: 9064.1064 - val_mae: 9064.7998\n",
      "Epoch 2961/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 85.7542 - mae: 86.4415 - val_loss: 9293.4629 - val_mae: 9294.1553\n",
      "Epoch 2962/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 74.9573 - mae: 75.6417 - val_loss: 9014.4531 - val_mae: 9015.1465\n",
      "Epoch 2963/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 79.1816 - mae: 79.8664 - val_loss: 9199.6084 - val_mae: 9200.2998\n",
      "Epoch 2964/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 87.8459 - mae: 88.5315 - val_loss: 9232.1709 - val_mae: 9232.8633\n",
      "Epoch 2965/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 88.9503 - mae: 89.6351 - val_loss: 8975.9639 - val_mae: 8976.6572\n",
      "Epoch 2966/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 77.2295 - mae: 77.9129 - val_loss: 9133.1299 - val_mae: 9133.8223\n",
      "Epoch 2967/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 72.0380 - mae: 72.7192 - val_loss: 9187.7012 - val_mae: 9188.3936\n",
      "Epoch 2968/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 79.2083 - mae: 79.8907 - val_loss: 9304.3242 - val_mae: 9305.0156\n",
      "Epoch 2969/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 74.8569 - mae: 75.5435 - val_loss: 9324.9854 - val_mae: 9325.6777\n",
      "Epoch 2970/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 75.5014 - mae: 76.1889 - val_loss: 9338.2725 - val_mae: 9338.9668\n",
      "Epoch 2971/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 78.8973 - mae: 79.5852 - val_loss: 9227.5508 - val_mae: 9228.2451\n",
      "Epoch 2972/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 71.8217 - mae: 72.5108 - val_loss: 9129.4502 - val_mae: 9130.1436\n",
      "Epoch 2973/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 74.9198 - mae: 75.6043 - val_loss: 9251.7109 - val_mae: 9252.4043\n",
      "Epoch 2974/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 88.1621 - mae: 88.8489 - val_loss: 9166.6826 - val_mae: 9167.3760\n",
      "Epoch 2975/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 77.3144 - mae: 77.9993 - val_loss: 9240.3496 - val_mae: 9241.0449\n",
      "Epoch 2976/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 72.8650 - mae: 73.5500 - val_loss: 9141.2393 - val_mae: 9141.9336\n",
      "Epoch 2977/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 78.7226 - mae: 79.4083 - val_loss: 9198.1670 - val_mae: 9198.8594\n",
      "Epoch 2978/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 76.0157 - mae: 76.6991 - val_loss: 9248.0928 - val_mae: 9248.7861\n",
      "Epoch 2979/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 65.6684 - mae: 66.3518 - val_loss: 9316.5312 - val_mae: 9317.2256\n",
      "Epoch 2980/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 87.2422 - mae: 87.9286 - val_loss: 9057.6250 - val_mae: 9058.3184\n",
      "Epoch 2981/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 74.5349 - mae: 75.2232 - val_loss: 9275.9248 - val_mae: 9276.6182\n",
      "Epoch 2982/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 74.9205 - mae: 75.6002 - val_loss: 9078.6289 - val_mae: 9079.3213\n",
      "Epoch 2983/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 79.4206 - mae: 80.1045 - val_loss: 9270.4297 - val_mae: 9271.1230\n",
      "Epoch 2984/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 78.2801 - mae: 78.9644 - val_loss: 9274.4668 - val_mae: 9275.1602\n",
      "Epoch 2985/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 87.5986 - mae: 88.2853 - val_loss: 9199.7598 - val_mae: 9200.4512\n",
      "Epoch 2986/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 68.7448 - mae: 69.4296 - val_loss: 9176.3359 - val_mae: 9177.0283\n",
      "Epoch 2987/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 70.4910 - mae: 71.1752 - val_loss: 9210.2959 - val_mae: 9210.9902\n",
      "Epoch 2988/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 74.6490 - mae: 75.3289 - val_loss: 9271.9199 - val_mae: 9272.6133\n",
      "Epoch 2989/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 90.3555 - mae: 91.0436 - val_loss: 9324.7822 - val_mae: 9325.4756\n",
      "Epoch 2990/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 82.9520 - mae: 83.6312 - val_loss: 9118.0732 - val_mae: 9118.7656\n",
      "Epoch 2991/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 80.0204 - mae: 80.7038 - val_loss: 9067.7100 - val_mae: 9068.4014\n",
      "Epoch 2992/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 67.1956 - mae: 67.8762 - val_loss: 9141.6152 - val_mae: 9142.3076\n",
      "Epoch 2993/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 76.5785 - mae: 77.2629 - val_loss: 9007.3594 - val_mae: 9008.0518\n",
      "Epoch 2994/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 72.1097 - mae: 72.7907 - val_loss: 9223.2979 - val_mae: 9223.9883\n",
      "Epoch 2995/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 67.7324 - mae: 68.4149 - val_loss: 9049.2148 - val_mae: 9049.9062\n",
      "Epoch 2996/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 87.1239 - mae: 87.8088 - val_loss: 9291.6914 - val_mae: 9292.3838\n",
      "Epoch 2997/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 85.8775 - mae: 86.5595 - val_loss: 9142.2969 - val_mae: 9142.9912\n",
      "Epoch 2998/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 81.4092 - mae: 82.0939 - val_loss: 9328.4902 - val_mae: 9329.1836\n",
      "Epoch 2999/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 67.4966 - mae: 68.1795 - val_loss: 9112.9902 - val_mae: 9113.6836\n",
      "Epoch 3000/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 82.5101 - mae: 83.1965 - val_loss: 9305.7256 - val_mae: 9306.4189\n",
      "Epoch 3001/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 76.0521 - mae: 76.7369 - val_loss: 9123.4150 - val_mae: 9124.1064\n",
      "Epoch 3002/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 84.0256 - mae: 84.7126 - val_loss: 9087.6504 - val_mae: 9088.3428\n",
      "Epoch 3003/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 86.5733 - mae: 87.2562 - val_loss: 8942.1465 - val_mae: 8942.8408\n",
      "Epoch 3004/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 82.4891 - mae: 83.1761 - val_loss: 9265.6475 - val_mae: 9266.3398\n",
      "Epoch 3005/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 80.0569 - mae: 80.7438 - val_loss: 9245.3447 - val_mae: 9246.0391\n",
      "Epoch 3006/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 79.2399 - mae: 79.9259 - val_loss: 9171.7900 - val_mae: 9172.4824\n",
      "Epoch 3007/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 80.7692 - mae: 81.4541 - val_loss: 9210.3223 - val_mae: 9211.0146\n",
      "Epoch 3008/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 77.6053 - mae: 78.2908 - val_loss: 9188.3525 - val_mae: 9189.0449\n",
      "Epoch 3009/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 76.8915 - mae: 77.5774 - val_loss: 9136.4414 - val_mae: 9137.1338\n",
      "Epoch 3010/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 73.2091 - mae: 73.8931 - val_loss: 9262.6162 - val_mae: 9263.3096\n",
      "Epoch 3011/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 76.8807 - mae: 77.5654 - val_loss: 9270.3857 - val_mae: 9271.0771\n",
      "Epoch 3012/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 82.9310 - mae: 83.6149 - val_loss: 9229.0957 - val_mae: 9229.7891\n",
      "Epoch 3013/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 73.5967 - mae: 74.2777 - val_loss: 9394.8154 - val_mae: 9395.5078\n",
      "Epoch 3014/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 77.2772 - mae: 77.9629 - val_loss: 9174.5352 - val_mae: 9175.2285\n",
      "Epoch 3015/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 84.8739 - mae: 85.5585 - val_loss: 9210.0098 - val_mae: 9210.7031\n",
      "Epoch 3016/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 86.9363 - mae: 87.6192 - val_loss: 9139.5498 - val_mae: 9140.2441\n",
      "Epoch 3017/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 81.0298 - mae: 81.7181 - val_loss: 9328.7266 - val_mae: 9329.4209\n",
      "Epoch 3018/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 83.6156 - mae: 84.3003 - val_loss: 9384.0850 - val_mae: 9384.7773\n",
      "Epoch 3019/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 75.9949 - mae: 76.6790 - val_loss: 9090.8906 - val_mae: 9091.5840\n",
      "Epoch 3020/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 72.8650 - mae: 73.5512 - val_loss: 9241.5840 - val_mae: 9242.2764\n",
      "Epoch 3021/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 78.7279 - mae: 79.4114 - val_loss: 9374.5430 - val_mae: 9375.2354\n",
      "Epoch 3022/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 80.2053 - mae: 80.8896 - val_loss: 9226.1895 - val_mae: 9226.8848\n",
      "Epoch 3023/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 75.8823 - mae: 76.5653 - val_loss: 9221.3887 - val_mae: 9222.0830\n",
      "Epoch 3024/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 76.5891 - mae: 77.2771 - val_loss: 9097.8213 - val_mae: 9098.5137\n",
      "Epoch 3025/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 79.8061 - mae: 80.4942 - val_loss: 8874.0039 - val_mae: 8874.6973\n",
      "Epoch 3026/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 89.4414 - mae: 90.1271 - val_loss: 9127.4512 - val_mae: 9128.1465\n",
      "Epoch 3027/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 89.1825 - mae: 89.8711 - val_loss: 9214.3447 - val_mae: 9215.0371\n",
      "Epoch 3028/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 84.4840 - mae: 85.1659 - val_loss: 9281.9189 - val_mae: 9282.6104\n",
      "Epoch 3029/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 77.6669 - mae: 78.3460 - val_loss: 9116.5508 - val_mae: 9117.2461\n",
      "Epoch 3030/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 68.7479 - mae: 69.4297 - val_loss: 9094.5986 - val_mae: 9095.2910\n",
      "Epoch 3031/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 77.0811 - mae: 77.7664 - val_loss: 9014.9678 - val_mae: 9015.6621\n",
      "Epoch 3032/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 79.1305 - mae: 79.8150 - val_loss: 9166.8857 - val_mae: 9167.5791\n",
      "Epoch 3033/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 82.7805 - mae: 83.4672 - val_loss: 9181.9365 - val_mae: 9182.6299\n",
      "Epoch 3034/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 80.6440 - mae: 81.3287 - val_loss: 9217.3496 - val_mae: 9218.0449\n",
      "Epoch 3035/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 73.8409 - mae: 74.5257 - val_loss: 9420.2773 - val_mae: 9420.9697\n",
      "Epoch 3036/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 74.4200 - mae: 75.1044 - val_loss: 9139.0156 - val_mae: 9139.7090\n",
      "Epoch 3037/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 77.8336 - mae: 78.5119 - val_loss: 9153.5371 - val_mae: 9154.2305\n",
      "Epoch 3038/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 72.9339 - mae: 73.6171 - val_loss: 9210.1406 - val_mae: 9210.8350\n",
      "Epoch 3039/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 72.4852 - mae: 73.1663 - val_loss: 9004.9893 - val_mae: 9005.6807\n",
      "Epoch 3040/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 92.0598 - mae: 92.7419 - val_loss: 9374.4287 - val_mae: 9375.1211\n",
      "Epoch 3041/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 85.3324 - mae: 86.0148 - val_loss: 9118.3047 - val_mae: 9118.9990\n",
      "Epoch 3042/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 73.3579 - mae: 74.0432 - val_loss: 9252.1631 - val_mae: 9252.8545\n",
      "Epoch 3043/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 88.4205 - mae: 89.1078 - val_loss: 9188.6074 - val_mae: 9189.3018\n",
      "Epoch 3044/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 77.6500 - mae: 78.3343 - val_loss: 9192.8242 - val_mae: 9193.5186\n",
      "Epoch 3045/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 81.1744 - mae: 81.8566 - val_loss: 9194.9336 - val_mae: 9195.6270\n",
      "Epoch 3046/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 77.3051 - mae: 77.9873 - val_loss: 9082.5732 - val_mae: 9083.2676\n",
      "Epoch 3047/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 80.9480 - mae: 81.6330 - val_loss: 9105.8164 - val_mae: 9106.5088\n",
      "Epoch 3048/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 70.7090 - mae: 71.3936 - val_loss: 9053.0078 - val_mae: 9053.7021\n",
      "Epoch 3049/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 76.9607 - mae: 77.6435 - val_loss: 9014.2080 - val_mae: 9014.9004\n",
      "Epoch 3050/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 87.1078 - mae: 87.7953 - val_loss: 9114.9170 - val_mae: 9115.6094\n",
      "Epoch 3051/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 74.6300 - mae: 75.3104 - val_loss: 9111.8809 - val_mae: 9112.5732\n",
      "Epoch 3052/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 64.2850 - mae: 64.9708 - val_loss: 9049.1211 - val_mae: 9049.8164\n",
      "Epoch 3053/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 74.1660 - mae: 74.8513 - val_loss: 9101.7627 - val_mae: 9102.4551\n",
      "Epoch 3054/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 87.2623 - mae: 87.9473 - val_loss: 9155.9893 - val_mae: 9156.6807\n",
      "Epoch 3055/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 68.6953 - mae: 69.3795 - val_loss: 9103.1650 - val_mae: 9103.8584\n",
      "Epoch 3056/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 77.1901 - mae: 77.8768 - val_loss: 9199.1768 - val_mae: 9199.8701\n",
      "Epoch 3057/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 76.4441 - mae: 77.1277 - val_loss: 9449.6396 - val_mae: 9450.3311\n",
      "Epoch 3058/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 92.1786 - mae: 92.8645 - val_loss: 9256.5586 - val_mae: 9257.2520\n",
      "Epoch 3059/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 80.5078 - mae: 81.1929 - val_loss: 9166.2285 - val_mae: 9166.9209\n",
      "Epoch 3060/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 72.1283 - mae: 72.8132 - val_loss: 9097.8730 - val_mae: 9098.5674\n",
      "Epoch 3061/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 73.2512 - mae: 73.9364 - val_loss: 9316.1865 - val_mae: 9316.8799\n",
      "Epoch 3062/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 66.6790 - mae: 67.3638 - val_loss: 9243.6025 - val_mae: 9244.2959\n",
      "Epoch 3063/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 82.9087 - mae: 83.5931 - val_loss: 9104.6484 - val_mae: 9105.3418\n",
      "Epoch 3064/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 74.7936 - mae: 75.4761 - val_loss: 9137.8574 - val_mae: 9138.5508\n",
      "Epoch 3065/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 70.5593 - mae: 71.2433 - val_loss: 9385.0322 - val_mae: 9385.7256\n",
      "Epoch 3066/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 78.7578 - mae: 79.4418 - val_loss: 9222.0547 - val_mae: 9222.7500\n",
      "Epoch 3067/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 81.9780 - mae: 82.6609 - val_loss: 9243.3730 - val_mae: 9244.0664\n",
      "Epoch 3068/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 86.0220 - mae: 86.7087 - val_loss: 9288.1172 - val_mae: 9288.8096\n",
      "Epoch 3069/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 76.4992 - mae: 77.1810 - val_loss: 9148.6152 - val_mae: 9149.3096\n",
      "Epoch 3070/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 75.2523 - mae: 75.9318 - val_loss: 9119.4658 - val_mae: 9120.1592\n",
      "Epoch 3071/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 71.4646 - mae: 72.1487 - val_loss: 9294.5967 - val_mae: 9295.2900\n",
      "Epoch 3072/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 84.6442 - mae: 85.3310 - val_loss: 9241.0488 - val_mae: 9241.7412\n",
      "Epoch 3073/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 87.8508 - mae: 88.5373 - val_loss: 9165.8418 - val_mae: 9166.5352\n",
      "Epoch 3074/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 88.7741 - mae: 89.4565 - val_loss: 9321.0029 - val_mae: 9321.6953\n",
      "Epoch 3075/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 81.4041 - mae: 82.0864 - val_loss: 9154.7266 - val_mae: 9155.4199\n",
      "Epoch 3076/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 84.3274 - mae: 85.0130 - val_loss: 9286.4863 - val_mae: 9287.1797\n",
      "Epoch 3077/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 74.0288 - mae: 74.7179 - val_loss: 9062.2559 - val_mae: 9062.9512\n",
      "Epoch 3078/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 64.8975 - mae: 65.5781 - val_loss: 9306.3105 - val_mae: 9307.0029\n",
      "Epoch 3079/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 87.6933 - mae: 88.3795 - val_loss: 9092.2725 - val_mae: 9092.9658\n",
      "Epoch 3080/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 86.2747 - mae: 86.9614 - val_loss: 9393.8320 - val_mae: 9394.5244\n",
      "Epoch 3081/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 102.4009 - mae: 103.0866 - val_loss: 9258.1553 - val_mae: 9258.8496\n",
      "Epoch 3082/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 82.6274 - mae: 83.3098 - val_loss: 9315.6807 - val_mae: 9316.3740\n",
      "Epoch 3083/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 70.7279 - mae: 71.4159 - val_loss: 9201.2930 - val_mae: 9201.9854\n",
      "Epoch 3084/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 70.6352 - mae: 71.3173 - val_loss: 9158.1494 - val_mae: 9158.8428\n",
      "Epoch 3085/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 73.4660 - mae: 74.1529 - val_loss: 9205.7393 - val_mae: 9206.4326\n",
      "Epoch 3086/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 75.0240 - mae: 75.7131 - val_loss: 9260.5459 - val_mae: 9261.2383\n",
      "Epoch 3087/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 85.5496 - mae: 86.2327 - val_loss: 9239.8604 - val_mae: 9240.5537\n",
      "Epoch 3088/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 81.9033 - mae: 82.5838 - val_loss: 9181.9209 - val_mae: 9182.6143\n",
      "Epoch 3089/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 76.9801 - mae: 77.6690 - val_loss: 9151.5166 - val_mae: 9152.2100\n",
      "Epoch 3090/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 72.9845 - mae: 73.6712 - val_loss: 9061.7529 - val_mae: 9062.4482\n",
      "Epoch 3091/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 83.1835 - mae: 83.8693 - val_loss: 9233.9658 - val_mae: 9234.6611\n",
      "Epoch 3092/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 77.4711 - mae: 78.1527 - val_loss: 9173.9141 - val_mae: 9174.6074\n",
      "Epoch 3093/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 69.2804 - mae: 69.9650 - val_loss: 9250.1484 - val_mae: 9250.8418\n",
      "Epoch 3094/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 79.5607 - mae: 80.2484 - val_loss: 9060.7305 - val_mae: 9061.4238\n",
      "Epoch 3095/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 74.7598 - mae: 75.4394 - val_loss: 9163.9248 - val_mae: 9164.6191\n",
      "Epoch 3096/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 73.8266 - mae: 74.5091 - val_loss: 9072.8076 - val_mae: 9073.5010\n",
      "Epoch 3097/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 73.7546 - mae: 74.4397 - val_loss: 9205.5918 - val_mae: 9206.2852\n",
      "Epoch 3098/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 89.0262 - mae: 89.7100 - val_loss: 9252.5967 - val_mae: 9253.2900\n",
      "Epoch 3099/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 71.0300 - mae: 71.7129 - val_loss: 9423.7295 - val_mae: 9424.4229\n",
      "Epoch 3100/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 90.7531 - mae: 91.4408 - val_loss: 9276.0273 - val_mae: 9276.7197\n",
      "Epoch 3101/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 72.6383 - mae: 73.3206 - val_loss: 9278.1963 - val_mae: 9278.8887\n",
      "Epoch 3102/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 69.9142 - mae: 70.5955 - val_loss: 9018.1758 - val_mae: 9018.8691\n",
      "Epoch 3103/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 81.0436 - mae: 81.7289 - val_loss: 9257.5664 - val_mae: 9258.2588\n",
      "Epoch 3104/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 77.6750 - mae: 78.3611 - val_loss: 9138.2725 - val_mae: 9138.9658\n",
      "Epoch 3105/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 93.9430 - mae: 94.6273 - val_loss: 9051.9229 - val_mae: 9052.6152\n",
      "Epoch 3106/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 83.3639 - mae: 84.0486 - val_loss: 8972.5156 - val_mae: 8973.2080\n",
      "Epoch 3107/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 73.3064 - mae: 73.9890 - val_loss: 9141.4727 - val_mae: 9142.1650\n",
      "Epoch 3108/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 74.4300 - mae: 75.1122 - val_loss: 9201.3926 - val_mae: 9202.0859\n",
      "Epoch 3109/5000\n",
      "46/46 [==============================] - 2s 41ms/step - loss: 79.5559 - mae: 80.2409 - val_loss: 9221.6592 - val_mae: 9222.3535\n",
      "Epoch 3110/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 74.4220 - mae: 75.1062 - val_loss: 9213.0615 - val_mae: 9213.7549\n",
      "Epoch 3111/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 84.7179 - mae: 85.4081 - val_loss: 9254.8311 - val_mae: 9255.5234\n",
      "Epoch 3112/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 73.8016 - mae: 74.4880 - val_loss: 9226.4033 - val_mae: 9227.0967\n",
      "Epoch 3113/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 85.4975 - mae: 86.1842 - val_loss: 9329.7695 - val_mae: 9330.4639\n",
      "Epoch 3114/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 85.0652 - mae: 85.7473 - val_loss: 9181.8027 - val_mae: 9182.4951\n",
      "Epoch 3115/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 73.5309 - mae: 74.2173 - val_loss: 9167.0645 - val_mae: 9167.7588\n",
      "Epoch 3116/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 72.4297 - mae: 73.1136 - val_loss: 9077.7764 - val_mae: 9078.4697\n",
      "Epoch 3117/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 86.2096 - mae: 86.8951 - val_loss: 9185.7490 - val_mae: 9186.4434\n",
      "Epoch 3118/5000\n",
      "46/46 [==============================] - 2s 32ms/step - loss: 87.8467 - mae: 88.5295 - val_loss: 8989.1787 - val_mae: 8989.8711\n",
      "Epoch 3119/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 76.7456 - mae: 77.4281 - val_loss: 9065.0977 - val_mae: 9065.7900\n",
      "Epoch 3120/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 76.0697 - mae: 76.7585 - val_loss: 9075.8936 - val_mae: 9076.5859\n",
      "Epoch 3121/5000\n",
      "46/46 [==============================] - 2s 40ms/step - loss: 76.6503 - mae: 77.3360 - val_loss: 9000.9883 - val_mae: 9001.6807\n",
      "Epoch 3122/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 76.5923 - mae: 77.2789 - val_loss: 9239.0439 - val_mae: 9239.7373\n",
      "Epoch 3123/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 75.1336 - mae: 75.8166 - val_loss: 9081.6572 - val_mae: 9082.3496\n",
      "Epoch 3124/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 81.9001 - mae: 82.5888 - val_loss: 8941.8857 - val_mae: 8942.5791\n",
      "Epoch 3125/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 85.2935 - mae: 85.9798 - val_loss: 9091.0029 - val_mae: 9091.6953\n",
      "Epoch 3126/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 72.5929 - mae: 73.2790 - val_loss: 9118.1182 - val_mae: 9118.8115\n",
      "Epoch 3127/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 83.0398 - mae: 83.7250 - val_loss: 9101.5049 - val_mae: 9102.1982\n",
      "Epoch 3128/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 87.6515 - mae: 88.3366 - val_loss: 9243.2158 - val_mae: 9243.9092\n",
      "Epoch 3129/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 79.8366 - mae: 80.5228 - val_loss: 9190.4453 - val_mae: 9191.1377\n",
      "Epoch 3130/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 79.6804 - mae: 80.3657 - val_loss: 9268.0244 - val_mae: 9268.7168\n",
      "Epoch 3131/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 77.5784 - mae: 78.2636 - val_loss: 9250.5215 - val_mae: 9251.2148\n",
      "Epoch 3132/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 83.4152 - mae: 84.1009 - val_loss: 9031.4502 - val_mae: 9032.1455\n",
      "Epoch 3133/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 78.8634 - mae: 79.5477 - val_loss: 8889.6680 - val_mae: 8890.3604\n",
      "Epoch 3134/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 95.5839 - mae: 96.2686 - val_loss: 9334.2480 - val_mae: 9334.9424\n",
      "Epoch 3135/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 83.3037 - mae: 83.9888 - val_loss: 9035.1377 - val_mae: 9035.8320\n",
      "Epoch 3136/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 84.0068 - mae: 84.6938 - val_loss: 9030.0957 - val_mae: 9030.7891\n",
      "Epoch 3137/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 74.7122 - mae: 75.3968 - val_loss: 9266.0859 - val_mae: 9266.7793\n",
      "Epoch 3138/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 76.5490 - mae: 77.2326 - val_loss: 9268.6875 - val_mae: 9269.3809\n",
      "Epoch 3139/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 79.4759 - mae: 80.1622 - val_loss: 9151.4717 - val_mae: 9152.1641\n",
      "Epoch 3140/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 69.2834 - mae: 69.9650 - val_loss: 9123.3105 - val_mae: 9124.0029\n",
      "Epoch 3141/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 83.8178 - mae: 84.5054 - val_loss: 8978.0967 - val_mae: 8978.7900\n",
      "Epoch 3142/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 75.2200 - mae: 75.9001 - val_loss: 9338.1299 - val_mae: 9338.8232\n",
      "Epoch 3143/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 77.9809 - mae: 78.6642 - val_loss: 9207.6113 - val_mae: 9208.3057\n",
      "Epoch 3144/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 78.4640 - mae: 79.1502 - val_loss: 9253.0283 - val_mae: 9253.7217\n",
      "Epoch 3145/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 86.8020 - mae: 87.4868 - val_loss: 9113.0928 - val_mae: 9113.7852\n",
      "Epoch 3146/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 79.1655 - mae: 79.8514 - val_loss: 9274.9736 - val_mae: 9275.6660\n",
      "Epoch 3147/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 75.9790 - mae: 76.6661 - val_loss: 9268.4990 - val_mae: 9269.1914\n",
      "Epoch 3148/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 82.1694 - mae: 82.8540 - val_loss: 9264.8643 - val_mae: 9265.5576\n",
      "Epoch 3149/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 83.1847 - mae: 83.8733 - val_loss: 9217.8896 - val_mae: 9218.5830\n",
      "Epoch 3150/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 77.2648 - mae: 77.9484 - val_loss: 9077.6787 - val_mae: 9078.3721\n",
      "Epoch 3151/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 64.5432 - mae: 65.2257 - val_loss: 9193.9805 - val_mae: 9194.6738\n",
      "Epoch 3152/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 77.9241 - mae: 78.6102 - val_loss: 9311.5039 - val_mae: 9312.1953\n",
      "Epoch 3153/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 75.1282 - mae: 75.8081 - val_loss: 9314.9941 - val_mae: 9315.6865\n",
      "Epoch 3154/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 74.9466 - mae: 75.6308 - val_loss: 9150.8633 - val_mae: 9151.5566\n",
      "Epoch 3155/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 72.3116 - mae: 72.9939 - val_loss: 9192.0654 - val_mae: 9192.7598\n",
      "Epoch 3156/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 71.1505 - mae: 71.8366 - val_loss: 9127.1396 - val_mae: 9127.8340\n",
      "Epoch 3157/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 86.9128 - mae: 87.5977 - val_loss: 9228.0400 - val_mae: 9228.7334\n",
      "Epoch 3158/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 74.2319 - mae: 74.9168 - val_loss: 9065.1113 - val_mae: 9065.8027\n",
      "Epoch 3159/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 74.9592 - mae: 75.6413 - val_loss: 9172.6631 - val_mae: 9173.3535\n",
      "Epoch 3160/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 86.3807 - mae: 87.0668 - val_loss: 9241.2529 - val_mae: 9241.9463\n",
      "Epoch 3161/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 72.4320 - mae: 73.1157 - val_loss: 9302.8584 - val_mae: 9303.5518\n",
      "Epoch 3162/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 75.2105 - mae: 75.8952 - val_loss: 9393.1504 - val_mae: 9393.8438\n",
      "Epoch 3163/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 90.4431 - mae: 91.1205 - val_loss: 9288.5713 - val_mae: 9289.2646\n",
      "Epoch 3164/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 73.0952 - mae: 73.7789 - val_loss: 9064.8037 - val_mae: 9065.4971\n",
      "Epoch 3165/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 80.7365 - mae: 81.4222 - val_loss: 9172.5303 - val_mae: 9173.2236\n",
      "Epoch 3166/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 72.9497 - mae: 73.6367 - val_loss: 8928.9082 - val_mae: 8929.6016\n",
      "Epoch 3167/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 81.5135 - mae: 82.1985 - val_loss: 9117.3145 - val_mae: 9118.0088\n",
      "Epoch 3168/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 91.5211 - mae: 92.2026 - val_loss: 9257.4121 - val_mae: 9258.1055\n",
      "Epoch 3169/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 71.9726 - mae: 72.6585 - val_loss: 9133.3926 - val_mae: 9134.0859\n",
      "Epoch 3170/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 69.8771 - mae: 70.5611 - val_loss: 9221.9775 - val_mae: 9222.6709\n",
      "Epoch 3171/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 69.0585 - mae: 69.7401 - val_loss: 9261.0967 - val_mae: 9261.7891\n",
      "Epoch 3172/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 85.8300 - mae: 86.5134 - val_loss: 9142.8916 - val_mae: 9143.5840\n",
      "Epoch 3173/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 73.1659 - mae: 73.8485 - val_loss: 9150.8809 - val_mae: 9151.5752\n",
      "Epoch 3174/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 69.6421 - mae: 70.3274 - val_loss: 9148.3242 - val_mae: 9149.0176\n",
      "Epoch 3175/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 73.7568 - mae: 74.4395 - val_loss: 9254.8896 - val_mae: 9255.5830\n",
      "Epoch 3176/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 68.9405 - mae: 69.6196 - val_loss: 9335.4150 - val_mae: 9336.1094\n",
      "Epoch 3177/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 66.9116 - mae: 67.5938 - val_loss: 9292.3496 - val_mae: 9293.0439\n",
      "Epoch 3178/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 73.6727 - mae: 74.3589 - val_loss: 9257.7549 - val_mae: 9258.4492\n",
      "Epoch 3179/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 81.5253 - mae: 82.2124 - val_loss: 9296.5801 - val_mae: 9297.2744\n",
      "Epoch 3180/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 79.8332 - mae: 80.5169 - val_loss: 9184.4766 - val_mae: 9185.1699\n",
      "Epoch 3181/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 76.8171 - mae: 77.4997 - val_loss: 9035.7383 - val_mae: 9036.4297\n",
      "Epoch 3182/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 92.8832 - mae: 93.5687 - val_loss: 9143.8516 - val_mae: 9144.5459\n",
      "Epoch 3183/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 81.8360 - mae: 82.5191 - val_loss: 9288.9531 - val_mae: 9289.6475\n",
      "Epoch 3184/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 68.2056 - mae: 68.8891 - val_loss: 9212.5410 - val_mae: 9213.2334\n",
      "Epoch 3185/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 72.1805 - mae: 72.8626 - val_loss: 9244.5723 - val_mae: 9245.2666\n",
      "Epoch 3186/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 83.7665 - mae: 84.4565 - val_loss: 9159.0059 - val_mae: 9159.6992\n",
      "Epoch 3187/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 76.5221 - mae: 77.2116 - val_loss: 9389.5674 - val_mae: 9390.2607\n",
      "Epoch 3188/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 76.6703 - mae: 77.3573 - val_loss: 9232.8594 - val_mae: 9233.5527\n",
      "Epoch 3189/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 73.7230 - mae: 74.4086 - val_loss: 9057.7627 - val_mae: 9058.4561\n",
      "Epoch 3190/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 69.8235 - mae: 70.5000 - val_loss: 9325.4561 - val_mae: 9326.1494\n",
      "Epoch 3191/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 68.4042 - mae: 69.0825 - val_loss: 9222.0791 - val_mae: 9222.7725\n",
      "Epoch 3192/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 70.6187 - mae: 71.3037 - val_loss: 9223.5059 - val_mae: 9224.1982\n",
      "Epoch 3193/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 65.7122 - mae: 66.3935 - val_loss: 9199.1240 - val_mae: 9199.8164\n",
      "Epoch 3194/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 76.7711 - mae: 77.4567 - val_loss: 9257.5322 - val_mae: 9258.2266\n",
      "Epoch 3195/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 79.8025 - mae: 80.4876 - val_loss: 9181.0322 - val_mae: 9181.7275\n",
      "Epoch 3196/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 76.0694 - mae: 76.7540 - val_loss: 9144.2070 - val_mae: 9144.9004\n",
      "Epoch 3197/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 75.7487 - mae: 76.4302 - val_loss: 9352.1191 - val_mae: 9352.8115\n",
      "Epoch 3198/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 88.6641 - mae: 89.3514 - val_loss: 9412.4639 - val_mae: 9413.1562\n",
      "Epoch 3199/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 77.8503 - mae: 78.5317 - val_loss: 9220.3057 - val_mae: 9220.9980\n",
      "Epoch 3200/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 78.5655 - mae: 79.2509 - val_loss: 9271.2832 - val_mae: 9271.9775\n",
      "Epoch 3201/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 63.8069 - mae: 64.4837 - val_loss: 9235.5439 - val_mae: 9236.2363\n",
      "Epoch 3202/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 72.6776 - mae: 73.3658 - val_loss: 9243.1318 - val_mae: 9243.8252\n",
      "Epoch 3203/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 84.8634 - mae: 85.5468 - val_loss: 9314.4277 - val_mae: 9315.1201\n",
      "Epoch 3204/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 76.4987 - mae: 77.1843 - val_loss: 9080.1084 - val_mae: 9080.8018\n",
      "Epoch 3205/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 75.8226 - mae: 76.5096 - val_loss: 9200.4229 - val_mae: 9201.1143\n",
      "Epoch 3206/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 73.2269 - mae: 73.9123 - val_loss: 9057.7217 - val_mae: 9058.4150\n",
      "Epoch 3207/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 75.1764 - mae: 75.8590 - val_loss: 9181.4404 - val_mae: 9182.1318\n",
      "Epoch 3208/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 80.3927 - mae: 81.0770 - val_loss: 9154.3018 - val_mae: 9154.9951\n",
      "Epoch 3209/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 73.9746 - mae: 74.6548 - val_loss: 9130.7822 - val_mae: 9131.4746\n",
      "Epoch 3210/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 84.8527 - mae: 85.5337 - val_loss: 9277.3643 - val_mae: 9278.0576\n",
      "Epoch 3211/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 69.2364 - mae: 69.9205 - val_loss: 9068.9502 - val_mae: 9069.6445\n",
      "Epoch 3212/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 87.1451 - mae: 87.8296 - val_loss: 9246.3027 - val_mae: 9246.9961\n",
      "Epoch 3213/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 85.4075 - mae: 86.0923 - val_loss: 9039.6104 - val_mae: 9040.3027\n",
      "Epoch 3214/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 81.2825 - mae: 81.9697 - val_loss: 9146.5635 - val_mae: 9147.2568\n",
      "Epoch 3215/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 78.4009 - mae: 79.0871 - val_loss: 9195.7383 - val_mae: 9196.4316\n",
      "Epoch 3216/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 86.7528 - mae: 87.4381 - val_loss: 9304.7568 - val_mae: 9305.4492\n",
      "Epoch 3217/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 71.6523 - mae: 72.3351 - val_loss: 9308.5576 - val_mae: 9309.2510\n",
      "Epoch 3218/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 75.4439 - mae: 76.1275 - val_loss: 9163.8555 - val_mae: 9164.5498\n",
      "Epoch 3219/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 75.1082 - mae: 75.7947 - val_loss: 9241.4102 - val_mae: 9242.1035\n",
      "Epoch 3220/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 80.1079 - mae: 80.7908 - val_loss: 9266.1406 - val_mae: 9266.8350\n",
      "Epoch 3221/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 84.2662 - mae: 84.9487 - val_loss: 9353.0723 - val_mae: 9353.7666\n",
      "Epoch 3222/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 72.6450 - mae: 73.3306 - val_loss: 9382.6387 - val_mae: 9383.3291\n",
      "Epoch 3223/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 79.7685 - mae: 80.4551 - val_loss: 9216.3857 - val_mae: 9217.0791\n",
      "Epoch 3224/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 77.7722 - mae: 78.4577 - val_loss: 9207.5625 - val_mae: 9208.2549\n",
      "Epoch 3225/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 72.2773 - mae: 72.9623 - val_loss: 9191.5312 - val_mae: 9192.2246\n",
      "Epoch 3226/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 75.9468 - mae: 76.6296 - val_loss: 9240.5498 - val_mae: 9241.2422\n",
      "Epoch 3227/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 74.4942 - mae: 75.1724 - val_loss: 9211.7900 - val_mae: 9212.4824\n",
      "Epoch 3228/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 75.4777 - mae: 76.1626 - val_loss: 9228.4561 - val_mae: 9229.1504\n",
      "Epoch 3229/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 72.2905 - mae: 72.9773 - val_loss: 9254.4893 - val_mae: 9255.1826\n",
      "Epoch 3230/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 73.5793 - mae: 74.2653 - val_loss: 9125.6621 - val_mae: 9126.3564\n",
      "Epoch 3231/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 77.3304 - mae: 78.0136 - val_loss: 9188.7979 - val_mae: 9189.4912\n",
      "Epoch 3232/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 66.5963 - mae: 67.2799 - val_loss: 9269.8311 - val_mae: 9270.5244\n",
      "Epoch 3233/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 71.5550 - mae: 72.2390 - val_loss: 9297.5869 - val_mae: 9298.2783\n",
      "Epoch 3234/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 68.9609 - mae: 69.6439 - val_loss: 9178.7705 - val_mae: 9179.4629\n",
      "Epoch 3235/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 69.6472 - mae: 70.3308 - val_loss: 9226.8389 - val_mae: 9227.5322\n",
      "Epoch 3236/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 73.9402 - mae: 74.6228 - val_loss: 9477.2891 - val_mae: 9477.9814\n",
      "Epoch 3237/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 82.6808 - mae: 83.3614 - val_loss: 9352.6562 - val_mae: 9353.3496\n",
      "Epoch 3238/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 83.6566 - mae: 84.3429 - val_loss: 9198.9434 - val_mae: 9199.6377\n",
      "Epoch 3239/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 74.3151 - mae: 74.9998 - val_loss: 9169.3398 - val_mae: 9170.0322\n",
      "Epoch 3240/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 79.0166 - mae: 79.7007 - val_loss: 9169.2939 - val_mae: 9169.9863\n",
      "Epoch 3241/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 81.2517 - mae: 81.9397 - val_loss: 9229.6592 - val_mae: 9230.3516\n",
      "Epoch 3242/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 71.7484 - mae: 72.4282 - val_loss: 9300.5898 - val_mae: 9301.2842\n",
      "Epoch 3243/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 70.8164 - mae: 71.5027 - val_loss: 9160.3135 - val_mae: 9161.0068\n",
      "Epoch 3244/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 75.6874 - mae: 76.3683 - val_loss: 9260.1348 - val_mae: 9260.8291\n",
      "Epoch 3245/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 72.6466 - mae: 73.3251 - val_loss: 9298.1445 - val_mae: 9298.8379\n",
      "Epoch 3246/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 74.9576 - mae: 75.6399 - val_loss: 9279.7119 - val_mae: 9280.4043\n",
      "Epoch 3247/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 86.2779 - mae: 86.9628 - val_loss: 9359.7959 - val_mae: 9360.4902\n",
      "Epoch 3248/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 79.1111 - mae: 79.7939 - val_loss: 9172.5625 - val_mae: 9173.2549\n",
      "Epoch 3249/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 74.0497 - mae: 74.7359 - val_loss: 9089.1006 - val_mae: 9089.7949\n",
      "Epoch 3250/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 67.3542 - mae: 68.0382 - val_loss: 9119.4219 - val_mae: 9120.1143\n",
      "Epoch 3251/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 66.0768 - mae: 66.7605 - val_loss: 9284.9980 - val_mae: 9285.6924\n",
      "Epoch 3252/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 83.1674 - mae: 83.8535 - val_loss: 8961.4805 - val_mae: 8962.1719\n",
      "Epoch 3253/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 83.5388 - mae: 84.2241 - val_loss: 9086.4990 - val_mae: 9087.1914\n",
      "Epoch 3254/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 76.4363 - mae: 77.1201 - val_loss: 9093.6719 - val_mae: 9094.3643\n",
      "Epoch 3255/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 69.1136 - mae: 69.7976 - val_loss: 9386.5010 - val_mae: 9387.1934\n",
      "Epoch 3256/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 69.8431 - mae: 70.5223 - val_loss: 9263.2432 - val_mae: 9263.9365\n",
      "Epoch 3257/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 76.7258 - mae: 77.4126 - val_loss: 9109.4424 - val_mae: 9110.1348\n",
      "Epoch 3258/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 93.0079 - mae: 93.6929 - val_loss: 9290.6582 - val_mae: 9291.3516\n",
      "Epoch 3259/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 71.1288 - mae: 71.8124 - val_loss: 9092.9600 - val_mae: 9093.6543\n",
      "Epoch 3260/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 68.0727 - mae: 68.7518 - val_loss: 9122.5762 - val_mae: 9123.2686\n",
      "Epoch 3261/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 74.8628 - mae: 75.5483 - val_loss: 9295.3867 - val_mae: 9296.0791\n",
      "Epoch 3262/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 66.2788 - mae: 66.9651 - val_loss: 9185.0361 - val_mae: 9185.7285\n",
      "Epoch 3263/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 70.9066 - mae: 71.5888 - val_loss: 9172.4893 - val_mae: 9173.1826\n",
      "Epoch 3264/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 71.0938 - mae: 71.7780 - val_loss: 9094.9746 - val_mae: 9095.6689\n",
      "Epoch 3265/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 70.4650 - mae: 71.1515 - val_loss: 9150.7871 - val_mae: 9151.4795\n",
      "Epoch 3266/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 65.4849 - mae: 66.1681 - val_loss: 9300.6250 - val_mae: 9301.3174\n",
      "Epoch 3267/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 78.4327 - mae: 79.1193 - val_loss: 9144.9951 - val_mae: 9145.6885\n",
      "Epoch 3268/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 68.5694 - mae: 69.2546 - val_loss: 9231.5420 - val_mae: 9232.2344\n",
      "Epoch 3269/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 94.0553 - mae: 94.7407 - val_loss: 9165.6152 - val_mae: 9166.3076\n",
      "Epoch 3270/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 69.9854 - mae: 70.6600 - val_loss: 9208.2432 - val_mae: 9208.9355\n",
      "Epoch 3271/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 70.0391 - mae: 70.7254 - val_loss: 9136.8867 - val_mae: 9137.5781\n",
      "Epoch 3272/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 75.5882 - mae: 76.2739 - val_loss: 9306.8135 - val_mae: 9307.5059\n",
      "Epoch 3273/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 73.5510 - mae: 74.2375 - val_loss: 9009.1748 - val_mae: 9009.8691\n",
      "Epoch 3274/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 72.7980 - mae: 73.4818 - val_loss: 9207.7168 - val_mae: 9208.4102\n",
      "Epoch 3275/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 68.1064 - mae: 68.7886 - val_loss: 9083.3545 - val_mae: 9084.0488\n",
      "Epoch 3276/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 77.9068 - mae: 78.5889 - val_loss: 9234.0898 - val_mae: 9234.7822\n",
      "Epoch 3277/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 76.1283 - mae: 76.8139 - val_loss: 9303.8115 - val_mae: 9304.5039\n",
      "Epoch 3278/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 81.8118 - mae: 82.4990 - val_loss: 9319.6016 - val_mae: 9320.2939\n",
      "Epoch 3279/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 91.5278 - mae: 92.2144 - val_loss: 9311.8428 - val_mae: 9312.5381\n",
      "Epoch 3280/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 74.4164 - mae: 75.1041 - val_loss: 9189.7432 - val_mae: 9190.4355\n",
      "Epoch 3281/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 75.9567 - mae: 76.6375 - val_loss: 9200.8320 - val_mae: 9201.5254\n",
      "Epoch 3282/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 69.0427 - mae: 69.7271 - val_loss: 9286.5977 - val_mae: 9287.2900\n",
      "Epoch 3283/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 68.0722 - mae: 68.7531 - val_loss: 9216.6582 - val_mae: 9217.3506\n",
      "Epoch 3284/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 72.5420 - mae: 73.2267 - val_loss: 9273.1357 - val_mae: 9273.8291\n",
      "Epoch 3285/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 68.6844 - mae: 69.3646 - val_loss: 9071.1973 - val_mae: 9071.8906\n",
      "Epoch 3286/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 76.3728 - mae: 77.0575 - val_loss: 9290.5801 - val_mae: 9291.2725\n",
      "Epoch 3287/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 84.9201 - mae: 85.6034 - val_loss: 9231.8271 - val_mae: 9232.5205\n",
      "Epoch 3288/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 69.4065 - mae: 70.0861 - val_loss: 9365.3857 - val_mae: 9366.0801\n",
      "Epoch 3289/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 78.6425 - mae: 79.3303 - val_loss: 9248.9023 - val_mae: 9249.5957\n",
      "Epoch 3290/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 73.2287 - mae: 73.9123 - val_loss: 9269.6514 - val_mae: 9270.3447\n",
      "Epoch 3291/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 72.0184 - mae: 72.7038 - val_loss: 9276.2285 - val_mae: 9276.9219\n",
      "Epoch 3292/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 67.6537 - mae: 68.3386 - val_loss: 9001.3535 - val_mae: 9002.0479\n",
      "Epoch 3293/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 69.4818 - mae: 70.1669 - val_loss: 9218.1016 - val_mae: 9218.7939\n",
      "Epoch 3294/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 77.4108 - mae: 78.1003 - val_loss: 9113.9365 - val_mae: 9114.6279\n",
      "Epoch 3295/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 67.6172 - mae: 68.2986 - val_loss: 9323.5010 - val_mae: 9324.1934\n",
      "Epoch 3296/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 92.7940 - mae: 93.4825 - val_loss: 9157.1172 - val_mae: 9157.8096\n",
      "Epoch 3297/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 81.6432 - mae: 82.3281 - val_loss: 9175.0840 - val_mae: 9175.7773\n",
      "Epoch 3298/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 65.9509 - mae: 66.6332 - val_loss: 9189.2236 - val_mae: 9189.9180\n",
      "Epoch 3299/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 61.5264 - mae: 62.2122 - val_loss: 9285.6396 - val_mae: 9286.3330\n",
      "Epoch 3300/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 68.3767 - mae: 69.0640 - val_loss: 9186.1025 - val_mae: 9186.7939\n",
      "Epoch 3301/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 65.6535 - mae: 66.3396 - val_loss: 9326.5039 - val_mae: 9327.1973\n",
      "Epoch 3302/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 68.1601 - mae: 68.8398 - val_loss: 9159.4727 - val_mae: 9160.1670\n",
      "Epoch 3303/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 75.0263 - mae: 75.7128 - val_loss: 9308.9404 - val_mae: 9309.6338\n",
      "Epoch 3304/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 67.4260 - mae: 68.1077 - val_loss: 9271.1768 - val_mae: 9271.8701\n",
      "Epoch 3305/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 74.4307 - mae: 75.1163 - val_loss: 9189.1006 - val_mae: 9189.7930\n",
      "Epoch 3306/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 81.0279 - mae: 81.7097 - val_loss: 9179.1992 - val_mae: 9179.8936\n",
      "Epoch 3307/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 81.2571 - mae: 81.9414 - val_loss: 9123.8994 - val_mae: 9124.5938\n",
      "Epoch 3308/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 71.6549 - mae: 72.3361 - val_loss: 9185.6270 - val_mae: 9186.3193\n",
      "Epoch 3309/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 85.6939 - mae: 86.3745 - val_loss: 9130.4502 - val_mae: 9131.1416\n",
      "Epoch 3310/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 80.8985 - mae: 81.5800 - val_loss: 9242.3340 - val_mae: 9243.0264\n",
      "Epoch 3311/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 74.0863 - mae: 74.7730 - val_loss: 9157.5469 - val_mae: 9158.2402\n",
      "Epoch 3312/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 78.7636 - mae: 79.4485 - val_loss: 9091.8906 - val_mae: 9092.5850\n",
      "Epoch 3313/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 71.0534 - mae: 71.7372 - val_loss: 9241.8584 - val_mae: 9242.5518\n",
      "Epoch 3314/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 64.6666 - mae: 65.3492 - val_loss: 9150.2568 - val_mae: 9150.9512\n",
      "Epoch 3315/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 69.7195 - mae: 70.4003 - val_loss: 9150.0332 - val_mae: 9150.7256\n",
      "Epoch 3316/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 71.0159 - mae: 71.6976 - val_loss: 9102.6768 - val_mae: 9103.3711\n",
      "Epoch 3317/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 73.0775 - mae: 73.7612 - val_loss: 9156.6201 - val_mae: 9157.3145\n",
      "Epoch 3318/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 71.5771 - mae: 72.2621 - val_loss: 9257.6094 - val_mae: 9258.3018\n",
      "Epoch 3319/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 78.8624 - mae: 79.5473 - val_loss: 9152.5195 - val_mae: 9153.2129\n",
      "Epoch 3320/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 76.2317 - mae: 76.9169 - val_loss: 9202.3691 - val_mae: 9203.0625\n",
      "Epoch 3321/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 80.8921 - mae: 81.5805 - val_loss: 9037.1572 - val_mae: 9037.8506\n",
      "Epoch 3322/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 94.2115 - mae: 94.8999 - val_loss: 9063.9443 - val_mae: 9064.6367\n",
      "Epoch 3323/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 75.1272 - mae: 75.8115 - val_loss: 9343.7666 - val_mae: 9344.4590\n",
      "Epoch 3324/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 81.4403 - mae: 82.1255 - val_loss: 9173.5273 - val_mae: 9174.2217\n",
      "Epoch 3325/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 61.3694 - mae: 62.0475 - val_loss: 9131.8457 - val_mae: 9132.5391\n",
      "Epoch 3326/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 89.1364 - mae: 89.8192 - val_loss: 9165.0518 - val_mae: 9165.7480\n",
      "Epoch 3327/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 80.0342 - mae: 80.7207 - val_loss: 9106.4590 - val_mae: 9107.1523\n",
      "Epoch 3328/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 82.7089 - mae: 83.3935 - val_loss: 9354.5293 - val_mae: 9355.2227\n",
      "Epoch 3329/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 79.2660 - mae: 79.9489 - val_loss: 9058.4795 - val_mae: 9059.1738\n",
      "Epoch 3330/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 77.3484 - mae: 78.0326 - val_loss: 9211.3682 - val_mae: 9212.0615\n",
      "Epoch 3331/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 74.5376 - mae: 75.2200 - val_loss: 9194.5283 - val_mae: 9195.2207\n",
      "Epoch 3332/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 68.9148 - mae: 69.6002 - val_loss: 9248.0410 - val_mae: 9248.7344\n",
      "Epoch 3333/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 68.3567 - mae: 69.0393 - val_loss: 9100.6729 - val_mae: 9101.3662\n",
      "Epoch 3334/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 69.4116 - mae: 70.0964 - val_loss: 9159.5801 - val_mae: 9160.2725\n",
      "Epoch 3335/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 65.5073 - mae: 66.1846 - val_loss: 9219.9795 - val_mae: 9220.6719\n",
      "Epoch 3336/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 59.9201 - mae: 60.6019 - val_loss: 9099.1123 - val_mae: 9099.8057\n",
      "Epoch 3337/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 74.9738 - mae: 75.6555 - val_loss: 9033.1084 - val_mae: 9033.8008\n",
      "Epoch 3338/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 82.3066 - mae: 82.9927 - val_loss: 9221.9434 - val_mae: 9222.6357\n",
      "Epoch 3339/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 68.9442 - mae: 69.6247 - val_loss: 9022.3027 - val_mae: 9022.9961\n",
      "Epoch 3340/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 75.7783 - mae: 76.4636 - val_loss: 9124.1572 - val_mae: 9124.8496\n",
      "Epoch 3341/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 71.7934 - mae: 72.4788 - val_loss: 9306.3076 - val_mae: 9307.0020\n",
      "Epoch 3342/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 80.3023 - mae: 80.9887 - val_loss: 9148.3438 - val_mae: 9149.0371\n",
      "Epoch 3343/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 67.3629 - mae: 68.0470 - val_loss: 9009.1133 - val_mae: 9009.8066\n",
      "Epoch 3344/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 85.0607 - mae: 85.7464 - val_loss: 9151.9307 - val_mae: 9152.6240\n",
      "Epoch 3345/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 70.1137 - mae: 70.7984 - val_loss: 9087.6523 - val_mae: 9088.3467\n",
      "Epoch 3346/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 79.9781 - mae: 80.6649 - val_loss: 9158.9307 - val_mae: 9159.6221\n",
      "Epoch 3347/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 67.3532 - mae: 68.0376 - val_loss: 9308.8545 - val_mae: 9309.5469\n",
      "Epoch 3348/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 75.4579 - mae: 76.1438 - val_loss: 9295.0781 - val_mae: 9295.7725\n",
      "Epoch 3349/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 68.2496 - mae: 68.9345 - val_loss: 9199.6416 - val_mae: 9200.3340\n",
      "Epoch 3350/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 68.4148 - mae: 69.0963 - val_loss: 9195.1670 - val_mae: 9195.8604\n",
      "Epoch 3351/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 67.3271 - mae: 68.0064 - val_loss: 9100.8232 - val_mae: 9101.5166\n",
      "Epoch 3352/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 72.8980 - mae: 73.5838 - val_loss: 9093.1719 - val_mae: 9093.8662\n",
      "Epoch 3353/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 78.5921 - mae: 79.2765 - val_loss: 9172.4248 - val_mae: 9173.1182\n",
      "Epoch 3354/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 61.9074 - mae: 62.5884 - val_loss: 9124.6221 - val_mae: 9125.3145\n",
      "Epoch 3355/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 75.5390 - mae: 76.2249 - val_loss: 9225.5947 - val_mae: 9226.2891\n",
      "Epoch 3356/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 64.6504 - mae: 65.3297 - val_loss: 9256.8867 - val_mae: 9257.5791\n",
      "Epoch 3357/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 71.1304 - mae: 71.8164 - val_loss: 9276.4854 - val_mae: 9277.1787\n",
      "Epoch 3358/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 70.3917 - mae: 71.0741 - val_loss: 9192.4160 - val_mae: 9193.1084\n",
      "Epoch 3359/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 68.0157 - mae: 68.7007 - val_loss: 9082.4512 - val_mae: 9083.1455\n",
      "Epoch 3360/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 66.0674 - mae: 66.7529 - val_loss: 9113.8340 - val_mae: 9114.5264\n",
      "Epoch 3361/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 69.2384 - mae: 69.9248 - val_loss: 9164.0508 - val_mae: 9164.7441\n",
      "Epoch 3362/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 74.2459 - mae: 74.9261 - val_loss: 9221.4658 - val_mae: 9222.1592\n",
      "Epoch 3363/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 66.7215 - mae: 67.4036 - val_loss: 9283.0752 - val_mae: 9283.7676\n",
      "Epoch 3364/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 76.0795 - mae: 76.7661 - val_loss: 9435.0410 - val_mae: 9435.7334\n",
      "Epoch 3365/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 77.5754 - mae: 78.2596 - val_loss: 9350.7119 - val_mae: 9351.4043\n",
      "Epoch 3366/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 90.2949 - mae: 90.9807 - val_loss: 9290.9590 - val_mae: 9291.6504\n",
      "Epoch 3367/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 75.0052 - mae: 75.6879 - val_loss: 9204.3076 - val_mae: 9205.0000\n",
      "Epoch 3368/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 66.6040 - mae: 67.2866 - val_loss: 9477.1914 - val_mae: 9477.8838\n",
      "Epoch 3369/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 77.6066 - mae: 78.2919 - val_loss: 9392.7871 - val_mae: 9393.4805\n",
      "Epoch 3370/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 77.1936 - mae: 77.8751 - val_loss: 9176.0771 - val_mae: 9176.7695\n",
      "Epoch 3371/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 76.7001 - mae: 77.3815 - val_loss: 9119.5205 - val_mae: 9120.2129\n",
      "Epoch 3372/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 68.6384 - mae: 69.3226 - val_loss: 9280.5586 - val_mae: 9281.2500\n",
      "Epoch 3373/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 84.3676 - mae: 85.0531 - val_loss: 9467.7949 - val_mae: 9468.4873\n",
      "Epoch 3374/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 85.2200 - mae: 85.9034 - val_loss: 9243.5518 - val_mae: 9244.2451\n",
      "Epoch 3375/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 68.6678 - mae: 69.3552 - val_loss: 9266.0068 - val_mae: 9266.7002\n",
      "Epoch 3376/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 78.4605 - mae: 79.1471 - val_loss: 9332.2969 - val_mae: 9332.9893\n",
      "Epoch 3377/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 82.9992 - mae: 83.6838 - val_loss: 9233.3525 - val_mae: 9234.0469\n",
      "Epoch 3378/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 63.5594 - mae: 64.2408 - val_loss: 9190.0283 - val_mae: 9190.7207\n",
      "Epoch 3379/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 74.2701 - mae: 74.9552 - val_loss: 9228.7393 - val_mae: 9229.4326\n",
      "Epoch 3380/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 73.0166 - mae: 73.6973 - val_loss: 9326.5762 - val_mae: 9327.2695\n",
      "Epoch 3381/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 71.3481 - mae: 72.0319 - val_loss: 9330.2607 - val_mae: 9330.9531\n",
      "Epoch 3382/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 78.5207 - mae: 79.2056 - val_loss: 9214.6133 - val_mae: 9215.3047\n",
      "Epoch 3383/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 81.7287 - mae: 82.4165 - val_loss: 9211.3701 - val_mae: 9212.0635\n",
      "Epoch 3384/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 77.8221 - mae: 78.5090 - val_loss: 9285.5654 - val_mae: 9286.2588\n",
      "Epoch 3385/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 68.3777 - mae: 69.0608 - val_loss: 9291.5859 - val_mae: 9292.2803\n",
      "Epoch 3386/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 68.9080 - mae: 69.5913 - val_loss: 9433.6445 - val_mae: 9434.3369\n",
      "Epoch 3387/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 77.3927 - mae: 78.0731 - val_loss: 9315.8867 - val_mae: 9316.5801\n",
      "Epoch 3388/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 70.4783 - mae: 71.1616 - val_loss: 9062.4639 - val_mae: 9063.1572\n",
      "Epoch 3389/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 84.8373 - mae: 85.5260 - val_loss: 9023.5928 - val_mae: 9024.2852\n",
      "Epoch 3390/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 77.8910 - mae: 78.5733 - val_loss: 9149.9668 - val_mae: 9150.6582\n",
      "Epoch 3391/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 69.4088 - mae: 70.0916 - val_loss: 9224.1357 - val_mae: 9224.8271\n",
      "Epoch 3392/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 85.8033 - mae: 86.4865 - val_loss: 9191.9639 - val_mae: 9192.6562\n",
      "Epoch 3393/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 68.2024 - mae: 68.8855 - val_loss: 9334.4023 - val_mae: 9335.0947\n",
      "Epoch 3394/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 80.7176 - mae: 81.4037 - val_loss: 9307.5654 - val_mae: 9308.2588\n",
      "Epoch 3395/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 78.8484 - mae: 79.5330 - val_loss: 9200.7012 - val_mae: 9201.3945\n",
      "Epoch 3396/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 60.4644 - mae: 61.1472 - val_loss: 9418.6240 - val_mae: 9419.3174\n",
      "Epoch 3397/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 70.7406 - mae: 71.4232 - val_loss: 9227.8203 - val_mae: 9228.5127\n",
      "Epoch 3398/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 73.8630 - mae: 74.5487 - val_loss: 9039.1553 - val_mae: 9039.8486\n",
      "Epoch 3399/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 93.4403 - mae: 94.1282 - val_loss: 9161.2217 - val_mae: 9161.9150\n",
      "Epoch 3400/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 79.7918 - mae: 80.4715 - val_loss: 9033.3779 - val_mae: 9034.0723\n",
      "Epoch 3401/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 80.3617 - mae: 81.0472 - val_loss: 9174.8662 - val_mae: 9175.5605\n",
      "Epoch 3402/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 78.3020 - mae: 78.9842 - val_loss: 9195.8438 - val_mae: 9196.5371\n",
      "Epoch 3403/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 66.8596 - mae: 67.5409 - val_loss: 9281.0049 - val_mae: 9281.6963\n",
      "Epoch 3404/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 84.5074 - mae: 85.1937 - val_loss: 9222.9561 - val_mae: 9223.6504\n",
      "Epoch 3405/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 72.7260 - mae: 73.4118 - val_loss: 9250.2109 - val_mae: 9250.9033\n",
      "Epoch 3406/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 90.4194 - mae: 91.1043 - val_loss: 9177.7695 - val_mae: 9178.4619\n",
      "Epoch 3407/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 71.8675 - mae: 72.5487 - val_loss: 9121.8330 - val_mae: 9122.5254\n",
      "Epoch 3408/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 63.6816 - mae: 64.3647 - val_loss: 9043.3477 - val_mae: 9044.0410\n",
      "Epoch 3409/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 69.4592 - mae: 70.1398 - val_loss: 9313.8555 - val_mae: 9314.5498\n",
      "Epoch 3410/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 80.5878 - mae: 81.2711 - val_loss: 9277.6885 - val_mae: 9278.3799\n",
      "Epoch 3411/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 77.4488 - mae: 78.1346 - val_loss: 9273.1719 - val_mae: 9273.8643\n",
      "Epoch 3412/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 71.2261 - mae: 71.9099 - val_loss: 9325.3213 - val_mae: 9326.0146\n",
      "Epoch 3413/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 84.0333 - mae: 84.7194 - val_loss: 9218.6914 - val_mae: 9219.3838\n",
      "Epoch 3414/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 76.4702 - mae: 77.1504 - val_loss: 9183.5586 - val_mae: 9184.2510\n",
      "Epoch 3415/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 86.2492 - mae: 86.9359 - val_loss: 9292.1943 - val_mae: 9292.8877\n",
      "Epoch 3416/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 73.0326 - mae: 73.7216 - val_loss: 9287.9287 - val_mae: 9288.6221\n",
      "Epoch 3417/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 86.3686 - mae: 87.0531 - val_loss: 9168.1045 - val_mae: 9168.7969\n",
      "Epoch 3418/5000\n",
      "46/46 [==============================] - 2s 40ms/step - loss: 66.1731 - mae: 66.8577 - val_loss: 9245.7295 - val_mae: 9246.4209\n",
      "Epoch 3419/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 87.1994 - mae: 87.8838 - val_loss: 9268.7334 - val_mae: 9269.4277\n",
      "Epoch 3420/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 83.9473 - mae: 84.6349 - val_loss: 9370.7109 - val_mae: 9371.4023\n",
      "Epoch 3421/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 78.7073 - mae: 79.3900 - val_loss: 9245.9971 - val_mae: 9246.6895\n",
      "Epoch 3422/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 72.3202 - mae: 73.0001 - val_loss: 9345.7461 - val_mae: 9346.4385\n",
      "Epoch 3423/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 80.8774 - mae: 81.5602 - val_loss: 9148.3896 - val_mae: 9149.0840\n",
      "Epoch 3424/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 74.0224 - mae: 74.7079 - val_loss: 9257.9385 - val_mae: 9258.6318\n",
      "Epoch 3425/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 68.6522 - mae: 69.3383 - val_loss: 9358.1914 - val_mae: 9358.8838\n",
      "Epoch 3426/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 70.5616 - mae: 71.2475 - val_loss: 9348.7812 - val_mae: 9349.4736\n",
      "Epoch 3427/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 73.8242 - mae: 74.5115 - val_loss: 9365.8643 - val_mae: 9366.5596\n",
      "Epoch 3428/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 93.7392 - mae: 94.4243 - val_loss: 9013.8545 - val_mae: 9014.5479\n",
      "Epoch 3429/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 76.4773 - mae: 77.1624 - val_loss: 9288.3662 - val_mae: 9289.0586\n",
      "Epoch 3430/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 73.1556 - mae: 73.8360 - val_loss: 9170.4287 - val_mae: 9171.1211\n",
      "Epoch 3431/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 83.0021 - mae: 83.6793 - val_loss: 9275.6055 - val_mae: 9276.2988\n",
      "Epoch 3432/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 65.8715 - mae: 66.5548 - val_loss: 9101.2627 - val_mae: 9101.9561\n",
      "Epoch 3433/5000\n",
      "46/46 [==============================] - 2s 41ms/step - loss: 79.8927 - mae: 80.5788 - val_loss: 8932.8408 - val_mae: 8933.5332\n",
      "Epoch 3434/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 95.6963 - mae: 96.3788 - val_loss: 9127.0430 - val_mae: 9127.7354\n",
      "Epoch 3435/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 72.1521 - mae: 72.8349 - val_loss: 9211.9873 - val_mae: 9212.6807\n",
      "Epoch 3436/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 67.8774 - mae: 68.5562 - val_loss: 8972.8291 - val_mae: 8973.5215\n",
      "Epoch 3437/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 69.5583 - mae: 70.2384 - val_loss: 9144.8135 - val_mae: 9145.5059\n",
      "Epoch 3438/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 82.1020 - mae: 82.7832 - val_loss: 9125.3691 - val_mae: 9126.0615\n",
      "Epoch 3439/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 76.0929 - mae: 76.7707 - val_loss: 9302.5420 - val_mae: 9303.2344\n",
      "Epoch 3440/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 88.4563 - mae: 89.1399 - val_loss: 9172.2070 - val_mae: 9172.9004\n",
      "Epoch 3441/5000\n",
      "46/46 [==============================] - 2s 32ms/step - loss: 74.1643 - mae: 74.8474 - val_loss: 9077.1611 - val_mae: 9077.8535\n",
      "Epoch 3442/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 80.2273 - mae: 80.9145 - val_loss: 9257.8936 - val_mae: 9258.5859\n",
      "Epoch 3443/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 76.5840 - mae: 77.2653 - val_loss: 9145.5537 - val_mae: 9146.2471\n",
      "Epoch 3444/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 63.7510 - mae: 64.4276 - val_loss: 9286.3428 - val_mae: 9287.0342\n",
      "Epoch 3445/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 65.7021 - mae: 66.3860 - val_loss: 9106.0898 - val_mae: 9106.7822\n",
      "Epoch 3446/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 73.0586 - mae: 73.7404 - val_loss: 9302.8447 - val_mae: 9303.5371\n",
      "Epoch 3447/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 74.1654 - mae: 74.8477 - val_loss: 9102.2197 - val_mae: 9102.9121\n",
      "Epoch 3448/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 76.3738 - mae: 77.0499 - val_loss: 9163.2422 - val_mae: 9163.9365\n",
      "Epoch 3449/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 67.9521 - mae: 68.6325 - val_loss: 9369.1768 - val_mae: 9369.8691\n",
      "Epoch 3450/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 69.7235 - mae: 70.4087 - val_loss: 9076.1025 - val_mae: 9076.7959\n",
      "Epoch 3451/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 88.2299 - mae: 88.9110 - val_loss: 9157.5693 - val_mae: 9158.2627\n",
      "Epoch 3452/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 84.3813 - mae: 85.0661 - val_loss: 9151.5938 - val_mae: 9152.2881\n",
      "Epoch 3453/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 72.8940 - mae: 73.5763 - val_loss: 9129.3887 - val_mae: 9130.0811\n",
      "Epoch 3454/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 80.3862 - mae: 81.0714 - val_loss: 9330.3838 - val_mae: 9331.0762\n",
      "Epoch 3455/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 67.3601 - mae: 68.0471 - val_loss: 9301.5391 - val_mae: 9302.2314\n",
      "Epoch 3456/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 80.7557 - mae: 81.4426 - val_loss: 9213.3438 - val_mae: 9214.0371\n",
      "Epoch 3457/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 67.6546 - mae: 68.3365 - val_loss: 9074.4951 - val_mae: 9075.1895\n",
      "Epoch 3458/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 79.8342 - mae: 80.5216 - val_loss: 9081.3027 - val_mae: 9081.9961\n",
      "Epoch 3459/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 74.0511 - mae: 74.7355 - val_loss: 9068.5322 - val_mae: 9069.2256\n",
      "Epoch 3460/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 71.3364 - mae: 72.0207 - val_loss: 9206.7178 - val_mae: 9207.4111\n",
      "Epoch 3461/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 69.4690 - mae: 70.1518 - val_loss: 9276.8311 - val_mae: 9277.5225\n",
      "Epoch 3462/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 75.0796 - mae: 75.7680 - val_loss: 9111.6465 - val_mae: 9112.3389\n",
      "Epoch 3463/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 69.1478 - mae: 69.8322 - val_loss: 9083.6084 - val_mae: 9084.3018\n",
      "Epoch 3464/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 68.2823 - mae: 68.9665 - val_loss: 9068.2607 - val_mae: 9068.9551\n",
      "Epoch 3465/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 87.1875 - mae: 87.8670 - val_loss: 9164.8135 - val_mae: 9165.5078\n",
      "Epoch 3466/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 69.6219 - mae: 70.3067 - val_loss: 9181.3438 - val_mae: 9182.0371\n",
      "Epoch 3467/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 72.2155 - mae: 72.8961 - val_loss: 9186.2197 - val_mae: 9186.9131\n",
      "Epoch 3468/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 65.8073 - mae: 66.4885 - val_loss: 9178.9746 - val_mae: 9179.6660\n",
      "Epoch 3469/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 72.9622 - mae: 73.6451 - val_loss: 9265.9180 - val_mae: 9266.6133\n",
      "Epoch 3470/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 72.6745 - mae: 73.3564 - val_loss: 9284.9932 - val_mae: 9285.6875\n",
      "Epoch 3471/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 77.7951 - mae: 78.4791 - val_loss: 9198.1738 - val_mae: 9198.8672\n",
      "Epoch 3472/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 81.4683 - mae: 82.1499 - val_loss: 9221.8154 - val_mae: 9222.5078\n",
      "Epoch 3473/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 82.1621 - mae: 82.8458 - val_loss: 9170.7021 - val_mae: 9171.3955\n",
      "Epoch 3474/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 66.6479 - mae: 67.3266 - val_loss: 9225.1396 - val_mae: 9225.8330\n",
      "Epoch 3475/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 74.7559 - mae: 75.4445 - val_loss: 9221.1172 - val_mae: 9221.8105\n",
      "Epoch 3476/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 69.6982 - mae: 70.3787 - val_loss: 9304.4717 - val_mae: 9305.1641\n",
      "Epoch 3477/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 76.0068 - mae: 76.6951 - val_loss: 9203.1543 - val_mae: 9203.8496\n",
      "Epoch 3478/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 69.4887 - mae: 70.1700 - val_loss: 8996.1133 - val_mae: 8996.8076\n",
      "Epoch 3479/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 70.1287 - mae: 70.8127 - val_loss: 9121.0352 - val_mae: 9121.7295\n",
      "Epoch 3480/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 65.2859 - mae: 65.9663 - val_loss: 9221.5459 - val_mae: 9222.2393\n",
      "Epoch 3481/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 71.1206 - mae: 71.8015 - val_loss: 9087.5186 - val_mae: 9088.2109\n",
      "Epoch 3482/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 71.7661 - mae: 72.4512 - val_loss: 9034.7344 - val_mae: 9035.4287\n",
      "Epoch 3483/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 65.9538 - mae: 66.6375 - val_loss: 9136.3525 - val_mae: 9137.0459\n",
      "Epoch 3484/5000\n",
      "46/46 [==============================] - 2s 40ms/step - loss: 73.5520 - mae: 74.2390 - val_loss: 9129.4971 - val_mae: 9130.1904\n",
      "Epoch 3485/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 67.1036 - mae: 67.7854 - val_loss: 9242.1992 - val_mae: 9242.8926\n",
      "Epoch 3486/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 72.7209 - mae: 73.4051 - val_loss: 9249.7686 - val_mae: 9250.4609\n",
      "Epoch 3487/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 66.8524 - mae: 67.5337 - val_loss: 9047.5781 - val_mae: 9048.2715\n",
      "Epoch 3488/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 67.7700 - mae: 68.4527 - val_loss: 9296.6484 - val_mae: 9297.3428\n",
      "Epoch 3489/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 79.6565 - mae: 80.3415 - val_loss: 9099.5840 - val_mae: 9100.2783\n",
      "Epoch 3490/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 66.9197 - mae: 67.6052 - val_loss: 9286.4785 - val_mae: 9287.1719\n",
      "Epoch 3491/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 79.8744 - mae: 80.5594 - val_loss: 9354.2871 - val_mae: 9354.9814\n",
      "Epoch 3492/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 71.4887 - mae: 72.1722 - val_loss: 9270.7598 - val_mae: 9271.4531\n",
      "Epoch 3493/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 71.4979 - mae: 72.1834 - val_loss: 9158.3223 - val_mae: 9159.0146\n",
      "Epoch 3494/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 66.0609 - mae: 66.7452 - val_loss: 9234.1973 - val_mae: 9234.8896\n",
      "Epoch 3495/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 63.3941 - mae: 64.0770 - val_loss: 9279.8486 - val_mae: 9280.5410\n",
      "Epoch 3496/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 69.4180 - mae: 70.1041 - val_loss: 9315.8408 - val_mae: 9316.5342\n",
      "Epoch 3497/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 66.8506 - mae: 67.5373 - val_loss: 9245.2852 - val_mae: 9245.9785\n",
      "Epoch 3498/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 69.1760 - mae: 69.8603 - val_loss: 9224.5146 - val_mae: 9225.2090\n",
      "Epoch 3499/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 74.9421 - mae: 75.6235 - val_loss: 9206.0332 - val_mae: 9206.7266\n",
      "Epoch 3500/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 66.3656 - mae: 67.0491 - val_loss: 9186.0488 - val_mae: 9186.7422\n",
      "Epoch 3501/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 69.2363 - mae: 69.9230 - val_loss: 9303.4688 - val_mae: 9304.1611\n",
      "Epoch 3502/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 83.8036 - mae: 84.4890 - val_loss: 9205.8447 - val_mae: 9206.5381\n",
      "Epoch 3503/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 62.0176 - mae: 62.7004 - val_loss: 9172.5898 - val_mae: 9173.2842\n",
      "Epoch 3504/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 73.2047 - mae: 73.8902 - val_loss: 9054.8916 - val_mae: 9055.5850\n",
      "Epoch 3505/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 67.3725 - mae: 68.0550 - val_loss: 9131.1143 - val_mae: 9131.8086\n",
      "Epoch 3506/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 66.8427 - mae: 67.5230 - val_loss: 9014.6094 - val_mae: 9015.3008\n",
      "Epoch 3507/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 100.7708 - mae: 101.4586 - val_loss: 9377.0771 - val_mae: 9377.7715\n",
      "Epoch 3508/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 78.0534 - mae: 78.7399 - val_loss: 9242.6045 - val_mae: 9243.2979\n",
      "Epoch 3509/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 60.5620 - mae: 61.2453 - val_loss: 9129.8379 - val_mae: 9130.5312\n",
      "Epoch 3510/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 68.5415 - mae: 69.2238 - val_loss: 9219.9941 - val_mae: 9220.6885\n",
      "Epoch 3511/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 70.0765 - mae: 70.7597 - val_loss: 9310.6436 - val_mae: 9311.3359\n",
      "Epoch 3512/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 64.9439 - mae: 65.6271 - val_loss: 9130.1064 - val_mae: 9130.8008\n",
      "Epoch 3513/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 63.8872 - mae: 64.5704 - val_loss: 9124.2988 - val_mae: 9124.9922\n",
      "Epoch 3514/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 71.3205 - mae: 72.0057 - val_loss: 9061.4727 - val_mae: 9062.1641\n",
      "Epoch 3515/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 68.7487 - mae: 69.4333 - val_loss: 9221.5947 - val_mae: 9222.2871\n",
      "Epoch 3516/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 70.3198 - mae: 71.0055 - val_loss: 9273.8486 - val_mae: 9274.5420\n",
      "Epoch 3517/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 80.4206 - mae: 81.1080 - val_loss: 9272.4824 - val_mae: 9273.1748\n",
      "Epoch 3518/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 69.2965 - mae: 69.9783 - val_loss: 9271.6064 - val_mae: 9272.2998\n",
      "Epoch 3519/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 75.5332 - mae: 76.2177 - val_loss: 9281.9834 - val_mae: 9282.6748\n",
      "Epoch 3520/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 77.5763 - mae: 78.2613 - val_loss: 9347.8330 - val_mae: 9348.5254\n",
      "Epoch 3521/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 65.0619 - mae: 65.7425 - val_loss: 9168.9316 - val_mae: 9169.6260\n",
      "Epoch 3522/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 78.9220 - mae: 79.6069 - val_loss: 9061.6631 - val_mae: 9062.3555\n",
      "Epoch 3523/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 89.9353 - mae: 90.6226 - val_loss: 9109.4141 - val_mae: 9110.1074\n",
      "Epoch 3524/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 71.3329 - mae: 72.0201 - val_loss: 9170.1348 - val_mae: 9170.8271\n",
      "Epoch 3525/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 56.6496 - mae: 57.3327 - val_loss: 9234.4697 - val_mae: 9235.1631\n",
      "Epoch 3526/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 70.8706 - mae: 71.5537 - val_loss: 9113.1553 - val_mae: 9113.8496\n",
      "Epoch 3527/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 77.9182 - mae: 78.6015 - val_loss: 9153.7051 - val_mae: 9154.3984\n",
      "Epoch 3528/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 77.5659 - mae: 78.2511 - val_loss: 9120.1104 - val_mae: 9120.8037\n",
      "Epoch 3529/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 73.8457 - mae: 74.5317 - val_loss: 9094.4531 - val_mae: 9095.1484\n",
      "Epoch 3530/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 64.6082 - mae: 65.2858 - val_loss: 9162.1758 - val_mae: 9162.8691\n",
      "Epoch 3531/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 69.8887 - mae: 70.5727 - val_loss: 9207.2803 - val_mae: 9207.9727\n",
      "Epoch 3532/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 68.0003 - mae: 68.6871 - val_loss: 9273.4199 - val_mae: 9274.1123\n",
      "Epoch 3533/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 66.7750 - mae: 67.4553 - val_loss: 9142.5771 - val_mae: 9143.2695\n",
      "Epoch 3534/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 73.9706 - mae: 74.6492 - val_loss: 9145.0625 - val_mae: 9145.7549\n",
      "Epoch 3535/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 63.5514 - mae: 64.2311 - val_loss: 9144.3867 - val_mae: 9145.0791\n",
      "Epoch 3536/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 72.8124 - mae: 73.4958 - val_loss: 9229.5498 - val_mae: 9230.2432\n",
      "Epoch 3537/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 66.0601 - mae: 66.7428 - val_loss: 9126.2266 - val_mae: 9126.9189\n",
      "Epoch 3538/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 79.0872 - mae: 79.7719 - val_loss: 9153.7451 - val_mae: 9154.4385\n",
      "Epoch 3539/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 64.6804 - mae: 65.3635 - val_loss: 9227.0566 - val_mae: 9227.7510\n",
      "Epoch 3540/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 72.1730 - mae: 72.8549 - val_loss: 9158.0146 - val_mae: 9158.7070\n",
      "Epoch 3541/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 73.9572 - mae: 74.6411 - val_loss: 9178.8770 - val_mae: 9179.5703\n",
      "Epoch 3542/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 65.9833 - mae: 66.6608 - val_loss: 9158.3760 - val_mae: 9159.0684\n",
      "Epoch 3543/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 73.0410 - mae: 73.7205 - val_loss: 9239.5391 - val_mae: 9240.2305\n",
      "Epoch 3544/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 76.0048 - mae: 76.6874 - val_loss: 9328.8730 - val_mae: 9329.5664\n",
      "Epoch 3545/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 89.0596 - mae: 89.7443 - val_loss: 9232.7451 - val_mae: 9233.4375\n",
      "Epoch 3546/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 74.6296 - mae: 75.3146 - val_loss: 9230.9863 - val_mae: 9231.6787\n",
      "Epoch 3547/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 64.8977 - mae: 65.5829 - val_loss: 9184.0371 - val_mae: 9184.7295\n",
      "Epoch 3548/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 80.1760 - mae: 80.8624 - val_loss: 9022.5010 - val_mae: 9023.1924\n",
      "Epoch 3549/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 75.2399 - mae: 75.9279 - val_loss: 9156.5029 - val_mae: 9157.1973\n",
      "Epoch 3550/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 73.0548 - mae: 73.7408 - val_loss: 9299.0342 - val_mae: 9299.7266\n",
      "Epoch 3551/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 64.3395 - mae: 65.0221 - val_loss: 9227.5439 - val_mae: 9228.2363\n",
      "Epoch 3552/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 88.2947 - mae: 88.9802 - val_loss: 9105.2559 - val_mae: 9105.9482\n",
      "Epoch 3553/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 87.7448 - mae: 88.4295 - val_loss: 9142.6689 - val_mae: 9143.3613\n",
      "Epoch 3554/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 64.9039 - mae: 65.5880 - val_loss: 9267.5840 - val_mae: 9268.2773\n",
      "Epoch 3555/5000\n",
      "46/46 [==============================] - 2s 45ms/step - loss: 66.8089 - mae: 67.4937 - val_loss: 9104.2881 - val_mae: 9104.9805\n",
      "Epoch 3556/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 80.4122 - mae: 81.0938 - val_loss: 9228.0723 - val_mae: 9228.7656\n",
      "Epoch 3557/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 66.0180 - mae: 66.7018 - val_loss: 9215.7412 - val_mae: 9216.4346\n",
      "Epoch 3558/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 79.4621 - mae: 80.1440 - val_loss: 9079.0000 - val_mae: 9079.6924\n",
      "Epoch 3559/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 70.4476 - mae: 71.1324 - val_loss: 9293.0742 - val_mae: 9293.7676\n",
      "Epoch 3560/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 91.3062 - mae: 91.9909 - val_loss: 9227.2432 - val_mae: 9227.9365\n",
      "Epoch 3561/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 72.7417 - mae: 73.4254 - val_loss: 9285.1504 - val_mae: 9285.8447\n",
      "Epoch 3562/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 79.3181 - mae: 80.0029 - val_loss: 9303.5332 - val_mae: 9304.2256\n",
      "Epoch 3563/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 76.4153 - mae: 77.1021 - val_loss: 9142.7363 - val_mae: 9143.4297\n",
      "Epoch 3564/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 81.5931 - mae: 82.2800 - val_loss: 9161.8984 - val_mae: 9162.5918\n",
      "Epoch 3565/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 67.9735 - mae: 68.6546 - val_loss: 9081.1152 - val_mae: 9081.8086\n",
      "Epoch 3566/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 86.7174 - mae: 87.4033 - val_loss: 9122.7539 - val_mae: 9123.4473\n",
      "Epoch 3567/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 82.3495 - mae: 83.0388 - val_loss: 9126.7812 - val_mae: 9127.4727\n",
      "Epoch 3568/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 67.7553 - mae: 68.4365 - val_loss: 9078.9580 - val_mae: 9079.6514\n",
      "Epoch 3569/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 91.9407 - mae: 92.6292 - val_loss: 9255.3896 - val_mae: 9256.0820\n",
      "Epoch 3570/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 77.8163 - mae: 78.5041 - val_loss: 9165.0312 - val_mae: 9165.7236\n",
      "Epoch 3571/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 73.0251 - mae: 73.7094 - val_loss: 9185.2422 - val_mae: 9185.9346\n",
      "Epoch 3572/5000\n",
      "46/46 [==============================] - 2s 48ms/step - loss: 66.6368 - mae: 67.3201 - val_loss: 9097.6807 - val_mae: 9098.3750\n",
      "Epoch 3573/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 71.4342 - mae: 72.1176 - val_loss: 9151.5225 - val_mae: 9152.2158\n",
      "Epoch 3574/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 61.0709 - mae: 61.7509 - val_loss: 9086.2803 - val_mae: 9086.9746\n",
      "Epoch 3575/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 75.5978 - mae: 76.2849 - val_loss: 9396.9316 - val_mae: 9397.6250\n",
      "Epoch 3576/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 81.6356 - mae: 82.3197 - val_loss: 9252.7383 - val_mae: 9253.4307\n",
      "Epoch 3577/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 77.2115 - mae: 77.8973 - val_loss: 9199.9873 - val_mae: 9200.6807\n",
      "Epoch 3578/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 70.4506 - mae: 71.1373 - val_loss: 9219.1553 - val_mae: 9219.8496\n",
      "Epoch 3579/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 68.4165 - mae: 69.1006 - val_loss: 9230.0332 - val_mae: 9230.7256\n",
      "Epoch 3580/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 74.9996 - mae: 75.6800 - val_loss: 9168.4463 - val_mae: 9169.1406\n",
      "Epoch 3581/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 79.0281 - mae: 79.7148 - val_loss: 9381.9766 - val_mae: 9382.6680\n",
      "Epoch 3582/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 64.7859 - mae: 65.4683 - val_loss: 8986.6504 - val_mae: 8987.3428\n",
      "Epoch 3583/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 90.4834 - mae: 91.1745 - val_loss: 9179.2695 - val_mae: 9179.9629\n",
      "Epoch 3584/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 68.6951 - mae: 69.3794 - val_loss: 9280.9189 - val_mae: 9281.6123\n",
      "Epoch 3585/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 67.7766 - mae: 68.4599 - val_loss: 9344.3604 - val_mae: 9345.0537\n",
      "Epoch 3586/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 92.7981 - mae: 93.4861 - val_loss: 9069.8037 - val_mae: 9070.4971\n",
      "Epoch 3587/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 68.0490 - mae: 68.7314 - val_loss: 9044.9062 - val_mae: 9045.6006\n",
      "Epoch 3588/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 68.7168 - mae: 69.4029 - val_loss: 9123.3730 - val_mae: 9124.0664\n",
      "Epoch 3589/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 71.2735 - mae: 71.9576 - val_loss: 9248.5479 - val_mae: 9249.2412\n",
      "Epoch 3590/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 65.1115 - mae: 65.7990 - val_loss: 9250.2275 - val_mae: 9250.9199\n",
      "Epoch 3591/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 67.5601 - mae: 68.2441 - val_loss: 9304.4072 - val_mae: 9305.1006\n",
      "Epoch 3592/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 66.1879 - mae: 66.8717 - val_loss: 9126.1777 - val_mae: 9126.8701\n",
      "Epoch 3593/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 66.3003 - mae: 66.9832 - val_loss: 9165.8105 - val_mae: 9166.5039\n",
      "Epoch 3594/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 65.2438 - mae: 65.9256 - val_loss: 9226.7568 - val_mae: 9227.4502\n",
      "Epoch 3595/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 66.2496 - mae: 66.9294 - val_loss: 9057.1494 - val_mae: 9057.8418\n",
      "Epoch 3596/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 66.6260 - mae: 67.3059 - val_loss: 9229.6680 - val_mae: 9230.3613\n",
      "Epoch 3597/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 68.6374 - mae: 69.3174 - val_loss: 9211.3936 - val_mae: 9212.0869\n",
      "Epoch 3598/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 76.7252 - mae: 77.4114 - val_loss: 9150.5508 - val_mae: 9151.2441\n",
      "Epoch 3599/5000\n",
      "46/46 [==============================] - 2s 45ms/step - loss: 64.2991 - mae: 64.9795 - val_loss: 9126.5820 - val_mae: 9127.2744\n",
      "Epoch 3600/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 76.3362 - mae: 77.0160 - val_loss: 9067.0488 - val_mae: 9067.7412\n",
      "Epoch 3601/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 76.0850 - mae: 76.7652 - val_loss: 9112.6543 - val_mae: 9113.3477\n",
      "Epoch 3602/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 73.1576 - mae: 73.8408 - val_loss: 9321.1104 - val_mae: 9321.8047\n",
      "Epoch 3603/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 80.2222 - mae: 80.9036 - val_loss: 9167.4639 - val_mae: 9168.1562\n",
      "Epoch 3604/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 74.6388 - mae: 75.3206 - val_loss: 9228.7090 - val_mae: 9229.4004\n",
      "Epoch 3605/5000\n",
      "46/46 [==============================] - 2s 42ms/step - loss: 70.6288 - mae: 71.3134 - val_loss: 9158.6875 - val_mae: 9159.3809\n",
      "Epoch 3606/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 79.0425 - mae: 79.7282 - val_loss: 9314.1738 - val_mae: 9314.8672\n",
      "Epoch 3607/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 65.2600 - mae: 65.9432 - val_loss: 9107.0537 - val_mae: 9107.7471\n",
      "Epoch 3608/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 86.5602 - mae: 87.2490 - val_loss: 9152.2119 - val_mae: 9152.9043\n",
      "Epoch 3609/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 75.8272 - mae: 76.5152 - val_loss: 9091.8652 - val_mae: 9092.5586\n",
      "Epoch 3610/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 71.3648 - mae: 72.0511 - val_loss: 9209.3770 - val_mae: 9210.0693\n",
      "Epoch 3611/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 77.6474 - mae: 78.3309 - val_loss: 9131.0557 - val_mae: 9131.7500\n",
      "Epoch 3612/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 80.1192 - mae: 80.8022 - val_loss: 9018.4248 - val_mae: 9019.1182\n",
      "Epoch 3613/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 71.2199 - mae: 71.9004 - val_loss: 9038.1289 - val_mae: 9038.8232\n",
      "Epoch 3614/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 67.6944 - mae: 68.3769 - val_loss: 9331.3555 - val_mae: 9332.0488\n",
      "Epoch 3615/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 72.6412 - mae: 73.3191 - val_loss: 9384.2236 - val_mae: 9384.9180\n",
      "Epoch 3616/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 84.1238 - mae: 84.8133 - val_loss: 9163.2930 - val_mae: 9163.9854\n",
      "Epoch 3617/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 65.5634 - mae: 66.2458 - val_loss: 9168.0615 - val_mae: 9168.7549\n",
      "Epoch 3618/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 67.7713 - mae: 68.4520 - val_loss: 9156.3027 - val_mae: 9156.9941\n",
      "Epoch 3619/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 65.7413 - mae: 66.4258 - val_loss: 9124.4521 - val_mae: 9125.1445\n",
      "Epoch 3620/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 68.9161 - mae: 69.6003 - val_loss: 9133.7266 - val_mae: 9134.4199\n",
      "Epoch 3621/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 69.4533 - mae: 70.1372 - val_loss: 9165.0908 - val_mae: 9165.7842\n",
      "Epoch 3622/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 70.3217 - mae: 71.0050 - val_loss: 9293.1113 - val_mae: 9293.8027\n",
      "Epoch 3623/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 78.2217 - mae: 78.9030 - val_loss: 9164.5430 - val_mae: 9165.2354\n",
      "Epoch 3624/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 63.4789 - mae: 64.1608 - val_loss: 9269.3662 - val_mae: 9270.0605\n",
      "Epoch 3625/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 89.7938 - mae: 90.4753 - val_loss: 9135.9707 - val_mae: 9136.6621\n",
      "Epoch 3626/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 61.6077 - mae: 62.2914 - val_loss: 9291.4355 - val_mae: 9292.1299\n",
      "Epoch 3627/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 75.1315 - mae: 75.8109 - val_loss: 9252.6514 - val_mae: 9253.3467\n",
      "Epoch 3628/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 71.6180 - mae: 72.3046 - val_loss: 9123.3916 - val_mae: 9124.0840\n",
      "Epoch 3629/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 55.2397 - mae: 55.9198 - val_loss: 9199.4385 - val_mae: 9200.1309\n",
      "Epoch 3630/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 73.4322 - mae: 74.1156 - val_loss: 9140.6826 - val_mae: 9141.3750\n",
      "Epoch 3631/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 69.0895 - mae: 69.7734 - val_loss: 9278.1885 - val_mae: 9278.8818\n",
      "Epoch 3632/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 70.9555 - mae: 71.6371 - val_loss: 9253.6221 - val_mae: 9254.3154\n",
      "Epoch 3633/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 73.7257 - mae: 74.4116 - val_loss: 9247.8545 - val_mae: 9248.5488\n",
      "Epoch 3634/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 73.7069 - mae: 74.3914 - val_loss: 9208.6621 - val_mae: 9209.3555\n",
      "Epoch 3635/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 84.6456 - mae: 85.3311 - val_loss: 9127.2773 - val_mae: 9127.9697\n",
      "Epoch 3636/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 74.1881 - mae: 74.8729 - val_loss: 9121.9268 - val_mae: 9122.6201\n",
      "Epoch 3637/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 68.3537 - mae: 69.0397 - val_loss: 9338.1055 - val_mae: 9338.8008\n",
      "Epoch 3638/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 82.5155 - mae: 83.2010 - val_loss: 9315.6396 - val_mae: 9316.3330\n",
      "Epoch 3639/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 75.3084 - mae: 75.9930 - val_loss: 9306.5596 - val_mae: 9307.2529\n",
      "Epoch 3640/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 71.7773 - mae: 72.4603 - val_loss: 9115.7061 - val_mae: 9116.3994\n",
      "Epoch 3641/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 67.0797 - mae: 67.7659 - val_loss: 8995.4268 - val_mae: 8996.1191\n",
      "Epoch 3642/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 72.6574 - mae: 73.3409 - val_loss: 9010.9033 - val_mae: 9011.5967\n",
      "Epoch 3643/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 80.9768 - mae: 81.6624 - val_loss: 9157.3174 - val_mae: 9158.0107\n",
      "Epoch 3644/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 65.2637 - mae: 65.9454 - val_loss: 9248.9189 - val_mae: 9249.6123\n",
      "Epoch 3645/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 72.9401 - mae: 73.6245 - val_loss: 9164.5225 - val_mae: 9165.2148\n",
      "Epoch 3646/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 68.6399 - mae: 69.3252 - val_loss: 9220.0879 - val_mae: 9220.7822\n",
      "Epoch 3647/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 70.4755 - mae: 71.1614 - val_loss: 9267.3320 - val_mae: 9268.0254\n",
      "Epoch 3648/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 66.2329 - mae: 66.9173 - val_loss: 9175.7158 - val_mae: 9176.4082\n",
      "Epoch 3649/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 66.9900 - mae: 67.6708 - val_loss: 9097.2842 - val_mae: 9097.9775\n",
      "Epoch 3650/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 78.4885 - mae: 79.1741 - val_loss: 9311.5898 - val_mae: 9312.2832\n",
      "Epoch 3651/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 69.5104 - mae: 70.1947 - val_loss: 9149.1602 - val_mae: 9149.8525\n",
      "Epoch 3652/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 76.4811 - mae: 77.1661 - val_loss: 9303.4355 - val_mae: 9304.1299\n",
      "Epoch 3653/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 79.6731 - mae: 80.3572 - val_loss: 9080.7344 - val_mae: 9081.4268\n",
      "Epoch 3654/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 71.3522 - mae: 72.0394 - val_loss: 9310.9277 - val_mae: 9311.6201\n",
      "Epoch 3655/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 69.9188 - mae: 70.5989 - val_loss: 9248.8545 - val_mae: 9249.5479\n",
      "Epoch 3656/5000\n",
      "46/46 [==============================] - 2s 42ms/step - loss: 68.8434 - mae: 69.5267 - val_loss: 9127.3232 - val_mae: 9128.0156\n",
      "Epoch 3657/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 84.0715 - mae: 84.7514 - val_loss: 9112.9248 - val_mae: 9113.6191\n",
      "Epoch 3658/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 67.0491 - mae: 67.7366 - val_loss: 9102.3984 - val_mae: 9103.0898\n",
      "Epoch 3659/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 58.7712 - mae: 59.4515 - val_loss: 9207.5156 - val_mae: 9208.2090\n",
      "Epoch 3660/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 66.5851 - mae: 67.2681 - val_loss: 9367.1240 - val_mae: 9367.8193\n",
      "Epoch 3661/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 67.7171 - mae: 68.3962 - val_loss: 9241.7725 - val_mae: 9242.4668\n",
      "Epoch 3662/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 84.9358 - mae: 85.6211 - val_loss: 9029.8906 - val_mae: 9030.5840\n",
      "Epoch 3663/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 77.9842 - mae: 78.6715 - val_loss: 9123.3086 - val_mae: 9124.0010\n",
      "Epoch 3664/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 71.8911 - mae: 72.5708 - val_loss: 9309.4648 - val_mae: 9310.1582\n",
      "Epoch 3665/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 75.0442 - mae: 75.7249 - val_loss: 9149.5664 - val_mae: 9150.2598\n",
      "Epoch 3666/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 65.4701 - mae: 66.1530 - val_loss: 9156.1123 - val_mae: 9156.8057\n",
      "Epoch 3667/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 80.2176 - mae: 80.9043 - val_loss: 9203.2383 - val_mae: 9203.9326\n",
      "Epoch 3668/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 60.6159 - mae: 61.2962 - val_loss: 9204.9590 - val_mae: 9205.6533\n",
      "Epoch 3669/5000\n",
      "46/46 [==============================] - 2s 48ms/step - loss: 68.4724 - mae: 69.1568 - val_loss: 9370.4092 - val_mae: 9371.1025\n",
      "Epoch 3670/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 76.3504 - mae: 77.0307 - val_loss: 9234.9951 - val_mae: 9235.6885\n",
      "Epoch 3671/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 62.3607 - mae: 63.0421 - val_loss: 9324.7637 - val_mae: 9325.4580\n",
      "Epoch 3672/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 77.8364 - mae: 78.5233 - val_loss: 8930.5234 - val_mae: 8931.2168\n",
      "Epoch 3673/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 78.0887 - mae: 78.7755 - val_loss: 9153.5068 - val_mae: 9154.2002\n",
      "Epoch 3674/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 67.1582 - mae: 67.8433 - val_loss: 9183.7617 - val_mae: 9184.4541\n",
      "Epoch 3675/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 81.6008 - mae: 82.2870 - val_loss: 9289.0684 - val_mae: 9289.7598\n",
      "Epoch 3676/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 77.4034 - mae: 78.0905 - val_loss: 9185.1689 - val_mae: 9185.8623\n",
      "Epoch 3677/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 72.6385 - mae: 73.3177 - val_loss: 9024.0518 - val_mae: 9024.7451\n",
      "Epoch 3678/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 76.6700 - mae: 77.3557 - val_loss: 9109.2773 - val_mae: 9109.9717\n",
      "Epoch 3679/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 67.5770 - mae: 68.2616 - val_loss: 9118.6162 - val_mae: 9119.3086\n",
      "Epoch 3680/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 65.8195 - mae: 66.5014 - val_loss: 9182.1689 - val_mae: 9182.8633\n",
      "Epoch 3681/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 70.7322 - mae: 71.4196 - val_loss: 9094.7988 - val_mae: 9095.4912\n",
      "Epoch 3682/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 64.1037 - mae: 64.7855 - val_loss: 9316.2178 - val_mae: 9316.9121\n",
      "Epoch 3683/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 78.1071 - mae: 78.7937 - val_loss: 8967.8877 - val_mae: 8968.5820\n",
      "Epoch 3684/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 81.5714 - mae: 82.2576 - val_loss: 9129.3252 - val_mae: 9130.0186\n",
      "Epoch 3685/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 78.8490 - mae: 79.5333 - val_loss: 9268.5557 - val_mae: 9269.2490\n",
      "Epoch 3686/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 62.6684 - mae: 63.3554 - val_loss: 9157.7295 - val_mae: 9158.4229\n",
      "Epoch 3687/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 80.8377 - mae: 81.5250 - val_loss: 9252.0381 - val_mae: 9252.7324\n",
      "Epoch 3688/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 68.5936 - mae: 69.2782 - val_loss: 9193.8779 - val_mae: 9194.5713\n",
      "Epoch 3689/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 67.4260 - mae: 68.1032 - val_loss: 9099.1611 - val_mae: 9099.8555\n",
      "Epoch 3690/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 76.3757 - mae: 77.0638 - val_loss: 9293.6338 - val_mae: 9294.3281\n",
      "Epoch 3691/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 74.8041 - mae: 75.4879 - val_loss: 9116.1992 - val_mae: 9116.8906\n",
      "Epoch 3692/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 60.0019 - mae: 60.6855 - val_loss: 9226.8105 - val_mae: 9227.5049\n",
      "Epoch 3693/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 73.7801 - mae: 74.4651 - val_loss: 8998.3906 - val_mae: 8999.0850\n",
      "Epoch 3694/5000\n",
      "46/46 [==============================] - 2s 47ms/step - loss: 72.1965 - mae: 72.8744 - val_loss: 9327.6162 - val_mae: 9328.3066\n",
      "Epoch 3695/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 79.6588 - mae: 80.3439 - val_loss: 9173.3320 - val_mae: 9174.0244\n",
      "Epoch 3696/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 64.3591 - mae: 65.0431 - val_loss: 9126.9287 - val_mae: 9127.6221\n",
      "Epoch 3697/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 67.7715 - mae: 68.4565 - val_loss: 9230.1348 - val_mae: 9230.8291\n",
      "Epoch 3698/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 62.9018 - mae: 63.5833 - val_loss: 9088.9727 - val_mae: 9089.6660\n",
      "Epoch 3699/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 74.4663 - mae: 75.1469 - val_loss: 9111.8896 - val_mae: 9112.5840\n",
      "Epoch 3700/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 65.8874 - mae: 66.5706 - val_loss: 9038.5684 - val_mae: 9039.2617\n",
      "Epoch 3701/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 72.7101 - mae: 73.3954 - val_loss: 9175.3506 - val_mae: 9176.0439\n",
      "Epoch 3702/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 64.0031 - mae: 64.6870 - val_loss: 9243.4131 - val_mae: 9244.1055\n",
      "Epoch 3703/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 63.8731 - mae: 64.5558 - val_loss: 9052.2061 - val_mae: 9052.8994\n",
      "Epoch 3704/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 77.5282 - mae: 78.2128 - val_loss: 9241.0107 - val_mae: 9241.7041\n",
      "Epoch 3705/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 78.8359 - mae: 79.5229 - val_loss: 9020.0986 - val_mae: 9020.7920\n",
      "Epoch 3706/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 60.5299 - mae: 61.2145 - val_loss: 9084.2490 - val_mae: 9084.9414\n",
      "Epoch 3707/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 69.6366 - mae: 70.3197 - val_loss: 9234.9941 - val_mae: 9235.6865\n",
      "Epoch 3708/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 72.5640 - mae: 73.2471 - val_loss: 9101.4385 - val_mae: 9102.1328\n",
      "Epoch 3709/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 63.3795 - mae: 64.0583 - val_loss: 9063.2236 - val_mae: 9063.9160\n",
      "Epoch 3710/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 75.2633 - mae: 75.9442 - val_loss: 9135.8232 - val_mae: 9136.5156\n",
      "Epoch 3711/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 66.0442 - mae: 66.7300 - val_loss: 9045.1445 - val_mae: 9045.8379\n",
      "Epoch 3712/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 72.5318 - mae: 73.2148 - val_loss: 9141.9570 - val_mae: 9142.6504\n",
      "Epoch 3713/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 79.0776 - mae: 79.7624 - val_loss: 8983.0850 - val_mae: 8983.7783\n",
      "Epoch 3714/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 83.2656 - mae: 83.9517 - val_loss: 9072.8682 - val_mae: 9073.5596\n",
      "Epoch 3715/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 78.9129 - mae: 79.5949 - val_loss: 9279.1934 - val_mae: 9279.8857\n",
      "Epoch 3716/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 76.3352 - mae: 77.0244 - val_loss: 9067.2549 - val_mae: 9067.9502\n",
      "Epoch 3717/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 76.6062 - mae: 77.2895 - val_loss: 9168.5859 - val_mae: 9169.2803\n",
      "Epoch 3718/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 60.5455 - mae: 61.2307 - val_loss: 9173.8311 - val_mae: 9174.5234\n",
      "Epoch 3719/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 67.7590 - mae: 68.4421 - val_loss: 9306.6895 - val_mae: 9307.3818\n",
      "Epoch 3720/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 81.4471 - mae: 82.1300 - val_loss: 9066.4473 - val_mae: 9067.1406\n",
      "Epoch 3721/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 59.9995 - mae: 60.6839 - val_loss: 9325.4717 - val_mae: 9326.1660\n",
      "Epoch 3722/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 70.3972 - mae: 71.0793 - val_loss: 9344.0449 - val_mae: 9344.7383\n",
      "Epoch 3723/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 68.3748 - mae: 69.0556 - val_loss: 9134.9248 - val_mae: 9135.6182\n",
      "Epoch 3724/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 72.5635 - mae: 73.2446 - val_loss: 9114.6455 - val_mae: 9115.3389\n",
      "Epoch 3725/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 67.0291 - mae: 67.7136 - val_loss: 9059.1748 - val_mae: 9059.8672\n",
      "Epoch 3726/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 82.0512 - mae: 82.7343 - val_loss: 9229.5635 - val_mae: 9230.2559\n",
      "Epoch 3727/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 71.0473 - mae: 71.7324 - val_loss: 9126.4297 - val_mae: 9127.1221\n",
      "Epoch 3728/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 76.8554 - mae: 77.5384 - val_loss: 9156.4502 - val_mae: 9157.1445\n",
      "Epoch 3729/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 60.6069 - mae: 61.2885 - val_loss: 8992.7354 - val_mae: 8993.4297\n",
      "Epoch 3730/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 75.5089 - mae: 76.1935 - val_loss: 9277.1377 - val_mae: 9277.8311\n",
      "Epoch 3731/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 71.7644 - mae: 72.4493 - val_loss: 9088.3398 - val_mae: 9089.0322\n",
      "Epoch 3732/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 69.3606 - mae: 70.0413 - val_loss: 9301.0312 - val_mae: 9301.7236\n",
      "Epoch 3733/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 91.0331 - mae: 91.7147 - val_loss: 9247.1924 - val_mae: 9247.8848\n",
      "Epoch 3734/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 69.0376 - mae: 69.7191 - val_loss: 9218.5996 - val_mae: 9219.2910\n",
      "Epoch 3735/5000\n",
      "46/46 [==============================] - 3s 58ms/step - loss: 66.9906 - mae: 67.6714 - val_loss: 9187.5225 - val_mae: 9188.2158\n",
      "Epoch 3736/5000\n",
      "46/46 [==============================] - 2s 43ms/step - loss: 71.1414 - mae: 71.8267 - val_loss: 9228.9873 - val_mae: 9229.6807\n",
      "Epoch 3737/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 75.0233 - mae: 75.7037 - val_loss: 9097.8789 - val_mae: 9098.5713\n",
      "Epoch 3738/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 79.3348 - mae: 80.0184 - val_loss: 9272.1514 - val_mae: 9272.8438\n",
      "Epoch 3739/5000\n",
      "46/46 [==============================] - 2s 42ms/step - loss: 80.4507 - mae: 81.1363 - val_loss: 9162.8291 - val_mae: 9163.5225\n",
      "Epoch 3740/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 65.4491 - mae: 66.1310 - val_loss: 8922.8750 - val_mae: 8923.5684\n",
      "Epoch 3741/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 70.4788 - mae: 71.1611 - val_loss: 9229.0137 - val_mae: 9229.7080\n",
      "Epoch 3742/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 68.3628 - mae: 69.0462 - val_loss: 9220.4248 - val_mae: 9221.1191\n",
      "Epoch 3743/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 67.1409 - mae: 67.8199 - val_loss: 9107.3037 - val_mae: 9107.9961\n",
      "Epoch 3744/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 64.1869 - mae: 64.8700 - val_loss: 9043.5908 - val_mae: 9044.2842\n",
      "Epoch 3745/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 73.6729 - mae: 74.3558 - val_loss: 9045.2725 - val_mae: 9045.9658\n",
      "Epoch 3746/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 77.8366 - mae: 78.5172 - val_loss: 9098.9980 - val_mae: 9099.6904\n",
      "Epoch 3747/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 62.3927 - mae: 63.0753 - val_loss: 9318.6709 - val_mae: 9319.3643\n",
      "Epoch 3748/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 71.2775 - mae: 71.9622 - val_loss: 9126.3242 - val_mae: 9127.0166\n",
      "Epoch 3749/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 83.8139 - mae: 84.4972 - val_loss: 8995.6484 - val_mae: 8996.3418\n",
      "Epoch 3750/5000\n",
      "46/46 [==============================] - 2s 40ms/step - loss: 56.2775 - mae: 56.9591 - val_loss: 9165.8438 - val_mae: 9166.5381\n",
      "Epoch 3751/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 64.8803 - mae: 65.5661 - val_loss: 9108.7119 - val_mae: 9109.4053\n",
      "Epoch 3752/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 64.8728 - mae: 65.5585 - val_loss: 9303.3076 - val_mae: 9304.0000\n",
      "Epoch 3753/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 68.0755 - mae: 68.7590 - val_loss: 9092.8467 - val_mae: 9093.5391\n",
      "Epoch 3754/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 66.2508 - mae: 66.9372 - val_loss: 9031.0752 - val_mae: 9031.7686\n",
      "Epoch 3755/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 65.3514 - mae: 66.0341 - val_loss: 9265.0430 - val_mae: 9265.7354\n",
      "Epoch 3756/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 79.2575 - mae: 79.9402 - val_loss: 9101.6270 - val_mae: 9102.3203\n",
      "Epoch 3757/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 75.1993 - mae: 75.8844 - val_loss: 9013.2314 - val_mae: 9013.9248\n",
      "Epoch 3758/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 76.0079 - mae: 76.6960 - val_loss: 9191.1504 - val_mae: 9191.8457\n",
      "Epoch 3759/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 70.7329 - mae: 71.4154 - val_loss: 9071.3594 - val_mae: 9072.0537\n",
      "Epoch 3760/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 70.9524 - mae: 71.6386 - val_loss: 9176.6455 - val_mae: 9177.3389\n",
      "Epoch 3761/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 71.9713 - mae: 72.6574 - val_loss: 9062.0029 - val_mae: 9062.6953\n",
      "Epoch 3762/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 65.1599 - mae: 65.8442 - val_loss: 9104.0459 - val_mae: 9104.7383\n",
      "Epoch 3763/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 76.3255 - mae: 77.0107 - val_loss: 9222.4502 - val_mae: 9223.1455\n",
      "Epoch 3764/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 56.0464 - mae: 56.7230 - val_loss: 8925.0752 - val_mae: 8925.7686\n",
      "Epoch 3765/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 66.4941 - mae: 67.1741 - val_loss: 9203.1895 - val_mae: 9203.8809\n",
      "Epoch 3766/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 69.9657 - mae: 70.6440 - val_loss: 9272.5586 - val_mae: 9273.2510\n",
      "Epoch 3767/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 82.2704 - mae: 82.9581 - val_loss: 9205.1572 - val_mae: 9205.8516\n",
      "Epoch 3768/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 69.3665 - mae: 70.0482 - val_loss: 9135.1943 - val_mae: 9135.8877\n",
      "Epoch 3769/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 67.0759 - mae: 67.7588 - val_loss: 9232.7764 - val_mae: 9233.4697\n",
      "Epoch 3770/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 71.2012 - mae: 71.8789 - val_loss: 9108.7090 - val_mae: 9109.4014\n",
      "Epoch 3771/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 64.7984 - mae: 65.4809 - val_loss: 9328.7148 - val_mae: 9329.4092\n",
      "Epoch 3772/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 77.5370 - mae: 78.2189 - val_loss: 9389.8428 - val_mae: 9390.5361\n",
      "Epoch 3773/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 64.3409 - mae: 65.0251 - val_loss: 9114.9170 - val_mae: 9115.6094\n",
      "Epoch 3774/5000\n",
      "46/46 [==============================] - 2s 42ms/step - loss: 67.4327 - mae: 68.1161 - val_loss: 9160.3623 - val_mae: 9161.0557\n",
      "Epoch 3775/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 71.3379 - mae: 72.0180 - val_loss: 9135.3721 - val_mae: 9136.0654\n",
      "Epoch 3776/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 60.6599 - mae: 61.3432 - val_loss: 9165.5234 - val_mae: 9166.2168\n",
      "Epoch 3777/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 66.8590 - mae: 67.5398 - val_loss: 9021.7969 - val_mae: 9022.4893\n",
      "Epoch 3778/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 71.8527 - mae: 72.5342 - val_loss: 9233.1650 - val_mae: 9233.8594\n",
      "Epoch 3779/5000\n",
      "46/46 [==============================] - 2s 41ms/step - loss: 57.9123 - mae: 58.5953 - val_loss: 8969.0967 - val_mae: 8969.7891\n",
      "Epoch 3780/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 79.9621 - mae: 80.6410 - val_loss: 9109.9414 - val_mae: 9110.6357\n",
      "Epoch 3781/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 74.9078 - mae: 75.5925 - val_loss: 9154.7822 - val_mae: 9155.4756\n",
      "Epoch 3782/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 70.8603 - mae: 71.5387 - val_loss: 9234.5869 - val_mae: 9235.2803\n",
      "Epoch 3783/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 60.8510 - mae: 61.5315 - val_loss: 9289.6914 - val_mae: 9290.3848\n",
      "Epoch 3784/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 79.9431 - mae: 80.6248 - val_loss: 9164.4043 - val_mae: 9165.0996\n",
      "Epoch 3785/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 55.2035 - mae: 55.8830 - val_loss: 9154.9668 - val_mae: 9155.6592\n",
      "Epoch 3786/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 74.0794 - mae: 74.7646 - val_loss: 9189.4082 - val_mae: 9190.1025\n",
      "Epoch 3787/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 80.0635 - mae: 80.7465 - val_loss: 9135.9678 - val_mae: 9136.6611\n",
      "Epoch 3788/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 69.9853 - mae: 70.6718 - val_loss: 9164.7891 - val_mae: 9165.4814\n",
      "Epoch 3789/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 70.8698 - mae: 71.5527 - val_loss: 9107.1279 - val_mae: 9107.8223\n",
      "Epoch 3790/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 63.0756 - mae: 63.7569 - val_loss: 9255.0557 - val_mae: 9255.7500\n",
      "Epoch 3791/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 83.4119 - mae: 84.0939 - val_loss: 9118.7637 - val_mae: 9119.4580\n",
      "Epoch 3792/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 69.0373 - mae: 69.7206 - val_loss: 9137.1221 - val_mae: 9137.8154\n",
      "Epoch 3793/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 70.7222 - mae: 71.4083 - val_loss: 9176.2236 - val_mae: 9176.9180\n",
      "Epoch 3794/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 63.9579 - mae: 64.6356 - val_loss: 9132.4385 - val_mae: 9133.1318\n",
      "Epoch 3795/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 68.0237 - mae: 68.7070 - val_loss: 9225.4814 - val_mae: 9226.1738\n",
      "Epoch 3796/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 61.4255 - mae: 62.1066 - val_loss: 9276.6680 - val_mae: 9277.3613\n",
      "Epoch 3797/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 82.8149 - mae: 83.4993 - val_loss: 9185.4209 - val_mae: 9186.1123\n",
      "Epoch 3798/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 71.3550 - mae: 72.0389 - val_loss: 9067.1514 - val_mae: 9067.8457\n",
      "Epoch 3799/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 80.0257 - mae: 80.7048 - val_loss: 9180.9629 - val_mae: 9181.6562\n",
      "Epoch 3800/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 62.6529 - mae: 63.3400 - val_loss: 9235.3096 - val_mae: 9236.0029\n",
      "Epoch 3801/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 64.2204 - mae: 64.9051 - val_loss: 9341.6318 - val_mae: 9342.3252\n",
      "Epoch 3802/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 74.2127 - mae: 74.8913 - val_loss: 9115.5039 - val_mae: 9116.1963\n",
      "Epoch 3803/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 75.7421 - mae: 76.4274 - val_loss: 9174.3154 - val_mae: 9175.0088\n",
      "Epoch 3804/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 70.4027 - mae: 71.0780 - val_loss: 9166.6729 - val_mae: 9167.3662\n",
      "Epoch 3805/5000\n",
      "46/46 [==============================] - 2s 40ms/step - loss: 68.4664 - mae: 69.1478 - val_loss: 9165.4854 - val_mae: 9166.1777\n",
      "Epoch 3806/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 66.3697 - mae: 67.0519 - val_loss: 9290.8447 - val_mae: 9291.5391\n",
      "Epoch 3807/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 77.2858 - mae: 77.9745 - val_loss: 8997.3799 - val_mae: 8998.0732\n",
      "Epoch 3808/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 80.4839 - mae: 81.1668 - val_loss: 9172.2080 - val_mae: 9172.9004\n",
      "Epoch 3809/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 71.5763 - mae: 72.2609 - val_loss: 9172.6953 - val_mae: 9173.3887\n",
      "Epoch 3810/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 68.7832 - mae: 69.4683 - val_loss: 9241.3613 - val_mae: 9242.0547\n",
      "Epoch 3811/5000\n",
      "46/46 [==============================] - 2s 43ms/step - loss: 74.1511 - mae: 74.8326 - val_loss: 9005.6064 - val_mae: 9006.3008\n",
      "Epoch 3812/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 66.1374 - mae: 66.8216 - val_loss: 9160.6035 - val_mae: 9161.2969\n",
      "Epoch 3813/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 69.3394 - mae: 70.0234 - val_loss: 9118.2812 - val_mae: 9118.9736\n",
      "Epoch 3814/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 63.0147 - mae: 63.6973 - val_loss: 9305.1387 - val_mae: 9305.8301\n",
      "Epoch 3815/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 66.5811 - mae: 67.2660 - val_loss: 9136.9619 - val_mae: 9137.6553\n",
      "Epoch 3816/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 76.4188 - mae: 77.1003 - val_loss: 9426.9863 - val_mae: 9427.6807\n",
      "Epoch 3817/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 83.4727 - mae: 84.1587 - val_loss: 9165.9102 - val_mae: 9166.6035\n",
      "Epoch 3818/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 66.2733 - mae: 66.9556 - val_loss: 9281.2070 - val_mae: 9281.9004\n",
      "Epoch 3819/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 67.8659 - mae: 68.5532 - val_loss: 8974.6045 - val_mae: 8975.2969\n",
      "Epoch 3820/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 75.4310 - mae: 76.1118 - val_loss: 9130.1641 - val_mae: 9130.8574\n",
      "Epoch 3821/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 66.5883 - mae: 67.2712 - val_loss: 9132.6270 - val_mae: 9133.3193\n",
      "Epoch 3822/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 68.1267 - mae: 68.8122 - val_loss: 9059.9062 - val_mae: 9060.5996\n",
      "Epoch 3823/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 81.4140 - mae: 82.0987 - val_loss: 9275.5361 - val_mae: 9276.2275\n",
      "Epoch 3824/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 68.3510 - mae: 69.0324 - val_loss: 9195.0508 - val_mae: 9195.7451\n",
      "Epoch 3825/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 68.6445 - mae: 69.3274 - val_loss: 9117.1641 - val_mae: 9117.8574\n",
      "Epoch 3826/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 72.4193 - mae: 73.1037 - val_loss: 9032.3691 - val_mae: 9033.0625\n",
      "Epoch 3827/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 82.5070 - mae: 83.1906 - val_loss: 9177.2715 - val_mae: 9177.9648\n",
      "Epoch 3828/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 72.8272 - mae: 73.5133 - val_loss: 9163.1455 - val_mae: 9163.8389\n",
      "Epoch 3829/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 62.6943 - mae: 63.3722 - val_loss: 9184.7090 - val_mae: 9185.4014\n",
      "Epoch 3830/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 65.0378 - mae: 65.7210 - val_loss: 9029.6729 - val_mae: 9030.3672\n",
      "Epoch 3831/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 73.4548 - mae: 74.1374 - val_loss: 9209.8252 - val_mae: 9210.5186\n",
      "Epoch 3832/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 75.9481 - mae: 76.6296 - val_loss: 9143.6689 - val_mae: 9144.3633\n",
      "Epoch 3833/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 64.9173 - mae: 65.6007 - val_loss: 9096.6279 - val_mae: 9097.3213\n",
      "Epoch 3834/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 64.0387 - mae: 64.7213 - val_loss: 9181.3652 - val_mae: 9182.0586\n",
      "Epoch 3835/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 66.5391 - mae: 67.2211 - val_loss: 9330.2949 - val_mae: 9330.9893\n",
      "Epoch 3836/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 71.9240 - mae: 72.6057 - val_loss: 9198.3027 - val_mae: 9198.9961\n",
      "Epoch 3837/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 63.5014 - mae: 64.1822 - val_loss: 9214.5098 - val_mae: 9215.2012\n",
      "Epoch 3838/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 68.7919 - mae: 69.4775 - val_loss: 9223.0303 - val_mae: 9223.7236\n",
      "Epoch 3839/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 75.4139 - mae: 76.0979 - val_loss: 9281.2402 - val_mae: 9281.9316\n",
      "Epoch 3840/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 68.5677 - mae: 69.2560 - val_loss: 9183.3428 - val_mae: 9184.0342\n",
      "Epoch 3841/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 64.6849 - mae: 65.3710 - val_loss: 9183.1680 - val_mae: 9183.8604\n",
      "Epoch 3842/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 66.6408 - mae: 67.3274 - val_loss: 9233.8535 - val_mae: 9234.5459\n",
      "Epoch 3843/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 63.8371 - mae: 64.5173 - val_loss: 9107.5537 - val_mae: 9108.2490\n",
      "Epoch 3844/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 72.2579 - mae: 72.9413 - val_loss: 9066.6445 - val_mae: 9067.3389\n",
      "Epoch 3845/5000\n",
      "46/46 [==============================] - 2s 41ms/step - loss: 68.1778 - mae: 68.8582 - val_loss: 9088.8252 - val_mae: 9089.5186\n",
      "Epoch 3846/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 68.9447 - mae: 69.6290 - val_loss: 9063.5039 - val_mae: 9064.1963\n",
      "Epoch 3847/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 70.0123 - mae: 70.6966 - val_loss: 9164.1611 - val_mae: 9164.8545\n",
      "Epoch 3848/5000\n",
      "46/46 [==============================] - 2s 41ms/step - loss: 64.0121 - mae: 64.6968 - val_loss: 9107.1611 - val_mae: 9107.8535\n",
      "Epoch 3849/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 67.3121 - mae: 67.9930 - val_loss: 9077.4434 - val_mae: 9078.1377\n",
      "Epoch 3850/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 67.4754 - mae: 68.1587 - val_loss: 9187.9160 - val_mae: 9188.6104\n",
      "Epoch 3851/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 70.7335 - mae: 71.4089 - val_loss: 9103.9678 - val_mae: 9104.6602\n",
      "Epoch 3852/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 68.2174 - mae: 68.9045 - val_loss: 9123.5645 - val_mae: 9124.2568\n",
      "Epoch 3853/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 83.2810 - mae: 83.9699 - val_loss: 9199.8730 - val_mae: 9200.5654\n",
      "Epoch 3854/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 70.0990 - mae: 70.7820 - val_loss: 9191.6572 - val_mae: 9192.3506\n",
      "Epoch 3855/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 66.9149 - mae: 67.5984 - val_loss: 9078.1094 - val_mae: 9078.8018\n",
      "Epoch 3856/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 63.5360 - mae: 64.2171 - val_loss: 9118.0723 - val_mae: 9118.7656\n",
      "Epoch 3857/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 62.8539 - mae: 63.5340 - val_loss: 9272.8867 - val_mae: 9273.5801\n",
      "Epoch 3858/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 69.1723 - mae: 69.8535 - val_loss: 9160.9756 - val_mae: 9161.6689\n",
      "Epoch 3859/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 70.7142 - mae: 71.3993 - val_loss: 9216.4092 - val_mae: 9217.1025\n",
      "Epoch 3860/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 66.6286 - mae: 67.3055 - val_loss: 9141.8252 - val_mae: 9142.5176\n",
      "Epoch 3861/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 71.9796 - mae: 72.6642 - val_loss: 9119.6426 - val_mae: 9120.3359\n",
      "Epoch 3862/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 67.0461 - mae: 67.7250 - val_loss: 9011.1318 - val_mae: 9011.8242\n",
      "Epoch 3863/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 85.8084 - mae: 86.4914 - val_loss: 9106.0596 - val_mae: 9106.7520\n",
      "Epoch 3864/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 71.7711 - mae: 72.4564 - val_loss: 9112.4727 - val_mae: 9113.1650\n",
      "Epoch 3865/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 62.0405 - mae: 62.7236 - val_loss: 9078.4678 - val_mae: 9079.1621\n",
      "Epoch 3866/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 75.3687 - mae: 76.0511 - val_loss: 9163.2119 - val_mae: 9163.9053\n",
      "Epoch 3867/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 63.6697 - mae: 64.3541 - val_loss: 9140.8223 - val_mae: 9141.5137\n",
      "Epoch 3868/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 63.1716 - mae: 63.8549 - val_loss: 9107.2627 - val_mae: 9107.9561\n",
      "Epoch 3869/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 60.0532 - mae: 60.7365 - val_loss: 9088.6221 - val_mae: 9089.3145\n",
      "Epoch 3870/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 60.9555 - mae: 61.6394 - val_loss: 9166.8379 - val_mae: 9167.5312\n",
      "Epoch 3871/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 75.4801 - mae: 76.1590 - val_loss: 9289.9072 - val_mae: 9290.5996\n",
      "Epoch 3872/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 67.7064 - mae: 68.3873 - val_loss: 9110.2451 - val_mae: 9110.9395\n",
      "Epoch 3873/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 66.7099 - mae: 67.3947 - val_loss: 8967.4951 - val_mae: 8968.1875\n",
      "Epoch 3874/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 77.5737 - mae: 78.2605 - val_loss: 9037.7471 - val_mae: 9038.4414\n",
      "Epoch 3875/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 73.9659 - mae: 74.6497 - val_loss: 9251.1260 - val_mae: 9251.8174\n",
      "Epoch 3876/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 63.7734 - mae: 64.4556 - val_loss: 9216.9248 - val_mae: 9217.6172\n",
      "Epoch 3877/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 72.6721 - mae: 73.3557 - val_loss: 9149.9365 - val_mae: 9150.6289\n",
      "Epoch 3878/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 65.5489 - mae: 66.2342 - val_loss: 8956.9443 - val_mae: 8957.6377\n",
      "Epoch 3879/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 80.8613 - mae: 81.5426 - val_loss: 8851.9355 - val_mae: 8852.6289\n",
      "Epoch 3880/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 87.1655 - mae: 87.8518 - val_loss: 9045.5771 - val_mae: 9046.2695\n",
      "Epoch 3881/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 62.4341 - mae: 63.1137 - val_loss: 9260.2119 - val_mae: 9260.9043\n",
      "Epoch 3882/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 60.1007 - mae: 60.7879 - val_loss: 9137.6680 - val_mae: 9138.3604\n",
      "Epoch 3883/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 65.4308 - mae: 66.1129 - val_loss: 9096.7969 - val_mae: 9097.4893\n",
      "Epoch 3884/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 62.6256 - mae: 63.3049 - val_loss: 9054.4717 - val_mae: 9055.1660\n",
      "Epoch 3885/5000\n",
      "46/46 [==============================] - 2s 52ms/step - loss: 62.5905 - mae: 63.2738 - val_loss: 9106.1904 - val_mae: 9106.8818\n",
      "Epoch 3886/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 68.1172 - mae: 68.7988 - val_loss: 9038.6953 - val_mae: 9039.3867\n",
      "Epoch 3887/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 75.6417 - mae: 76.3270 - val_loss: 9107.5840 - val_mae: 9108.2783\n",
      "Epoch 3888/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 70.0642 - mae: 70.7466 - val_loss: 9153.5186 - val_mae: 9154.2119\n",
      "Epoch 3889/5000\n",
      "46/46 [==============================] - 2s 40ms/step - loss: 66.6179 - mae: 67.3021 - val_loss: 9140.0215 - val_mae: 9140.7129\n",
      "Epoch 3890/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 60.4847 - mae: 61.1690 - val_loss: 9060.6318 - val_mae: 9061.3252\n",
      "Epoch 3891/5000\n",
      "46/46 [==============================] - 3s 55ms/step - loss: 62.9332 - mae: 63.6139 - val_loss: 9202.5586 - val_mae: 9203.2520\n",
      "Epoch 3892/5000\n",
      "46/46 [==============================] - 3s 59ms/step - loss: 70.9554 - mae: 71.6354 - val_loss: 9329.5420 - val_mae: 9330.2334\n",
      "Epoch 3893/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 80.7459 - mae: 81.4294 - val_loss: 9239.0293 - val_mae: 9239.7217\n",
      "Epoch 3894/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 72.8690 - mae: 73.5510 - val_loss: 9105.2666 - val_mae: 9105.9600\n",
      "Epoch 3895/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 64.5044 - mae: 65.1899 - val_loss: 9098.2500 - val_mae: 9098.9453\n",
      "Epoch 3896/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 62.8586 - mae: 63.5410 - val_loss: 9000.0488 - val_mae: 9000.7402\n",
      "Epoch 3897/5000\n",
      "46/46 [==============================] - 2s 42ms/step - loss: 72.8828 - mae: 73.5667 - val_loss: 9483.9854 - val_mae: 9484.6777\n",
      "Epoch 3898/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 92.1876 - mae: 92.8758 - val_loss: 9206.5830 - val_mae: 9207.2764\n",
      "Epoch 3899/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 73.2698 - mae: 73.9536 - val_loss: 9101.7041 - val_mae: 9102.3975\n",
      "Epoch 3900/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 69.2617 - mae: 69.9486 - val_loss: 9250.5518 - val_mae: 9251.2461\n",
      "Epoch 3901/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 68.0874 - mae: 68.7665 - val_loss: 9162.6729 - val_mae: 9163.3652\n",
      "Epoch 3902/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 59.5368 - mae: 60.2217 - val_loss: 9112.4268 - val_mae: 9113.1201\n",
      "Epoch 3903/5000\n",
      "46/46 [==============================] - 2s 45ms/step - loss: 62.7845 - mae: 63.4663 - val_loss: 9113.5566 - val_mae: 9114.2500\n",
      "Epoch 3904/5000\n",
      "46/46 [==============================] - 2s 41ms/step - loss: 61.1628 - mae: 61.8468 - val_loss: 9056.2139 - val_mae: 9056.9072\n",
      "Epoch 3905/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 62.4266 - mae: 63.1063 - val_loss: 9135.1670 - val_mae: 9135.8613\n",
      "Epoch 3906/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 65.0008 - mae: 65.6795 - val_loss: 9073.0205 - val_mae: 9073.7148\n",
      "Epoch 3907/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 65.0175 - mae: 65.6920 - val_loss: 9148.9502 - val_mae: 9149.6436\n",
      "Epoch 3908/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 77.8377 - mae: 78.5206 - val_loss: 9033.4922 - val_mae: 9034.1846\n",
      "Epoch 3909/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 69.2333 - mae: 69.9171 - val_loss: 9224.1963 - val_mae: 9224.8916\n",
      "Epoch 3910/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 76.0025 - mae: 76.6879 - val_loss: 9121.7598 - val_mae: 9122.4541\n",
      "Epoch 3911/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 73.8443 - mae: 74.5271 - val_loss: 9158.3691 - val_mae: 9159.0635\n",
      "Epoch 3912/5000\n",
      "46/46 [==============================] - 2s 41ms/step - loss: 63.0643 - mae: 63.7459 - val_loss: 9091.4961 - val_mae: 9092.1885\n",
      "Epoch 3913/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 66.9415 - mae: 67.6239 - val_loss: 9076.0312 - val_mae: 9076.7246\n",
      "Epoch 3914/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 63.2835 - mae: 63.9683 - val_loss: 9090.4092 - val_mae: 9091.1035\n",
      "Epoch 3915/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 65.3237 - mae: 66.0048 - val_loss: 9120.2109 - val_mae: 9120.9043\n",
      "Epoch 3916/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 75.0872 - mae: 75.7703 - val_loss: 8983.6357 - val_mae: 8984.3281\n",
      "Epoch 3917/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 68.3837 - mae: 69.0650 - val_loss: 9387.2607 - val_mae: 9387.9531\n",
      "Epoch 3918/5000\n",
      "46/46 [==============================] - 2s 32ms/step - loss: 76.9289 - mae: 77.6106 - val_loss: 9080.7520 - val_mae: 9081.4463\n",
      "Epoch 3919/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 63.8003 - mae: 64.4831 - val_loss: 9190.6172 - val_mae: 9191.3105\n",
      "Epoch 3920/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 64.7315 - mae: 65.4135 - val_loss: 9112.6768 - val_mae: 9113.3691\n",
      "Epoch 3921/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 69.3914 - mae: 70.0772 - val_loss: 9180.5273 - val_mae: 9181.2197\n",
      "Epoch 3922/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 70.5966 - mae: 71.2748 - val_loss: 9184.7549 - val_mae: 9185.4492\n",
      "Epoch 3923/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 67.8434 - mae: 68.5210 - val_loss: 9131.8105 - val_mae: 9132.5029\n",
      "Epoch 3924/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 72.6664 - mae: 73.3500 - val_loss: 9181.5020 - val_mae: 9182.1953\n",
      "Epoch 3925/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 89.2471 - mae: 89.9337 - val_loss: 9051.3027 - val_mae: 9051.9951\n",
      "Epoch 3926/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 72.4104 - mae: 73.0929 - val_loss: 9126.8662 - val_mae: 9127.5596\n",
      "Epoch 3927/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 67.1756 - mae: 67.8615 - val_loss: 9180.9941 - val_mae: 9181.6875\n",
      "Epoch 3928/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 72.5702 - mae: 73.2580 - val_loss: 9200.8662 - val_mae: 9201.5586\n",
      "Epoch 3929/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 62.2542 - mae: 62.9318 - val_loss: 9219.5596 - val_mae: 9220.2529\n",
      "Epoch 3930/5000\n",
      "46/46 [==============================] - 2s 41ms/step - loss: 67.2294 - mae: 67.9124 - val_loss: 9173.9912 - val_mae: 9174.6846\n",
      "Epoch 3931/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 69.6899 - mae: 70.3730 - val_loss: 8988.4053 - val_mae: 8989.0977\n",
      "Epoch 3932/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 74.9454 - mae: 75.6280 - val_loss: 9218.4756 - val_mae: 9219.1689\n",
      "Epoch 3933/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 65.9126 - mae: 66.5992 - val_loss: 9113.2256 - val_mae: 9113.9180\n",
      "Epoch 3934/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 54.5278 - mae: 55.2064 - val_loss: 9136.1416 - val_mae: 9136.8340\n",
      "Epoch 3935/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 61.8725 - mae: 62.5479 - val_loss: 9178.4062 - val_mae: 9179.0996\n",
      "Epoch 3936/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 68.6933 - mae: 69.3748 - val_loss: 9084.4805 - val_mae: 9085.1738\n",
      "Epoch 3937/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 65.2615 - mae: 65.9471 - val_loss: 9015.3604 - val_mae: 9016.0547\n",
      "Epoch 3938/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 78.7714 - mae: 79.4545 - val_loss: 9064.4883 - val_mae: 9065.1807\n",
      "Epoch 3939/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 70.3123 - mae: 70.9942 - val_loss: 9118.2373 - val_mae: 9118.9307\n",
      "Epoch 3940/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 58.1108 - mae: 58.7958 - val_loss: 8952.4482 - val_mae: 8953.1406\n",
      "Epoch 3941/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 65.4205 - mae: 66.1041 - val_loss: 9148.5205 - val_mae: 9149.2148\n",
      "Epoch 3942/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 65.7766 - mae: 66.4622 - val_loss: 9125.7451 - val_mae: 9126.4385\n",
      "Epoch 3943/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 66.4015 - mae: 67.0839 - val_loss: 9156.4639 - val_mae: 9157.1553\n",
      "Epoch 3944/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 75.8959 - mae: 76.5815 - val_loss: 9320.8535 - val_mae: 9321.5498\n",
      "Epoch 3945/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 68.4164 - mae: 69.0992 - val_loss: 9309.5137 - val_mae: 9310.2070\n",
      "Epoch 3946/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 67.9812 - mae: 68.6652 - val_loss: 9148.0684 - val_mae: 9148.7607\n",
      "Epoch 3947/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 66.3558 - mae: 67.0376 - val_loss: 9364.5439 - val_mae: 9365.2354\n",
      "Epoch 3948/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 76.0313 - mae: 76.7189 - val_loss: 9054.3047 - val_mae: 9054.9971\n",
      "Epoch 3949/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 76.5223 - mae: 77.2092 - val_loss: 9134.0410 - val_mae: 9134.7344\n",
      "Epoch 3950/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 59.4531 - mae: 60.1308 - val_loss: 9115.6309 - val_mae: 9116.3242\n",
      "Epoch 3951/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 68.9365 - mae: 69.6189 - val_loss: 9217.8838 - val_mae: 9218.5771\n",
      "Epoch 3952/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 62.9484 - mae: 63.6325 - val_loss: 9077.8223 - val_mae: 9078.5146\n",
      "Epoch 3953/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 77.2698 - mae: 77.9539 - val_loss: 9030.3340 - val_mae: 9031.0264\n",
      "Epoch 3954/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 63.8647 - mae: 64.5504 - val_loss: 9187.6855 - val_mae: 9188.3799\n",
      "Epoch 3955/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 71.9041 - mae: 72.5832 - val_loss: 9196.4854 - val_mae: 9197.1787\n",
      "Epoch 3956/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 59.5691 - mae: 60.2509 - val_loss: 9127.4111 - val_mae: 9128.1045\n",
      "Epoch 3957/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 67.7903 - mae: 68.4687 - val_loss: 9128.0674 - val_mae: 9128.7588\n",
      "Epoch 3958/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 63.3812 - mae: 64.0615 - val_loss: 9076.5469 - val_mae: 9077.2393\n",
      "Epoch 3959/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 67.2498 - mae: 67.9351 - val_loss: 9052.7930 - val_mae: 9053.4873\n",
      "Epoch 3960/5000\n",
      "46/46 [==============================] - 2s 48ms/step - loss: 63.8282 - mae: 64.5146 - val_loss: 9313.0762 - val_mae: 9313.7676\n",
      "Epoch 3961/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 90.7454 - mae: 91.4323 - val_loss: 9058.0645 - val_mae: 9058.7578\n",
      "Epoch 3962/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 59.6103 - mae: 60.2918 - val_loss: 9036.5059 - val_mae: 9037.1982\n",
      "Epoch 3963/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 75.8127 - mae: 76.4982 - val_loss: 9094.8350 - val_mae: 9095.5273\n",
      "Epoch 3964/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 63.0584 - mae: 63.7415 - val_loss: 9142.0566 - val_mae: 9142.7500\n",
      "Epoch 3965/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 70.9887 - mae: 71.6683 - val_loss: 9068.5811 - val_mae: 9069.2754\n",
      "Epoch 3966/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 55.0050 - mae: 55.6889 - val_loss: 9086.9609 - val_mae: 9087.6533\n",
      "Epoch 3967/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 64.5894 - mae: 65.2753 - val_loss: 9009.9854 - val_mae: 9010.6807\n",
      "Epoch 3968/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 61.5492 - mae: 62.2351 - val_loss: 9117.5596 - val_mae: 9118.2529\n",
      "Epoch 3969/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 68.1828 - mae: 68.8617 - val_loss: 9165.0498 - val_mae: 9165.7432\n",
      "Epoch 3970/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 70.1133 - mae: 70.7924 - val_loss: 9035.5029 - val_mae: 9036.1953\n",
      "Epoch 3971/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 71.5043 - mae: 72.1882 - val_loss: 9111.6035 - val_mae: 9112.2969\n",
      "Epoch 3972/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 74.9889 - mae: 75.6693 - val_loss: 9048.3789 - val_mae: 9049.0713\n",
      "Epoch 3973/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 64.6913 - mae: 65.3706 - val_loss: 9068.0410 - val_mae: 9068.7334\n",
      "Epoch 3974/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 69.9125 - mae: 70.5989 - val_loss: 9137.7764 - val_mae: 9138.4678\n",
      "Epoch 3975/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 60.7986 - mae: 61.4787 - val_loss: 9214.2041 - val_mae: 9214.8984\n",
      "Epoch 3976/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 76.5747 - mae: 77.2602 - val_loss: 9158.2783 - val_mae: 9158.9717\n",
      "Epoch 3977/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 70.4077 - mae: 71.0921 - val_loss: 9208.8037 - val_mae: 9209.4961\n",
      "Epoch 3978/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 62.5106 - mae: 63.1905 - val_loss: 9189.6406 - val_mae: 9190.3340\n",
      "Epoch 3979/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 70.2403 - mae: 70.9243 - val_loss: 9081.9639 - val_mae: 9082.6572\n",
      "Epoch 3980/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 69.6359 - mae: 70.3176 - val_loss: 9143.2559 - val_mae: 9143.9502\n",
      "Epoch 3981/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 64.9411 - mae: 65.6252 - val_loss: 9261.2988 - val_mae: 9261.9922\n",
      "Epoch 3982/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 67.6885 - mae: 68.3670 - val_loss: 9068.2842 - val_mae: 9068.9775\n",
      "Epoch 3983/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 65.2179 - mae: 65.9032 - val_loss: 9296.3906 - val_mae: 9297.0859\n",
      "Epoch 3984/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 75.6743 - mae: 76.3545 - val_loss: 9107.2100 - val_mae: 9107.9023\n",
      "Epoch 3985/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 78.3438 - mae: 79.0285 - val_loss: 9033.1523 - val_mae: 9033.8467\n",
      "Epoch 3986/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 76.7650 - mae: 77.4510 - val_loss: 9156.6475 - val_mae: 9157.3408\n",
      "Epoch 3987/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 72.5395 - mae: 73.2158 - val_loss: 9167.6689 - val_mae: 9168.3613\n",
      "Epoch 3988/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 67.4573 - mae: 68.1451 - val_loss: 9113.3291 - val_mae: 9114.0244\n",
      "Epoch 3989/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 62.8031 - mae: 63.4824 - val_loss: 9176.8457 - val_mae: 9177.5391\n",
      "Epoch 3990/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 60.2542 - mae: 60.9391 - val_loss: 9003.6904 - val_mae: 9004.3838\n",
      "Epoch 3991/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 66.0386 - mae: 66.7170 - val_loss: 9065.3838 - val_mae: 9066.0781\n",
      "Epoch 3992/5000\n",
      "46/46 [==============================] - 2s 45ms/step - loss: 62.6963 - mae: 63.3756 - val_loss: 9114.2041 - val_mae: 9114.8975\n",
      "Epoch 3993/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 61.8198 - mae: 62.5007 - val_loss: 9154.4482 - val_mae: 9155.1406\n",
      "Epoch 3994/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 53.3665 - mae: 54.0476 - val_loss: 9215.4678 - val_mae: 9216.1611\n",
      "Epoch 3995/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 67.4715 - mae: 68.1583 - val_loss: 9088.8027 - val_mae: 9089.4951\n",
      "Epoch 3996/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 68.2466 - mae: 68.9296 - val_loss: 8992.2070 - val_mae: 8992.9004\n",
      "Epoch 3997/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 65.7721 - mae: 66.4563 - val_loss: 9295.8682 - val_mae: 9296.5615\n",
      "Epoch 3998/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 81.3957 - mae: 82.0790 - val_loss: 9302.8145 - val_mae: 9303.5078\n",
      "Epoch 3999/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 76.7801 - mae: 77.4614 - val_loss: 9206.8379 - val_mae: 9207.5303\n",
      "Epoch 4000/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 59.8942 - mae: 60.5749 - val_loss: 9155.5557 - val_mae: 9156.2500\n",
      "Epoch 4001/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 72.5750 - mae: 73.2603 - val_loss: 9047.9834 - val_mae: 9048.6758\n",
      "Epoch 4002/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 78.2393 - mae: 78.9242 - val_loss: 9072.3613 - val_mae: 9073.0557\n",
      "Epoch 4003/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 68.0415 - mae: 68.7244 - val_loss: 9111.9346 - val_mae: 9112.6270\n",
      "Epoch 4004/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 58.5988 - mae: 59.2800 - val_loss: 9005.1016 - val_mae: 9005.7939\n",
      "Epoch 4005/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 66.0039 - mae: 66.6832 - val_loss: 9169.4668 - val_mae: 9170.1592\n",
      "Epoch 4006/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 61.8024 - mae: 62.4778 - val_loss: 9084.3252 - val_mae: 9085.0186\n",
      "Epoch 4007/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 74.2696 - mae: 74.9537 - val_loss: 9178.1436 - val_mae: 9178.8350\n",
      "Epoch 4008/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 63.2296 - mae: 63.9121 - val_loss: 8988.1650 - val_mae: 8988.8584\n",
      "Epoch 4009/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 70.5487 - mae: 71.2351 - val_loss: 8964.1357 - val_mae: 8964.8291\n",
      "Epoch 4010/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 69.9528 - mae: 70.6336 - val_loss: 9074.1035 - val_mae: 9074.7959\n",
      "Epoch 4011/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 71.2271 - mae: 71.9096 - val_loss: 9137.5947 - val_mae: 9138.2881\n",
      "Epoch 4012/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 66.7077 - mae: 67.3896 - val_loss: 9152.8535 - val_mae: 9153.5479\n",
      "Epoch 4013/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 70.7724 - mae: 71.4565 - val_loss: 8964.0430 - val_mae: 8964.7354\n",
      "Epoch 4014/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 86.6480 - mae: 87.3325 - val_loss: 9091.6094 - val_mae: 9092.3027\n",
      "Epoch 4015/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 70.8954 - mae: 71.5771 - val_loss: 9147.1797 - val_mae: 9147.8730\n",
      "Epoch 4016/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 65.4570 - mae: 66.1408 - val_loss: 9039.0234 - val_mae: 9039.7168\n",
      "Epoch 4017/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 65.3524 - mae: 66.0392 - val_loss: 9126.9463 - val_mae: 9127.6387\n",
      "Epoch 4018/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 60.4008 - mae: 61.0837 - val_loss: 9122.5000 - val_mae: 9123.1914\n",
      "Epoch 4019/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 64.1190 - mae: 64.8048 - val_loss: 9124.6787 - val_mae: 9125.3721\n",
      "Epoch 4020/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 65.8371 - mae: 66.5228 - val_loss: 9089.7412 - val_mae: 9090.4346\n",
      "Epoch 4021/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 68.3614 - mae: 69.0459 - val_loss: 9189.0430 - val_mae: 9189.7344\n",
      "Epoch 4022/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 85.2582 - mae: 85.9422 - val_loss: 9132.2207 - val_mae: 9132.9141\n",
      "Epoch 4023/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 55.1977 - mae: 55.8789 - val_loss: 9130.6260 - val_mae: 9131.3164\n",
      "Epoch 4024/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 73.5626 - mae: 74.2453 - val_loss: 8974.6963 - val_mae: 8975.3896\n",
      "Epoch 4025/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 65.6707 - mae: 66.3541 - val_loss: 9022.6172 - val_mae: 9023.3115\n",
      "Epoch 4026/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 59.6517 - mae: 60.3325 - val_loss: 9266.4883 - val_mae: 9267.1816\n",
      "Epoch 4027/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 78.6117 - mae: 79.2957 - val_loss: 9249.1133 - val_mae: 9249.8066\n",
      "Epoch 4028/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 63.5994 - mae: 64.2812 - val_loss: 9175.7549 - val_mae: 9176.4473\n",
      "Epoch 4029/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 64.4529 - mae: 65.1337 - val_loss: 9165.4736 - val_mae: 9166.1670\n",
      "Epoch 4030/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 78.6644 - mae: 79.3512 - val_loss: 9105.7607 - val_mae: 9106.4531\n",
      "Epoch 4031/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 65.6365 - mae: 66.3144 - val_loss: 9016.3770 - val_mae: 9017.0693\n",
      "Epoch 4032/5000\n",
      "46/46 [==============================] - 2s 41ms/step - loss: 80.3077 - mae: 80.9924 - val_loss: 9105.5615 - val_mae: 9106.2539\n",
      "Epoch 4033/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 64.0207 - mae: 64.7035 - val_loss: 9092.4658 - val_mae: 9093.1582\n",
      "Epoch 4034/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 60.7762 - mae: 61.4573 - val_loss: 9364.4912 - val_mae: 9365.1826\n",
      "Epoch 4035/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 91.0205 - mae: 91.7021 - val_loss: 9106.9766 - val_mae: 9107.6689\n",
      "Epoch 4036/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 62.8282 - mae: 63.5107 - val_loss: 9289.0654 - val_mae: 9289.7588\n",
      "Epoch 4037/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 64.9996 - mae: 65.6832 - val_loss: 9029.8145 - val_mae: 9030.5078\n",
      "Epoch 4038/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 75.0699 - mae: 75.7512 - val_loss: 9155.5762 - val_mae: 9156.2695\n",
      "Epoch 4039/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 51.9758 - mae: 52.6588 - val_loss: 8957.1807 - val_mae: 8957.8750\n",
      "Epoch 4040/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 81.0718 - mae: 81.7572 - val_loss: 9129.2930 - val_mae: 9129.9863\n",
      "Epoch 4041/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 64.1475 - mae: 64.8278 - val_loss: 9077.7842 - val_mae: 9078.4766\n",
      "Epoch 4042/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 68.4983 - mae: 69.1818 - val_loss: 9040.0508 - val_mae: 9040.7432\n",
      "Epoch 4043/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 65.4518 - mae: 66.1369 - val_loss: 9166.7900 - val_mae: 9167.4824\n",
      "Epoch 4044/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 58.7887 - mae: 59.4711 - val_loss: 9023.9375 - val_mae: 9024.6299\n",
      "Epoch 4045/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 62.1914 - mae: 62.8757 - val_loss: 9010.1357 - val_mae: 9010.8301\n",
      "Epoch 4046/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 65.9858 - mae: 66.6663 - val_loss: 9154.8672 - val_mae: 9155.5596\n",
      "Epoch 4047/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 60.5024 - mae: 61.1798 - val_loss: 9176.5947 - val_mae: 9177.2871\n",
      "Epoch 4048/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 52.7151 - mae: 53.3972 - val_loss: 9206.9834 - val_mae: 9207.6748\n",
      "Epoch 4049/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 63.9396 - mae: 64.6236 - val_loss: 9114.4014 - val_mae: 9115.0947\n",
      "Epoch 4050/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 67.8356 - mae: 68.5169 - val_loss: 9092.9971 - val_mae: 9093.6895\n",
      "Epoch 4051/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 74.3715 - mae: 75.0556 - val_loss: 9184.3330 - val_mae: 9185.0264\n",
      "Epoch 4052/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 78.2166 - mae: 78.9019 - val_loss: 9312.5322 - val_mae: 9313.2246\n",
      "Epoch 4053/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 67.6656 - mae: 68.3515 - val_loss: 9194.2002 - val_mae: 9194.8936\n",
      "Epoch 4054/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 60.5025 - mae: 61.1861 - val_loss: 9029.6895 - val_mae: 9030.3828\n",
      "Epoch 4055/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 62.7849 - mae: 63.4636 - val_loss: 9249.6416 - val_mae: 9250.3340\n",
      "Epoch 4056/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 64.4588 - mae: 65.1454 - val_loss: 9085.9307 - val_mae: 9086.6230\n",
      "Epoch 4057/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 59.0408 - mae: 59.7215 - val_loss: 9212.7061 - val_mae: 9213.3984\n",
      "Epoch 4058/5000\n",
      "46/46 [==============================] - 2s 40ms/step - loss: 67.5053 - mae: 68.1919 - val_loss: 9154.9219 - val_mae: 9155.6152\n",
      "Epoch 4059/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 65.9271 - mae: 66.6056 - val_loss: 9067.0742 - val_mae: 9067.7666\n",
      "Epoch 4060/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 64.5342 - mae: 65.2172 - val_loss: 9237.9941 - val_mae: 9238.6875\n",
      "Epoch 4061/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 61.0275 - mae: 61.7138 - val_loss: 9227.3467 - val_mae: 9228.0400\n",
      "Epoch 4062/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 60.8982 - mae: 61.5812 - val_loss: 9154.0928 - val_mae: 9154.7852\n",
      "Epoch 4063/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 81.4915 - mae: 82.1805 - val_loss: 8959.5010 - val_mae: 8960.1943\n",
      "Epoch 4064/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 59.1236 - mae: 59.8084 - val_loss: 8964.7354 - val_mae: 8965.4287\n",
      "Epoch 4065/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 86.5094 - mae: 87.1943 - val_loss: 9131.4316 - val_mae: 9132.1240\n",
      "Epoch 4066/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 67.3269 - mae: 68.0099 - val_loss: 9142.7812 - val_mae: 9143.4756\n",
      "Epoch 4067/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 67.0030 - mae: 67.6859 - val_loss: 9086.2783 - val_mae: 9086.9717\n",
      "Epoch 4068/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 68.9532 - mae: 69.6377 - val_loss: 9242.5439 - val_mae: 9243.2363\n",
      "Epoch 4069/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 72.1876 - mae: 72.8716 - val_loss: 9108.3721 - val_mae: 9109.0645\n",
      "Epoch 4070/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 57.2073 - mae: 57.8884 - val_loss: 9039.9902 - val_mae: 9040.6846\n",
      "Epoch 4071/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 69.9494 - mae: 70.6321 - val_loss: 9084.8486 - val_mae: 9085.5420\n",
      "Epoch 4072/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 61.3292 - mae: 62.0156 - val_loss: 9107.7100 - val_mae: 9108.4014\n",
      "Epoch 4073/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 59.7735 - mae: 60.4534 - val_loss: 9170.9014 - val_mae: 9171.5957\n",
      "Epoch 4074/5000\n",
      "46/46 [==============================] - 2s 48ms/step - loss: 59.9880 - mae: 60.6709 - val_loss: 9135.9893 - val_mae: 9136.6826\n",
      "Epoch 4075/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 59.5856 - mae: 60.2657 - val_loss: 9112.0615 - val_mae: 9112.7539\n",
      "Epoch 4076/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 66.0442 - mae: 66.7220 - val_loss: 9098.1719 - val_mae: 9098.8652\n",
      "Epoch 4077/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 66.7583 - mae: 67.4404 - val_loss: 9124.5303 - val_mae: 9125.2227\n",
      "Epoch 4078/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 68.0709 - mae: 68.7513 - val_loss: 9194.1465 - val_mae: 9194.8398\n",
      "Epoch 4079/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 59.6222 - mae: 60.3061 - val_loss: 9196.0957 - val_mae: 9196.7891\n",
      "Epoch 4080/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 68.0382 - mae: 68.7207 - val_loss: 9252.8311 - val_mae: 9253.5264\n",
      "Epoch 4081/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 73.6369 - mae: 74.3229 - val_loss: 9164.5498 - val_mae: 9165.2432\n",
      "Epoch 4082/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 68.0048 - mae: 68.6907 - val_loss: 9073.1289 - val_mae: 9073.8213\n",
      "Epoch 4083/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 65.2745 - mae: 65.9559 - val_loss: 9154.7793 - val_mae: 9155.4727\n",
      "Epoch 4084/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 58.2621 - mae: 58.9395 - val_loss: 9140.5986 - val_mae: 9141.2900\n",
      "Epoch 4085/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 57.6236 - mae: 58.3074 - val_loss: 9223.5527 - val_mae: 9224.2471\n",
      "Epoch 4086/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 57.5655 - mae: 58.2451 - val_loss: 9158.9512 - val_mae: 9159.6455\n",
      "Epoch 4087/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 55.1500 - mae: 55.8272 - val_loss: 8970.0879 - val_mae: 8970.7822\n",
      "Epoch 4088/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 64.0155 - mae: 64.7003 - val_loss: 9151.6436 - val_mae: 9152.3379\n",
      "Epoch 4089/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 54.7261 - mae: 55.4056 - val_loss: 9155.1855 - val_mae: 9155.8799\n",
      "Epoch 4090/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 54.9961 - mae: 55.6645 - val_loss: 9036.1562 - val_mae: 9036.8496\n",
      "Epoch 4091/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 56.6282 - mae: 57.3122 - val_loss: 9093.5928 - val_mae: 9094.2861\n",
      "Epoch 4092/5000\n",
      "46/46 [==============================] - 2s 43ms/step - loss: 66.3307 - mae: 67.0125 - val_loss: 9084.7881 - val_mae: 9085.4824\n",
      "Epoch 4093/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 59.9464 - mae: 60.6322 - val_loss: 9037.1445 - val_mae: 9037.8389\n",
      "Epoch 4094/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 72.3331 - mae: 73.0153 - val_loss: 9240.8066 - val_mae: 9241.4990\n",
      "Epoch 4095/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 75.9835 - mae: 76.6695 - val_loss: 9188.2021 - val_mae: 9188.8955\n",
      "Epoch 4096/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 67.0079 - mae: 67.6906 - val_loss: 9036.3223 - val_mae: 9037.0146\n",
      "Epoch 4097/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 65.0386 - mae: 65.7181 - val_loss: 9149.7500 - val_mae: 9150.4424\n",
      "Epoch 4098/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 57.1079 - mae: 57.7881 - val_loss: 9057.7402 - val_mae: 9058.4326\n",
      "Epoch 4099/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 86.8737 - mae: 87.5577 - val_loss: 9121.6504 - val_mae: 9122.3428\n",
      "Epoch 4100/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 69.3000 - mae: 69.9838 - val_loss: 8986.4258 - val_mae: 8987.1191\n",
      "Epoch 4101/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 72.7765 - mae: 73.4617 - val_loss: 9105.3428 - val_mae: 9106.0361\n",
      "Epoch 4102/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 60.4773 - mae: 61.1610 - val_loss: 9248.3916 - val_mae: 9249.0840\n",
      "Epoch 4103/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 65.9410 - mae: 66.6230 - val_loss: 9153.7139 - val_mae: 9154.4092\n",
      "Epoch 4104/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 80.4531 - mae: 81.1367 - val_loss: 9064.1367 - val_mae: 9064.8301\n",
      "Epoch 4105/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 63.5779 - mae: 64.2613 - val_loss: 9107.6162 - val_mae: 9108.3096\n",
      "Epoch 4106/5000\n",
      "46/46 [==============================] - 2s 32ms/step - loss: 67.1934 - mae: 67.8762 - val_loss: 8975.4570 - val_mae: 8976.1504\n",
      "Epoch 4107/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 66.1749 - mae: 66.8571 - val_loss: 9026.4180 - val_mae: 9027.1113\n",
      "Epoch 4108/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 61.7092 - mae: 62.3914 - val_loss: 9061.9336 - val_mae: 9062.6279\n",
      "Epoch 4109/5000\n",
      "46/46 [==============================] - 2s 40ms/step - loss: 69.8655 - mae: 70.5465 - val_loss: 9004.1670 - val_mae: 9004.8594\n",
      "Epoch 4110/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 68.5819 - mae: 69.2651 - val_loss: 9130.2393 - val_mae: 9130.9326\n",
      "Epoch 4111/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 60.7669 - mae: 61.4489 - val_loss: 8967.9951 - val_mae: 8968.6865\n",
      "Epoch 4112/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 66.2008 - mae: 66.8823 - val_loss: 8886.5889 - val_mae: 8887.2832\n",
      "Epoch 4113/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 86.5882 - mae: 87.2738 - val_loss: 9151.7861 - val_mae: 9152.4795\n",
      "Epoch 4114/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 68.7367 - mae: 69.4194 - val_loss: 9096.6191 - val_mae: 9097.3125\n",
      "Epoch 4115/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 58.7381 - mae: 59.4245 - val_loss: 9095.1416 - val_mae: 9095.8350\n",
      "Epoch 4116/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 63.1355 - mae: 63.8213 - val_loss: 9121.9502 - val_mae: 9122.6426\n",
      "Epoch 4117/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 56.7195 - mae: 57.4027 - val_loss: 9199.9521 - val_mae: 9200.6475\n",
      "Epoch 4118/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 71.0083 - mae: 71.6894 - val_loss: 9178.3252 - val_mae: 9179.0176\n",
      "Epoch 4119/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 75.6566 - mae: 76.3417 - val_loss: 9192.1494 - val_mae: 9192.8438\n",
      "Epoch 4120/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 76.6931 - mae: 77.3725 - val_loss: 9114.2178 - val_mae: 9114.9092\n",
      "Epoch 4121/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 71.7288 - mae: 72.4094 - val_loss: 9096.8857 - val_mae: 9097.5791\n",
      "Epoch 4122/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 81.1743 - mae: 81.8584 - val_loss: 9213.0000 - val_mae: 9213.6934\n",
      "Epoch 4123/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 63.2517 - mae: 63.9385 - val_loss: 9204.3486 - val_mae: 9205.0410\n",
      "Epoch 4124/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 64.7653 - mae: 65.4467 - val_loss: 9003.4941 - val_mae: 9004.1875\n",
      "Epoch 4125/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 70.6916 - mae: 71.3742 - val_loss: 9056.7900 - val_mae: 9057.4834\n",
      "Epoch 4126/5000\n",
      "46/46 [==============================] - 2s 40ms/step - loss: 66.3871 - mae: 67.0697 - val_loss: 9159.0859 - val_mae: 9159.7793\n",
      "Epoch 4127/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 68.5207 - mae: 69.2042 - val_loss: 9236.7988 - val_mae: 9237.4932\n",
      "Epoch 4128/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 69.8884 - mae: 70.5712 - val_loss: 9026.5000 - val_mae: 9027.1934\n",
      "Epoch 4129/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 59.5541 - mae: 60.2394 - val_loss: 9033.2158 - val_mae: 9033.9102\n",
      "Epoch 4130/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 67.2548 - mae: 67.9349 - val_loss: 9014.8018 - val_mae: 9015.4951\n",
      "Epoch 4131/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 64.4230 - mae: 65.1039 - val_loss: 9012.0508 - val_mae: 9012.7432\n",
      "Epoch 4132/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 60.3474 - mae: 61.0261 - val_loss: 9053.9336 - val_mae: 9054.6270\n",
      "Epoch 4133/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 63.5209 - mae: 64.2012 - val_loss: 9140.1514 - val_mae: 9140.8457\n",
      "Epoch 4134/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 77.0440 - mae: 77.7255 - val_loss: 9193.8496 - val_mae: 9194.5430\n",
      "Epoch 4135/5000\n",
      "46/46 [==============================] - 2s 49ms/step - loss: 82.2370 - mae: 82.9197 - val_loss: 9124.1182 - val_mae: 9124.8115\n",
      "Epoch 4136/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 63.3500 - mae: 64.0330 - val_loss: 9276.0693 - val_mae: 9276.7617\n",
      "Epoch 4137/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 79.7714 - mae: 80.4505 - val_loss: 9216.9258 - val_mae: 9217.6182\n",
      "Epoch 4138/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 75.4234 - mae: 76.1090 - val_loss: 9123.7520 - val_mae: 9124.4453\n",
      "Epoch 4139/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 65.8291 - mae: 66.5129 - val_loss: 9203.9238 - val_mae: 9204.6162\n",
      "Epoch 4140/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 69.2278 - mae: 69.9099 - val_loss: 9122.1797 - val_mae: 9122.8721\n",
      "Epoch 4141/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 64.0454 - mae: 64.7262 - val_loss: 9060.0029 - val_mae: 9060.6973\n",
      "Epoch 4142/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 72.5673 - mae: 73.2548 - val_loss: 9289.6396 - val_mae: 9290.3330\n",
      "Epoch 4143/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 73.3557 - mae: 74.0382 - val_loss: 9187.4229 - val_mae: 9188.1152\n",
      "Epoch 4144/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 65.9781 - mae: 66.6656 - val_loss: 9140.7295 - val_mae: 9141.4229\n",
      "Epoch 4145/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 60.9409 - mae: 61.6220 - val_loss: 9326.5635 - val_mae: 9327.2578\n",
      "Epoch 4146/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 69.8014 - mae: 70.4882 - val_loss: 9214.4912 - val_mae: 9215.1846\n",
      "Epoch 4147/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 72.5535 - mae: 73.2394 - val_loss: 9115.6709 - val_mae: 9116.3643\n",
      "Epoch 4148/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 71.5688 - mae: 72.2531 - val_loss: 9113.3877 - val_mae: 9114.0811\n",
      "Epoch 4149/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 74.6402 - mae: 75.3195 - val_loss: 8960.7129 - val_mae: 8961.4053\n",
      "Epoch 4150/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 76.6515 - mae: 77.3333 - val_loss: 9082.5342 - val_mae: 9083.2275\n",
      "Epoch 4151/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 82.9974 - mae: 83.6793 - val_loss: 9205.7451 - val_mae: 9206.4395\n",
      "Epoch 4152/5000\n",
      "46/46 [==============================] - 2s 41ms/step - loss: 69.7530 - mae: 70.4377 - val_loss: 9043.7109 - val_mae: 9044.4033\n",
      "Epoch 4153/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 61.5485 - mae: 62.2339 - val_loss: 9121.2070 - val_mae: 9121.9014\n",
      "Epoch 4154/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 65.3301 - mae: 66.0138 - val_loss: 9162.8691 - val_mae: 9163.5615\n",
      "Epoch 4155/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 59.7737 - mae: 60.4578 - val_loss: 9123.9951 - val_mae: 9124.6875\n",
      "Epoch 4156/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 66.2655 - mae: 66.9458 - val_loss: 9176.6377 - val_mae: 9177.3320\n",
      "Epoch 4157/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 63.7869 - mae: 64.4682 - val_loss: 9196.3359 - val_mae: 9197.0283\n",
      "Epoch 4158/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 63.5351 - mae: 64.2161 - val_loss: 9174.0664 - val_mae: 9174.7598\n",
      "Epoch 4159/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 68.1701 - mae: 68.8557 - val_loss: 9188.0713 - val_mae: 9188.7637\n",
      "Epoch 4160/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 71.0004 - mae: 71.6815 - val_loss: 9026.8545 - val_mae: 9027.5498\n",
      "Epoch 4161/5000\n",
      "46/46 [==============================] - 2s 40ms/step - loss: 69.3092 - mae: 69.9906 - val_loss: 9071.4932 - val_mae: 9072.1846\n",
      "Epoch 4162/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 67.9746 - mae: 68.6588 - val_loss: 9119.4414 - val_mae: 9120.1348\n",
      "Epoch 4163/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 63.4517 - mae: 64.1369 - val_loss: 9088.6006 - val_mae: 9089.2939\n",
      "Epoch 4164/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 76.4441 - mae: 77.1292 - val_loss: 9035.8252 - val_mae: 9036.5176\n",
      "Epoch 4165/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 70.1980 - mae: 70.8785 - val_loss: 9113.5732 - val_mae: 9114.2666\n",
      "Epoch 4166/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 70.3447 - mae: 71.0313 - val_loss: 9071.1982 - val_mae: 9071.8906\n",
      "Epoch 4167/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 60.7932 - mae: 61.4744 - val_loss: 8908.7881 - val_mae: 8909.4795\n",
      "Epoch 4168/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 75.9620 - mae: 76.6468 - val_loss: 8990.1445 - val_mae: 8990.8379\n",
      "Epoch 4169/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 66.7302 - mae: 67.4131 - val_loss: 9167.6396 - val_mae: 9168.3311\n",
      "Epoch 4170/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 58.1342 - mae: 58.8185 - val_loss: 9008.6270 - val_mae: 9009.3213\n",
      "Epoch 4171/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 67.5915 - mae: 68.2728 - val_loss: 8942.0674 - val_mae: 8942.7607\n",
      "Epoch 4172/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 71.5220 - mae: 72.2061 - val_loss: 9134.4492 - val_mae: 9135.1416\n",
      "Epoch 4173/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 56.1980 - mae: 56.8790 - val_loss: 9092.2256 - val_mae: 9092.9199\n",
      "Epoch 4174/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 58.5818 - mae: 59.2635 - val_loss: 9175.1426 - val_mae: 9175.8350\n",
      "Epoch 4175/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 65.9221 - mae: 66.6049 - val_loss: 9039.9707 - val_mae: 9040.6641\n",
      "Epoch 4176/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 61.3971 - mae: 62.0789 - val_loss: 9027.0430 - val_mae: 9027.7363\n",
      "Epoch 4177/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 69.2121 - mae: 69.8939 - val_loss: 9216.3428 - val_mae: 9217.0352\n",
      "Epoch 4178/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 64.5994 - mae: 65.2825 - val_loss: 9164.1621 - val_mae: 9164.8555\n",
      "Epoch 4179/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 65.8607 - mae: 66.5411 - val_loss: 9193.7354 - val_mae: 9194.4287\n",
      "Epoch 4180/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 70.6484 - mae: 71.3342 - val_loss: 9146.2451 - val_mae: 9146.9385\n",
      "Epoch 4181/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 62.1463 - mae: 62.8293 - val_loss: 9051.3711 - val_mae: 9052.0635\n",
      "Epoch 4182/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 64.1135 - mae: 64.7963 - val_loss: 9017.5371 - val_mae: 9018.2305\n",
      "Epoch 4183/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 68.4766 - mae: 69.1568 - val_loss: 9221.4033 - val_mae: 9222.0947\n",
      "Epoch 4184/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 58.4472 - mae: 59.1320 - val_loss: 9310.4502 - val_mae: 9311.1436\n",
      "Epoch 4185/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 71.7779 - mae: 72.4629 - val_loss: 9124.8740 - val_mae: 9125.5674\n",
      "Epoch 4186/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 65.2471 - mae: 65.9271 - val_loss: 9022.6914 - val_mae: 9023.3848\n",
      "Epoch 4187/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 63.0129 - mae: 63.6917 - val_loss: 9244.0615 - val_mae: 9244.7549\n",
      "Epoch 4188/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 69.9847 - mae: 70.6675 - val_loss: 9076.2930 - val_mae: 9076.9854\n",
      "Epoch 4189/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 62.6520 - mae: 63.3342 - val_loss: 9121.5312 - val_mae: 9122.2236\n",
      "Epoch 4190/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 61.3914 - mae: 62.0696 - val_loss: 9164.9307 - val_mae: 9165.6240\n",
      "Epoch 4191/5000\n",
      "46/46 [==============================] - 2s 48ms/step - loss: 69.3012 - mae: 69.9831 - val_loss: 9138.0918 - val_mae: 9138.7861\n",
      "Epoch 4192/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 58.6259 - mae: 59.3091 - val_loss: 9087.1709 - val_mae: 9087.8643\n",
      "Epoch 4193/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 64.5036 - mae: 65.1866 - val_loss: 9132.7998 - val_mae: 9133.4932\n",
      "Epoch 4194/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 60.1770 - mae: 60.8605 - val_loss: 9165.4512 - val_mae: 9166.1465\n",
      "Epoch 4195/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 61.3332 - mae: 62.0152 - val_loss: 9239.3975 - val_mae: 9240.0908\n",
      "Epoch 4196/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 62.3980 - mae: 63.0790 - val_loss: 9095.2041 - val_mae: 9095.8984\n",
      "Epoch 4197/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 73.1264 - mae: 73.8093 - val_loss: 9024.8584 - val_mae: 9025.5518\n",
      "Epoch 4198/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 61.8338 - mae: 62.5139 - val_loss: 9098.0898 - val_mae: 9098.7822\n",
      "Epoch 4199/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 61.8191 - mae: 62.5000 - val_loss: 9093.4473 - val_mae: 9094.1416\n",
      "Epoch 4200/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 59.6786 - mae: 60.3616 - val_loss: 9135.9365 - val_mae: 9136.6309\n",
      "Epoch 4201/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 62.5664 - mae: 63.2492 - val_loss: 9226.2607 - val_mae: 9226.9541\n",
      "Epoch 4202/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 61.4265 - mae: 62.1106 - val_loss: 9146.4199 - val_mae: 9147.1133\n",
      "Epoch 4203/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 64.1465 - mae: 64.8366 - val_loss: 8977.6309 - val_mae: 8978.3262\n",
      "Epoch 4204/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 69.0817 - mae: 69.7636 - val_loss: 9213.4492 - val_mae: 9214.1436\n",
      "Epoch 4205/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 60.0962 - mae: 60.7863 - val_loss: 9214.5098 - val_mae: 9215.2021\n",
      "Epoch 4206/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 67.0702 - mae: 67.7537 - val_loss: 9216.1553 - val_mae: 9216.8486\n",
      "Epoch 4207/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 68.3958 - mae: 69.0773 - val_loss: 9099.0049 - val_mae: 9099.6973\n",
      "Epoch 4208/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 64.3302 - mae: 65.0134 - val_loss: 9148.2207 - val_mae: 9148.9141\n",
      "Epoch 4209/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 66.5801 - mae: 67.2628 - val_loss: 9168.4932 - val_mae: 9169.1855\n",
      "Epoch 4210/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 70.1471 - mae: 70.8353 - val_loss: 9075.4150 - val_mae: 9076.1094\n",
      "Epoch 4211/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 53.3606 - mae: 54.0450 - val_loss: 9133.1396 - val_mae: 9133.8320\n",
      "Epoch 4212/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 54.7324 - mae: 55.4135 - val_loss: 9096.0615 - val_mae: 9096.7549\n",
      "Epoch 4213/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 68.6497 - mae: 69.3377 - val_loss: 9233.9180 - val_mae: 9234.6123\n",
      "Epoch 4214/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 64.2719 - mae: 64.9562 - val_loss: 9034.7295 - val_mae: 9035.4219\n",
      "Epoch 4215/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 66.0582 - mae: 66.7381 - val_loss: 9184.5068 - val_mae: 9185.1992\n",
      "Epoch 4216/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 61.6984 - mae: 62.3820 - val_loss: 9217.5771 - val_mae: 9218.2695\n",
      "Epoch 4217/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 64.3153 - mae: 64.9955 - val_loss: 9216.6729 - val_mae: 9217.3643\n",
      "Epoch 4218/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 65.8690 - mae: 66.5542 - val_loss: 9011.1797 - val_mae: 9011.8730\n",
      "Epoch 4219/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 64.8769 - mae: 65.5595 - val_loss: 9270.5811 - val_mae: 9271.2754\n",
      "Epoch 4220/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 66.3977 - mae: 67.0801 - val_loss: 9227.1738 - val_mae: 9227.8652\n",
      "Epoch 4221/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 71.5851 - mae: 72.2713 - val_loss: 9242.6084 - val_mae: 9243.3008\n",
      "Epoch 4222/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 64.4886 - mae: 65.1722 - val_loss: 9117.1621 - val_mae: 9117.8555\n",
      "Epoch 4223/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 60.6967 - mae: 61.3805 - val_loss: 9129.4795 - val_mae: 9130.1738\n",
      "Epoch 4224/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 67.2249 - mae: 67.9110 - val_loss: 9122.5889 - val_mae: 9123.2832\n",
      "Epoch 4225/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 59.4099 - mae: 60.0904 - val_loss: 9139.5400 - val_mae: 9140.2344\n",
      "Epoch 4226/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 71.4278 - mae: 72.1100 - val_loss: 9066.9365 - val_mae: 9067.6289\n",
      "Epoch 4227/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 64.5382 - mae: 65.2187 - val_loss: 9032.5312 - val_mae: 9033.2236\n",
      "Epoch 4228/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 79.8311 - mae: 80.5153 - val_loss: 9138.3311 - val_mae: 9139.0244\n",
      "Epoch 4229/5000\n",
      "46/46 [==============================] - 2s 44ms/step - loss: 63.0570 - mae: 63.7389 - val_loss: 9134.6777 - val_mae: 9135.3711\n",
      "Epoch 4230/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 59.6751 - mae: 60.3582 - val_loss: 9108.4688 - val_mae: 9109.1611\n",
      "Epoch 4231/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 64.6950 - mae: 65.3814 - val_loss: 9185.6523 - val_mae: 9186.3467\n",
      "Epoch 4232/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 62.1403 - mae: 62.8245 - val_loss: 8999.4951 - val_mae: 9000.1885\n",
      "Epoch 4233/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 59.9383 - mae: 60.6146 - val_loss: 9095.7998 - val_mae: 9096.4922\n",
      "Epoch 4234/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 66.3902 - mae: 67.0750 - val_loss: 9052.2178 - val_mae: 9052.9111\n",
      "Epoch 4235/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 62.7299 - mae: 63.4116 - val_loss: 9073.4287 - val_mae: 9074.1211\n",
      "Epoch 4236/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 62.1857 - mae: 62.8680 - val_loss: 9104.4033 - val_mae: 9105.0957\n",
      "Epoch 4237/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 69.0003 - mae: 69.6772 - val_loss: 9195.9629 - val_mae: 9196.6553\n",
      "Epoch 4238/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 66.2649 - mae: 66.9461 - val_loss: 9215.7666 - val_mae: 9216.4580\n",
      "Epoch 4239/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 73.2615 - mae: 73.9444 - val_loss: 9109.1504 - val_mae: 9109.8438\n",
      "Epoch 4240/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 73.4308 - mae: 74.1109 - val_loss: 9170.0430 - val_mae: 9170.7363\n",
      "Epoch 4241/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 75.1566 - mae: 75.8453 - val_loss: 9099.1875 - val_mae: 9099.8809\n",
      "Epoch 4242/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 67.6151 - mae: 68.2950 - val_loss: 9120.8760 - val_mae: 9121.5713\n",
      "Epoch 4243/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 66.9398 - mae: 67.6230 - val_loss: 9057.4395 - val_mae: 9058.1318\n",
      "Epoch 4244/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 69.7433 - mae: 70.4278 - val_loss: 9170.7529 - val_mae: 9171.4473\n",
      "Epoch 4245/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 62.6920 - mae: 63.3772 - val_loss: 9065.9229 - val_mae: 9066.6162\n",
      "Epoch 4246/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 65.1885 - mae: 65.8715 - val_loss: 9217.7168 - val_mae: 9218.4102\n",
      "Epoch 4247/5000\n",
      "46/46 [==============================] - 2s 43ms/step - loss: 65.8403 - mae: 66.5238 - val_loss: 9234.8809 - val_mae: 9235.5742\n",
      "Epoch 4248/5000\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 76.2569 - mae: 76.9404 - val_loss: 8942.1426 - val_mae: 8942.8369\n",
      "Epoch 4249/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 74.5534 - mae: 75.2364 - val_loss: 9188.7988 - val_mae: 9189.4912\n",
      "Epoch 4250/5000\n",
      "46/46 [==============================] - 2s 41ms/step - loss: 73.8562 - mae: 74.5432 - val_loss: 9199.9307 - val_mae: 9200.6240\n",
      "Epoch 4251/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 67.3880 - mae: 68.0709 - val_loss: 9038.6250 - val_mae: 9039.3193\n",
      "Epoch 4252/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 74.3271 - mae: 75.0116 - val_loss: 9174.6787 - val_mae: 9175.3711\n",
      "Epoch 4253/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 63.0652 - mae: 63.7528 - val_loss: 9126.7305 - val_mae: 9127.4238\n",
      "Epoch 4254/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 65.6355 - mae: 66.3233 - val_loss: 9257.8047 - val_mae: 9258.4961\n",
      "Epoch 4255/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 63.4289 - mae: 64.1099 - val_loss: 9244.0312 - val_mae: 9244.7256\n",
      "Epoch 4256/5000\n",
      "46/46 [==============================] - 2s 48ms/step - loss: 77.6254 - mae: 78.3132 - val_loss: 9128.0732 - val_mae: 9128.7656\n",
      "Epoch 4257/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 57.3937 - mae: 58.0767 - val_loss: 9239.6699 - val_mae: 9240.3643\n",
      "Epoch 4258/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 60.8889 - mae: 61.5671 - val_loss: 9157.3145 - val_mae: 9158.0068\n",
      "Epoch 4259/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 80.5656 - mae: 81.2492 - val_loss: 9116.9873 - val_mae: 9117.6807\n",
      "Epoch 4260/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 69.2673 - mae: 69.9494 - val_loss: 9329.6250 - val_mae: 9330.3174\n",
      "Epoch 4261/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 71.3756 - mae: 72.0570 - val_loss: 9070.3037 - val_mae: 9070.9961\n",
      "Epoch 4262/5000\n",
      "46/46 [==============================] - 2s 42ms/step - loss: 61.2185 - mae: 61.8978 - val_loss: 8969.7021 - val_mae: 8970.3955\n",
      "Epoch 4263/5000\n",
      "46/46 [==============================] - 2s 32ms/step - loss: 67.7379 - mae: 68.4207 - val_loss: 9119.7285 - val_mae: 9120.4219\n",
      "Epoch 4264/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 71.7809 - mae: 72.4663 - val_loss: 9130.3164 - val_mae: 9131.0098\n",
      "Epoch 4265/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 56.4392 - mae: 57.1178 - val_loss: 9188.1953 - val_mae: 9188.8887\n",
      "Epoch 4266/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 77.7764 - mae: 78.4604 - val_loss: 9222.3633 - val_mae: 9223.0566\n",
      "Epoch 4267/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 81.1699 - mae: 81.8512 - val_loss: 9210.2715 - val_mae: 9210.9619\n",
      "Epoch 4268/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 63.3215 - mae: 64.0035 - val_loss: 9140.3096 - val_mae: 9141.0029\n",
      "Epoch 4269/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 65.2668 - mae: 65.9508 - val_loss: 9070.6758 - val_mae: 9071.3682\n",
      "Epoch 4270/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 64.3173 - mae: 64.9999 - val_loss: 9056.5322 - val_mae: 9057.2256\n",
      "Epoch 4271/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 63.5184 - mae: 64.2003 - val_loss: 9130.5303 - val_mae: 9131.2236\n",
      "Epoch 4272/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 68.8338 - mae: 69.5143 - val_loss: 9207.4922 - val_mae: 9208.1855\n",
      "Epoch 4273/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 53.5703 - mae: 54.2505 - val_loss: 9060.8320 - val_mae: 9061.5254\n",
      "Epoch 4274/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 55.0910 - mae: 55.7748 - val_loss: 9246.0488 - val_mae: 9246.7422\n",
      "Epoch 4275/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 63.6109 - mae: 64.2905 - val_loss: 9192.1045 - val_mae: 9192.7988\n",
      "Epoch 4276/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 62.0496 - mae: 62.7309 - val_loss: 9161.2197 - val_mae: 9161.9111\n",
      "Epoch 4277/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 64.3055 - mae: 64.9856 - val_loss: 9174.4990 - val_mae: 9175.1914\n",
      "Epoch 4278/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 62.7719 - mae: 63.4565 - val_loss: 9259.9414 - val_mae: 9260.6338\n",
      "Epoch 4279/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 51.3408 - mae: 52.0227 - val_loss: 9183.6201 - val_mae: 9184.3125\n",
      "Epoch 4280/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 57.5124 - mae: 58.1965 - val_loss: 9281.7852 - val_mae: 9282.4795\n",
      "Epoch 4281/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 75.6943 - mae: 76.3802 - val_loss: 9154.4746 - val_mae: 9155.1689\n",
      "Epoch 4282/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 60.2319 - mae: 60.9138 - val_loss: 9196.1299 - val_mae: 9196.8232\n",
      "Epoch 4283/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 63.2223 - mae: 63.9018 - val_loss: 9183.9346 - val_mae: 9184.6270\n",
      "Epoch 4284/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 64.7763 - mae: 65.4613 - val_loss: 9151.2783 - val_mae: 9151.9727\n",
      "Epoch 4285/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 66.5112 - mae: 67.1932 - val_loss: 9018.6699 - val_mae: 9019.3643\n",
      "Epoch 4286/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 73.2181 - mae: 73.9010 - val_loss: 9161.6797 - val_mae: 9162.3730\n",
      "Epoch 4287/5000\n",
      "46/46 [==============================] - 2s 42ms/step - loss: 63.5928 - mae: 64.2776 - val_loss: 9167.3008 - val_mae: 9167.9932\n",
      "Epoch 4288/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 65.3657 - mae: 66.0469 - val_loss: 9027.8018 - val_mae: 9028.4941\n",
      "Epoch 4289/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 61.3635 - mae: 62.0431 - val_loss: 9050.5293 - val_mae: 9051.2236\n",
      "Epoch 4290/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 64.7085 - mae: 65.3922 - val_loss: 8932.4072 - val_mae: 8933.1016\n",
      "Epoch 4291/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 63.4293 - mae: 64.1098 - val_loss: 9217.8584 - val_mae: 9218.5508\n",
      "Epoch 4292/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 60.9236 - mae: 61.6024 - val_loss: 9102.4482 - val_mae: 9103.1426\n",
      "Epoch 4293/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 60.1465 - mae: 60.8280 - val_loss: 9191.5312 - val_mae: 9192.2246\n",
      "Epoch 4294/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 67.3076 - mae: 67.9894 - val_loss: 9144.0449 - val_mae: 9144.7373\n",
      "Epoch 4295/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 73.1070 - mae: 73.7906 - val_loss: 9117.0020 - val_mae: 9117.6943\n",
      "Epoch 4296/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 79.6536 - mae: 80.3415 - val_loss: 9079.0059 - val_mae: 9079.6973\n",
      "Epoch 4297/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 73.7725 - mae: 74.4576 - val_loss: 9158.5859 - val_mae: 9159.2783\n",
      "Epoch 4298/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 66.1894 - mae: 66.8706 - val_loss: 9086.4150 - val_mae: 9087.1084\n",
      "Epoch 4299/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 67.7826 - mae: 68.4703 - val_loss: 9100.1172 - val_mae: 9100.8096\n",
      "Epoch 4300/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 66.2192 - mae: 66.9025 - val_loss: 9058.1113 - val_mae: 9058.8037\n",
      "Epoch 4301/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 61.7361 - mae: 62.4210 - val_loss: 9128.7969 - val_mae: 9129.4912\n",
      "Epoch 4302/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 64.4117 - mae: 65.0956 - val_loss: 9086.7324 - val_mae: 9087.4258\n",
      "Epoch 4303/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 67.3198 - mae: 68.0014 - val_loss: 9079.2959 - val_mae: 9079.9893\n",
      "Epoch 4304/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 63.5601 - mae: 64.2419 - val_loss: 9077.5635 - val_mae: 9078.2568\n",
      "Epoch 4305/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 63.9008 - mae: 64.5851 - val_loss: 9093.3232 - val_mae: 9094.0156\n",
      "Epoch 4306/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 69.2568 - mae: 69.9380 - val_loss: 8984.8984 - val_mae: 8985.5928\n",
      "Epoch 4307/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 76.0750 - mae: 76.7591 - val_loss: 9153.4541 - val_mae: 9154.1475\n",
      "Epoch 4308/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 73.1692 - mae: 73.8525 - val_loss: 9077.1738 - val_mae: 9077.8662\n",
      "Epoch 4309/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 70.4038 - mae: 71.0891 - val_loss: 9063.7773 - val_mae: 9064.4697\n",
      "Epoch 4310/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 78.3691 - mae: 79.0580 - val_loss: 9092.2461 - val_mae: 9092.9395\n",
      "Epoch 4311/5000\n",
      "46/46 [==============================] - 2s 47ms/step - loss: 70.4513 - mae: 71.1386 - val_loss: 9183.6924 - val_mae: 9184.3857\n",
      "Epoch 4312/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 61.2687 - mae: 61.9453 - val_loss: 9162.1748 - val_mae: 9162.8672\n",
      "Epoch 4313/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 58.0489 - mae: 58.7256 - val_loss: 9126.7402 - val_mae: 9127.4336\n",
      "Epoch 4314/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 66.2956 - mae: 66.9805 - val_loss: 9179.0879 - val_mae: 9179.7812\n",
      "Epoch 4315/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 72.2737 - mae: 72.9611 - val_loss: 9127.3438 - val_mae: 9128.0361\n",
      "Epoch 4316/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 70.2724 - mae: 70.9567 - val_loss: 9058.9229 - val_mae: 9059.6152\n",
      "Epoch 4317/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 64.1084 - mae: 64.7910 - val_loss: 9073.0625 - val_mae: 9073.7578\n",
      "Epoch 4318/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 71.0592 - mae: 71.7446 - val_loss: 8976.3584 - val_mae: 8977.0518\n",
      "Epoch 4319/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 77.9155 - mae: 78.6019 - val_loss: 9136.0410 - val_mae: 9136.7344\n",
      "Epoch 4320/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 65.1668 - mae: 65.8462 - val_loss: 9120.7588 - val_mae: 9121.4512\n",
      "Epoch 4321/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 76.2573 - mae: 76.9421 - val_loss: 9181.4277 - val_mae: 9182.1211\n",
      "Epoch 4322/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 74.7195 - mae: 75.4037 - val_loss: 9001.3994 - val_mae: 9002.0918\n",
      "Epoch 4323/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 69.4299 - mae: 70.1141 - val_loss: 9048.8164 - val_mae: 9049.5078\n",
      "Epoch 4324/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 77.3731 - mae: 78.0579 - val_loss: 9070.5488 - val_mae: 9071.2422\n",
      "Epoch 4325/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 67.0646 - mae: 67.7505 - val_loss: 9100.0693 - val_mae: 9100.7617\n",
      "Epoch 4326/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 61.8173 - mae: 62.4997 - val_loss: 9239.1338 - val_mae: 9239.8262\n",
      "Epoch 4327/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 65.4559 - mae: 66.1349 - val_loss: 9114.4160 - val_mae: 9115.1074\n",
      "Epoch 4328/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 70.9578 - mae: 71.6404 - val_loss: 9066.5049 - val_mae: 9067.1982\n",
      "Epoch 4329/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 60.0080 - mae: 60.6923 - val_loss: 9039.8506 - val_mae: 9040.5449\n",
      "Epoch 4330/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 62.0807 - mae: 62.7622 - val_loss: 9058.7969 - val_mae: 9059.4893\n",
      "Epoch 4331/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 65.8147 - mae: 66.4983 - val_loss: 8979.5059 - val_mae: 8980.2002\n",
      "Epoch 4332/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 68.1394 - mae: 68.8225 - val_loss: 9058.9160 - val_mae: 9059.6064\n",
      "Epoch 4333/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 61.4355 - mae: 62.1210 - val_loss: 9175.1152 - val_mae: 9175.8086\n",
      "Epoch 4334/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 58.7253 - mae: 59.4072 - val_loss: 9098.9502 - val_mae: 9099.6445\n",
      "Epoch 4335/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 61.7366 - mae: 62.4201 - val_loss: 9184.5303 - val_mae: 9185.2236\n",
      "Epoch 4336/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 57.7213 - mae: 58.4047 - val_loss: 9150.9941 - val_mae: 9151.6865\n",
      "Epoch 4337/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 58.5906 - mae: 59.2749 - val_loss: 9128.5449 - val_mae: 9129.2393\n",
      "Epoch 4338/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 59.0512 - mae: 59.7308 - val_loss: 9267.2266 - val_mae: 9267.9209\n",
      "Epoch 4339/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 63.7037 - mae: 64.3900 - val_loss: 9129.1602 - val_mae: 9129.8545\n",
      "Epoch 4340/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 66.3610 - mae: 67.0447 - val_loss: 9265.3271 - val_mae: 9266.0195\n",
      "Epoch 4341/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 70.6272 - mae: 71.3113 - val_loss: 9030.8398 - val_mae: 9031.5322\n",
      "Epoch 4342/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 60.2697 - mae: 60.9545 - val_loss: 9120.6143 - val_mae: 9121.3057\n",
      "Epoch 4343/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 68.5042 - mae: 69.1842 - val_loss: 9112.6836 - val_mae: 9113.3770\n",
      "Epoch 4344/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 62.2674 - mae: 62.9493 - val_loss: 9168.4951 - val_mae: 9169.1875\n",
      "Epoch 4345/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 65.4558 - mae: 66.1369 - val_loss: 9239.5127 - val_mae: 9240.2051\n",
      "Epoch 4346/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 61.8326 - mae: 62.5178 - val_loss: 8940.8135 - val_mae: 8941.5068\n",
      "Epoch 4347/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 87.9958 - mae: 88.6806 - val_loss: 9288.9463 - val_mae: 9289.6387\n",
      "Epoch 4348/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 74.4639 - mae: 75.1512 - val_loss: 9301.2910 - val_mae: 9301.9834\n",
      "Epoch 4349/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 64.7416 - mae: 65.4268 - val_loss: 9200.3408 - val_mae: 9201.0332\n",
      "Epoch 4350/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 68.2528 - mae: 68.9363 - val_loss: 9019.2812 - val_mae: 9019.9746\n",
      "Epoch 4351/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 61.3250 - mae: 62.0070 - val_loss: 9027.9854 - val_mae: 9028.6787\n",
      "Epoch 4352/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 58.5702 - mae: 59.2517 - val_loss: 9056.2900 - val_mae: 9056.9824\n",
      "Epoch 4353/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 68.8643 - mae: 69.5446 - val_loss: 8992.0459 - val_mae: 8992.7383\n",
      "Epoch 4354/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 75.2357 - mae: 75.9174 - val_loss: 9125.7783 - val_mae: 9126.4717\n",
      "Epoch 4355/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 62.9935 - mae: 63.6731 - val_loss: 9085.8848 - val_mae: 9086.5771\n",
      "Epoch 4356/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 67.1135 - mae: 67.8026 - val_loss: 8961.7559 - val_mae: 8962.4492\n",
      "Epoch 4357/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 72.0598 - mae: 72.7448 - val_loss: 9314.2627 - val_mae: 9314.9551\n",
      "Epoch 4358/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 79.1117 - mae: 79.7986 - val_loss: 9139.3633 - val_mae: 9140.0566\n",
      "Epoch 4359/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 58.4163 - mae: 59.0956 - val_loss: 9061.3535 - val_mae: 9062.0459\n",
      "Epoch 4360/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 69.9860 - mae: 70.6695 - val_loss: 9061.3867 - val_mae: 9062.0811\n",
      "Epoch 4361/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 62.6192 - mae: 63.3017 - val_loss: 9135.7656 - val_mae: 9136.4590\n",
      "Epoch 4362/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 58.8109 - mae: 59.4942 - val_loss: 9011.8955 - val_mae: 9012.5869\n",
      "Epoch 4363/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 71.1063 - mae: 71.7834 - val_loss: 9051.9785 - val_mae: 9052.6709\n",
      "Epoch 4364/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 64.3799 - mae: 65.0610 - val_loss: 9212.0762 - val_mae: 9212.7705\n",
      "Epoch 4365/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 70.2871 - mae: 70.9732 - val_loss: 9198.6953 - val_mae: 9199.3867\n",
      "Epoch 4366/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 70.0890 - mae: 70.7733 - val_loss: 9209.3467 - val_mae: 9210.0400\n",
      "Epoch 4367/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 66.7089 - mae: 67.3894 - val_loss: 9177.1045 - val_mae: 9177.7988\n",
      "Epoch 4368/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 67.1352 - mae: 67.8194 - val_loss: 9145.2520 - val_mae: 9145.9473\n",
      "Epoch 4369/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 58.0637 - mae: 58.7441 - val_loss: 9210.8057 - val_mae: 9211.4980\n",
      "Epoch 4370/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 57.9328 - mae: 58.6114 - val_loss: 9176.1123 - val_mae: 9176.8066\n",
      "Epoch 4371/5000\n",
      "46/46 [==============================] - 2s 32ms/step - loss: 62.8283 - mae: 63.5108 - val_loss: 9072.2588 - val_mae: 9072.9521\n",
      "Epoch 4372/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 61.5852 - mae: 62.2669 - val_loss: 9109.4658 - val_mae: 9110.1602\n",
      "Epoch 4373/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 67.7699 - mae: 68.4478 - val_loss: 9186.4551 - val_mae: 9187.1504\n",
      "Epoch 4374/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 56.2954 - mae: 56.9695 - val_loss: 9132.4209 - val_mae: 9133.1133\n",
      "Epoch 4375/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 68.6336 - mae: 69.3158 - val_loss: 9021.5938 - val_mae: 9022.2881\n",
      "Epoch 4376/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 64.7258 - mae: 65.4069 - val_loss: 9079.1445 - val_mae: 9079.8369\n",
      "Epoch 4377/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 66.6946 - mae: 67.3763 - val_loss: 9053.2812 - val_mae: 9053.9746\n",
      "Epoch 4378/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 66.5281 - mae: 67.2111 - val_loss: 9202.8467 - val_mae: 9203.5410\n",
      "Epoch 4379/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 63.2722 - mae: 63.9561 - val_loss: 9032.1631 - val_mae: 9032.8555\n",
      "Epoch 4380/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 62.7598 - mae: 63.4387 - val_loss: 9358.8701 - val_mae: 9359.5625\n",
      "Epoch 4381/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 85.0693 - mae: 85.7560 - val_loss: 9120.1338 - val_mae: 9120.8271\n",
      "Epoch 4382/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 62.0650 - mae: 62.7470 - val_loss: 9277.6465 - val_mae: 9278.3389\n",
      "Epoch 4383/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 68.4604 - mae: 69.1411 - val_loss: 9196.3779 - val_mae: 9197.0703\n",
      "Epoch 4384/5000\n",
      "46/46 [==============================] - 2s 45ms/step - loss: 66.1695 - mae: 66.8543 - val_loss: 9147.1484 - val_mae: 9147.8418\n",
      "Epoch 4385/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 64.3551 - mae: 65.0396 - val_loss: 9127.0742 - val_mae: 9127.7676\n",
      "Epoch 4386/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 63.5179 - mae: 64.2009 - val_loss: 9094.2178 - val_mae: 9094.9102\n",
      "Epoch 4387/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 61.6572 - mae: 62.3403 - val_loss: 9105.0605 - val_mae: 9105.7529\n",
      "Epoch 4388/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 62.7462 - mae: 63.4297 - val_loss: 9165.2158 - val_mae: 9165.9092\n",
      "Epoch 4389/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 68.5115 - mae: 69.1952 - val_loss: 9258.3535 - val_mae: 9259.0479\n",
      "Epoch 4390/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 65.6021 - mae: 66.2848 - val_loss: 9043.6885 - val_mae: 9044.3828\n",
      "Epoch 4391/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 69.6530 - mae: 70.3360 - val_loss: 9232.2500 - val_mae: 9232.9443\n",
      "Epoch 4392/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 75.4620 - mae: 76.1444 - val_loss: 9068.7129 - val_mae: 9069.4062\n",
      "Epoch 4393/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 63.6159 - mae: 64.3004 - val_loss: 9130.0371 - val_mae: 9130.7305\n",
      "Epoch 4394/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 66.6930 - mae: 67.3768 - val_loss: 9093.6582 - val_mae: 9094.3506\n",
      "Epoch 4395/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 69.4797 - mae: 70.1665 - val_loss: 9076.8877 - val_mae: 9077.5811\n",
      "Epoch 4396/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 63.2066 - mae: 63.8894 - val_loss: 9237.9111 - val_mae: 9238.6045\n",
      "Epoch 4397/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 64.9475 - mae: 65.6302 - val_loss: 9110.9805 - val_mae: 9111.6748\n",
      "Epoch 4398/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 60.9981 - mae: 61.6827 - val_loss: 9131.9844 - val_mae: 9132.6758\n",
      "Epoch 4399/5000\n",
      "46/46 [==============================] - 2s 41ms/step - loss: 65.2077 - mae: 65.8943 - val_loss: 9174.7686 - val_mae: 9175.4629\n",
      "Epoch 4400/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 60.2964 - mae: 60.9773 - val_loss: 9004.7871 - val_mae: 9005.4805\n",
      "Epoch 4401/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 65.3211 - mae: 66.0004 - val_loss: 9128.1055 - val_mae: 9128.7988\n",
      "Epoch 4402/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 65.1428 - mae: 65.8248 - val_loss: 9043.5352 - val_mae: 9044.2275\n",
      "Epoch 4403/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 64.7139 - mae: 65.3963 - val_loss: 9092.7295 - val_mae: 9093.4219\n",
      "Epoch 4404/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 62.3918 - mae: 63.0742 - val_loss: 9132.8682 - val_mae: 9133.5615\n",
      "Epoch 4405/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 61.5547 - mae: 62.2361 - val_loss: 9134.6592 - val_mae: 9135.3525\n",
      "Epoch 4406/5000\n",
      "46/46 [==============================] - 2s 41ms/step - loss: 69.7511 - mae: 70.4333 - val_loss: 9137.6055 - val_mae: 9138.2959\n",
      "Epoch 4407/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 63.4784 - mae: 64.1615 - val_loss: 9055.9697 - val_mae: 9056.6621\n",
      "Epoch 4408/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 64.1452 - mae: 64.8258 - val_loss: 9175.0010 - val_mae: 9175.6934\n",
      "Epoch 4409/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 59.3612 - mae: 60.0448 - val_loss: 9089.8672 - val_mae: 9090.5596\n",
      "Epoch 4410/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 68.2164 - mae: 68.9009 - val_loss: 8993.6484 - val_mae: 8994.3408\n",
      "Epoch 4411/5000\n",
      "46/46 [==============================] - 2s 45ms/step - loss: 74.2984 - mae: 74.9850 - val_loss: 9135.0537 - val_mae: 9135.7490\n",
      "Epoch 4412/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 66.1600 - mae: 66.8433 - val_loss: 9120.2129 - val_mae: 9120.9053\n",
      "Epoch 4413/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 67.8238 - mae: 68.5057 - val_loss: 8938.3955 - val_mae: 8939.0879\n",
      "Epoch 4414/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 85.5949 - mae: 86.2808 - val_loss: 9139.7188 - val_mae: 9140.4131\n",
      "Epoch 4415/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 75.3331 - mae: 76.0195 - val_loss: 9159.0869 - val_mae: 9159.7793\n",
      "Epoch 4416/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 67.4220 - mae: 68.1016 - val_loss: 9174.8037 - val_mae: 9175.4980\n",
      "Epoch 4417/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 72.1638 - mae: 72.8508 - val_loss: 9074.4951 - val_mae: 9075.1875\n",
      "Epoch 4418/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 63.3036 - mae: 63.9862 - val_loss: 8980.4404 - val_mae: 8981.1328\n",
      "Epoch 4419/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 62.7371 - mae: 63.4147 - val_loss: 9197.3574 - val_mae: 9198.0508\n",
      "Epoch 4420/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 68.8547 - mae: 69.5376 - val_loss: 9292.9580 - val_mae: 9293.6504\n",
      "Epoch 4421/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 69.4206 - mae: 70.0993 - val_loss: 9084.0049 - val_mae: 9084.6973\n",
      "Epoch 4422/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 72.0871 - mae: 72.7676 - val_loss: 8964.3994 - val_mae: 8965.0928\n",
      "Epoch 4423/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 67.6401 - mae: 68.3217 - val_loss: 8992.7041 - val_mae: 8993.3965\n",
      "Epoch 4424/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 59.0458 - mae: 59.7262 - val_loss: 9059.1426 - val_mae: 9059.8359\n",
      "Epoch 4425/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 62.3768 - mae: 63.0540 - val_loss: 9079.1377 - val_mae: 9079.8301\n",
      "Epoch 4426/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 62.8183 - mae: 63.5050 - val_loss: 8957.8535 - val_mae: 8958.5469\n",
      "Epoch 4427/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 64.8284 - mae: 65.5114 - val_loss: 9207.5879 - val_mae: 9208.2812\n",
      "Epoch 4428/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 65.3635 - mae: 66.0489 - val_loss: 9087.8555 - val_mae: 9088.5498\n",
      "Epoch 4429/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 63.3400 - mae: 64.0281 - val_loss: 9060.6924 - val_mae: 9061.3867\n",
      "Epoch 4430/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 58.3807 - mae: 59.0614 - val_loss: 9145.3359 - val_mae: 9146.0293\n",
      "Epoch 4431/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 60.7054 - mae: 61.3862 - val_loss: 8886.5244 - val_mae: 8887.2178\n",
      "Epoch 4432/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 75.7330 - mae: 76.4200 - val_loss: 9273.2559 - val_mae: 9273.9502\n",
      "Epoch 4433/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 70.8242 - mae: 71.5099 - val_loss: 9174.6846 - val_mae: 9175.3760\n",
      "Epoch 4434/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 67.0472 - mae: 67.7308 - val_loss: 8996.7197 - val_mae: 8997.4131\n",
      "Epoch 4435/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 65.8820 - mae: 66.5563 - val_loss: 9044.0576 - val_mae: 9044.7500\n",
      "Epoch 4436/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 64.1660 - mae: 64.8518 - val_loss: 9033.4756 - val_mae: 9034.1699\n",
      "Epoch 4437/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 61.6489 - mae: 62.3314 - val_loss: 8938.0371 - val_mae: 8938.7305\n",
      "Epoch 4438/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 74.5542 - mae: 75.2371 - val_loss: 9191.0693 - val_mae: 9191.7617\n",
      "Epoch 4439/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 63.4072 - mae: 64.0912 - val_loss: 9144.6152 - val_mae: 9145.3076\n",
      "Epoch 4440/5000\n",
      "46/46 [==============================] - 2s 40ms/step - loss: 71.1406 - mae: 71.8284 - val_loss: 9163.3457 - val_mae: 9164.0391\n",
      "Epoch 4441/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 56.7674 - mae: 57.4469 - val_loss: 8905.2959 - val_mae: 8905.9883\n",
      "Epoch 4442/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 68.9655 - mae: 69.6479 - val_loss: 9087.6768 - val_mae: 9088.3711\n",
      "Epoch 4443/5000\n",
      "46/46 [==============================] - 2s 41ms/step - loss: 65.8270 - mae: 66.5106 - val_loss: 9159.2578 - val_mae: 9159.9512\n",
      "Epoch 4444/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 59.7098 - mae: 60.3931 - val_loss: 9029.9453 - val_mae: 9030.6377\n",
      "Epoch 4445/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 72.3596 - mae: 73.0387 - val_loss: 8930.1084 - val_mae: 8930.7998\n",
      "Epoch 4446/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 66.9798 - mae: 67.6637 - val_loss: 9036.0195 - val_mae: 9036.7129\n",
      "Epoch 4447/5000\n",
      "46/46 [==============================] - 2s 41ms/step - loss: 62.8280 - mae: 63.5115 - val_loss: 9089.2695 - val_mae: 9089.9639\n",
      "Epoch 4448/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 51.3027 - mae: 51.9815 - val_loss: 9080.6357 - val_mae: 9081.3301\n",
      "Epoch 4449/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 59.0273 - mae: 59.7099 - val_loss: 9223.7490 - val_mae: 9224.4434\n",
      "Epoch 4450/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 78.2793 - mae: 78.9620 - val_loss: 9218.8789 - val_mae: 9219.5723\n",
      "Epoch 4451/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 82.2823 - mae: 82.9712 - val_loss: 8957.8037 - val_mae: 8958.4980\n",
      "Epoch 4452/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 69.0256 - mae: 69.7115 - val_loss: 9045.6523 - val_mae: 9046.3467\n",
      "Epoch 4453/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 70.1153 - mae: 70.8002 - val_loss: 9022.9639 - val_mae: 9023.6562\n",
      "Epoch 4454/5000\n",
      "46/46 [==============================] - 2s 51ms/step - loss: 56.8439 - mae: 57.5251 - val_loss: 9130.2207 - val_mae: 9130.9141\n",
      "Epoch 4455/5000\n",
      "46/46 [==============================] - 2s 51ms/step - loss: 69.6814 - mae: 70.3664 - val_loss: 9115.7217 - val_mae: 9116.4150\n",
      "Epoch 4456/5000\n",
      "46/46 [==============================] - 2s 42ms/step - loss: 52.6100 - mae: 53.2921 - val_loss: 9146.7451 - val_mae: 9147.4365\n",
      "Epoch 4457/5000\n",
      "46/46 [==============================] - 2s 42ms/step - loss: 70.3780 - mae: 71.0620 - val_loss: 9077.9219 - val_mae: 9078.6152\n",
      "Epoch 4458/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 58.0963 - mae: 58.7800 - val_loss: 9095.7607 - val_mae: 9096.4541\n",
      "Epoch 4459/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 76.0732 - mae: 76.7589 - val_loss: 9020.5215 - val_mae: 9021.2158\n",
      "Epoch 4460/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 58.6329 - mae: 59.3161 - val_loss: 9089.3369 - val_mae: 9090.0312\n",
      "Epoch 4461/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 66.9490 - mae: 67.6321 - val_loss: 9119.4072 - val_mae: 9120.1006\n",
      "Epoch 4462/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 57.2760 - mae: 57.9551 - val_loss: 9068.5615 - val_mae: 9069.2549\n",
      "Epoch 4463/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 58.9459 - mae: 59.6234 - val_loss: 9196.4160 - val_mae: 9197.1094\n",
      "Epoch 4464/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 69.8748 - mae: 70.5548 - val_loss: 9162.6582 - val_mae: 9163.3516\n",
      "Epoch 4465/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 74.9739 - mae: 75.6591 - val_loss: 9274.7324 - val_mae: 9275.4258\n",
      "Epoch 4466/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 67.4782 - mae: 68.1604 - val_loss: 9077.5439 - val_mae: 9078.2373\n",
      "Epoch 4467/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 56.7303 - mae: 57.4148 - val_loss: 9180.0400 - val_mae: 9180.7334\n",
      "Epoch 4468/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 73.6429 - mae: 74.3268 - val_loss: 9005.7627 - val_mae: 9006.4541\n",
      "Epoch 4469/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 62.1166 - mae: 62.8005 - val_loss: 8937.2930 - val_mae: 8937.9854\n",
      "Epoch 4470/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 63.7966 - mae: 64.4781 - val_loss: 9135.7900 - val_mae: 9136.4824\n",
      "Epoch 4471/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 65.5031 - mae: 66.1837 - val_loss: 9253.1895 - val_mae: 9253.8828\n",
      "Epoch 4472/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 68.0422 - mae: 68.7189 - val_loss: 9100.1230 - val_mae: 9100.8154\n",
      "Epoch 4473/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 73.4646 - mae: 74.1457 - val_loss: 9081.9600 - val_mae: 9082.6543\n",
      "Epoch 4474/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 61.5165 - mae: 62.2035 - val_loss: 8953.8457 - val_mae: 8954.5381\n",
      "Epoch 4475/5000\n",
      "46/46 [==============================] - 2s 44ms/step - loss: 69.2151 - mae: 69.8954 - val_loss: 9136.8174 - val_mae: 9137.5098\n",
      "Epoch 4476/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 69.6034 - mae: 70.2861 - val_loss: 9050.1426 - val_mae: 9050.8350\n",
      "Epoch 4477/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 64.0680 - mae: 64.7501 - val_loss: 9224.8701 - val_mae: 9225.5625\n",
      "Epoch 4478/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 64.8402 - mae: 65.5197 - val_loss: 9081.3730 - val_mae: 9082.0684\n",
      "Epoch 4479/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 62.2055 - mae: 62.8907 - val_loss: 9040.5830 - val_mae: 9041.2764\n",
      "Epoch 4480/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 62.9789 - mae: 63.6579 - val_loss: 9071.3252 - val_mae: 9072.0186\n",
      "Epoch 4481/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 55.7675 - mae: 56.4519 - val_loss: 8995.9814 - val_mae: 8996.6729\n",
      "Epoch 4482/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 62.8626 - mae: 63.5500 - val_loss: 9099.1514 - val_mae: 9099.8447\n",
      "Epoch 4483/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 59.6954 - mae: 60.3750 - val_loss: 9203.5215 - val_mae: 9204.2148\n",
      "Epoch 4484/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 60.1586 - mae: 60.8393 - val_loss: 9110.6279 - val_mae: 9111.3193\n",
      "Epoch 4485/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 73.1404 - mae: 73.8233 - val_loss: 8940.3799 - val_mae: 8941.0723\n",
      "Epoch 4486/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 63.1457 - mae: 63.8293 - val_loss: 9086.0986 - val_mae: 9086.7920\n",
      "Epoch 4487/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 58.2927 - mae: 58.9714 - val_loss: 9278.2998 - val_mae: 9278.9941\n",
      "Epoch 4488/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 71.0936 - mae: 71.7717 - val_loss: 9053.3066 - val_mae: 9053.9980\n",
      "Epoch 4489/5000\n",
      "46/46 [==============================] - 2s 42ms/step - loss: 63.4358 - mae: 64.1186 - val_loss: 9004.5078 - val_mae: 9005.2002\n",
      "Epoch 4490/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 76.1632 - mae: 76.8481 - val_loss: 9185.6445 - val_mae: 9186.3389\n",
      "Epoch 4491/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 69.0280 - mae: 69.7040 - val_loss: 9046.7168 - val_mae: 9047.4102\n",
      "Epoch 4492/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 66.3619 - mae: 67.0436 - val_loss: 9173.7041 - val_mae: 9174.3965\n",
      "Epoch 4493/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 58.5503 - mae: 59.2315 - val_loss: 9115.9590 - val_mae: 9116.6523\n",
      "Epoch 4494/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 66.3390 - mae: 67.0197 - val_loss: 9025.7256 - val_mae: 9026.4189\n",
      "Epoch 4495/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 73.5076 - mae: 74.1921 - val_loss: 8937.7988 - val_mae: 8938.4912\n",
      "Epoch 4496/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 69.3895 - mae: 70.0728 - val_loss: 9175.8770 - val_mae: 9176.5703\n",
      "Epoch 4497/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 65.1422 - mae: 65.8260 - val_loss: 9037.0059 - val_mae: 9037.7002\n",
      "Epoch 4498/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 67.2012 - mae: 67.8798 - val_loss: 9099.0791 - val_mae: 9099.7734\n",
      "Epoch 4499/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 61.5356 - mae: 62.2199 - val_loss: 9063.3359 - val_mae: 9064.0283\n",
      "Epoch 4500/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 63.9281 - mae: 64.6131 - val_loss: 9002.2158 - val_mae: 9002.9092\n",
      "Epoch 4501/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 68.2381 - mae: 68.9177 - val_loss: 9183.3613 - val_mae: 9184.0547\n",
      "Epoch 4502/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 65.2485 - mae: 65.9324 - val_loss: 9181.1943 - val_mae: 9181.8857\n",
      "Epoch 4503/5000\n",
      "46/46 [==============================] - 2s 40ms/step - loss: 57.8331 - mae: 58.5160 - val_loss: 9101.0137 - val_mae: 9101.7070\n",
      "Epoch 4504/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 69.1111 - mae: 69.7906 - val_loss: 9044.7949 - val_mae: 9045.4873\n",
      "Epoch 4505/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 54.4887 - mae: 55.1707 - val_loss: 9122.4648 - val_mae: 9123.1592\n",
      "Epoch 4506/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 59.1602 - mae: 59.8458 - val_loss: 9057.2285 - val_mae: 9057.9209\n",
      "Epoch 4507/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 57.1111 - mae: 57.7937 - val_loss: 9056.5830 - val_mae: 9057.2764\n",
      "Epoch 4508/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 67.0045 - mae: 67.6884 - val_loss: 9043.5615 - val_mae: 9044.2539\n",
      "Epoch 4509/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 57.4892 - mae: 58.1694 - val_loss: 9103.6396 - val_mae: 9104.3311\n",
      "Epoch 4510/5000\n",
      "46/46 [==============================] - 2s 41ms/step - loss: 62.5023 - mae: 63.1818 - val_loss: 9147.7090 - val_mae: 9148.4023\n",
      "Epoch 4511/5000\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 68.6708 - mae: 69.3542 - val_loss: 9129.9570 - val_mae: 9130.6514\n",
      "Epoch 4512/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 57.5198 - mae: 58.2050 - val_loss: 9080.0889 - val_mae: 9080.7822\n",
      "Epoch 4513/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 61.6033 - mae: 62.2762 - val_loss: 9043.1035 - val_mae: 9043.7959\n",
      "Epoch 4514/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 61.7853 - mae: 62.4702 - val_loss: 9097.2754 - val_mae: 9097.9688\n",
      "Epoch 4515/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 73.8401 - mae: 74.5215 - val_loss: 9024.8828 - val_mae: 9025.5752\n",
      "Epoch 4516/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 84.8627 - mae: 85.5485 - val_loss: 9050.7334 - val_mae: 9051.4268\n",
      "Epoch 4517/5000\n",
      "46/46 [==============================] - 3s 58ms/step - loss: 69.9612 - mae: 70.6454 - val_loss: 8966.0195 - val_mae: 8966.7139\n",
      "Epoch 4518/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 63.1069 - mae: 63.7910 - val_loss: 9141.9619 - val_mae: 9142.6543\n",
      "Epoch 4519/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 64.4135 - mae: 65.0963 - val_loss: 9218.6621 - val_mae: 9219.3545\n",
      "Epoch 4520/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 61.5550 - mae: 62.2296 - val_loss: 9118.2285 - val_mae: 9118.9209\n",
      "Epoch 4521/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 69.3427 - mae: 70.0266 - val_loss: 9028.2881 - val_mae: 9028.9814\n",
      "Epoch 4522/5000\n",
      "46/46 [==============================] - 2s 43ms/step - loss: 64.8905 - mae: 65.5706 - val_loss: 8978.9229 - val_mae: 8979.6152\n",
      "Epoch 4523/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 67.1514 - mae: 67.8367 - val_loss: 9005.6914 - val_mae: 9006.3838\n",
      "Epoch 4524/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 58.4807 - mae: 59.1633 - val_loss: 9013.3545 - val_mae: 9014.0488\n",
      "Epoch 4525/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 63.3190 - mae: 64.0023 - val_loss: 9191.7422 - val_mae: 9192.4365\n",
      "Epoch 4526/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 67.6406 - mae: 68.3222 - val_loss: 9084.1396 - val_mae: 9084.8340\n",
      "Epoch 4527/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 71.8218 - mae: 72.5084 - val_loss: 9070.5430 - val_mae: 9071.2363\n",
      "Epoch 4528/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 75.7216 - mae: 76.4084 - val_loss: 9089.7158 - val_mae: 9090.4082\n",
      "Epoch 4529/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 55.0932 - mae: 55.7718 - val_loss: 8998.2080 - val_mae: 8998.9004\n",
      "Epoch 4530/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 64.7306 - mae: 65.4113 - val_loss: 9038.1650 - val_mae: 9038.8574\n",
      "Epoch 4531/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 66.5262 - mae: 67.2093 - val_loss: 9117.9111 - val_mae: 9118.6045\n",
      "Epoch 4532/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 64.5491 - mae: 65.2368 - val_loss: 9110.2246 - val_mae: 9110.9170\n",
      "Epoch 4533/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 62.0327 - mae: 62.7098 - val_loss: 9062.5518 - val_mae: 9063.2461\n",
      "Epoch 4534/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 60.3537 - mae: 61.0322 - val_loss: 9100.8945 - val_mae: 9101.5889\n",
      "Epoch 4535/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 56.9742 - mae: 57.6518 - val_loss: 9111.9932 - val_mae: 9112.6855\n",
      "Epoch 4536/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 89.3674 - mae: 90.0561 - val_loss: 9112.9072 - val_mae: 9113.6016\n",
      "Epoch 4537/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 61.4324 - mae: 62.1138 - val_loss: 9183.8623 - val_mae: 9184.5557\n",
      "Epoch 4538/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 70.1015 - mae: 70.7852 - val_loss: 9165.1963 - val_mae: 9165.8887\n",
      "Epoch 4539/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 64.3189 - mae: 65.0031 - val_loss: 9025.5254 - val_mae: 9026.2178\n",
      "Epoch 4540/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 61.3928 - mae: 62.0729 - val_loss: 9014.9023 - val_mae: 9015.5947\n",
      "Epoch 4541/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 62.7337 - mae: 63.4141 - val_loss: 9107.7930 - val_mae: 9108.4854\n",
      "Epoch 4542/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 70.0847 - mae: 70.7705 - val_loss: 9233.5469 - val_mae: 9234.2412\n",
      "Epoch 4543/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 64.4808 - mae: 65.1635 - val_loss: 9109.8691 - val_mae: 9110.5625\n",
      "Epoch 4544/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 60.0662 - mae: 60.7534 - val_loss: 8984.5352 - val_mae: 8985.2275\n",
      "Epoch 4545/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 55.0972 - mae: 55.7805 - val_loss: 9077.2930 - val_mae: 9077.9863\n",
      "Epoch 4546/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 64.6652 - mae: 65.3466 - val_loss: 8959.0508 - val_mae: 8959.7451\n",
      "Epoch 4547/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 73.3683 - mae: 74.0548 - val_loss: 9037.7139 - val_mae: 9038.4072\n",
      "Epoch 4548/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 71.1140 - mae: 71.8005 - val_loss: 9129.3965 - val_mae: 9130.0898\n",
      "Epoch 4549/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 71.0582 - mae: 71.7397 - val_loss: 9138.8018 - val_mae: 9139.4951\n",
      "Epoch 4550/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 78.6034 - mae: 79.2851 - val_loss: 9143.4531 - val_mae: 9144.1475\n",
      "Epoch 4551/5000\n",
      "46/46 [==============================] - 2s 32ms/step - loss: 64.6013 - mae: 65.2797 - val_loss: 9118.6416 - val_mae: 9119.3350\n",
      "Epoch 4552/5000\n",
      "46/46 [==============================] - 2s 32ms/step - loss: 64.9113 - mae: 65.5951 - val_loss: 9075.2188 - val_mae: 9075.9121\n",
      "Epoch 4553/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 64.4874 - mae: 65.1726 - val_loss: 9070.5791 - val_mae: 9071.2734\n",
      "Epoch 4554/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 62.1976 - mae: 62.8794 - val_loss: 9041.5449 - val_mae: 9042.2393\n",
      "Epoch 4555/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 62.5604 - mae: 63.2460 - val_loss: 9184.0693 - val_mae: 9184.7627\n",
      "Epoch 4556/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 59.2052 - mae: 59.8885 - val_loss: 9124.1729 - val_mae: 9124.8672\n",
      "Epoch 4557/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 65.2691 - mae: 65.9485 - val_loss: 9016.5752 - val_mae: 9017.2686\n",
      "Epoch 4558/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 62.1885 - mae: 62.8741 - val_loss: 8993.7236 - val_mae: 8994.4160\n",
      "Epoch 4559/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 65.0319 - mae: 65.7171 - val_loss: 9131.2344 - val_mae: 9131.9277\n",
      "Epoch 4560/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 65.7974 - mae: 66.4812 - val_loss: 9134.0342 - val_mae: 9134.7275\n",
      "Epoch 4561/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 57.7774 - mae: 58.4572 - val_loss: 9054.0547 - val_mae: 9054.7490\n",
      "Epoch 4562/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 59.4328 - mae: 60.1195 - val_loss: 9099.2529 - val_mae: 9099.9473\n",
      "Epoch 4563/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 59.2250 - mae: 59.9051 - val_loss: 8990.1230 - val_mae: 8990.8164\n",
      "Epoch 4564/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 53.6263 - mae: 54.3034 - val_loss: 8961.1035 - val_mae: 8961.7979\n",
      "Epoch 4565/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 65.0749 - mae: 65.7545 - val_loss: 9060.4473 - val_mae: 9061.1416\n",
      "Epoch 4566/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 61.4441 - mae: 62.1239 - val_loss: 8964.8584 - val_mae: 8965.5527\n",
      "Epoch 4567/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 60.7881 - mae: 61.4691 - val_loss: 9025.9824 - val_mae: 9026.6729\n",
      "Epoch 4568/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 61.4259 - mae: 62.1064 - val_loss: 9087.2568 - val_mae: 9087.9512\n",
      "Epoch 4569/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 56.9078 - mae: 57.5892 - val_loss: 9116.8184 - val_mae: 9117.5107\n",
      "Epoch 4570/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 59.5056 - mae: 60.1830 - val_loss: 8958.8213 - val_mae: 8959.5146\n",
      "Epoch 4571/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 61.3683 - mae: 62.0535 - val_loss: 9079.9707 - val_mae: 9080.6641\n",
      "Epoch 4572/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 70.0594 - mae: 70.7462 - val_loss: 8998.7979 - val_mae: 8999.4902\n",
      "Epoch 4573/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 66.0030 - mae: 66.6834 - val_loss: 9094.1553 - val_mae: 9094.8486\n",
      "Epoch 4574/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 65.0893 - mae: 65.7729 - val_loss: 9062.6025 - val_mae: 9063.2959\n",
      "Epoch 4575/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 67.8616 - mae: 68.5407 - val_loss: 9039.8496 - val_mae: 9040.5430\n",
      "Epoch 4576/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 68.9376 - mae: 69.6206 - val_loss: 9109.3926 - val_mae: 9110.0850\n",
      "Epoch 4577/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 63.5150 - mae: 64.1978 - val_loss: 9158.6172 - val_mae: 9159.3096\n",
      "Epoch 4578/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 59.1667 - mae: 59.8517 - val_loss: 9170.1611 - val_mae: 9170.8545\n",
      "Epoch 4579/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 64.7691 - mae: 65.4493 - val_loss: 8975.7568 - val_mae: 8976.4502\n",
      "Epoch 4580/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 66.0528 - mae: 66.7346 - val_loss: 9126.8516 - val_mae: 9127.5449\n",
      "Epoch 4581/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 68.2547 - mae: 68.9394 - val_loss: 9094.2881 - val_mae: 9094.9805\n",
      "Epoch 4582/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 71.4454 - mae: 72.1256 - val_loss: 9154.5469 - val_mae: 9155.2393\n",
      "Epoch 4583/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 61.7444 - mae: 62.4259 - val_loss: 9221.9863 - val_mae: 9222.6797\n",
      "Epoch 4584/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 61.6439 - mae: 62.3240 - val_loss: 9076.3711 - val_mae: 9077.0635\n",
      "Epoch 4585/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 62.9598 - mae: 63.6416 - val_loss: 9120.8252 - val_mae: 9121.5186\n",
      "Epoch 4586/5000\n",
      "46/46 [==============================] - 2s 43ms/step - loss: 56.8573 - mae: 57.5397 - val_loss: 8950.2246 - val_mae: 8950.9180\n",
      "Epoch 4587/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 72.6817 - mae: 73.3642 - val_loss: 9179.4570 - val_mae: 9180.1514\n",
      "Epoch 4588/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 66.4969 - mae: 67.1807 - val_loss: 9370.6309 - val_mae: 9371.3242\n",
      "Epoch 4589/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 65.1934 - mae: 65.8780 - val_loss: 9162.6475 - val_mae: 9163.3398\n",
      "Epoch 4590/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 65.2561 - mae: 65.9360 - val_loss: 9213.8955 - val_mae: 9214.5889\n",
      "Epoch 4591/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 60.8411 - mae: 61.5192 - val_loss: 9248.6484 - val_mae: 9249.3408\n",
      "Epoch 4592/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 65.9199 - mae: 66.6008 - val_loss: 9057.6699 - val_mae: 9058.3652\n",
      "Epoch 4593/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 55.3176 - mae: 55.9998 - val_loss: 9234.6660 - val_mae: 9235.3584\n",
      "Epoch 4594/5000\n",
      "46/46 [==============================] - 2s 48ms/step - loss: 71.6115 - mae: 72.2842 - val_loss: 8987.3867 - val_mae: 8988.0791\n",
      "Epoch 4595/5000\n",
      "46/46 [==============================] - 2s 40ms/step - loss: 68.8130 - mae: 69.4913 - val_loss: 9243.9580 - val_mae: 9244.6514\n",
      "Epoch 4596/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 68.9710 - mae: 69.6587 - val_loss: 9000.0078 - val_mae: 9000.7002\n",
      "Epoch 4597/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 70.1727 - mae: 70.8577 - val_loss: 9008.5801 - val_mae: 9009.2725\n",
      "Epoch 4598/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 79.4600 - mae: 80.1437 - val_loss: 9059.5879 - val_mae: 9060.2822\n",
      "Epoch 4599/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 60.0097 - mae: 60.6923 - val_loss: 9065.8262 - val_mae: 9066.5186\n",
      "Epoch 4600/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 64.3303 - mae: 65.0113 - val_loss: 9090.1914 - val_mae: 9090.8848\n",
      "Epoch 4601/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 62.1468 - mae: 62.8263 - val_loss: 9137.0049 - val_mae: 9137.6973\n",
      "Epoch 4602/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 67.1158 - mae: 67.7939 - val_loss: 9060.9883 - val_mae: 9061.6826\n",
      "Epoch 4603/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 61.3506 - mae: 62.0325 - val_loss: 9038.5342 - val_mae: 9039.2266\n",
      "Epoch 4604/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 57.6462 - mae: 58.3276 - val_loss: 9102.7266 - val_mae: 9103.4180\n",
      "Epoch 4605/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 59.9432 - mae: 60.6226 - val_loss: 9227.1748 - val_mae: 9227.8682\n",
      "Epoch 4606/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 61.8430 - mae: 62.5271 - val_loss: 9055.0752 - val_mae: 9055.7695\n",
      "Epoch 4607/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 64.0170 - mae: 64.7007 - val_loss: 9154.4932 - val_mae: 9155.1865\n",
      "Epoch 4608/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 69.6235 - mae: 70.3007 - val_loss: 9256.7461 - val_mae: 9257.4395\n",
      "Epoch 4609/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 77.8688 - mae: 78.5520 - val_loss: 9121.3174 - val_mae: 9122.0117\n",
      "Epoch 4610/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 58.0076 - mae: 58.6899 - val_loss: 9112.1562 - val_mae: 9112.8496\n",
      "Epoch 4611/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 56.1985 - mae: 56.8773 - val_loss: 9081.6953 - val_mae: 9082.3887\n",
      "Epoch 4612/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 52.1707 - mae: 52.8484 - val_loss: 9127.5986 - val_mae: 9128.2920\n",
      "Epoch 4613/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 61.6452 - mae: 62.3231 - val_loss: 9084.3340 - val_mae: 9085.0264\n",
      "Epoch 4614/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 58.2778 - mae: 58.9635 - val_loss: 9127.9473 - val_mae: 9128.6396\n",
      "Epoch 4615/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 61.0885 - mae: 61.7735 - val_loss: 9100.4072 - val_mae: 9101.1016\n",
      "Epoch 4616/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 69.4521 - mae: 70.1363 - val_loss: 9108.1865 - val_mae: 9108.8799\n",
      "Epoch 4617/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 60.2808 - mae: 60.9646 - val_loss: 9094.0391 - val_mae: 9094.7314\n",
      "Epoch 4618/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 59.2509 - mae: 59.9354 - val_loss: 9031.7373 - val_mae: 9032.4307\n",
      "Epoch 4619/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 60.1264 - mae: 60.8086 - val_loss: 9211.2656 - val_mae: 9211.9580\n",
      "Epoch 4620/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 73.5783 - mae: 74.2634 - val_loss: 9023.7100 - val_mae: 9024.4023\n",
      "Epoch 4621/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 72.2434 - mae: 72.9260 - val_loss: 9120.1895 - val_mae: 9120.8828\n",
      "Epoch 4622/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 64.5552 - mae: 65.2370 - val_loss: 9079.1426 - val_mae: 9079.8350\n",
      "Epoch 4623/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 59.3964 - mae: 60.0768 - val_loss: 9095.8311 - val_mae: 9096.5234\n",
      "Epoch 4624/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 69.2233 - mae: 69.9050 - val_loss: 9012.3438 - val_mae: 9013.0391\n",
      "Epoch 4625/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 60.4487 - mae: 61.1298 - val_loss: 9101.1504 - val_mae: 9101.8438\n",
      "Epoch 4626/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 58.8838 - mae: 59.5621 - val_loss: 9071.2002 - val_mae: 9071.8936\n",
      "Epoch 4627/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 60.2637 - mae: 60.9463 - val_loss: 9250.1631 - val_mae: 9250.8564\n",
      "Epoch 4628/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 57.7650 - mae: 58.4417 - val_loss: 9081.9297 - val_mae: 9082.6221\n",
      "Epoch 4629/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 64.0741 - mae: 64.7543 - val_loss: 9061.3418 - val_mae: 9062.0352\n",
      "Epoch 4630/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 63.5737 - mae: 64.2551 - val_loss: 9203.0820 - val_mae: 9203.7754\n",
      "Epoch 4631/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 60.3130 - mae: 60.9903 - val_loss: 9147.1592 - val_mae: 9147.8525\n",
      "Epoch 4632/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 59.0036 - mae: 59.6908 - val_loss: 9014.5527 - val_mae: 9015.2461\n",
      "Epoch 4633/5000\n",
      "46/46 [==============================] - 2s 45ms/step - loss: 60.0113 - mae: 60.6953 - val_loss: 9127.8916 - val_mae: 9128.5850\n",
      "Epoch 4634/5000\n",
      "46/46 [==============================] - 2s 47ms/step - loss: 67.3375 - mae: 68.0244 - val_loss: 9173.7080 - val_mae: 9174.4014\n",
      "Epoch 4635/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 65.0024 - mae: 65.6880 - val_loss: 8972.6348 - val_mae: 8973.3271\n",
      "Epoch 4636/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 70.2515 - mae: 70.9351 - val_loss: 9050.7861 - val_mae: 9051.4775\n",
      "Epoch 4637/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 69.2659 - mae: 69.9530 - val_loss: 9180.6729 - val_mae: 9181.3643\n",
      "Epoch 4638/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 60.1086 - mae: 60.7893 - val_loss: 9075.9414 - val_mae: 9076.6357\n",
      "Epoch 4639/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 48.4437 - mae: 49.1276 - val_loss: 9310.7949 - val_mae: 9311.4873\n",
      "Epoch 4640/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 61.9797 - mae: 62.6596 - val_loss: 9110.4150 - val_mae: 9111.1064\n",
      "Epoch 4641/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 61.2975 - mae: 61.9764 - val_loss: 9158.2607 - val_mae: 9158.9541\n",
      "Epoch 4642/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 62.0933 - mae: 62.7769 - val_loss: 9118.6836 - val_mae: 9119.3770\n",
      "Epoch 4643/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 65.5400 - mae: 66.2248 - val_loss: 9026.4736 - val_mae: 9027.1660\n",
      "Epoch 4644/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 62.8862 - mae: 63.5713 - val_loss: 9092.4990 - val_mae: 9093.1934\n",
      "Epoch 4645/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 59.7967 - mae: 60.4756 - val_loss: 9107.9580 - val_mae: 9108.6523\n",
      "Epoch 4646/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 52.9904 - mae: 53.6737 - val_loss: 9203.3291 - val_mae: 9204.0215\n",
      "Epoch 4647/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 62.7596 - mae: 63.4379 - val_loss: 9101.1494 - val_mae: 9101.8418\n",
      "Epoch 4648/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 54.8277 - mae: 55.5087 - val_loss: 9080.0820 - val_mae: 9080.7754\n",
      "Epoch 4649/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 60.0341 - mae: 60.7177 - val_loss: 9155.0264 - val_mae: 9155.7188\n",
      "Epoch 4650/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 66.3967 - mae: 67.0799 - val_loss: 9105.0498 - val_mae: 9105.7441\n",
      "Epoch 4651/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 61.4283 - mae: 62.1058 - val_loss: 8996.5781 - val_mae: 8997.2715\n",
      "Epoch 4652/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 57.1428 - mae: 57.8222 - val_loss: 9231.3213 - val_mae: 9232.0137\n",
      "Epoch 4653/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 65.1992 - mae: 65.8824 - val_loss: 9067.7637 - val_mae: 9068.4561\n",
      "Epoch 4654/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 68.0450 - mae: 68.7261 - val_loss: 8930.2715 - val_mae: 8930.9648\n",
      "Epoch 4655/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 67.1014 - mae: 67.7865 - val_loss: 9092.8047 - val_mae: 9093.4980\n",
      "Epoch 4656/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 62.5718 - mae: 63.2546 - val_loss: 9102.8672 - val_mae: 9103.5605\n",
      "Epoch 4657/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 79.9297 - mae: 80.6140 - val_loss: 9034.5498 - val_mae: 9035.2441\n",
      "Epoch 4658/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 58.7031 - mae: 59.3844 - val_loss: 9162.9502 - val_mae: 9163.6455\n",
      "Epoch 4659/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 66.3124 - mae: 66.9930 - val_loss: 9047.5381 - val_mae: 9048.2295\n",
      "Epoch 4660/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 64.8094 - mae: 65.4930 - val_loss: 9087.8984 - val_mae: 9088.5928\n",
      "Epoch 4661/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 66.1834 - mae: 66.8624 - val_loss: 9179.6553 - val_mae: 9180.3496\n",
      "Epoch 4662/5000\n",
      "46/46 [==============================] - 2s 43ms/step - loss: 60.4712 - mae: 61.1562 - val_loss: 9109.3428 - val_mae: 9110.0361\n",
      "Epoch 4663/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 70.7480 - mae: 71.4301 - val_loss: 8906.4570 - val_mae: 8907.1504\n",
      "Epoch 4664/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 58.7942 - mae: 59.4765 - val_loss: 9055.6865 - val_mae: 9056.3799\n",
      "Epoch 4665/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 61.3137 - mae: 61.9918 - val_loss: 9173.0205 - val_mae: 9173.7139\n",
      "Epoch 4666/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 61.8287 - mae: 62.5104 - val_loss: 9141.7480 - val_mae: 9142.4424\n",
      "Epoch 4667/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 59.1104 - mae: 59.7910 - val_loss: 9077.1230 - val_mae: 9077.8164\n",
      "Epoch 4668/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 54.3407 - mae: 55.0215 - val_loss: 9035.0771 - val_mae: 9035.7695\n",
      "Epoch 4669/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 73.4481 - mae: 74.1334 - val_loss: 9088.9697 - val_mae: 9089.6631\n",
      "Epoch 4670/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 66.8443 - mae: 67.5271 - val_loss: 9042.4727 - val_mae: 9043.1660\n",
      "Epoch 4671/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 61.2522 - mae: 61.9351 - val_loss: 9111.4111 - val_mae: 9112.1045\n",
      "Epoch 4672/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 55.5698 - mae: 56.2497 - val_loss: 9264.2783 - val_mae: 9264.9717\n",
      "Epoch 4673/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 63.9480 - mae: 64.6275 - val_loss: 9069.2227 - val_mae: 9069.9150\n",
      "Epoch 4674/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 59.3844 - mae: 60.0682 - val_loss: 9027.0703 - val_mae: 9027.7646\n",
      "Epoch 4675/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 57.8186 - mae: 58.5008 - val_loss: 9072.9434 - val_mae: 9073.6357\n",
      "Epoch 4676/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 63.0053 - mae: 63.6911 - val_loss: 9016.0820 - val_mae: 9016.7744\n",
      "Epoch 4677/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 60.7287 - mae: 61.4147 - val_loss: 9175.5625 - val_mae: 9176.2559\n",
      "Epoch 4678/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 66.4930 - mae: 67.1776 - val_loss: 9165.8594 - val_mae: 9166.5518\n",
      "Epoch 4679/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 58.4437 - mae: 59.1245 - val_loss: 9175.7197 - val_mae: 9176.4121\n",
      "Epoch 4680/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 63.8961 - mae: 64.5795 - val_loss: 9036.8457 - val_mae: 9037.5400\n",
      "Epoch 4681/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 53.0802 - mae: 53.7562 - val_loss: 8891.6162 - val_mae: 8892.3086\n",
      "Epoch 4682/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 70.4257 - mae: 71.1124 - val_loss: 9076.2725 - val_mae: 9076.9658\n",
      "Epoch 4683/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 62.6312 - mae: 63.3103 - val_loss: 9162.8613 - val_mae: 9163.5557\n",
      "Epoch 4684/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 71.7125 - mae: 72.3973 - val_loss: 9065.9893 - val_mae: 9066.6816\n",
      "Epoch 4685/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 61.5711 - mae: 62.2526 - val_loss: 9106.0088 - val_mae: 9106.7021\n",
      "Epoch 4686/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 56.9329 - mae: 57.6138 - val_loss: 9077.2754 - val_mae: 9077.9688\n",
      "Epoch 4687/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 67.5293 - mae: 68.2092 - val_loss: 9187.9746 - val_mae: 9188.6680\n",
      "Epoch 4688/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 66.1306 - mae: 66.8159 - val_loss: 9180.5557 - val_mae: 9181.2500\n",
      "Epoch 4689/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 63.7870 - mae: 64.4731 - val_loss: 9112.0371 - val_mae: 9112.7295\n",
      "Epoch 4690/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 59.7046 - mae: 60.3872 - val_loss: 9014.7832 - val_mae: 9015.4775\n",
      "Epoch 4691/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 60.6277 - mae: 61.3142 - val_loss: 9030.3721 - val_mae: 9031.0654\n",
      "Epoch 4692/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 60.0562 - mae: 60.7410 - val_loss: 9128.5762 - val_mae: 9129.2686\n",
      "Epoch 4693/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 69.3067 - mae: 69.9873 - val_loss: 9095.0146 - val_mae: 9095.7080\n",
      "Epoch 4694/5000\n",
      "46/46 [==============================] - 2s 40ms/step - loss: 60.8969 - mae: 61.5791 - val_loss: 9136.0000 - val_mae: 9136.6924\n",
      "Epoch 4695/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 57.4461 - mae: 58.1271 - val_loss: 9124.9287 - val_mae: 9125.6230\n",
      "Epoch 4696/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 63.5210 - mae: 64.2003 - val_loss: 8990.2285 - val_mae: 8990.9219\n",
      "Epoch 4697/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 58.4427 - mae: 59.1241 - val_loss: 8938.0293 - val_mae: 8938.7236\n",
      "Epoch 4698/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 67.4288 - mae: 68.1106 - val_loss: 9233.1504 - val_mae: 9233.8457\n",
      "Epoch 4699/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 56.9022 - mae: 57.5875 - val_loss: 8946.8604 - val_mae: 8947.5547\n",
      "Epoch 4700/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 58.5749 - mae: 59.2511 - val_loss: 9145.5088 - val_mae: 9146.2021\n",
      "Epoch 4701/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 69.7154 - mae: 70.3983 - val_loss: 9234.5137 - val_mae: 9235.2061\n",
      "Epoch 4702/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 72.4261 - mae: 73.1116 - val_loss: 9183.6943 - val_mae: 9184.3877\n",
      "Epoch 4703/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 65.0622 - mae: 65.7453 - val_loss: 9166.9951 - val_mae: 9167.6865\n",
      "Epoch 4704/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 60.9986 - mae: 61.6841 - val_loss: 9108.7715 - val_mae: 9109.4648\n",
      "Epoch 4705/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 60.5563 - mae: 61.2362 - val_loss: 9060.1973 - val_mae: 9060.8896\n",
      "Epoch 4706/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 59.1861 - mae: 59.8650 - val_loss: 9140.1250 - val_mae: 9140.8193\n",
      "Epoch 4707/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 59.0112 - mae: 59.6943 - val_loss: 9088.1973 - val_mae: 9088.8896\n",
      "Epoch 4708/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 62.6327 - mae: 63.3189 - val_loss: 9118.6963 - val_mae: 9119.3877\n",
      "Epoch 4709/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 59.6785 - mae: 60.3586 - val_loss: 9182.4365 - val_mae: 9183.1299\n",
      "Epoch 4710/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 62.5027 - mae: 63.1896 - val_loss: 9075.3965 - val_mae: 9076.0898\n",
      "Epoch 4711/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 56.6801 - mae: 57.3608 - val_loss: 9096.7812 - val_mae: 9097.4746\n",
      "Epoch 4712/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 60.0936 - mae: 60.7752 - val_loss: 9175.1553 - val_mae: 9175.8486\n",
      "Epoch 4713/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 56.8263 - mae: 57.5029 - val_loss: 9117.2061 - val_mae: 9117.9014\n",
      "Epoch 4714/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 64.8184 - mae: 65.5028 - val_loss: 9082.7363 - val_mae: 9083.4287\n",
      "Epoch 4715/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 59.0660 - mae: 59.7525 - val_loss: 9104.0811 - val_mae: 9104.7744\n",
      "Epoch 4716/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 64.2045 - mae: 64.8858 - val_loss: 9104.0908 - val_mae: 9104.7842\n",
      "Epoch 4717/5000\n",
      "46/46 [==============================] - 2s 45ms/step - loss: 57.5495 - mae: 58.2307 - val_loss: 9156.2256 - val_mae: 9156.9180\n",
      "Epoch 4718/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 63.9470 - mae: 64.6321 - val_loss: 9072.3828 - val_mae: 9073.0762\n",
      "Epoch 4719/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 55.2464 - mae: 55.9256 - val_loss: 9146.8477 - val_mae: 9147.5410\n",
      "Epoch 4720/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 54.5405 - mae: 55.2194 - val_loss: 9103.2285 - val_mae: 9103.9229\n",
      "Epoch 4721/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 61.7618 - mae: 62.4460 - val_loss: 9169.4941 - val_mae: 9170.1885\n",
      "Epoch 4722/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 55.8903 - mae: 56.5743 - val_loss: 9101.0352 - val_mae: 9101.7285\n",
      "Epoch 4723/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 65.9869 - mae: 66.6688 - val_loss: 9038.1670 - val_mae: 9038.8604\n",
      "Epoch 4724/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 61.0123 - mae: 61.6939 - val_loss: 9104.6191 - val_mae: 9105.3125\n",
      "Epoch 4725/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 54.6740 - mae: 55.3602 - val_loss: 9028.7148 - val_mae: 9029.4072\n",
      "Epoch 4726/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 68.6679 - mae: 69.3511 - val_loss: 9145.7656 - val_mae: 9146.4600\n",
      "Epoch 4727/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 57.3472 - mae: 58.0313 - val_loss: 9054.7627 - val_mae: 9055.4570\n",
      "Epoch 4728/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 69.7010 - mae: 70.3848 - val_loss: 9206.7061 - val_mae: 9207.4004\n",
      "Epoch 4729/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 67.8213 - mae: 68.5045 - val_loss: 9038.6035 - val_mae: 9039.2969\n",
      "Epoch 4730/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 62.4581 - mae: 63.1366 - val_loss: 9114.7295 - val_mae: 9115.4229\n",
      "Epoch 4731/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 58.1455 - mae: 58.8265 - val_loss: 9068.1699 - val_mae: 9068.8633\n",
      "Epoch 4732/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 58.7216 - mae: 59.3984 - val_loss: 9053.3057 - val_mae: 9054.0000\n",
      "Epoch 4733/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 70.8686 - mae: 71.5526 - val_loss: 9132.5439 - val_mae: 9133.2363\n",
      "Epoch 4734/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 59.2567 - mae: 59.9383 - val_loss: 9077.7334 - val_mae: 9078.4258\n",
      "Epoch 4735/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 58.6996 - mae: 59.3772 - val_loss: 9020.3203 - val_mae: 9021.0137\n",
      "Epoch 4736/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 60.6751 - mae: 61.3554 - val_loss: 9163.4170 - val_mae: 9164.1104\n",
      "Epoch 4737/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 62.1462 - mae: 62.8286 - val_loss: 9198.7109 - val_mae: 9199.4033\n",
      "Epoch 4738/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 60.6034 - mae: 61.2843 - val_loss: 8992.9473 - val_mae: 8993.6416\n",
      "Epoch 4739/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 68.5279 - mae: 69.2120 - val_loss: 9149.2520 - val_mae: 9149.9453\n",
      "Epoch 4740/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 63.3761 - mae: 64.0642 - val_loss: 9168.2529 - val_mae: 9168.9463\n",
      "Epoch 4741/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 69.9916 - mae: 70.6800 - val_loss: 9066.2715 - val_mae: 9066.9639\n",
      "Epoch 4742/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 58.0446 - mae: 58.7278 - val_loss: 9129.7432 - val_mae: 9130.4375\n",
      "Epoch 4743/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 55.2302 - mae: 55.9116 - val_loss: 9020.0088 - val_mae: 9020.7021\n",
      "Epoch 4744/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 58.8957 - mae: 59.5767 - val_loss: 9219.0508 - val_mae: 9219.7461\n",
      "Epoch 4745/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 70.8336 - mae: 71.5130 - val_loss: 9163.8877 - val_mae: 9164.5801\n",
      "Epoch 4746/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 63.0431 - mae: 63.7284 - val_loss: 9119.7666 - val_mae: 9120.4590\n",
      "Epoch 4747/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 65.1991 - mae: 65.8810 - val_loss: 9041.0322 - val_mae: 9041.7266\n",
      "Epoch 4748/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 69.1099 - mae: 69.7928 - val_loss: 9088.2627 - val_mae: 9088.9561\n",
      "Epoch 4749/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 62.0623 - mae: 62.7404 - val_loss: 9047.1484 - val_mae: 9047.8398\n",
      "Epoch 4750/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 53.0295 - mae: 53.7113 - val_loss: 9099.8926 - val_mae: 9100.5859\n",
      "Epoch 4751/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 59.9341 - mae: 60.6137 - val_loss: 9067.9844 - val_mae: 9068.6777\n",
      "Epoch 4752/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 59.1627 - mae: 59.8401 - val_loss: 9034.5508 - val_mae: 9035.2432\n",
      "Epoch 4753/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 62.3016 - mae: 62.9837 - val_loss: 9058.0000 - val_mae: 9058.6934\n",
      "Epoch 4754/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 57.7159 - mae: 58.3954 - val_loss: 8988.3203 - val_mae: 8989.0127\n",
      "Epoch 4755/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 64.1156 - mae: 64.7986 - val_loss: 9201.5176 - val_mae: 9202.2109\n",
      "Epoch 4756/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 64.9963 - mae: 65.6773 - val_loss: 9141.1299 - val_mae: 9141.8223\n",
      "Epoch 4757/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 56.1165 - mae: 56.7929 - val_loss: 9098.3076 - val_mae: 9099.0000\n",
      "Epoch 4758/5000\n",
      "46/46 [==============================] - 2s 49ms/step - loss: 66.6520 - mae: 67.3371 - val_loss: 9067.7041 - val_mae: 9068.3965\n",
      "Epoch 4759/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 58.1982 - mae: 58.8790 - val_loss: 9091.6738 - val_mae: 9092.3672\n",
      "Epoch 4760/5000\n",
      "46/46 [==============================] - 2s 43ms/step - loss: 78.7583 - mae: 79.4464 - val_loss: 9253.4990 - val_mae: 9254.1924\n",
      "Epoch 4761/5000\n",
      "46/46 [==============================] - 2s 43ms/step - loss: 68.1018 - mae: 68.7866 - val_loss: 9205.3740 - val_mae: 9206.0664\n",
      "Epoch 4762/5000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 57.0559 - mae: 57.7347 - val_loss: 9015.7246 - val_mae: 9016.4160\n",
      "Epoch 4763/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 57.3006 - mae: 57.9839 - val_loss: 9094.9619 - val_mae: 9095.6553\n",
      "Epoch 4764/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 74.5904 - mae: 75.2680 - val_loss: 9005.0459 - val_mae: 9005.7393\n",
      "Epoch 4765/5000\n",
      "46/46 [==============================] - 2s 42ms/step - loss: 86.5105 - mae: 87.1939 - val_loss: 9035.7852 - val_mae: 9036.4785\n",
      "Epoch 4766/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 65.1271 - mae: 65.8084 - val_loss: 9147.9844 - val_mae: 9148.6777\n",
      "Epoch 4767/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 55.4368 - mae: 56.1197 - val_loss: 9111.5225 - val_mae: 9112.2158\n",
      "Epoch 4768/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 61.1237 - mae: 61.8032 - val_loss: 9043.3193 - val_mae: 9044.0117\n",
      "Epoch 4769/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 65.8486 - mae: 66.5331 - val_loss: 9185.5928 - val_mae: 9186.2861\n",
      "Epoch 4770/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 53.6727 - mae: 54.3512 - val_loss: 9105.1816 - val_mae: 9105.8760\n",
      "Epoch 4771/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 69.1626 - mae: 69.8480 - val_loss: 9145.5430 - val_mae: 9146.2354\n",
      "Epoch 4772/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 68.5044 - mae: 69.1881 - val_loss: 9092.3584 - val_mae: 9093.0527\n",
      "Epoch 4773/5000\n",
      "46/46 [==============================] - 2s 40ms/step - loss: 62.2503 - mae: 62.9328 - val_loss: 9195.9922 - val_mae: 9196.6846\n",
      "Epoch 4774/5000\n",
      "46/46 [==============================] - 2s 39ms/step - loss: 56.8665 - mae: 57.5503 - val_loss: 9067.3564 - val_mae: 9068.0508\n",
      "Epoch 4775/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 62.6036 - mae: 63.2842 - val_loss: 9142.8633 - val_mae: 9143.5576\n",
      "Epoch 4776/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 52.2261 - mae: 52.9035 - val_loss: 9117.5020 - val_mae: 9118.1963\n",
      "Epoch 4777/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 67.0989 - mae: 67.7814 - val_loss: 8994.2090 - val_mae: 8994.9033\n",
      "Epoch 4778/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 56.5316 - mae: 57.2138 - val_loss: 8983.3672 - val_mae: 8984.0615\n",
      "Epoch 4779/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 55.5574 - mae: 56.2414 - val_loss: 9068.2764 - val_mae: 9068.9697\n",
      "Epoch 4780/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 63.9594 - mae: 64.6451 - val_loss: 9072.6699 - val_mae: 9073.3633\n",
      "Epoch 4781/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 61.9919 - mae: 62.6774 - val_loss: 9018.0205 - val_mae: 9018.7129\n",
      "Epoch 4782/5000\n",
      "46/46 [==============================] - 2s 41ms/step - loss: 75.2454 - mae: 75.9288 - val_loss: 9242.2734 - val_mae: 9242.9658\n",
      "Epoch 4783/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 67.0211 - mae: 67.7051 - val_loss: 9104.4170 - val_mae: 9105.1094\n",
      "Epoch 4784/5000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 73.6236 - mae: 74.3059 - val_loss: 9163.9336 - val_mae: 9164.6260\n",
      "Epoch 4785/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 72.1653 - mae: 72.8469 - val_loss: 9140.1982 - val_mae: 9140.8916\n",
      "Epoch 4786/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 59.6984 - mae: 60.3787 - val_loss: 9138.2373 - val_mae: 9138.9307\n",
      "Epoch 4787/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 69.0366 - mae: 69.7158 - val_loss: 9207.8184 - val_mae: 9208.5127\n",
      "Epoch 4788/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 61.5666 - mae: 62.2465 - val_loss: 9032.4639 - val_mae: 9033.1572\n",
      "Epoch 4789/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 77.4070 - mae: 78.0948 - val_loss: 9154.9482 - val_mae: 9155.6406\n",
      "Epoch 4790/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 62.0218 - mae: 62.7046 - val_loss: 9207.1533 - val_mae: 9207.8486\n",
      "Epoch 4791/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 64.2255 - mae: 64.9063 - val_loss: 9083.3848 - val_mae: 9084.0771\n",
      "Epoch 4792/5000\n",
      "46/46 [==============================] - 2s 38ms/step - loss: 65.0355 - mae: 65.7207 - val_loss: 9145.2607 - val_mae: 9145.9531\n",
      "Epoch 4793/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 57.7332 - mae: 58.4171 - val_loss: 9022.0186 - val_mae: 9022.7109\n",
      "Epoch 4794/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 64.2756 - mae: 64.9582 - val_loss: 8994.9531 - val_mae: 8995.6465\n",
      "Epoch 4795/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 65.0193 - mae: 65.7006 - val_loss: 9028.1982 - val_mae: 9028.8906\n",
      "Epoch 4796/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 53.9214 - mae: 54.5987 - val_loss: 9162.3350 - val_mae: 9163.0283\n",
      "Epoch 4797/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 60.3276 - mae: 61.0076 - val_loss: 9052.1445 - val_mae: 9052.8389\n",
      "Epoch 4798/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 55.7434 - mae: 56.4239 - val_loss: 9142.9424 - val_mae: 9143.6357\n",
      "Epoch 4799/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 63.2372 - mae: 63.9215 - val_loss: 9073.6641 - val_mae: 9074.3584\n",
      "Epoch 4800/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 66.3992 - mae: 67.0820 - val_loss: 9083.8467 - val_mae: 9084.5400\n",
      "Epoch 4801/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 58.6865 - mae: 59.3694 - val_loss: 9105.0137 - val_mae: 9105.7061\n",
      "Epoch 4802/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 66.7080 - mae: 67.3928 - val_loss: 9070.5371 - val_mae: 9071.2305\n",
      "Epoch 4803/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 60.0882 - mae: 60.7729 - val_loss: 9032.4814 - val_mae: 9033.1748\n",
      "Epoch 4804/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 53.7615 - mae: 54.4440 - val_loss: 8966.5830 - val_mae: 8967.2773\n",
      "Epoch 4805/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 60.4361 - mae: 61.1186 - val_loss: 9117.7412 - val_mae: 9118.4355\n",
      "Epoch 4806/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 58.3238 - mae: 59.0101 - val_loss: 9010.0420 - val_mae: 9010.7344\n",
      "Epoch 4807/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 60.3963 - mae: 61.0786 - val_loss: 9122.4629 - val_mae: 9123.1562\n",
      "Epoch 4808/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 59.4557 - mae: 60.1394 - val_loss: 8942.4512 - val_mae: 8943.1455\n",
      "Epoch 4809/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 79.9601 - mae: 80.6436 - val_loss: 9028.4307 - val_mae: 9029.1230\n",
      "Epoch 4810/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 68.8364 - mae: 69.5196 - val_loss: 9076.1523 - val_mae: 9076.8467\n",
      "Epoch 4811/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 67.0180 - mae: 67.7022 - val_loss: 9238.5166 - val_mae: 9239.2090\n",
      "Epoch 4812/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 64.3329 - mae: 65.0211 - val_loss: 9175.5283 - val_mae: 9176.2217\n",
      "Epoch 4813/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 58.3254 - mae: 59.0113 - val_loss: 9323.4512 - val_mae: 9324.1455\n",
      "Epoch 4814/5000\n",
      "46/46 [==============================] - 2s 50ms/step - loss: 66.2773 - mae: 66.9567 - val_loss: 8999.7500 - val_mae: 9000.4434\n",
      "Epoch 4815/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 64.1868 - mae: 64.8691 - val_loss: 9167.0088 - val_mae: 9167.7021\n",
      "Epoch 4816/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 54.6652 - mae: 55.3452 - val_loss: 9190.0107 - val_mae: 9190.7051\n",
      "Epoch 4817/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 65.4811 - mae: 66.1648 - val_loss: 9131.1797 - val_mae: 9131.8721\n",
      "Epoch 4818/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 67.0133 - mae: 67.6937 - val_loss: 8988.0781 - val_mae: 8988.7715\n",
      "Epoch 4819/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 61.8381 - mae: 62.5176 - val_loss: 9209.6611 - val_mae: 9210.3535\n",
      "Epoch 4820/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 61.6005 - mae: 62.2848 - val_loss: 9148.6113 - val_mae: 9149.3037\n",
      "Epoch 4821/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 60.7529 - mae: 61.4323 - val_loss: 9071.3652 - val_mae: 9072.0586\n",
      "Epoch 4822/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 58.2827 - mae: 58.9676 - val_loss: 9204.8213 - val_mae: 9205.5137\n",
      "Epoch 4823/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 58.8950 - mae: 59.5720 - val_loss: 9124.4316 - val_mae: 9125.1250\n",
      "Epoch 4824/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 53.5362 - mae: 54.2162 - val_loss: 9201.3008 - val_mae: 9201.9941\n",
      "Epoch 4825/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 58.8551 - mae: 59.5350 - val_loss: 9203.9238 - val_mae: 9204.6191\n",
      "Epoch 4826/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 74.8811 - mae: 75.5658 - val_loss: 9344.7451 - val_mae: 9345.4404\n",
      "Epoch 4827/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 74.7130 - mae: 75.3976 - val_loss: 9231.9844 - val_mae: 9232.6777\n",
      "Epoch 4828/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 57.2844 - mae: 57.9664 - val_loss: 9104.9961 - val_mae: 9105.6885\n",
      "Epoch 4829/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 53.2805 - mae: 53.9634 - val_loss: 9161.1074 - val_mae: 9161.8008\n",
      "Epoch 4830/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 63.9597 - mae: 64.6430 - val_loss: 9169.4795 - val_mae: 9170.1738\n",
      "Epoch 4831/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 72.0447 - mae: 72.7264 - val_loss: 8979.2734 - val_mae: 8979.9658\n",
      "Epoch 4832/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 65.6769 - mae: 66.3558 - val_loss: 9331.1143 - val_mae: 9331.8076\n",
      "Epoch 4833/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 67.1215 - mae: 67.8068 - val_loss: 9128.6143 - val_mae: 9129.3076\n",
      "Epoch 4834/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 71.9017 - mae: 72.5851 - val_loss: 9163.0566 - val_mae: 9163.7500\n",
      "Epoch 4835/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 54.6900 - mae: 55.3697 - val_loss: 8915.4268 - val_mae: 8916.1201\n",
      "Epoch 4836/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 59.7349 - mae: 60.4200 - val_loss: 9086.7891 - val_mae: 9087.4824\n",
      "Epoch 4837/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 53.5479 - mae: 54.2260 - val_loss: 9120.4707 - val_mae: 9121.1650\n",
      "Epoch 4838/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 59.5061 - mae: 60.1876 - val_loss: 9065.3818 - val_mae: 9066.0752\n",
      "Epoch 4839/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 60.8794 - mae: 61.5645 - val_loss: 9197.2793 - val_mae: 9197.9736\n",
      "Epoch 4840/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 65.8512 - mae: 66.5343 - val_loss: 9253.1504 - val_mae: 9253.8428\n",
      "Epoch 4841/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 71.8719 - mae: 72.5569 - val_loss: 9094.5078 - val_mae: 9095.1992\n",
      "Epoch 4842/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 58.7908 - mae: 59.4710 - val_loss: 9197.4629 - val_mae: 9198.1543\n",
      "Epoch 4843/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 60.4696 - mae: 61.1521 - val_loss: 9162.9062 - val_mae: 9163.5977\n",
      "Epoch 4844/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 70.1770 - mae: 70.8604 - val_loss: 9178.1992 - val_mae: 9178.8936\n",
      "Epoch 4845/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 68.4845 - mae: 69.1646 - val_loss: 9022.1533 - val_mae: 9022.8467\n",
      "Epoch 4846/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 66.0522 - mae: 66.7403 - val_loss: 9036.2334 - val_mae: 9036.9277\n",
      "Epoch 4847/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 60.8489 - mae: 61.5299 - val_loss: 9000.4365 - val_mae: 9001.1289\n",
      "Epoch 4848/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 56.3018 - mae: 56.9816 - val_loss: 9112.6553 - val_mae: 9113.3496\n",
      "Epoch 4849/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 48.0409 - mae: 48.7192 - val_loss: 9070.9658 - val_mae: 9071.6602\n",
      "Epoch 4850/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 66.6686 - mae: 67.3481 - val_loss: 9070.7129 - val_mae: 9071.4053\n",
      "Epoch 4851/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 67.4680 - mae: 68.1472 - val_loss: 8958.9424 - val_mae: 8959.6348\n",
      "Epoch 4852/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 73.3019 - mae: 73.9890 - val_loss: 9120.2021 - val_mae: 9120.8955\n",
      "Epoch 4853/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 62.8595 - mae: 63.5456 - val_loss: 9072.6621 - val_mae: 9073.3555\n",
      "Epoch 4854/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 54.2819 - mae: 54.9609 - val_loss: 9143.2168 - val_mae: 9143.9092\n",
      "Epoch 4855/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 52.8406 - mae: 53.5163 - val_loss: 9021.1797 - val_mae: 9021.8740\n",
      "Epoch 4856/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 69.0585 - mae: 69.7408 - val_loss: 9067.4512 - val_mae: 9068.1455\n",
      "Epoch 4857/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 53.6001 - mae: 54.2794 - val_loss: 9087.1553 - val_mae: 9087.8496\n",
      "Epoch 4858/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 55.2940 - mae: 55.9709 - val_loss: 9131.4365 - val_mae: 9132.1299\n",
      "Epoch 4859/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 50.9336 - mae: 51.6140 - val_loss: 9018.1055 - val_mae: 9018.7979\n",
      "Epoch 4860/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 61.4872 - mae: 62.1708 - val_loss: 9087.6680 - val_mae: 9088.3613\n",
      "Epoch 4861/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 58.9787 - mae: 59.6582 - val_loss: 9174.0469 - val_mae: 9174.7402\n",
      "Epoch 4862/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 68.9271 - mae: 69.6125 - val_loss: 9079.4326 - val_mae: 9080.1260\n",
      "Epoch 4863/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 73.6044 - mae: 74.2894 - val_loss: 9089.0850 - val_mae: 9089.7773\n",
      "Epoch 4864/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 60.8971 - mae: 61.5778 - val_loss: 9194.1504 - val_mae: 9194.8438\n",
      "Epoch 4865/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 57.9419 - mae: 58.6241 - val_loss: 9215.1416 - val_mae: 9215.8340\n",
      "Epoch 4866/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 64.9740 - mae: 65.6527 - val_loss: 8995.1846 - val_mae: 8995.8779\n",
      "Epoch 4867/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 59.2279 - mae: 59.9113 - val_loss: 9165.7002 - val_mae: 9166.3936\n",
      "Epoch 4868/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 53.9503 - mae: 54.6307 - val_loss: 9146.4531 - val_mae: 9147.1465\n",
      "Epoch 4869/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 55.8148 - mae: 56.4969 - val_loss: 9178.3574 - val_mae: 9179.0518\n",
      "Epoch 4870/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 59.8838 - mae: 60.5655 - val_loss: 9227.1309 - val_mae: 9227.8232\n",
      "Epoch 4871/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 62.2167 - mae: 62.8965 - val_loss: 9083.7256 - val_mae: 9084.4199\n",
      "Epoch 4872/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 61.8812 - mae: 62.5610 - val_loss: 9077.5498 - val_mae: 9078.2432\n",
      "Epoch 4873/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 83.4312 - mae: 84.1144 - val_loss: 9131.9619 - val_mae: 9132.6543\n",
      "Epoch 4874/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 61.6472 - mae: 62.3325 - val_loss: 9018.5557 - val_mae: 9019.2480\n",
      "Epoch 4875/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 68.8030 - mae: 69.4889 - val_loss: 9194.2949 - val_mae: 9194.9873\n",
      "Epoch 4876/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 71.9471 - mae: 72.6299 - val_loss: 9188.5449 - val_mae: 9189.2373\n",
      "Epoch 4877/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 63.2172 - mae: 63.9025 - val_loss: 9083.7891 - val_mae: 9084.4824\n",
      "Epoch 4878/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 57.0558 - mae: 57.7371 - val_loss: 9138.8184 - val_mae: 9139.5107\n",
      "Epoch 4879/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 71.1812 - mae: 71.8659 - val_loss: 9124.5518 - val_mae: 9125.2451\n",
      "Epoch 4880/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 65.2330 - mae: 65.9153 - val_loss: 9197.5703 - val_mae: 9198.2627\n",
      "Epoch 4881/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 74.0887 - mae: 74.7726 - val_loss: 9089.6162 - val_mae: 9090.3086\n",
      "Epoch 4882/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 56.1889 - mae: 56.8661 - val_loss: 9145.6191 - val_mae: 9146.3125\n",
      "Epoch 4883/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 56.7120 - mae: 57.3908 - val_loss: 9127.2910 - val_mae: 9127.9854\n",
      "Epoch 4884/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 60.0967 - mae: 60.7818 - val_loss: 9110.2275 - val_mae: 9110.9199\n",
      "Epoch 4885/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 62.0874 - mae: 62.7690 - val_loss: 9151.0312 - val_mae: 9151.7227\n",
      "Epoch 4886/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 64.6040 - mae: 65.2897 - val_loss: 9103.8037 - val_mae: 9104.4961\n",
      "Epoch 4887/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 56.9247 - mae: 57.6021 - val_loss: 9083.1992 - val_mae: 9083.8906\n",
      "Epoch 4888/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 56.8001 - mae: 57.4804 - val_loss: 9165.4922 - val_mae: 9166.1855\n",
      "Epoch 4889/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 64.0220 - mae: 64.7048 - val_loss: 9069.6631 - val_mae: 9070.3555\n",
      "Epoch 4890/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 54.5577 - mae: 55.2343 - val_loss: 9016.6924 - val_mae: 9017.3867\n",
      "Epoch 4891/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 56.8081 - mae: 57.4893 - val_loss: 9178.9033 - val_mae: 9179.5957\n",
      "Epoch 4892/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 59.0724 - mae: 59.7572 - val_loss: 9107.5088 - val_mae: 9108.2021\n",
      "Epoch 4893/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 58.8999 - mae: 59.5754 - val_loss: 8964.1807 - val_mae: 8964.8750\n",
      "Epoch 4894/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 62.7163 - mae: 63.4018 - val_loss: 9184.5234 - val_mae: 9185.2168\n",
      "Epoch 4895/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 71.9956 - mae: 72.6800 - val_loss: 9115.3281 - val_mae: 9116.0215\n",
      "Epoch 4896/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 61.9268 - mae: 62.6082 - val_loss: 9085.1865 - val_mae: 9085.8789\n",
      "Epoch 4897/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 60.2966 - mae: 60.9753 - val_loss: 9065.0244 - val_mae: 9065.7168\n",
      "Epoch 4898/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 57.7976 - mae: 58.4730 - val_loss: 9082.4717 - val_mae: 9083.1660\n",
      "Epoch 4899/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 54.2521 - mae: 54.9333 - val_loss: 9022.5430 - val_mae: 9023.2354\n",
      "Epoch 4900/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 68.8987 - mae: 69.5837 - val_loss: 9120.3906 - val_mae: 9121.0850\n",
      "Epoch 4901/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 61.2184 - mae: 61.8999 - val_loss: 9039.0537 - val_mae: 9039.7471\n",
      "Epoch 4902/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 59.4392 - mae: 60.1201 - val_loss: 9148.0654 - val_mae: 9148.7578\n",
      "Epoch 4903/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 64.5407 - mae: 65.2183 - val_loss: 9092.8281 - val_mae: 9093.5215\n",
      "Epoch 4904/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 66.0220 - mae: 66.7043 - val_loss: 9116.9775 - val_mae: 9117.6699\n",
      "Epoch 4905/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 57.0048 - mae: 57.6868 - val_loss: 9052.5674 - val_mae: 9053.2617\n",
      "Epoch 4906/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 61.8882 - mae: 62.5716 - val_loss: 9135.1729 - val_mae: 9135.8682\n",
      "Epoch 4907/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 52.6807 - mae: 53.3622 - val_loss: 9095.5723 - val_mae: 9096.2676\n",
      "Epoch 4908/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 61.7058 - mae: 62.3903 - val_loss: 9122.6162 - val_mae: 9123.3086\n",
      "Epoch 4909/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 56.8777 - mae: 57.5574 - val_loss: 9168.3516 - val_mae: 9169.0449\n",
      "Epoch 4910/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 57.7363 - mae: 58.4153 - val_loss: 9224.8398 - val_mae: 9225.5322\n",
      "Epoch 4911/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 57.1313 - mae: 57.8081 - val_loss: 9088.7666 - val_mae: 9089.4600\n",
      "Epoch 4912/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 55.6381 - mae: 56.3195 - val_loss: 9117.3154 - val_mae: 9118.0098\n",
      "Epoch 4913/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 59.7466 - mae: 60.4269 - val_loss: 9145.9111 - val_mae: 9146.6045\n",
      "Epoch 4914/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 65.8969 - mae: 66.5812 - val_loss: 9105.1260 - val_mae: 9105.8184\n",
      "Epoch 4915/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 63.2619 - mae: 63.9452 - val_loss: 9154.1172 - val_mae: 9154.8105\n",
      "Epoch 4916/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 56.7644 - mae: 57.4455 - val_loss: 9094.6641 - val_mae: 9095.3564\n",
      "Epoch 4917/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 63.1807 - mae: 63.8604 - val_loss: 9160.0947 - val_mae: 9160.7871\n",
      "Epoch 4918/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 57.0810 - mae: 57.7631 - val_loss: 9042.5059 - val_mae: 9043.1992\n",
      "Epoch 4919/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 57.5419 - mae: 58.2205 - val_loss: 9181.4463 - val_mae: 9182.1396\n",
      "Epoch 4920/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 71.0336 - mae: 71.7164 - val_loss: 9223.6230 - val_mae: 9224.3154\n",
      "Epoch 4921/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 50.7966 - mae: 51.4784 - val_loss: 9207.4863 - val_mae: 9208.1787\n",
      "Epoch 4922/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 66.3860 - mae: 67.0727 - val_loss: 9137.2305 - val_mae: 9137.9238\n",
      "Epoch 4923/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 70.5856 - mae: 71.2655 - val_loss: 9112.1270 - val_mae: 9112.8203\n",
      "Epoch 4924/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 55.2776 - mae: 55.9598 - val_loss: 9086.4502 - val_mae: 9087.1436\n",
      "Epoch 4925/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 67.0022 - mae: 67.6826 - val_loss: 9073.8574 - val_mae: 9074.5498\n",
      "Epoch 4926/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 70.0089 - mae: 70.6928 - val_loss: 9149.7100 - val_mae: 9150.4033\n",
      "Epoch 4927/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 61.0510 - mae: 61.7317 - val_loss: 9181.4453 - val_mae: 9182.1387\n",
      "Epoch 4928/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 66.0672 - mae: 66.7528 - val_loss: 9074.1064 - val_mae: 9074.7988\n",
      "Epoch 4929/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 56.6071 - mae: 57.2877 - val_loss: 9144.7021 - val_mae: 9145.3955\n",
      "Epoch 4930/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 61.1240 - mae: 61.8108 - val_loss: 9139.9688 - val_mae: 9140.6611\n",
      "Epoch 4931/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 71.3209 - mae: 72.0015 - val_loss: 9060.4375 - val_mae: 9061.1299\n",
      "Epoch 4932/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 57.1370 - mae: 57.8216 - val_loss: 9076.0723 - val_mae: 9076.7656\n",
      "Epoch 4933/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 57.3989 - mae: 58.0796 - val_loss: 9068.1953 - val_mae: 9068.8887\n",
      "Epoch 4934/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 66.2194 - mae: 66.9075 - val_loss: 9117.6953 - val_mae: 9118.3887\n",
      "Epoch 4935/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 66.6739 - mae: 67.3557 - val_loss: 9161.6406 - val_mae: 9162.3340\n",
      "Epoch 4936/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 66.6488 - mae: 67.3311 - val_loss: 9017.2070 - val_mae: 9017.9004\n",
      "Epoch 4937/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 68.1704 - mae: 68.8580 - val_loss: 9156.8496 - val_mae: 9157.5449\n",
      "Epoch 4938/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 70.0004 - mae: 70.6860 - val_loss: 9321.3037 - val_mae: 9321.9990\n",
      "Epoch 4939/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 66.5618 - mae: 67.2441 - val_loss: 9098.5459 - val_mae: 9099.2393\n",
      "Epoch 4940/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 52.2738 - mae: 52.9544 - val_loss: 9087.4482 - val_mae: 9088.1426\n",
      "Epoch 4941/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 52.2064 - mae: 52.8874 - val_loss: 9123.6494 - val_mae: 9124.3418\n",
      "Epoch 4942/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 55.0700 - mae: 55.7523 - val_loss: 9116.9873 - val_mae: 9117.6807\n",
      "Epoch 4943/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 57.4816 - mae: 58.1624 - val_loss: 9186.1123 - val_mae: 9186.8047\n",
      "Epoch 4944/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 57.9183 - mae: 58.5997 - val_loss: 9116.0381 - val_mae: 9116.7305\n",
      "Epoch 4945/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 59.5942 - mae: 60.2786 - val_loss: 9057.4570 - val_mae: 9058.1514\n",
      "Epoch 4946/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 69.5678 - mae: 70.2514 - val_loss: 9104.5889 - val_mae: 9105.2822\n",
      "Epoch 4947/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 56.3882 - mae: 57.0722 - val_loss: 8965.1533 - val_mae: 8965.8467\n",
      "Epoch 4948/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 63.7102 - mae: 64.3864 - val_loss: 9049.8994 - val_mae: 9050.5928\n",
      "Epoch 4949/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 67.0362 - mae: 67.7168 - val_loss: 9030.7910 - val_mae: 9031.4834\n",
      "Epoch 4950/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 58.2998 - mae: 58.9829 - val_loss: 9079.7354 - val_mae: 9080.4287\n",
      "Epoch 4951/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 72.2186 - mae: 72.8995 - val_loss: 9182.9180 - val_mae: 9183.6123\n",
      "Epoch 4952/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 63.9480 - mae: 64.6296 - val_loss: 9065.6621 - val_mae: 9066.3555\n",
      "Epoch 4953/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 54.4679 - mae: 55.1559 - val_loss: 9158.2021 - val_mae: 9158.8955\n",
      "Epoch 4954/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 57.7993 - mae: 58.4783 - val_loss: 8984.1641 - val_mae: 8984.8594\n",
      "Epoch 4955/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 70.1997 - mae: 70.8830 - val_loss: 8949.8916 - val_mae: 8950.5830\n",
      "Epoch 4956/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 61.6013 - mae: 62.2850 - val_loss: 9154.7412 - val_mae: 9155.4326\n",
      "Epoch 4957/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 63.3908 - mae: 64.0710 - val_loss: 9179.6191 - val_mae: 9180.3105\n",
      "Epoch 4958/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 59.6753 - mae: 60.3604 - val_loss: 8941.8584 - val_mae: 8942.5527\n",
      "Epoch 4959/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 65.4764 - mae: 66.1569 - val_loss: 9166.2178 - val_mae: 9166.9111\n",
      "Epoch 4960/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 82.5725 - mae: 83.2607 - val_loss: 9071.7080 - val_mae: 9072.4014\n",
      "Epoch 4961/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 59.7208 - mae: 60.4015 - val_loss: 9125.0586 - val_mae: 9125.7520\n",
      "Epoch 4962/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 54.6264 - mae: 55.3000 - val_loss: 9002.5840 - val_mae: 9003.2783\n",
      "Epoch 4963/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 69.2265 - mae: 69.9038 - val_loss: 9060.3760 - val_mae: 9061.0693\n",
      "Epoch 4964/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 63.6219 - mae: 64.3050 - val_loss: 9111.0850 - val_mae: 9111.7793\n",
      "Epoch 4965/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 63.9579 - mae: 64.6399 - val_loss: 9076.8613 - val_mae: 9077.5537\n",
      "Epoch 4966/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 55.7214 - mae: 56.4048 - val_loss: 9188.4482 - val_mae: 9189.1406\n",
      "Epoch 4967/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 72.9499 - mae: 73.6342 - val_loss: 9059.4844 - val_mae: 9060.1777\n",
      "Epoch 4968/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 64.4042 - mae: 65.0858 - val_loss: 9026.9365 - val_mae: 9027.6299\n",
      "Epoch 4969/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 65.7637 - mae: 66.4459 - val_loss: 9169.0840 - val_mae: 9169.7773\n",
      "Epoch 4970/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 60.9737 - mae: 61.6566 - val_loss: 9180.5928 - val_mae: 9181.2861\n",
      "Epoch 4971/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 62.5694 - mae: 63.2530 - val_loss: 8944.6572 - val_mae: 8945.3516\n",
      "Epoch 4972/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 63.9543 - mae: 64.6368 - val_loss: 9159.4824 - val_mae: 9160.1748\n",
      "Epoch 4973/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 67.4725 - mae: 68.1583 - val_loss: 9085.1260 - val_mae: 9085.8193\n",
      "Epoch 4974/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 62.1165 - mae: 62.7980 - val_loss: 9150.7188 - val_mae: 9151.4121\n",
      "Epoch 4975/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 78.2830 - mae: 78.9639 - val_loss: 9048.9951 - val_mae: 9049.6885\n",
      "Epoch 4976/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 71.0408 - mae: 71.7259 - val_loss: 9028.7393 - val_mae: 9029.4307\n",
      "Epoch 4977/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 53.3723 - mae: 54.0518 - val_loss: 9086.7090 - val_mae: 9087.4014\n",
      "Epoch 4978/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 58.4747 - mae: 59.1538 - val_loss: 9119.9229 - val_mae: 9120.6152\n",
      "Epoch 4979/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 61.3601 - mae: 62.0445 - val_loss: 9237.0918 - val_mae: 9237.7842\n",
      "Epoch 4980/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 76.8063 - mae: 77.4883 - val_loss: 9203.5303 - val_mae: 9204.2227\n",
      "Epoch 4981/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 55.4792 - mae: 56.1570 - val_loss: 9103.2910 - val_mae: 9103.9854\n",
      "Epoch 4982/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 65.4415 - mae: 66.1206 - val_loss: 9092.4756 - val_mae: 9093.1689\n",
      "Epoch 4983/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 64.9427 - mae: 65.6309 - val_loss: 9049.8154 - val_mae: 9050.5078\n",
      "Epoch 4984/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 64.5015 - mae: 65.1886 - val_loss: 9152.3838 - val_mae: 9153.0771\n",
      "Epoch 4985/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 59.3972 - mae: 60.0779 - val_loss: 9126.3848 - val_mae: 9127.0791\n",
      "Epoch 4986/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 54.6275 - mae: 55.3109 - val_loss: 9144.5645 - val_mae: 9145.2578\n",
      "Epoch 4987/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 56.9503 - mae: 57.6343 - val_loss: 9114.9551 - val_mae: 9115.6494\n",
      "Epoch 4988/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 63.4045 - mae: 64.0845 - val_loss: 9004.6768 - val_mae: 9005.3691\n",
      "Epoch 4989/5000\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 62.4826 - mae: 63.1660 - val_loss: 8962.3418 - val_mae: 8963.0352\n",
      "Epoch 4990/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 63.6715 - mae: 64.3575 - val_loss: 9132.9951 - val_mae: 9133.6895\n",
      "Epoch 4991/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 53.1410 - mae: 53.8243 - val_loss: 8975.0107 - val_mae: 8975.7051\n",
      "Epoch 4992/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 69.5096 - mae: 70.1920 - val_loss: 9205.6133 - val_mae: 9206.3076\n",
      "Epoch 4993/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 53.7062 - mae: 54.3861 - val_loss: 9093.4365 - val_mae: 9094.1299\n",
      "Epoch 4994/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 57.5657 - mae: 58.2493 - val_loss: 9078.5137 - val_mae: 9079.2061\n",
      "Epoch 4995/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 61.2296 - mae: 61.9121 - val_loss: 9050.3496 - val_mae: 9051.0420\n",
      "Epoch 4996/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 67.5698 - mae: 68.2512 - val_loss: 9143.8408 - val_mae: 9144.5342\n",
      "Epoch 4997/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 62.9891 - mae: 63.6680 - val_loss: 9023.4912 - val_mae: 9024.1846\n",
      "Epoch 4998/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 63.3127 - mae: 63.9944 - val_loss: 9215.2383 - val_mae: 9215.9307\n",
      "Epoch 4999/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 66.7814 - mae: 67.4640 - val_loss: 9003.2686 - val_mae: 9003.9609\n",
      "Epoch 5000/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 66.5148 - mae: 67.1972 - val_loss: 9079.4990 - val_mae: 9080.1924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc008052220>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(X_train,Y_train,validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2dd3a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "adf8814a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred=regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0b0f090b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9990210045840386"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for check\n",
    "Y_train_pred=regressor.predict(X_train)\n",
    "r2_score(Y_train, Y_train_pred) #training score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e757ef47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2:0.3289147112445364\n"
     ]
    }
   ],
   "source": [
    "r2=r2_score(Y_test[:-30],y_pred[:-30]) #score/ r^2\n",
    "print(f'r2:{r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "00ef1c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2_oos\n",
    "def r2_oos(ret, pred):\n",
    "    sum_of_sq_res = np.nansum(np.power((ret-pred), 2))\n",
    "    sum_of_sq_total = np.nansum(np.power(ret, 2))\n",
    "    \n",
    "    return 1-sum_of_sq_res/sum_of_sq_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8969a6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_oos:0.9181680230673898\n"
     ]
    }
   ],
   "source": [
    "r2_oos = r2_oos(Y_test[:-30], y_pred[:-30])\n",
    "print(f'r2_oos:{r2_oos}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "70b87143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae:8630.756097837935\n",
      "rmse:11084.832551513839\n",
      "mape:24.996031025506138\n"
     ]
    }
   ],
   "source": [
    "mae=mean_absolute_error(Y_test[:-30],y_pred[:-30]) #mae\n",
    "print(f'mae:{mae}')\n",
    "\n",
    "rmse=np.sqrt(mean_squared_error(Y_test[:-30],y_pred[:-30])) #rmse\n",
    "print(f'rmse:{rmse}')\n",
    "\n",
    "mape=mean_absolute_percentage_error(Y_test[:-30],y_pred[:-30]) #mape\n",
    "print(f'mape:{mape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb36caf",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a5641115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y_test</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-06-01</th>\n",
       "      <td>33731.0</td>\n",
       "      <td>44551.234375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-02</th>\n",
       "      <td>33285.0</td>\n",
       "      <td>43986.269531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-03</th>\n",
       "      <td>34298.0</td>\n",
       "      <td>42827.039062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-04</th>\n",
       "      <td>35271.0</td>\n",
       "      <td>43842.292969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-05</th>\n",
       "      <td>34100.0</td>\n",
       "      <td>44061.710938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>22087.283203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>21704.134766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>21419.484375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>21278.337891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>20728.130859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>546 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Y_test        y_pred\n",
       "Date                             \n",
       "2021-06-01  33731.0  44551.234375\n",
       "2021-06-02  33285.0  43986.269531\n",
       "2021-06-03  34298.0  42827.039062\n",
       "2021-06-04  35271.0  43842.292969\n",
       "2021-06-05  34100.0  44061.710938\n",
       "...             ...           ...\n",
       "2022-11-24      NaN  22087.283203\n",
       "2022-11-25      NaN  21704.134766\n",
       "2022-11-26      NaN  21419.484375\n",
       "2022-11-27      NaN  21278.337891\n",
       "2022-11-28      NaN  20728.130859\n",
       "\n",
       "[546 rows x 2 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_df = pd.DataFrame(zip(Y_test,y_pred),columns=['Y_test','y_pred'])\n",
    "pre_df.index = Y_test.index\n",
    "pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4f4bd2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df['pred_returns'] = pre_df['y_pred'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0b94aa95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y_test</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>pred_returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-06-01</th>\n",
       "      <td>33731.0</td>\n",
       "      <td>44551.234375</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-02</th>\n",
       "      <td>33285.0</td>\n",
       "      <td>43986.269531</td>\n",
       "      <td>-0.012681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-03</th>\n",
       "      <td>34298.0</td>\n",
       "      <td>42827.039062</td>\n",
       "      <td>-0.026354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-04</th>\n",
       "      <td>35271.0</td>\n",
       "      <td>43842.292969</td>\n",
       "      <td>0.023706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-05</th>\n",
       "      <td>34100.0</td>\n",
       "      <td>44061.710938</td>\n",
       "      <td>0.005005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>22087.283203</td>\n",
       "      <td>-0.002899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>21704.134766</td>\n",
       "      <td>-0.017347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>21419.484375</td>\n",
       "      <td>-0.013115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>21278.337891</td>\n",
       "      <td>-0.006590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>20728.130859</td>\n",
       "      <td>-0.025858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>546 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Y_test        y_pred  pred_returns\n",
       "Date                                           \n",
       "2021-06-01  33731.0  44551.234375           NaN\n",
       "2021-06-02  33285.0  43986.269531     -0.012681\n",
       "2021-06-03  34298.0  42827.039062     -0.026354\n",
       "2021-06-04  35271.0  43842.292969      0.023706\n",
       "2021-06-05  34100.0  44061.710938      0.005005\n",
       "...             ...           ...           ...\n",
       "2022-11-24      NaN  22087.283203     -0.002899\n",
       "2022-11-25      NaN  21704.134766     -0.017347\n",
       "2022-11-26      NaN  21419.484375     -0.013115\n",
       "2022-11-27      NaN  21278.337891     -0.006590\n",
       "2022-11-28      NaN  20728.130859     -0.025858\n",
       "\n",
       "[546 rows x 3 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0d9674c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkEAAAGhCAYAAABmqGCyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAADA/klEQVR4nOydeXwU5f3HPzOzZ7LZ3CEk3OEMIIecioBY6kVUSm31p2IRbQFrL221FfWnUqu/Vilaj2Kx9awtbUVF8EQDHmAxiMpNOMIRcid7XzPz+2N2Zmf2ym6yIdnk+369lOzsM888+2Sz89nvyYiiKIIgCIIgCKKPwXb3AgiCIAiCILoDEkEEQRAEQfRJSAQRBEEQBNEnIRFEEARBEESfhEQQQRAEQRB9EhJBBEEQBEH0SUgEEQRBEATRJyERRBAEQRBEn0TX3Qvo6YiiCEFIfT1JlmW6ZN6+CO1l6qC9TB20l6mB9jF19KW9ZFkGDMO0O45EUDsIgojmZmdK59TpWOTmZsJmcyEQEFI6d1+D9jJ10F6mDtrL1ED7mDr62l7m5WWC49oXQeQOIwiCIAiiT0IiiCAIgiCIPgmJIIIgCIIg+iQkggiCIAiC6JOQCCIIgiAIok9C2WEEQRAEEQVBEMDzge5eRkoQBAYeDwefzwueT+80eY7TgWVTY8MhEUQQBEEQKkRRhM3WDLfb0d1LSSmNjSwEoXekx5vNFliteQnVAooHiSCCIAiCUCELIIslFwaDsdM32p4CxzFpbwUSRRE+nxcORwsAIDs7v1PzkQgiCIIgiCCCwCsCyGKxdvdyUopOx/aKQokGgxEA4HC0ICsrt1OuMQqMJgiCIIggPM8DCN1oiZ6J/PvpbMwWiSCCIAiCCKO3uMB6K6n6/ZAIIgiCIAiiT0IiiCAIgiCIPgmJIKLXIYrpnf1AEASRCm677Ue47rrvwu/3Rzy3cuWduOqqS+F0xi8DsGnTm3j33be7ZH21taexbt2f0djY0CXzJwKJIKJX4dvzPhwv/Bh84/HuXgpBEES38stf/ga1tafx8svPa47v2PEZPvroA/zsZ3cgM9MSd45Nm97E++93nQj661+fJRFEEKnC+8lLgNcJz7a/dfdSCIIgupVBgwbj+ut/gBde+CtOnToJAPB6vXjssUdw/vkXYO7ci7p5hd0P1QkieimU2UEQBHHDDUvw/vvv4NFHH8Hjjz+JF1/8K5qbm7BmzTPtnvvjH/8QX35ZBQCYNWsKAGDJkluwdOmPAACffvox/vrXZ1FdfRgZGWbMnXsRbr31ZzCbzQCAQCCAP//5SWzZ8h6am5tgtVoxalQ57r33QRw8uB8/+ckyAMDNNy9WrvnxxztT+vrbg0QQ0TtJUV8ZgiAIIFip2N89hQYNerbDKeF6vR6//OVv8JOfLMNzz/0Fr7zyAn70o1tRXFzc7rm3334XHnzwHhiNJtx6688AAEVFRQCADz98H/fd9xtcdlkFli79EZqaGvHMM3+C3W7D/ff/DgDw4ot/xYYN/8by5bdh6NBhaGtrxeefb4ff78OoUaPxi1/cicceewS/+c19GDRoSIdeX2chEUT0ShiyBBEEkSJEUcTvXqrC4VNt3XL94QOy8evrJndYCE2adC4uvXQB1q59CiNHjsZ3v3tNQucNHToMGRmZyMjIwLhx45XjoijiySfXYN68+bjrrnuU43l5efjVr36OG2+8GcOGlWHfvj2YNm06vvOdq5UxahfckCFDAQDDhpVh9OjyDr22zkJfl4neCVmCCIJIJWn+ver6638AALjmmuvBcVyn5jpx4jjOnKnFvHnzEQgElP8mTjwXDMPgwIF9AICRI0fjs88+xbp1f8a+fXt6ZPNWsgQRvZQ0/8QiCKLHwDAMfn3d5LR0h8no9frgv52/7be2tgIAfvObO6I+X1d3BgCwePFNYBgGb7/9Fv7612eRk5OL73znaixZckuPqchNIojonZAliCCIFMIwDIyGzllQegtWazYA4Oc//xXGjh0X8XxBQSEAwGAwYOnSH2Hp0h/h5MkTeOutN/Dcc2tRUlKKSy65/KyuORYkgoheSs/4lkEQBJHO6PV6eL0+zbHBg4egqKgfTp8+hUWLvpfQPAMGDMSPfnQrXn/9Pzh+/JgyN4CI+c8mJIKI3glZggiCIDrN4MFD8fbbG/Hxx1tRUFCAgoJCFBQU4sc//jnuv/9ueDxuzJw5C2azGWfO1OKzzz7GD394KwYNGoxf//p2jBo1BiNGjILZbMYnn2yFzdaGyZOldPuBAweD4zi89dbr4DgWOp3urAdIkwgieicMiSCCIIjOct11i3Hq1AmsWnUfHA67Uido3rxvISvLgueffw7vvrsZAFBc3B/Tp5+HvLx8AMD48ROwZcv7ePXVl8DzPAYOHIz77luFqVOnAwBycnLw85//Cq+88gLeeWcTeJ4/63WCGJEaLcWF5wU0NztTOqdOxyI3NxMtLU4EAj0vWj6dCN9L+9ofSMcHT4L54p927+LSDHpfpg7ay9TQHfvo9/vQ1FSL/Pz+0OsNZ+WaZwudju0178f2fk95eZnguPa/DNPXZaJ30kMyDwiCIIieC7nDiN4JucMIgiDiwvM84jmDdLreLxF6/ysk+iZkCSIIgojL979/Fc6cqY35/NmOz+kOSAQRvROyBBEEQcTlkUdWw+/vvvT0ngCJIKJ3QiKIIAgiLmVlw7t7Cd0O3SmI3gm5wwiCIIh2IBFE9E7IEkQQBEG0A90piF5JT2nORxAEQfRcSAQRvROyBBEEQRDtQHcKondCIoggCIJoB7pTEL0TEkEEQRBEOyR9p3j//fdx9dVXY/LkyTjvvPPw4x//GEeOHIkYV1lZiauuugrjx4/H/Pnz8fLLL0edb926dZg3bx7Gjx+PRYsWYceOHRFjHA4H7r33XkyfPh2TJk3CsmXLcOrUqYhxR48exdKlSzFx4kTMnDkTq1atgsfjSfYlEr0BigkiCIJIS/bv34tZs6agqqrrizUmJYI+/fRT/PjHP8bQoUPxxBNP4J577sHRo0exZMkSOBwOZdyuXbuwYsUKlJeX49lnn8XChQuxatUqrF+/XjPfunXrsHr1alx33XVYu3YtBg8ejFtuuQUHDhzQjLv99tuxZcsW3HPPPVi9ejXq6+uxZMkSjcCx2Wy48cYb4XQ68fjjj+POO+/Em2++iZUrV3ZkX4g0RBRVjQHJEkQQBEG0Q1LFEt966y2UlJTgkUceUbJvSktLcfXVV+OLL77AnDlzAABPPvkkysvL8dBDDwEAZsyYgdraWqxZswaLFi0Cy7Lw+Xx4+umnsXjxYixduhQAMG3aNFRUVOCZZ57B6tWrAQC7d+/GRx99hLVr1yrzjxw5EvPnz8drr72Ga6+9FgDw6quvwmazYcOGDcjLywMAcByHO+64A8uXL0dZWVln94ro6QhqEUSWIIIgiO7A6/XAaDR19zISIqmvy4FAAJmZmZr046ysLM0Yn8+H7du34/LLL9ccr6ioQENDA/bu3QsAqKqqgt1ux4IFC5QxHMfhsssuQ2VlpdLUrbKyElarFbNnz1bGlZSUYPLkyaisrFSObd26FTNnzlQEEABcfPHFMBgMmnFEL4YsQQRBdBGiKEL0e7vnvzhNTmPx8ceVmDVrCk6cqNEcdzgcuOii87F+/avtzjFr1hS8+OLf8NRTa7Bgwbcwf/5s/Pa3/wuXy6mMqaraiVmzpuDTTz/GypW/wre/PQf33HMXAMBut+MPf3gYV155MS68cCZuuul6fP759ojr/O1vf8EVV1yM+fMvwG9+80u0tDQn/Xo7SlKWoO9+97v4wQ9+gBdffBFXXnklbDYbHnnkEZSVlWHmzJkAgJqaGvj9fgwbNkxz7vDhUnnu6upqjBs3DtXV1QAQMa6srAxOpxN1dXUoLi5GdXU1hg4dGlH3Zfjw4fj444+Vx9XV1Vi0aJFmjMFgwKBBg5RrdRSdLrU3VI5jNf8SHUe9lyIb+qBgOTblv7feDr0vUwftZWrojn0UhEgrsiiKcL3xWwh1h8/aOtRw/UbAfMVvkqp/NnPmLBQWFuGtt97AsmU/Vozj77//NkRRxMUXX5rQPP/+9z8wcuRo3H33/+L06dN45pk/wefz4v77f6cZ9/vfP4Rvf/tSPPTQd8GyLPx+P37+81vR3NyEW25ZgcLCIrz77ib88pc/xXPPvay07Pj3v/+Bv/zlGVx77Q2YMmUaPv98O/7v/x5K+HVyHNOpz/qkRNDUqVPxpz/9CbfffjtWrVoFQBIjzz33HAwGAwCgra0NAGC1WjXnyo/l5202GwwGA0wmrcksOzsbANDa2ori4mLYbLYIa5M8nzyXPF/4NaONSxaWZZCbm9nh8+NhtZq7ZN6+iNVqBm8Q0Bp8bDYbu+z31tuh92XqoL1MDWdzHz0eDo2NrObmKopi9xZgZaQv48msQadjsWDBFdi48XUsX34rOI4DAGza9CZmz56LvLzchOYxGAz4/e8fU843m0343e8exC23LMOQIUMVgTp79hzcdttPlfM2bnwdhw4dxEsvvYqhQyVjx/nnn48TJ07ghRfW4be/fQQ8z+Oll/6GSy+9HD/96c+VMS0tTXj33bfBxfkyKwgMWJZFdnZGhI5IhqREUFVVFX75y19i0aJFmDdvHhwOB5555hnccsst+Pvf/w6LxaKMjfXLUh+PNkY2+7U3Lt7x8Pk68+YVBBE2m6vD50eD41hYrWbYbG7wvND+CURM1Hvpd9iV4x5vAC0tzjhnEuHQ+zJ10F6mhu7YR5/PC0EQwPMiAoHQNc0VvwEC3dRxXWcAz4sAknOLXXbZFfjb39bhk08+wfnnz8KxY0ewd+8e3HLLcs1ri8d5510AUWSU8bNmzYUoPoCvv/4aAwYMVn4v06efr5nzs88+Q1lZGfr3HwCPJ7Rv5547Fe+//w4CAQFnzpxBQ0MDZs2aqzl3zpx5ePfdt8HzQsx18rwIQRDQ1uaC281HPG+1mhOyICYlglatWoUZM2bg7rvvVr2gczF79mysX78eS5YsUSw54dYXm80WXJhV+dfr9cLr9cJoNEaMk+exWq2ora2NWEu45cdqtSrnqrHb7Z0Oik70zZIs8X7BRHLwvICA3688FlR/tERy0PsyddBepoazuY+S2IiEYRhAb4z6XE+lf/8STJ06HRs3vo7zzpuFN9/cgOLi/jj33GkJz5Gbq7UYWa1W6HQ6NDU1xh3X1taKgwcPYO7cGRFzylalxsbGqOfm5uZFnBOLcLGaLEmJoOrqasybN09zLC8vD0VFRaipkYKvBg0aBL1ejyNHjmiCmQ8flnypsiCR/62urkZ5ebnmGpmZmejXr58y7tNPP42w6Bw+fFgjbsrKyiJif3w+H2pqaiJihYheiiZ4MPlAQoIgiN5GRcVVuP/+lWhoqMc772zGwoVXg2UTj6FpaWnRPLbZbAgEAsjPL9AcD/e4WK3ZKCsbgV//+p6YcxcUFES9xtkMjE4qmqikpAR79uzRHGtoaEB9fT1KS0sBSP7DGTNmYPPmzZpxGzduRGFhoSJ4Jk+ejKysLGzatEkZw/M8Nm/ejDlz5igbOmfOHNhsNmzbtk0ZV1tbi6qqKiVlHgBmz56N7du3azbzvffeg8/n04wjejGCyiTagWwKgiCI3sYFF8xFVpYV99+/Em1tbbjssiuSOv+TT7aB50OfrVu3bgHDMBgzZmzc86ZMmYbTp0+hoKAQo0eXR/wHAIWFRcjPL8DWrR9qzv3www+SWmNnSMoSdN111+HBBx/EAw88gIsuugg2mw1//vOfkZGRgSuuCG3srbfeiuuvvx4rV65ERUUFqqqqsH79ejzwwAOKAjUYDFi+fDlWr16NvLw8lJeXY/369Thx4gQee+wxZa4JEyZg7ty5uPvuu3HXXXfBYrFgzZo1KC0txcKFC5Vx11xzDV566SWsWLECK1asQFNTEx5++GFUVFRQjaC+gkhuB4IgCDU6nQ6XXno5XnnlRUyfPgPFxcVJne/3+/DrX9+BhQu/i9ra03j66Scwd+5FGDJkaNzzLrnkcrz++n/w4x//CNdeez0GDhwEh8OBQ4cOwO/3Y9myH4PjOFx//Q+wZs0fkJeXj6lTp2PHjs+we/euzrzkpEhaBOn1erzyyit47bXXkJGRgfHjx+ORRx5BUVGRMm7SpEl46qmn8Nhjj2HDhg0oLi7GypUrcfXVV2vmu+mmmyCKIl588UU0NjZi5MiRWLt2LUaNGqUZ9+ijj+KRRx7B/fffD7/fj+nTp+OJJ57QRIRbrVY8//zzWLVqFW677TaYTCYsWLAAd9xxR0f2hUhH1MUSyRJEEAQBAJg9+0K88sqLqKi4KulzFy36PlpbW/Dgg/fC7/dj9uy5+MUvftXueQaDAY8//jSee24tXnjhOTQ1NSI7OwcjR47CwoUhLfDd734fDocd//nPerz22npMmTINv/zlb/CrX/0s6bV2BEbsSBWmPgTPC2huTm2WkU7HIjc3Ey0tTgqa7CTqvfQ2nIRr/W8AAIaJC2Cc9t1uXl16Qe/L1EF7mRq6Yx/9fh+ammqRn98fer3hrFyzq/nLX57Ba6+tx5tvvgOWTdz2MWvWFKxY8VP8z//c0IWr6xjt/Z7y8jJTnx1GED0aUZ0mSdqeIIi+TU3NMdTUHMe//vUPfOc7V8NgMJAoD4NEENF7IHcYQRCEwv/930PYu/cbTJ8+EzfcsETzXCAQiHkewzBKGntvh0QQ0XugwGiCIAiFP/1pbdTjtbWncfXVsbPEJk6cjD/9aS0+/nhnVy2tx0AiiOg9CJFVQwmCIAgtBQWF+MtfXoj5fEZGxllcTfdCIojoNahj/CnenyAIIjp6vV6p1dPXoRbHRO9BoMBogiBSA32R6tmk6vdDIojoPVBMEEEQnUQOCPb5vN28EiIe8u+H4zrn0CJ3GNF7ECk7jCCIzsGyHMxmCxwOqQWTwWCM6IuVrggCE7NBbLogiiJ8Pi8cjhaYzZak+qBFg0QQ0XsQyBJEEETnsVqlLuayEOotsCwLoZd8TprNFuX31BlIBBG9B5EaqBIE0XkYhkF2dj6ysnLB87Hr6aQTHMcgOzsDbW2utLcGcZyu0xYgGRJBRK9B1MQEpfcfOUEQ3Q/LsmDZ3tE6Q6djYTKZ4HbzVDVaBQVGE72HXmLmJQiCIM4OJIKI3oNA7jCCIAgicUgEEb0HSpEnCIIgkoBEENF7oJgggiAIIglIBBG9B00X+e5bBkEQBJEekAgieg2UHUYQBEEkA4kgovdAMUEEQRBEEpAIInoPlB1GEARBJAGJIKL3QO4wgiAIIglIBBG9BwqMJgiCIJKARBDRaxDVvcNIBREEQRDtQCKI6D0I5A4jCIIgEodEENF7oOwwgiAIIgmoizyR9viba8E3t2lFEBmCCIIgiHYgEUSkPSee/jEAQF8+T3WUVBBBEAQRH3KHEb0GwdGk/ExlggiCIIj2IBFE9B40yodUEEEQBBEfEkFEWiPGFD4kggiCIIj4kAgi0hu1CNL8fPaXQhAEQaQXJIKINCeW2iEVRBAEQcSHRBCR3sS0BJEIIgiCIOJDIojoRZDwIQiCIBKHRBCR5pD1hyAIgugYJIKI9CamC4wEEUEQBBGfpETQDTfcgFGjRkX976233lLGVVZW4qqrrsL48eMxf/58vPzyy1HnW7duHebNm4fx48dj0aJF2LFjR8QYh8OBe++9F9OnT8ekSZOwbNkynDp1KmLc0aNHsXTpUkycOBEzZ87EqlWr4PF4knl5RDoSS/iQBiIIgiDaIam2Gffddx8cDofm2PPPP493330XM2fOBADs2rULK1aswJVXXom77roLVVVVWLVqFQwGA66++mrlvHXr1mH16tX4+c9/jvLycqxfvx633HIL1q9fj1GjRinjbr/9duzZswf33HMPLBYLHn/8cSxZsgRvvPEGTCYTAMBms+HGG29ESUkJHn/8cTQ3N+N3v/sdWltb8Yc//KHDm0OkM6SCCIIgiPgkJYKGDx8ecez222/H+eefj7y8PADAk08+ifLycjz00EMAgBkzZqC2thZr1qzBokWLwLIsfD4fnn76aSxevBhLly4FAEybNg0VFRV45plnsHr1agDA7t278dFHH2Ht2rWYM2cOAGDkyJGYP38+XnvtNVx77bUAgFdffRU2mw0bNmxQ1sFxHO644w4sX74cZWVlHdkbIi2g7DCCIAiiY3QqJqiqqgonT55ERUUFAMDn82H79u24/PLLNeMqKirQ0NCAvXv3KufZ7XYsWLBAGcNxHC677DJUVlYqVYArKythtVoxe/ZsZVxJSQkmT56MyspK5djWrVsxc+ZMRQABwMUXXwyDwaAZR/RCSPgQBEEQHaRTImjjxo0wm8246KKLAAA1NTXw+/0YNmyYZpxsQaqurtb8Gz6urKwMTqcTdXV1yrihQ4eCYZiI+eQ55HHh1h6DwYBBgwZpxhG9EJXwEaltBkEQBJEESbnD1AQCAbz99tu46KKLkJGRAQBoa2sDAFitVs1Y+bH8vM1mg8FgUGJ6ZLKzswEAra2tKC4uhs1mQ1ZWVsS1rVarMpc8X/g1o43rKDpdapPoOI7V/Et0HAaM6ueQ8GGY1P/eejv0vkwdtJepgfYxddBeRqfDIuiTTz5BU1OTxqUlE265iXY82hjZDdbeuHjHw+dLZFw8WJZBbm5mp+aIhdVq7pJ5+xK8W0Rz8Gcdx4IP/mzQc132e+vt0PsyddBepgbax9RBe6mlwyJo48aNyMnJwaxZs5RjsiUn3Ppis9kAhCxCVqsVXq8XXq8XRqMxYpw8j9VqRW1tbcS1wy0/VqtVOVeN3W7vdFC0IIiw2VydmiMcjmNhtZphs7nB80JK5+5rMH638nMgwCs/+3w8Wlqc3bGktIXel6mD9jI10D6mjr62l1arOSGrV4dEkMfjwQcffICKigro9Xrl+KBBg6DX63HkyBFNMPPhw4cBQBEk8r/V1dUoLy9XxlVXVyMzMxP9+vVTxn366acRFp3Dhw9rxE1ZWVlE7I/P50NNTQ0WLVrUkZeoIRDomjcMzwtdNndfgVUJH1EdHyTQ3nYUel+mDtrL1ED7mDpoL7V0yDm4ZcsWOJ1OJStMxmAwYMaMGdi8ebPm+MaNG1FYWKgInsmTJyMrKwubNm1SxvA8j82bN2POnDmK4JkzZw5sNhu2bdumjKutrUVVVZWSMg8As2fPxvbt29HS0qIce++99+Dz+TTjiN4IBUATBEEQHaNDlqA333wTJSUlOPfccyOeu/XWW3H99ddj5cqVqKioQFVVFdavX48HHngALCtpLoPBgOXLl2P16tXIy8tTiiWeOHECjz32mDLXhAkTMHfuXNx999246667YLFYsGbNGpSWlmLhwoXKuGuuuQYvvfQSVqxYgRUrVqCpqQkPP/wwKioqqEZQb0etgQQhxhMEQRAEEUnSIqitrQ3btm3DjTfeGDXoeNKkSXjqqafw2GOPYcOGDSguLsbKlSs11aIB4KabboIoinjxxRfR2NiIkSNHYu3atZpq0QDw6KOP4pFHHsH9998Pv9+P6dOn44knntBkllmtVjz//PNYtWoVbrvtNphMJixYsAB33HFHsi+PSDs0Kkh1mEQQQRAEER9GFOluEQ+eF9DcnNoAW52ORW5uJlpanOSb7SSsz4a2v/1E+jlvAITmkwAA3eBJMF/80+5cWtpB78vUQXuZGmgfU0df28u8vMyEAqOpYACR3mgqRguqw6TtCYIgiPiQCCLSG7XYoZgggiAIIglIBBG9BpGED0EQBJEEJIKI9CaWJSjF7rD3/nsCX1U3pXROgiAIonshEUSkOV3fNPX4GTv+/sEh/HH9bggCWZsIgiB6CySCiLRGEwDdRcHQbU6f8vPJBkeXXIMgCII4+5AIInoPYtcERttdIRF06GRbnJEEQRBEOkEiiEhvzkJMkNoSdOhka8rmJQiCILoXEkFEmhO9TlAqaXOERFB9izvOSIIgCCKdIBFEpDexYoJSagnyKj/bVK4xgiAIIr0hEUSkOaLqp64JjLap3GE2p4+qURMEQfQSSAQR6c1Z6CKvjgkK8CLc3kDK5iYIgiC6DxJBRJoTIyYole4wh9YFphZFBEEQRPpCIohIb7q4ThAvCHAFLT+ZJh0ArXuMIAiCSF9IBBFpTtdmh3l9oTmLcs0AAJvLn/LrEARBEGcfEkFEeqPpmpF6q5DXzwMAOJZBntUEQLIEuTwB3Pfc5/j7+4dSch2CIAji7EMiiEhzNJHRUY/XNjnx7n9PIMAnbyny+CRXmMnAwZppACDFBL39+XGcqHfgvZ0nOrJogiAIogeg6+4FEERnSKR32NMb9uBkgwNnmpxYfMnopOb3+CRLkNHAoSBoCaptcsJOcUEEQRBpD1mCiN6JShDJTU8/+vJ00jV+vLII0nMoK80GABw+2Ybq07YULZQgCILoLkgEEelNAsHQJgOn/Hym2ZXU9J5gTJDJwGFIcRY4lkGb0wdeCImpjrjZCIIgiO6HRBDRO1FZfASVYGmxe6ONjonaEmTQcxjSPytijM9PIoggCCIdIRFEpDcx3FtyCw2vn4cvEBIp9iTT272KJUgKn7t85hDoOEYzxhfgk5qTIAiC6BmQCCLSnPgxPo4w0eNwJyeCPN5QdhgATBxegAeXTsed/zNJOSYLJYIgCCK9oOwwIr2JFegcPG53a7O47El2gZdjgoyquKJ+eRnol5cBg56Dx8eTO4wgCCJNIUsQ0asJtwTZk7QEqWOCwjHopD8fH1mCCIIg0hISQUR6EzPlXbYEhYmgJGOC1Nlh4RjJHUYQBJHWkAgi0pxY7jDpn3DR42jHHeYP8Hhqwzf4d2U1AJUlKIoIMuikY+QOIwiCSE9IBBHpTcy46KAlKCh6+uVlAJACo6tPt+Gxf3yJo7WRBQ/f33kSO/fX463PjsPn5xURZIriDjPqpT8fsgQRBEGkJySCiDQnvhVGLo44NFjfx+7y461Pj+Obo8148PmdcAezv2Q+3HVK+bm2yaXqHRaZQ2DQy5YgEkEEQRDpCIkgIq2JHRIkPVFTZwcAjB2SB0CyBLk8IRfZnqPNys8Otx+NbR7l8ckGR9TsMBlFBAXIHUYQBJGOkAgi0pzYgdEuTwANrZKoGTc0DyzDgBdE1Le6lVHqukH1LW7NDKcanXB7Y4sgcocRBEGkNySCiPQmTnLYiXrJCpRnNSLbYkRBjtQFvtURCo5Wu8PqWrR9xY6fsaOxTRJGcgd5NeQOIwiCSG+oWCKR5sRWQTV1Uvf4QUVSPFC/3IwIa4/N5cNfN+3DZ3vOKHE/wwdk4/DJNuw73gIAYBkG+dmRIshI2WEEQRBpDVmCiPQmRlCQ2xvAm58eAwAM6mcBAPTLM0eMqzrYgG1f1SLAi4prbOLwAkwdXaSMKcwxQcdF/qkYyB1GEASR1pAIItKc6CKoodWtiJpB/SRLUHEwTV47zhNxrH9eBuZOKlUeF+ZEiicgVEWa3GEEQRDpCbnDiPQmhiVI3eddtgRFE0HhlBRkYtywPLBsaIZYTVflmCCyBBEEQaQnHbIErV+/HldccQXGjx+PmTNnYtmyZZrnKysrcdVVV2H8+PGYP38+Xn755ajzrFu3DvPmzcP48eOxaNEi7NixI2KMw+HAvffei+nTp2PSpElYtmwZTp06FTHu6NGjWLp0KSZOnIiZM2di1apV8Hgiv+UTfQNGZSHKDwY1jxiQHXP8ZTMG45qLRuDX10+GXseBY1nMKO+nPBcNuZWGK6zWEEEQBJEeJC2CnnjiCTz88MOoqKjAunXr8MADD6CoKBQ/sWvXLqxYsQLl5eV49tlnsXDhQqxatQrr16/XzLNu3TqsXr0a1113HdauXYvBgwfjlltuwYEDBzTjbr/9dmzZsgX33HMPVq9ejfr6eixZskQjcGw2G2688UY4nU48/vjjuPPOO/Hmm29i5cqVyb48It2IWShI4taF48AwklVHr+Nw9YVlAIDSgkzNuOEDsvHtqQORadIrx5ZcNgb33DgF544qjDp3v1zJslTb5Ir6PEEQBNGzScodVl1djaeffhpr167FrFmzlOPz589Xfn7yySdRXl6Ohx56CAAwY8YM1NbWYs2aNVi0aBFYloXP58PTTz+NxYsXY+nSpQCAadOmoaKiAs888wxWr14NANi9ezc++ugjrF27FnPmzAEAjBw5EvPnz8drr72Ga6+9FgDw6quvwmazYcOGDcjLk4ricRyHO+64A8uXL0dZWVlH94fo8cR2h50/rhjnjirSHL90+mCMGZwLr4/HI6/sUo4XRsn+0utYDO1vjXnl/gWSCGqxe+H2BmA2kneZIAginUjKEvSf//wHAwcO1AggNT6fD9u3b8fll1+uOV5RUYGGhgbs3bsXAFBVVQW73Y4FCxYoYziOw2WXXYbKykqIwW/3lZWVsFqtmD17tjKupKQEkydPRmVlpXJs69atmDlzpiKAAODiiy+GwWDQjCN6IXG6yOdajVGfGVJsRW6W9rmC7OjBz/HINOmRbTEAAE43OZM+nyAIguhekvrqunv3bowcORJPPvkkXnrpJdjtdkycOBF33303xowZg5qaGvj9fgwbNkxz3vDhwwFIlqRx48ahulrq0B0+rqysDE6nE3V1dSguLkZ1dTWGDh2quDPU83388cfK4+rqaixatEgzxmAwYNCgQcq1OoNOl9okOi6Ybs1FSbsmkoNnmajHGUYSNrF+d1mZBuXnolwzMjP0Uce1R2mBBW2OZtQ1uzFqUG6H5ugp0PsyddBepgbax9RBexmdpERQQ0MD9uzZg0OHDuH++++HXq/Hn/70JyxZsgTvvvsu2traAABWq9aFID+Wn7fZbDAYDDCZtC6I7GwpcLW1tRXFxcWw2WzIysqKWIfValXmkucLv2a0cR2BZRnk5ma2P7ADWK3JWx8ILa5GAyJ7wUsMLMmO+bvLUu396CF5Hf4dlw3Mwd5jzai3ebrsfXK2ofdl6qC9TA20j6mD9lJLUiJIFEW4XC488cQTGDFiBABg7NixuOiii/CPf/wDkydPBoAIy42M+ni0MbIbrL1x8Y6Hz5fIuHgIggibLbWBrxzHwmo1w2Zzg+ep2nBnCLiiZwAyEKGDiJaW9t1Ug4ssCY2LRv9cScjvO9LU4Tl6CvS+TB20l6mB9jF19LW9tFrNCVm9khJB2dnZKCgoUAQQABQVFWHYsGE4fPgwLrzwQgCIsL7YbLbgoqzKv16vF16vF0ajMWKcbBGyWq2ora2NWEe45cdqtSrnqrHb7SkJig50UZdwnhe6bO6+ghDnjznDoIu7v3MmlqCmzo7zxhZ3+PcwOFiI8dgZOzzeQNTK0ukGvS9TB+1laqB9TB20l1qS+sSOJShEUQTLshg0aBD0ej2OHDmief7w4cOa8+V/w+N1qqurkZmZiX79+injjh49qliI1POp11JWVhYxl8/nQ01NDWWG9Xpip8hntRPnc+Mlo3HPjVOjdohPlH55Gcgw6uAPCNh/vAUPPv9fvP7x0Q7PRxAEQZw9khJBc+fORWNjIw4ePKgcq6urw5EjRzBq1CgYDAbMmDEDmzdv1py3ceNGFBYWory8HAAwefJkZGVlYdOmTcoYnuexefNmzJkzR3FhzZkzBzabDdu2bVPG1dbWoqqqSkmZB4DZs2dj+/btaGlpUY6999578Pl8mnFELySGBmKZUEXnroRlGAwulqxBq9fvxtFaO4kggiCINCEpd9j8+fMxduxY3HbbbfjpT38Kg8GAJ598Enl5efje974HALj11ltx/fXXY+XKlaioqEBVVRXWr1+PBx54ACwraS6DwYDly5dj9erVyMvLQ3l5OdavX48TJ07gscceU643YcIEzJ07F3fffTfuuusuWCwWrFmzBqWlpVi4cKEy7pprrsFLL72EFStWYMWKFWhqalIKOpIlqLcTXQXFSBrrEvKC6fZqg6UgimA7GY9GEARBdC1JiSCO4/Dss8/ioYcewr333otAIICpU6fi0UcfRUaGVDhu0qRJeOqpp/DYY49hw4YNKC4uxsqVK3H11Vdr5rrpppsgiiJefPFFNDY2YuTIkVi7di1GjRqlGffoo4/ikUcewf333w+/34/p06fjiSee0GSWWa1WPP/881i1ahVuu+02mEwmLFiwAHfccUdH94VIF2LUCeLOogCxqtLtZRwuf9TjBEEQRM+BEcMDbggNPC+guTm1WT86HYvc3Ey0tDgpQK2TCMd3wvnOnyKO29hslN685qys4Z3Pa/CPLYc1x/53yVSIItDq8GLC8IKzso7OQu/L1EF7mRpoH1NHX9vLvLzMhLLD0j+VhejbxIkJOltkR7H4tDl9uP9v/8Waf32FM83UW4wgCKInQiKISHO6PyYomttLLXya2qLXMiIIgiC6FxJBRHoTw5t7NoOSo1mCjp4O1a3yBwS0Orz4aNcpeHyBs7YugiAIIj4kgog0p2dZggzBXmXVp0MFQx1uP9769DheeOcAVjy2Fdu+On32FkcQBEHEhEQQkd7EsgSdRRWUaQ4VZSzKlfryNLSGXGAOtx8HTrQqj/+6aT9qqes8QRBEt0MiiEhrYqU2nk1LkNr1tuC8IRHP290+WMzaahT7jrdEjCMIgiDOLiSCiPQmZkzQ2V3G/y6ZilsXjse0Mf0wfEC25jmn2w+3jwcAlJVKPe/217Se3QUSBEEQEZAIItKbGCLobBdrHtQvC+eOKgQAXHH+EM1zdpcfbq8UED1phDTmQE1LRE88giAI4uxCIohIc2IIiW4UGOOG5uOBpdMwdmgegKAlKCiCxgzOBccysLv8lDpPEATRzZAIIoguYEChBZdNHwQAsLv9cHsld1hWhh4DiiwAgCO1tpjnEwRBEF0PiSAivYll8ekBriZLhpQ63+rwIcBLZeozjDoM6y/FBR2rtXfb2giCIAgSQUTaE0vsdL8IysqQUudlVxgAmAw6DOmfBQA4XkciiCAIojshEUSkNz3A4hMLa6YBOlUDP5OBA8sysAYtRFQ9miAIonshEUSkOT3XHcYyDApzTMpjs1GqFSQLI3+g+9dIEATRlyERRKQ3PVxHFOWYlZ9DIkjK3+cFoVvWRBAEQUiQCCLSmoZWV/uDupHCXLUI4gCoLUEkggiCILoTEkFEWrP3aHP0J3qAOwwIswQZtO4wXugZayQIguirkAgi0pbdhxtxrLYt6nNiD/GTFednKD+Hu8PktHmCIAiieyARRKQlXxxowJp/fdXdy2iXMYNzseC8wSjINmHK6CIAIUsQiSCCIIjuRdf+EILoeVSfkixAMVuE9RB3GMey+M7sMnxndplyLCSCesYaCYIg+ipkCSLSkhaHFwCUpqWR9FyBobjDAgI1USUIguhGSAQRaUmrXRJBGcGMqwh6sLbQ6aQ/OxGAQCKIIAii2yARRKQlsiUopgjqwSpIx4b+7AJUMJEgCKLbIBFEpB2iKCqWIHNMEdRz0elCkUwBKphIEATRbZAIItIOtzcAX7DQYIYh/WL7WYZRAroDVDCRIAii2yARRKQdLUErUKZJB46LkR/Wg2NtGIZR4oIoQ4wgCKL7IBFEpB2tDh8AICfLCIixLCk9W1xQwUSCIIjuh0QQkXY43H4AQJZZH3tQz9ZAVDCRIAiiB0AiiEg7XN4AACDDpI/j9urZKogKJhIEQXQ/JIKItMPlkSxBGUYdYoudni0uyB1GEATR/ZAIIlLOJ1/X4qvqpi6bP2QJ0qWrIYjcYQRBED0AEkFESqltcmLdW/vwx/W7u6wlhNsTFEFGHbnDCIIgiA5DIohIKY1tHuVnZ1CspBrZEmQ2xXGH9XBtIbvD/GQJIgiC6DZIBBEpxeb0KT+3BVtbpBqX2hLU09VODLigJYjvpAiSe4+12L3UjJUgCCJJ0q/cLtGjaVJZglqdPpTGavLeCWRLUKZJDzhijerZgkAfFEF+XsD6jw7D7eVxw7dHgmFiFH+MwhP//gon6h244Jz+eG3bUSycPQwV5w3pohUTBEH0PpKyBP3nP//BqFGjIv77wx/+oBlXWVmJq666CuPHj8f8+fPx8ssvR51v3bp1mDdvHsaPH49FixZhx44dEWMcDgfuvfdeTJ8+HZMmTcKyZctw6tSpiHFHjx7F0qVLMXHiRMycOROrVq2Cx+OJGEd0LU220J7bHL44IzuOYgky6dK2WKJc6drj5bF5ew0+2nUKR2ptCZ8viCJ2HWpEY5sHr207CgB4beuRLlkrQRBEb6VDlqC//OUvyMrKUh7369dP+XnXrl1YsWIFrrzyStx1112oqqrCqlWrYDAYcPXVVyvj1q1bh9WrV+PnP/85ysvLsX79etxyyy1Yv349Ro0apYy7/fbbsWfPHtxzzz2wWCx4/PHHsWTJErzxxhswmUwAAJvNhhtvvBElJSV4/PHH0dzcjN/97ndobW2NEGhE16IWQa3OLnKHedXusBj0bA2kWILkFiAAcOSUDWUl2Qmd7wwWjCQIgiA6TodE0NixY5GXlxf1uSeffBLl5eV46KGHAAAzZsxAbW0t1qxZg0WLFoFlWfh8Pjz99NNYvHgxli5dCgCYNm0aKioq8Mwzz2D16tUAgN27d+Ojjz7C2rVrMWfOHADAyJEjMX/+fLz22mu49tprAQCvvvoqbDYbNmzYoKyL4zjccccdWL58OcrKyjryMokOoHaHtZ0NS1Ca1gmSY4JaVXFTB060Yv7UgQmdb3NFiqAkPGkEQRAEUhwY7fP5sH37dlx++eWa4xUVFWhoaMDevXsBAFVVVbDb7ViwYIEyhuM4XHbZZaisrFQCPCsrK2G1WjF79mxlXElJCSZPnozKykrl2NatWzFz5kyNMLv44othMBg044iupfpUG+pb3Mrj1i4IjPYHeKW2jjluinzPRh90h7Wo9ujwqbaEz7c7IwVmXMsYQRAEEUGHPjUXLFiAlpYWlJSU4Hvf+x5uvvlmcByHmpoa+P1+DBs2TDN++PDhAIDq6mqMGzcO1dXVABAxrqysDE6nE3V1dSguLkZ1dTWGDh0aESw6fPhwfPzxx8rj6upqLFq0SDPGYDBg0KBByrU6g9zxO1XIVgD5356O3eXDkdM2nFOWHzNwt6HFjWde3wMRUvp3gBdhc/pSvneOYLVohgEsGXp445g/OI5JKtD4bKLXcQC01jKb0wc/L0jirh3kfeBYBrwgCUGXNwCWZcCyHXvN6fa+7MnQXqYG2sfUQXsZnaREUGFhIW677TZMmDABDMNgy5Yt+OMf/4i6ujrce++9aGuTvslarVbNefJj+XmbzQaDwaDE9MhkZ0vxEK2trSguLobNZtPEHqnnk+eS5wu/ZrRxHYFlGeTmZnZqjlhYreYumTfV/PbFL3DgeAt+fPVEXDxjcMTz/gCP//3rf9Fk86C0MBPXfns0/vDyF/D4hZTvXZuHBwBkZRiQl2dBs4GDO8bY3NwMMEzP/IPPzDQAiLSWuQIiSorb37OAKAmdGeP64/vzR+Inj34EUQT0JgOswbk7Srq8L9MB2svUQPuYOmgvtSQlgi644AJccMEFyuNZs2bBaDTi+eefx7Jly5Tjsb59q49HGyO7wdobF+94+HydtQQIggibzdWpOcLhOBZWqxk2m7vTdWLOBgeOtwAANn5cjWmjCiKe37b7NI6caoPFrMcvr52kBEe7PH60tDhTupYTp1sBANYMPVpanPB6YwcItzQ7wbA9UwTxAUnM2cNiew4da8J/vzmNfcdacOui8TAELUbhnGmUagOY9CxyzDpkmHRweQI4cboVJQUdE57p9r7sydBepgbax9TR1/bSajUnZPXqdBDBpZdeiueeew779u1DaWkpAERYX2w2W3BRVuVfr9cLr9cLo9EYMU62CFmtVtTW1kZcM9zyY7ValXPV2O32lARFBwJd84bheaHL5k4Vbm+o6rMgiFHXe6ZJEolTRxfBmmFQXDweH5/y1ydnU2VlGBAICBCE2PMHAjx6qCEIXAxxXtvoVFLet39zBueN6x91XKu8D2Y9AgEBFrMeLk8ArXYvinI6900vHd6X6QLtZWqgfUwdtJdaUnqLGDRoEPR6PY4c0dYrOXz4MAAogkT+Nzxep7q6GpmZmUrKfVlZGY4ePRpRCffw4cMacVNWVhYxl8/nQ01NDWWGdQJRFFH55Wnlcas9eqCz3SWJHtkNYzRI1guvj0/5mmSBld1Jl093E/4NxZqhBwBUnw6Jeac7etsRURRRfVr6opEV3Icss3S+g1LnCYIgEqbTImjTpk3gOA7l5eUwGAyYMWMGNm/erBmzceNGFBYWory8HAAwefJkZGVlYdOmTcoYnuexefNmzJkzR3FhzZkzBzabDdu2bVPG1dbWoqqqSkmZB4DZs2dj+/btaGlpUY6999578Pl8mnFEcnxztBn//PCw8tjm8qPN6UN9qxuvvH8Qja1u5TgQupGb9EER5OeVtg6pwhYmuOJnh/XczLFwEVdWKlk/v6puUo61RckAA4D3vziJUw2SmzE3S7KkWkgEEQRBJE1S7rClS5dixowZGDlyJADggw8+wD//+U8sXrwYhYVSf4Rbb70V119/PVauXImKigpUVVVh/fr1eOCBB8AG4zMMBgOWL1+O1atXIy8vTymWeOLECTz22GPK9SZMmIC5c+fi7rvvxl133QWLxYI1a9agtLQUCxcuVMZdc801eOmll7BixQqsWLECTU1NePjhh1FRUUGWoE5w/Iw94tiRU2348nAjtn1Vi/d3nsSf75irCJOsDK0lCAB8fh4mQ+pStyMsQfFEUM/VQJg1vj9aHV689dlxAMC4oXnYdahRM6ah1Q1RFHGywYnSgkwl62vbbsk6N2V0EcYNlcpCWGMEWhMEQRCxSeruNHToUPzrX//CmTNnIAgChgwZgt/85je44YYblDGTJk3CU089hcceewwbNmxAcXExVq5cqakWDQA33XQTRFHEiy++iMbGRowcORJr167VVIsGgEcffRSPPPII7r//fvj9fkyfPh1PPPGEJrPMarXi+eefx6pVq3DbbbfBZDJhwYIFuOOOOzqyJ0SQ+qClpyjHjGElVmzfW4e9x1s09Wx27K1TmqbKN2KDjgUDSYN4fakVQRGWoLhKp+eqIKOBw6I5ZSgtyMT+mhZcMKEE/648olTDBiQR9K/KamzeXoPFl4zC3ImlaGrz4GSDEwwDLL54FHRBt1pBtvT30NhKrWIIgiASJam708qVKxMaN2fOnHbdUAzD4Oabb8bNN98cd5zFYsGDDz6IBx98MO64oUOHYt26dQmtj0iMhmDhwysvGAo9x2L73jrsO94CjyrWZ9ehBiUmKCvoDmMYBkYDB4+Ph8fPI1YjiG+ONiE3y4TSJLKZwgVXXKGTBoUUZ4wtxoyxxQCA4vwMHFHFBNW1uHEsaI174+OjmDuxFHuPNQMAykqyFRcYABQGg6EbWmMVDCAIgiDC6aG5M0RPQLYEFeaYMXpwLgDgdKNT0+/qy8ONcHslUaSOczHq4wdHHztjw2P/2I17/rJDqQDdHsfP2HGi3qG9Vs/XOQkzflg+ACDHIr02dWZeUW4GAKA5uPelhVrhKIugehJBBEEQCUN19omo+AO8kg1WlGOGxaxHYY4JDUF3S6ZJB4OeUwQRxzKaSsdGAwc4peDocNZ/dBibt9coj/cea8E5Zflx13OqwYH/+3uVcm35pp+ugdHRuHjaQBTlmDFheAHufW4Hmm0hsSlbwMLjr2Tk/Wi1e+EP8EpFaoIgCCI2ZAkiotLY5oEISczIbq5B/ULVu4vzMzC8NOTosmYaNIUpTTEsQTV1do0AAoDP99W1u573vzgJt5fH8NJsPLB0ukpwpWdgdDRMBh1mjitGhkmH/vlaS0+z3QNRFJWeYeHZZVkZehj1HERIvzsiPrVNzqgCnSCIvgWJICIqciXjbJW4UYug4aXZSlo3APTL1RbokzPEPGEi6P0vTkZcK5GbtlyF+oIJ/ZW0cIneYwlSUxImgnx+AU5PQLEIycJUhmEYxRpUU+eIqK1FhPj0m1rc/ewOPPmfr2mfCKKPQyKIiIoz2KAz0xS62Q5QxaHMnzIQZSWhqt3zJg/QnK8UTAz7ti1Xlx5emo2xQ6Q4I3XsSyxsSmq8UftEL72H5VlDr1O2r31YdRIHTwb782VEFoscN0xKl//zG3uw9JEP8cf1u1FTF1nmoC/j8gTw3Fv7AUh1sNR1mQiC6HuQCCLg8QVw158/wx/X70arwwtBFOHySMIk0xSK8xk/LB+zJ/THjZeMQp7VhMHFWRhWYsXoQTmYPLJQM6fsDgu3BDXbJYvO9+cNx1UXDAOQmAhqi+EGSvfssFgMKLQoP8sWOLmdBoCoTVKnjSnSPP6qugkvvHOgi1aYnpxpdmkKeKorohME0fegwGgCB2paUd/iRn2LG7/40ye49qIRirTIUIkgHcfiB5eO0TxeuXhK1DmjWYIEQUSrXRIzeVaTUhMnXCiFIwiiEhCcbdHe/HurO2Ps0DzceMkolBRkYtvuWhwPs+iEiyCR92MAU48Jw3Kx+0iocvqR0zZUn25DWUmsQgV9C7mitlzHau+xZvj8PAx6CiQniL4IWYII1DW7NI///sEhON2R7rBkMEaxBLU5fRBEESzDIDvTAHNQKLm9gbhixu72QxSlG1d4LExvyg4LZ87EUowYkIOZY/tFPKcWpwDgqXwO7tdXYdnIWjx31zw8d9c8nD9Oqj/0QZQ4rL6K7OYdPTgXeVYjfAEB+463tHMWQRC9FRJBBE40OCKOye6w8JttosiWoI2fHlPS6JuDwc05WQawqpR6XhDhj9PVuM0hd47Xg2OTeMv2EivRqMG56JeXoTnGhnWhDxz+DADg+2KDcuyiKVKc1n/31St72NeRLUGZZj3GDJJi0tb86ys8+PxOHDtjw6ff1CbkniUIondAIqgPI4oinB4/Tgabcao5US+5XzpqCVK3ynjz02MAQoX+8rKkFg9GA6cE/brjuMRCVaKNUZ4NFzpMlDHpDcswuOt/JmH5VeMAhAKgo8KHmq4OKbZiWIkVvCDiv/vru3qZaYFs4bSY9SjODwnLo7U2PPC3nfjLxn14f+eJ7loeQRBnGRJBfZi3PjuO2/64TWmU+tPvnoOiYKq7nIWU2UFL0OQRBcrP/91XhwAvoCVoCZIzn1iGgckYconFQgmKtkQGA0dYe9QWkl5iCQKAbIsRU0cX4ffLz8OPF45P+Lxpo6Vg6Z0HGrpqaWmF0x0K+C8Os67JhDeyJQii90IiqI/CCwL+s/WI8phjGYwblocRA7QBtBkdtASVFlrwl19diGyLAU5PAN8cbVbqAeVbQ81vZYtRPBEk1yyyhscDAfFFUC8kP9uUVBDvuaMkEXToRCu5eRCKCbKY9RoXo9yAFoCm8jlBEL0bEkF9lP3HWzWPC3PM4FhWUxAR6LglCABYlsG5wdT5x//1lVIosUhVWDEjeMPxxLlBu7zSjSu6IIstgsQ0D4xOCi6KlQxB0aRjISIUD9OXcajcYUU5offhjLH98IvvTwAQsjymK57t/4DrjYcg8vT7Joj2IBHURwkvEicLk8HhIsjcMUuQzESVWyx0rdA3cNkd5vLGjglyykHaCX1DV72le5E7LBaiKEIUBDDG0J6KAW0QtGzZkIPd+zLqIqBqi9rQ/lYlVq3Vnt5B5P6vNoM/cxCBozu7eykE0eMhu28f5dDJVs3j/KA7YGCRRXO8M5YgABg9KBfZmQbNt2t1iw1z0B3m8cW+QbvjZar1MXeYGjHgg/NfK8GaswE2dEMXXW1grKHCiRkmHdqcPnKHQWsJAoBfXz8Zx8/YMXF4AdxBIe7yBuD180qZh3RF9Lm7ewkE0eMhS1AfxOMLoKZOmxbPBrOqzEadUq3YYtZHrUycDHJBRXWcRY6q95dJtlLEdYfFsQT1chEUOLUXzvW/QaA2svKz0FQD0VYPvu4QREfIsifYtJlgGQnscV9BCYw2S3syYkAOvjVlIBiGgdnIwaCTPhJ7RUmBALnDCKI9SAT1AUTeD9/X70II3igPnWyDIIrIt5qUD/2xQ0Np18uuHIubF4zB7340Azqu82+R/GyT0icM0Na4yQi6w+LGBMWtWRQnRb4XuMPc76yB0HIa7o0PRzwXLnZkfLve1BSfVOoxtdTC/cEz4JtPdc1iezhtDq8iBKOJe4ZhkGORBHqrI73jggBA5NP/NRBEV0PusG4gUH8Epzf9G/qpVwO5g7r8er6dr8G3exN8ez+A5fuPYNdBKV16/LA8XDJjMGrO2DFheL4yvqQgEyUFmbGmi4rgsYMxZoJhooum7104HMfO2DF3UqnmeCg7LHZMUHxLUNhjRv6fGOXJNESO74ki6IS2uqin8LUHIDqawGRJ8ViyeBx48BUE3GfA1x1C5rV/ANPLrGbtIae+D+2fFbP+VY7FgPpWN1p7hSWIRBBBtAdZgrqBwKl98Bz7Gp6qjWflev7D2wEAYlsdfCe+hlj9KQBg8shCFOWYMWV0UaduiIETX8H5wm3w7Xwt5piCHDP+b/l5uGzGYM1xOTYjXuaSy5NEdhiY3lUvURetQKSE0HYm4hhjkgLbBXsD+LrDcH+4Frk6LwZxjch0S+NFRxP4U3u7Zr1BfH4eL7xzAJ/tiVxjd9Bi9+Ktz44DQESzXzX52VK8WkNr+sfTUHYYQbQPiaBuQFcyGgDgr/kKYsAH/9Gd4Fu7spt1SCh4Nz+K7+i3YpipBaMH58Y5J3G82/8BIOiGEZKLO7FkhETQ10easOdYc8QY2RJkjuIOi0iDZxgoKqgXuMMYY8giFx7oGk0Esbkl0lhHE1xv/R6BQ59ievMb+F7mdu25MVxpqWLT9uP4aNcpPPumVmw5PX4IQmK/F4fbj5376zVd3zvKfyqr0WTzwGjgMHNsccxx/fIkEVTXnP4iiCxBBNE+JIK6Aa5oKLjMHMDvgev1B+F570/wfPSXLrmW6HVCdEY2iJya25qSeB9RFCG6bcpj/uSeiDHeL16H57O/R22SmhW0BDW0urH6n7vx6KtfarKYArwAn1/qKxY1Uy3qDTL9TUGC2wbH338J0RkSheHuL6EtXMgwYK1Ss1Wh5bTiSst3H8dAXTNEAGx+0BLXhVaCo7U2vPnJMeWx/Pv84IuT+Omaj/HM698kNM//vbILT234Bp9903lr0rFgVfTF3x6FPFWxznD6Bcs3nGlxxRzTkxHFUA8+kQKjCaJdSAR1AwzDwjSoHAAgNEl9ioT6I/FO6TB8U03U48P10eNJkkVoOQ3RY1ceB058rb1+43H4vngN/q/fgWiLvKbsDjvVGOpfpv5ZndFkNrQvgjQxSWlsCfJ//S5Eu7bVhdAS6gYv+tyAP8xawTBKHJD/4MeapwIiiy8zzgObL8WgdZWrRBRFPPXaNxr73JlmF+wuH15+7yAEUUyohUd9qxsng419t+/t3Hs1wAs40yyJmhEDs+OOlS1B9c3pKYKgEkEI9IK4JoLoYkgEdRP6/JKwA7G/nXYG0SbdcBQLQJAix0G43vo9+JZT0n8Nxzo2v0PbZ4mvr9Y89u95X/lZsEXe/CwZkVk6J+rs+OCLk7h33ec4VS/dCM1GDiyboIVHGZa+IiiaeA2c3q/8LMgWIn2o5hJEAWxQBMnWOcaUhYbSOfhNy/exg5sKRheMqwr4wAsCdu5PbYf5ZpsXTcEecbLl7kyzS2mZIhOvLhQA7FQ1fI2XOZgIdc0u8IIIo4HTtGyJhmwJsrn8SmHFtEJQiSCKCSKIdiER1E3o88JEkN8DUf0BliKEoDWBKxqGjKsfwr/cM3A0IN0o+VN74Fp/N1zrV8L12v/C8/ELEFSurUQQ/dLNjcmSgk2FxuMQ/dJNVRQFBI7tilgL33AU/v1bIYqC4g5Tc6LegZffO4iTDQ68EXSrxK4WHScmKA0RHM3wfPoy+BNfRTzHn9qruBRlFydrydeMYVSP2ZwSZCy8D47RFfBCL5Ua4IL7zfvx9/cP4akN3+Cldw+mbP3Vp6XGu0OKs3DuKOk9UdfsiqjCHC6KwnG4Qjfwkw3OTsUFyZbF0oLMdhMAzEad0uD3b5v3xx3bI9G4wygmiCDag0RQNxEhgoBI90YKEOySpYbJKoQvowjb3CPxV/tcMLkDVaOkG4x/7xZ4t/41qfnlYF02txRMZi4gCuAbjoKvOwzHszdB9IaKMsqWIPd7f4Jn63Pwf/WOUrROzVdHQoX/7O54mWGIbuxJXw0E72evwP/Ne5FPsDqIzmbwtfvhfvdx+A9+AgBgLHmaYVzeAEBvBpNVAPOCO8FmFSh1gtzeAJigCHK53NhSJdUL+uJgA27741Y8/q/d8Ac6J8QPn5RE0PDSbKVBaW2TKyLlvLE1vgjyBkIlE7x+HvUtHf/bOK0SQYlw3fyRAIAvDjSkX6q8SJYggkgGEkHdhD6vf8Qx2aqSSmTrC2stUNLQXZwFmd99AFzJGAAAVzoW+nMukca7WpO7gGwJMpjB9RsOAODrDsO3e3PEUNHeANHnVqobe3f8A/y+LREBz8220I1HvoHFdGOI4Tft9M4O44OxYYbJV4DJDAkceW/dGx9B4FgVAoc/AwCwmdoMP8aYCcu1v0fmd38LNkOKf7EGXY6tDi/EoAiy2bQxL05PADv3N2DXwc5ljclxPEP6Z2FgsPJ4Tb0DLeEiqC2+qPH5tXWjmtqxHMXjVENyImjSiEKlTtaeo81RA/p7LAJZgggiGUgEdROc2RJxTLaqiKII3573wTce6/R1xKAliM0qVERQVoYeDMPANHsJDJOvhPmi5dANkjpoJxtMKa+Z0ZtUIuiQJliaLSqTpj66M6Kpo/e//4YlijUoHLm3Wbto3GFpdPMCIHocSjaY4ZxLlJo/AMCVlkc9Ry2UlGMmCxh9qL5QQY4JHMvAFxAQ7BoBtzu6CPlvJ4OQZQFbkG3G4GJp/XXNLtQ2akVXe+4wOSNQxu7u+A1dcYcVRv7NxWJIcO3r3tqHDduOdvjaZxtRExhNIogg2oNEUDdiufI3MF7wA6XZpSwo/Ae2wvvJS3D95387Nb8Y8EEMWnaYrALYXdrmkay1CMYpC6Wbps6gnJPUNTSWoBEAJEsQH8x605VNh2n2D5Txnsp1AACu/yjpgM+NfubQNYcXGbAoYwcGcNou97EDWqPEBKWnBgLfLO0Zk1UAxpAB0wU3AqwOhikLoYshgsItQdHgWFZxTbW5pU3xuqXf28ThBSjKNeOSaVLW2M69Zzps+RBEEc12ad68LCOyMgzK7+2LYJVyWRg129pxh4VZgtQxQsnwzuc1SmZYMlXQB/ULCdA3Pz3WoWt3C5qYoDRz5RFEN0AiqBvRl46GYcxcMHKGT1AE8TWhoNjOBEuLvuC3b4YBY7TAEfw2HS0YGUERlPS3RzmOSW+S0q85HeB1SsdZDqa5t4DLGwjThT/UBu0WDgWbI7kEy7OlG2K2xYAfDdiH2aYDuCP7Lc1lCmJZgiJu2OlrCZLLJXDBNHauaBgsS56GcfKV0n4FLW0AJLGnN0tikm2/23n/fEkEtbql95PfK90gL542EA//aCauumAoDDoWjW0efFXdhLc+O5Z0dpTd5UeAF8Eg1CR3SP8szZiBRZI1Jl6FcCDkDpMD4tsbH436Vjf+seWw8jjHkngz4DGqQqLZSZzX7ajdYV3gXieI3gb1DusBMAbpBi8GBYU6Lkd0NimZV0kjCxqdEQzDKN+mo6WlM1wHLUG+kCWI4XRgs/tDCFo02NwSMJz0FtOPOA+6gefA/e7j4M8chK50LER7I4TWWlwwGBg7cyr65WfC88/XIUKSMQb44YMk2ApykighkKY9sYQWKVCZzRugHJMDmRmWQ8YVd0NoPS1Zf1gdwLDS/nJ6QIjdew0A+udnAmhAs0tAGQAh2FxTdhEZ9BzGDs3DrkONePTVL5XzLp85JOH1y9adbItBKcR5wTklOHSyDTandL3hpdn4+Kva9kVQMEA7z2qCq8HRIRFUp6r1c05ZflKtYQYWWbDiqnF4asM3cLj8EERR0/i3xyKq3gckggiiXUgE9QAYg/QtXfS5IdgbIahq7QitZ8B2UATJgka+kcqZVpZ2LEGiKCZ8w1DcYcE6R2xOcUgE5WubwzImC8wVd0F0tYHNzAVfdwg4CgQ+fxVW/QYwl/8KoitU3XqksQHfeKUsuoQDo9XrTqeAVgBCay0AKbU9GgzDgMstjTiuHzYV/gPbYp4HhCwwXx+zY2oWoAePAYUWzXthwvACpckoEApKTxRZBKkrMp9Tlo8/3jYLx8/Y4XD7levZE7QEFWSbcLKDIqgx2P/LYtZj6eVjkj5/4ogCMAB4QYTd5Ud2lM7zgOS6M+jYntGQVv2eF3iIggCGZYNPCQCYnrFOgughkDusJyAXSvS54f7gKc1T0fpDJcLeY81446MD0oOgwFECo6OIIDkmCKLQrlVBg/xtUxZB2aG+TFzeoIjhDMMqcSxcMGBanse14QGADxXGu2SYHzqORf/8jOjCDe3pnPQQQZ7P/g73R8+Cb5YqQstuwkQxnncdjOddD/Plv4w5ZuLwAljMegQguc70DI9zyrQ1hqaX99P0kzNGq9Adhy+DAiovK7Lp6+DiLIwdmqf8Hp1uf9zYIzkwOjdYs8fegZighmDw9YzyfsiKYv1sDx3HKq6wWDFM+4634KePb8PL76Wu1lKnCHefB78kiEIArvUr4X57dTcsiiB6LiSCegCMQYoJEn0uCA1Sp2uuv9RkVbYOJMvGT49h7xEp00dkg01KFXdYNEuQ6saVREClkh0WfA3qGzibPzDqOTLcwHOgL58XcVwOFB+Wx2LNT2bh3hunxvn2Gh4YzSKUIt/++rsbobUW/q/fQeDgJ0AwhkstJBOB0ZtgGPetuEHSeh2LmxeMUSpG68ErxQxlMs16/P4ns3Htt6QA9/aqOqvZd7wFnwR7fBUFqy5HQ37vBXgRHl9ssS0HRssWQLUlyOMLxE2Zr/zyFG56eAve3iFV3S7IMccc2x6yVUtdtkFZo4/H7/++Cz6/oNRc6m5EMWxPZRHkbIHQehr8ycT6thFEX4FEUA9AcYe5WhWfPjdgnHTMm1gPI6G1FoGaL5XHjW0eGBhpLl/Q6xnXHcZyQQGRXFxQuDtMHfzM5UdagtQwDAPTrMUwzVsWWkb+YBjGzZfm9jpgNupgNLQf+KudWFldcud1A35VRW0AgN6kSW9PJeeUFeAn358CACjM0mFof2vUcabgfnu8iVsEz6jiby6eFlv8GvUc9DrpfeaM4+LyBYslytWb1SLo3nWf45dPf4qG1uhp/uENVwsTLa8QBdmqJWe9qdm+V3udHlFPKNw9LFt1ZQuRKHRJZXqCSFdIBPUEgoHRcnVncDowxuC36QSrvjr/+Wu43/6jUmzP4+Ohh/RNPhAUQXJwqjVaYDTDhOKCkqk065Ozw6Rv21ww64sbOB6MKbG6LOoaOFz/kWCMUiqz6HHEOiVE1Jig9Il54FXCFQB0QfHbVcjxYQY29o1QrjCdjCXIFcwkO39ccbuup0TigmR3WIFVel85VO4zucbQV9VNEefxgoBjdXbNsVRYgqJZnj7cpbX+uDrZ4yxZRI8DgZPfaEVNLHeY2kIkUCVpgpDplAhyOp2YPXs2Ro0aha+/1nYPr6ysxFVXXYXx48dj/vz5ePnll6POsW7dOsybNw/jx4/HokWLsGPHjogxDocD9957L6ZPn45JkyZh2bJlOHUq0vx89OhRLF26FBMnTsTMmTOxatUqeDw9P0NCtgQJNqlaL2O0gGGlG1Ei3b5FIfThyzfVgBcEONx+xRLkF6Vv9ooIihHgGaoVlIQ7TKkTZFLmyLj6tzBf8ouE52DNIYuETiWe1C03Yi8g/ACjBEeLPdwSJIqCUhAz4+rfwnz5r2C64Adde1FV77BYmIKxQO447qpw3EGrkTlmj7cQ6rigaAR4Abwg/e5kS5A/IMDr5zU9xHyByPV9tOs0fH4BJgOHiyYPwLQxRQlXio5GYVBANbS6Ud/qRkuwB5rPz6OmTvv+bIniMutKXG88BPemP8C//6PQwfAvBfJ+qcVRgEQQQch0SgQ99dRT4PnID6Jdu3ZhxYoVKC8vx7PPPouFCxdi1apVWL9+vWbcunXrsHr1alx33XVYu3YtBg8ejFtuuQUHDhzQjLv99tuxZcsW3HPPPVi9ejXq6+uxZMkSjcCx2Wy48cYb4XQ68fjjj+POO+/Em2++iZUrV3bmJZ4VGLPU3kBuJ8EYMwG527fQ/rdL0dEcmovllNgfPSOd6xU5+AOC8k01lghKtlaQyPuV9cnuMEAKfk42AyXjqntguvCH0A08B4wxKII8iWQnhccExX6qpyHam6S9ZnVgs4uhKy1P2HrWUeSYoHjiWnGHJSWCpPdBhilxERTLEqRumZGVYYAh6D6zufwaF50/rKr0oZOtSoDykOIsXPftkVh25TiwbMctg4XB0gy7DjVi5bM78MDz/4U/wKMpGChtNHAYECwz0Gw/uyJIaD0NAAgc+W/oYJgIEhV3GK86dnYtVgTRk+lwinx1dTVeeeUV3Hnnnbjvvvs0zz355JMoLy/HQw89BACYMWMGamtrsWbNGixatAgsy8Ln8+Hpp5/G4sWLsXTpUgDAtGnTUFFRgWeeeQarV0tZDLt378ZHH32EtWvXYs6cOQCAkSNHYv78+Xjttddw7bXXAgBeffVV2Gw2bNiwAXl5UisBjuNwxx13YPny5SgrK0NPRe7xJMMYM6U6MIAmWyoWsgUJkEzktqAIMkD64PMKHOwuSdhwLBPRq0u5rs4AEYnHBGmKsek7HncBSJlicraYYglKxB0WpnSYYLFEMcpzPQXBVg9P5XPgGyTXJZvbH0wCBQ9TgmwJ8nsgCnzU65qMsghK/GYpi6BkLEGxMr68QXHDMgx0HANrpgGNbR7YHD6o9YwjrJijOr3/onPjB+UnSqHKlRbgBbQ5fPjRHyoxrESyXhZYTcizGnGywYGWKHFDZwWd6ktNDHeYJuOTGqsShEKHLUG//e1vcc0112Do0KGa4z6fD9u3b8fll1+uOV5RUYGGhgbs3bsXAFBVVQW73Y4FCxYoYziOw2WXXYbKykrF/19ZWQmr1YrZs2cr40pKSjB58mRUVlYqx7Zu3YqZM2cqAggALr74YhgMBs24ngiTkaN9bMxUYjcScYepe4yJHjtsQcEjW4I8PIs2lSssppVGzhBL1B3mD47j9Cm9iSvWEN7XviCLKBgdcof1RA0ktNXB/fZq8LX7FYsba+131q4vv68AwPWvldpeU0HMBjkmKHFLkCsJEVQcbOGxc3991GBi2c1l0EsWRbk+T5vTC5cnJMzaHNr3xp6jkkX0h1eUR2S+dZRYlcqPnLYBkHraKcHTZ9kdJsOoMjsjfp+ioP0XSOiLFUH0FTpkCXr77bexf/9+PP7449izZ4/muZqaGvj9fgwbNkxzfPhwqeR/dXU1xo0bh+pqqSBg+LiysjI4nU7U1dWhuLgY1dXVGDp0aMSNe/jw4fj444+Vx9XV1Vi0aJFmjMFgwKBBg5RrdRSdLrXx41ywmq78r5iVo3meNVvAGaQPfoYPxL2+9+Cn8H3+L+Wx39mGgzWtAKRaMADgCrAwBG8e2ZmGmPOxegMEAKwQ/5oyvCgJNEYXe86OIHKZUraawIMLuMCaYluZmDi9w3QcAy7Fv7vO4N23Fa7Kv0W4OHX5A1L+HouFaAzdMIXWWnB+B9igCJffj5lBS43Xx4PlmIQqJcuCKStD3+5rmT91IN7+vAaHT7XheJ0DwwdoLaFyPJDRwEGnY5UWHA5PAFaVq8zm8inXsrt8OFEvWQ7PKStI2X6Gz2MycBpxWJhjVpr71tTblfHhf+NdCWswKtcVWe3fA8cCnI6FyISOs+DP2vuts5zNfezt0F5GJ2kR5Ha78fDDD+MXv/gFLJbI+IW2tjYAgNWqTb+VH8vP22w2GAwGmMJucNnZ0gdia2sriouLYbPZkJWl7T8kzyfPJc8Xfs1o45KFZRnk5nY8sDIeVqtsas+EzWSBEHT/mLNzkJGTBQekD6x416+v2695vGffcbxpPybNH9xaN8/CEvwMzM8xx5zPY8pAAECGEchK4DV73CxsAFhj7Dk7it1sAe9sg8XAwxhnbjfHQm0r4zgWPCNJI6vVBENuJgSfG56avdDl9IOhYECsqboUURRwfMc/ASEAY8kI9Ft0BwJtDbBVvYv8WVeAy+ya91jkOsxoVT3O0gdgCNvfwoLQ37U5w4gMU/RClWrkuj5FBZZ23wu5uZmYMKIQO/fVodHhw9Sw8XVBi4rZqENubiaK8qSWH96ACEYX+siyu/zKtWpbJVdUUa4Zgwe031Q2GUoLLTjV4MCSBWNRccEw1DY6cOvvPwQAFBdacP45JfjnlsPYfbgJdTYvAgEBOVlGWK3qv/HUIooi5NrqpsxMZR9crQaoncjZWUboczPhbtMrx7MydDB10WdaV9FV+9gXob3UkrQIevrpp5Gfn4/vfOc7ccfFcrmoj0cbI5vH2xsX73j4fJ0pEy8IImy2xGr1JArHsbBazbDZ3OD5oJnabAWCIsgrGiG4JGsB7/ehpSV2gLDXYdM8zmBCcQkZnAAIgN0LBBqktOEMAxdzPrmasLPNgUCca8r4m1sBSMUY462xI4j6DABtqN/yd2RefFvM32EgLEOI9/mUhJi2Nhd0eiecW56Fb/82AID1ut+Dyz577ieZQF01BJcN0Blhrvg17LwOsAyCfvbNsPkA+FK7f4nSeuYM9PoCAKH3pdftA8swEEQRZ+rtyI1SATocOeaM9wcSei/kBSsxHz/VGjG+sUl6rGMZtLQ4YdJL31zrGh3IMoXcrs02j3LuoWOSK6wwx5zy9+LPrj4Hh0+24bzxxXDY3bAYVN+kBQEWA4thJVYcOW3DLx/fpjxlMnC45YqxmDq6KKXrAQDRH3K9eXlWec3+sM+qtlYHOMYJf1toT2ytdrgz4u9RZz83fUer4Nm5Aay1CJnfvrXDc0X9rCQ6RF/bS6vVnJDVKykRdOrUKTz33HN48skn4XBIN2yXy6X863Q6FUtOuPXFZrMFF2ZV/vV6vfB6vTCqTPTyOHkeq9WK2trIqsnhlh+r1aqcq8Zut3c6KDoQ6Jo3DM8LytyMORtokbI9RL0ZfFCQiAFf3OvzjlYAgGHCZfDt3gRLUASZDByGFpqAOikmqDHoKrBk6GPOJ1eW5n2ehF4z7w0KLp0x5XvEmK1Aay38R3bCV1sNrmhY9IFhMSXqmwPPC0BAQKDppHLMW/0FDOdcktK1JoL36JcApBIAvMgCXfSeSpaAsw1M2FoEQYTJwMHlDcDh8kVtsxKOHBht0HEJvRfkStB1zS7N+AAvwB6MX9MH55IDqVvsXk0wtcfHo77ZhTyrSelzVpRrTvl7Mc/IY0qhGzwvQg40+9EVY7HrUANmje+PQEDANReNwO//vgt+1bU9Ph5P/OsrPHdXZFX0ziK4QyJGZEJ7zge0rtaAn4cYEDTHeZ8v4neumbu1Fq43fwf9+EtgnHhZh9bn/u8GCI3HwDccg7/t+2At+e2fFAf1ZyXROWgvtSTlHDx58iT8fj9++MMfYurUqZg6dSqWLZOq/S5evBhLlizBoEGDoNfrceTIEc25hw8fBgBFkMj/hsfrVFdXIzMzE/369VPGHT16NCKA8vDhwxpxU1ZWFjGXz+dDTU1Nj84Mk2FUGWKMMRPg5DpB8YMYxWDHebnzuIX1YsroIvzp57ORnyn9en2iDjv2SC00+sVpaRCqE5RgdlgwgJrRJd+XqT0M5y5Ufg6c3hdvFdpHvA/hxRJFd0gcB47vgu+b9yHYG1KxzIThz0ip21wXF0NMFvXeqAlliLUfHC2IopK6npFAYDSgrr8TslwGeAEPPr8TT22QWjtkmqW55MBom8unFGWU+SYYDF3XIn0ZK47z/u4o7g+ehuu1/0Xg1F7l2PTyflh25TgY9dI+DS/Nxu3fnxj1/FiVrTuD6FNZfNSZX4L270FoOg5P5XPa93s7yRa+b96D6LbB9/k/O75AXvUZQnWJiB5MUiJozJgxeOGFFzT//frXvwYA3H///bjvvvtgMBgwY8YMbN68WXPuxo0bUVhYiPJyqTrw5MmTkZWVhU2bNiljeJ7H5s2bMWfOHMV8OmfOHNhsNmzbFjIz19bWoqqqSkmZB4DZs2dj+/btaGkJdSF/77334PP5NON6KrrBkwGdEVzxSHADxoayeOKIIFEUIbolixubJ6UEmxg/sk1SMKssZvzgFKkwalBOnEUkVydIyQ7rgjYPupLRMM78HwCQMqliEJFdFPCpssPE4B6FbvR87QF4P30Jng+fTfmao67P40Dg2C7wQSGnaRrbA4gpguQMsQSqIHu8vPL+SiQ7DAAKgvV31AJh16FGJbgZAEYNzAEA5Fik99fJege+PhKqiQUAf9u8H69+cEhp29EvL7XxDmLAp/Tb8n39btyxIwZkY9b4yOa3XxyQBIgoiorbsNP4QvumzggL7x3m+XAt/Ae2wvvJS6Ex7YgguW5ZImNjokrVT6b4KkGcbZJyh1mtVkyfPj3qc2PHjsXYsWMBALfeeiuuv/56rFy5EhUVFaiqqsL69evxwAMPgGUl3WUwGLB8+XKsXr0aeXl5KC8vx/r163HixAk89thjyrwTJkzA3Llzcffdd+Ouu+6CxWLBmjVrUFpaioULQ9aCa665Bi+99BJWrFiBFStWoKmpCQ8//DAqKirSwhKkL5sG3bBQo1BB/nbH+2P650WvQ/kWyOYUQwADFiJy9X7lXECyBAFAtsWAojgtBORU20Q/tGSRpU7RTSVcidRElj9zCKIggGGjaPZwESSEWS78HmUfmKxCiMFvxLJlpisRfW44/3GXqvI1Aza3tMuv2x7my34J9/t/AnzuOCIocUuQ7ArTcazSF6w9CrOl96HLG0Cbw4tsixFbd5/WjBk3VHKhDOmfhdGDcrC/phVHa6X1ThpRoNQFeve/JwBI9j+5cGGq4OtD1uX2KpgzDIObLh8DlgW27g658L8+0oQMkw4Halrx2Z4zuOHiUbhwUufeB6JKBGne81FKHkTQTrFERlXBXWg5Da5gcLLL06bqU10iogfTJblykyZNwlNPPYWvv/4aS5cuxb///W+sXLkSV199tWbcTTfdhJ/97Gd48cUXccstt+DYsWNYu3YtRo0apRn36KOPYu7cubj//vvxs5/9DEVFRfjrX/+qySyzWq14/vnnkZGRgdtuuw0PP/wwFixYgFWrVnXFS+wSNMHgcrFEiEpTVTWCswXuTX+QxhotYDg9vKx0Y8nW+yC4WsGfksoXyG0zygfnxg1QlOsVCY3HE1uwLJa6wB0GAGzuAKm4n98D0V4fY1SUYkDKa1RZgXRG6MeoLILGrs+O4Wv3a26cjDkrupA7y+gGjIVx+vcBAEInRND2PWdwz7odOHxKskZmGBOvFWU0cBhYJAmWf2+VXOcNLaEbe77VhIH9pOdZhsEPLhujOX98WT5+dvU5mDi8QDk2d1Kp0uurIwRqvoTjxZ8gcPxLpaoyr3KBCQ3HEnIVL7xgGEYMyMbVF40AAOw73oK/bd6Pz/ZIDVdffOeApip2R4jtDktABLVXJ0glYITmk3EGJjZHMg2ZCeJs0+GK0TLTp0+PaHMBSG6s9txQDMPg5ptvxs033xx3nMViwYMPPogHH3ww7rihQ4di3bp17S86HdCpglH5QKiCNCQTteuNhxSrhuh14MvDjTD7DDDrXMhm3XC9+TtlvNxF/qoLYgQXy5ccei68n74M/sxBOF65HWxOf5gv/QUYJvqNWw5C7ipLEMOyYPMGQGg4Cr7pBNjs4kTPDC4w5O5hzFYYxn4LgaNVEBqOAAFvpzNgYsE31cD3xQal4jWTVQjwfhinXd3OmWcP+dt+u+6wOFWj174pCYQ/vyGJ7axY7VhicN38kXj45Sp88lUtppf3Q6tTej/dUlGOEQOyNfWJwjvBD+6XhaH9rTinrABVBxtw5LQNl89M3mKhxv3OE4DIw/3OH8GYspD5/YfBn1a5YoUAAtU7oB91Qdx5si1G3PODqcjNzcS2XacUV52ar6qbMKUTWWNaS5Dqd5SAJahdF5daBLVE9mhMCE2vMnKHET2X7v9aSkSHDRNBKgLVOxQBBAC6oVPw+L++gkOUbhT9Tn0Isa1OeX7e1KG49wdTNC0Aol4yMxdcqfSNW3Q0gT/5DUS5s30UFLdZF8QEKWvKlQK+Y34jjVJxWPWkYulgzFYwehMyKu6SnuIDgC+1pQ9kfLs2InCsSnG5GacuguX6P0I/8vwuuV5HkLN1hOYT4FtCbihX9S7wbfUwt2MJilbpOTtJETRiQDaMeile7dFXv1Q6x48elIuCbO17lWEYTCiT1pxtMWBo/5DLZvLIQnx3blnC8UjREFxtGour6LGDP3MQfN0hAIC+XMrw8n21Oer5sThvfHTh3uLopDBQvXc1XeTDRRATxTrXnghSzSe4OlhjTWMJIncY0XMhEdRDYVgWCFpgwr+5+fZ9BAAwTF0E88U/hfH86wEADkESI6a2Y5rxE0cUYkhxZCHJaJhm3Qj9uPnKY3Vfsgi6OCYIALj8DoggdWB0UATJneoZnQEwSDfYDn/AtwPfVCP9wOnAmLOh62EZYQDA5g8CN3A8wAfg/ewVAIC/5iuceXUVbP9cGbeT/Kff1GLpIx9GHE9WBDEMo7TQUBOrCet180di7sQS/Pr6c5O6TiLIrmM17nfWAACYrAIYpkjxh0LL6aTcO7POKVF+rjhvCMYOldr6OGM0j00U0afqUxbPHRaleWx7GadaMRjdUtguIlmCiPSARFBPhpObqIY+MEVRgNAo3WT1Q6dCN3iS0vZAtgQBwfgevVnqUJ4TmbESCza7H0znXQdu0EQA8UWQ2MUxQUAo642PGZsQv0GY2h2mzBncLzmzLpWIPrdihcu85vfIvPb3Xd4ZviMwDAPT+TcADAP+5Dfgm0/Cd+QL6Um/B6ZgQcBwd1hDqxt/2Ri9ZEF2ZvJiuH++VgRxLKN0jQ+nIMeMxZeMjhvc31HkFHLd0Cka1zMA6MumgzFalOazclmKRCjINuEX35uAX3xvAhbOHqZYsBydFUH+6CIoondYNMHTXtkNdWZXDHdpu+tTr4kCo4keTKdjgoguhNMDAZ/mm5voaJJqcLA6MFapSaQQrA3iFEIiSDdsKoznXgUx4FOsIMnAWgvBQxJBMWNnujgmCAjVPxJt9RD9XjAq15sY8EGwN8U5W4TQJAV5M1mhAFrGnA201iZ1M0sUvvkEABFMZi7YzNS2b0g1rLUIuiHnInB0J3w7/6P5HVtZyd0i1//Zc6wZmz47rnFDhWNN0hIEhPqUyWSYdF0Sp9UeokNKvWdzS8EFfOBPfAUAYLL7wTBlERiGAZORA9HeAMHVBtaaeDzPuGGhQoFy4cfOiiDEEEEJZYe1GxOkEjBue5ILi7IOsgQRPRgSQT0YhtNLdg7Vh5bQKqXestn9lM7tct8mhxgSCFzBEKkbfQezoOQPef9Xb8P/9bswXfhD6IfP0IxRUuS7MibIbAVjtkJ02yC0nAJXNAyi3wP/oU/h/Xy9pl6KQvAmKgb8CJyU3By6geNDT8uWoC4QQXJmHZs/KOVzdwWGcxcicKwKgWNVmuPZgmQlky1Bj776JQAp0wkARg7Mgc3pQ77ViD3HpGPZluRFUFGu1qqTSJ+yrkBwSGKateQrf2MAwOWUKBl9TEY2RHtDp943lmAByE67w1TCQtS4w9rPOmvXMqO2BHlsHUsgECgmiEgPSAT1ZKK4w4SWoAhSubjkdFuXSgSxBUM6dWnNN11RgK/qdXCFQ7QZWoo7rOtEECC5xPhTe8A3nwBfXy2Jn7hxGdIHduDI50DACyYjB2x+KHOIzQoGBbeeSfla+aCrsiO1VboDLq8UuhHnIXDwY81xS6AFgCVmYPRlMwbhnLIC7DrYEBJBHbAEXTipFDV1dnzytfS7MOq7x0MvOiVLEGPJA2NQZaIZQiKNzciBgM6JZ9kSZO+0JUhlXRGTtQQlniIPPiBZnQxJuiA1liBKkSd6LhQT1IORawWp3WGKJUglgjxBEaRTxVIkEwcUDbZgsOSOk4s3ttbC+Y+74D+wTYkTChVL7LqYICDkEvPv3wrvpy9LH6p6E7hBE+Ke59+7BQCgHzlL802WLRwKAODrDsG95c/wfPpKytYqu9/YNBFBACIsfACQYzuEiYZj8Hr9SjFEGQZAWalUVbhQZcnpiDtMx7FYcmmoBlDcZL8uQhRFCI6QCII+9JoY1c1fbm0jdiKgXnb/pdIShHjZYdEQ2rMEaYWv6OmAS0yTHUbuMKLnQpagnoxcK0hVB0QO4GRVndC9wW/r1cwQsEUnoCsp73RRPjYjRwrqNZjheuv3EOqk3m+eSqkOk+lbK0Jugy50hwEAVzwC/q/fgRCs3qsbNhWmi1aAYRjY1/5AGsSwoQ9eleDhBk1UMnuUY8HWFULLaQjB9HBD+TywOYnWIYqOyPuVuipcfvqIIK5kjBTv4mpF5thZcO75GFmNX2OJBTjgOYlT9cM140sLM5EZdFupyy50xBIEAKwqg0kQukEF+VxKjA2bmQ9Gr0owMIQCt5Viop0QQaGYoPbbkcRDGxitihlMoFhie+6p8OBqvvkEYDCDNWUlvkBNnSByhxE9FxJBPRm5VpA6OywYu8Bk5inH5PoqrN6IzKvuTd3lgx/6pvNvgOs/92me87z/lPJzVwZGA4AumKkmY5z5P4plx3r9o8jwNaHxkw1Kg0uGYSECYIvKYP7WciV2SobNzFVu+jL+IztgnHxlp9YptJySvkUbM8F0smv22YRhOZgv/xVYnw0FI8fD29KIQLBI4CgcwaHqXZrxIwbkKD8b9RxWXDUOfl5AVkbnLYKBbhBBshUIxkwweqPGHaYWQfLfQ+DgNvgHjYd+2LSkr5UVFEFePw9/QEi4zUgE/pCLSUxx24zwNHvPu0+AycyTvhSx7VcFl0RU6PdIliCiJ0PusB4Mo3SSl0SQKIoQgrELrCUkguTAaLmjdarhCgZDPzp29W/GnMQ3xA7AcDoYz7sOYFiYLvyhJuuKsxYiY8S5muBsw8TLoSubAfMlP4sp0HTDpmoe+3a9Bb7+SKfWyQeDorn8Qd2S4dQZuNwS6EvLwRpMsFxxJxwX/QYfeSQ3Vc7JTzRjhw/I1jyeMroIM8d2zoomIyTS9iHFCMG6TooLWeUOU8fCqDMMfVVvdOhaZqNOqYTdmQwxMaC2BCXpDkuiTpByyNmceLp8RJo+WYKInguJoJ5MeCd5rzNUoDAziggydI0IAkKuADXG6d+D+ds/BZtV2GXXlTGMmw/LTWuhH3Fe1OdNkxcAAHRlM6AfdQHMFy2La743zvwfZF7/R1gW/wmMtR/A++D75r1OrVGu35RO8UDRYFgOxsJB+Ngj9fDLdx2BAX6MHpSDCyeXYsqojrd7aI+OxBV1FkW8BmPFYrnDuOJRMJwbLJrYfBKCqip7ojAMg6wM6e/62JkOFiIEtIHRSfYOSzQ7TDd0iva8REVQ2BqodxjRkyER1JMJswTJViDGlKUJRu5qSxAAMFFq3uhHz4FuyKQuu2bEGrjY3ltd8QhYFv8Jpnk/SmwuhgGbkQPGZIFhwqUAwppSdgA+GBSdLplh8TAZODQIVrQKGWAhYqCuGRedOxA3fHtUx104cfjpd8/BsBIrbgprlHo2EBqPAZDKSgAIc4epLEEsC+O5V4IrLQcABIK1hJJlerkUz7f+w+qo7UcSQYwlglJYJ4jNHwTz5b8KHe6oJYjcYUQPhkRQD4YJswTJBd3UViAgFBjdlSKIzcyJXN9Z6MSeDIzJ0iE3lHKjUwebJokoCBCaTgCAJh0/XZGtiscDkgvoJ9Z30E+M00Klk0wYXoCVi6egf/7ZfU+JoqC0OVHKSqizw4yRbT24QqkRsbqeUDJccf5QMADONLtgdyXvKhIFQSqYKpN0A9X2KkYHhRnLQldaDq50rHS8gyKI6gQRPRkSQd2MPyAgwMf44AqrExQtHgg4S5agjJ5d/bgzyO4PzbfrJBEdjdI3Xk6fRLf7ngvHsjDqOUUEAUB29TvduKIuIuAPZYYFY37U7rBo9XHkzMyOuMMAqSp2TpYUq9bQFqXYZ3uEWVbU2VxiAsUSE40JYoLNV+WWMx11h1GdIKInQyKoGwnwAu5+djvu/9t/IUQxi8suL9mnLn/oqgM0gZAIMnSpCAoFw3IDz4G54tdddq2zjiKCOmYJEgUBfLCEAJtT3OnyBD2FS6YPwj5/qAEoTn+tuG9EQUDg1N707wultqJwwZt+jJgg5ViwkGjc5sLtUJgtXaOhNboIEhxNCBz/Mqq7LCLbSuMOS8C9lmjFaLlSdlAECQn22gtPsRd5EkFEz4VS5LuRE/UONLZJN95Wuxd5VpN2gF7rppGzWLhgU1GZs2IJMmdDN+J8KS5i9k1pl/0UD+Wm10ER5N3+KvzfvAsAYHNK2hmdPlw5aygOn2zFqpqrsDJnAyCK8FQ+BzZbEgG+//4b+vKLYJp1g+Y8/6FPwTfVwDj9e2CYni0INZaToOUDqtgzJo4lSHQ0QuQDcWPVYlGYY8bBk21obI3+nnP+406AD8B86S+gG3iO9slwi2WSMUHtClc5OyzCEpRg0cSImCASQUTPhURQN3K6wan8XNfsihBBjMpCIYqiKnZB25fK55M+dLo0O4xhYL7wli6bvzuR0+s7YgkSBUERQEDnK3X3NH505Ti88r4BLudgZNiOI3Bwm+Z5/94PNCJIcDTB8+FaAFJ9J13J6LO63qQRQjd8WdgzmXnQjbwAjN4QtcQCY84GdAapubGjEUwH3J9ykcloliBRFBSXFV97MEIExbUEJeIOS7ROUNASxCoiKMEikeEVpzvhZiaIroZEUDdyssGh/Pz1kWbsPNiAivOGIMciffDKWSqizy31NvI6AYYDm1uqmSdkCerZ37p7LIolyJt0s0i5TYbM2SgXcDaxmPX4YcVY+A98C55PXoz6rV4MeCG6bHC/9wSE1tOh47Z6oMeLoKAgUBUBZBgG5rlLY57CMAxYa5GUJt9S26EYsIIc6T0nW4LVyAkQAACDKfL5MFGRdLHERHuHMVp3WIctQX4PRFHo8VZBom9CIqgbUYugtz+XrDyCIOLGS4I3DpU7TKlBk1sSyhoLcjbcYb2ZUAyIGOxLlngF7ECNNk2aKx6RwpX1HPSjLoB+1AUQPHY4X7lDE5zLn9oH//5KxV0rI9g6Fjh8VpEFBJfc3w5XNAxC80kEzhzoUJmIgmzpb7spigiSW7kAALzasg2i3wv3G7+VHuiM0u8hxe4wWVTJ1aGZYL2thHuIyZYkhgu61sRgE9bI+CqC6G5ImncjJ+odEcfU5nG1O0xxheUPijinrln6oJT7EhFJojNC7jyfjEtM5APw7/sQAGCYuggZ37kfrLXrCgn2BFhTFgxjL9Icc7/zRwSO74oY29HsqbNJ6Iaf3PdBrr/0RYUPthdJlmyLlPTQ5oq0rAmtp0I/qzKyRCEA354PQgNlIaqq8JxI77D26wSFWYJMFumw1xnrjOjn6w2KhU30dSALjiDOAiSCugmn249mW6SvXC6pL4piyB3m94SCosNE0JlmF2rqHeBYBmOHalPnicRgGCZk/UlCBAVqdkN0tYIxZ8NwziW9okhiIhgmVUA/bj704+ZHfV434nwAQODoTviPVZ3NpSUPH+kOSwRZBAlNxztUZNMa7LPm9fHw+LTuKb45JILktHTB0QTXv+6F7/N/htZQEiwsmXTvMD4igyv8eQDKnigZcgFvuzWGgFB2GMNwyrkkgoieComgbuBMswu3r9kKAMjNMuLab43AhDKp4WaLw4v3d57AT9ZswysfScX3RJ87ZlB01UGpq/yYIbkpaWDZV2E6kCYvNBwFAOiGTIpwUfZmGIMZpvOug3HGNaHCnYYMmOYsRdYP/wbjpAplrOe9JxA4vgsiH4CQqDvlbBJ2w08U1pInVVEXRa37KkFMBg6GYAyfzam1Bgn11crPsgjyfr4+FG/FcDDNWQqjHJAuiiFRk4gIAuLHBYnawGjJjSV9OQtUb4d7y58hep0Q2s5EFzfqwOpgdh2JIKKnQjFB3cAX++txKhgPVFqYiflTBqJ8cC52VzfhVIMTr7x/CABwqM4DZAcDJYO1NsItQbWNkola3dmb6AAdEEF8c7BCdFjJgr4Cw3IwX3o7hJZT0A2bGsqushaCsRZJgdGiCPc7a6QT9CZkXHE3uPwetF+KCEr+o5DNKQHvbIHQchpcv+FJncswDKwZBjS2eWBz+lEUrEUqehyaStSi2yZlhp7aBwAwTPkOdCVjwBWP0LqnBB7g2JAA4XTxhQ7vlzLcoiGEucNkMeNzwfPRXwAAzpPfQPTYwQ08BxmX/kJ7vsqdxhjMUj/5Trak6YtQMPnZgXa4GxgftPoAQHbQepObFRmM6xGDH1JBAcRY8iNaVcjZJXLxNaJjKF3ok0jnFZpPAgDYvAFdsaS0gMsrhb5smiajjmE5ZF79W1h+8DT05fNCg/0eeD5+vhtWGRsxmB3GJGkJAqTCmADAd7B9hhIX5FQFmctWILlvoMcG0VYvpaezOsntKgffq9cshFmC2PiWyXhuLTGsYjQQ2SJHDpLmo/VPU1nXyB3WMfxH/gvH31ZEjbUjUguJoG5gUL8s6Dhp60cNkr4Cmo06TWPK7104HF5R++003AoEhESQnG1CdIxk3WGi1wnR0QQA4PqwCIoFw+klt9msxTDNW6b0nxLqDkNwtgAAvJ//C+73/pRYMG9X0UF3GBAqjKkuC5AM2ZmS8Fa7wwLHpJuebkiwgzsfkAonAmALh2gaJ2tFUFDUyPE47RVwjFcrKKxOEBAKjo6Ai2JNUluC9KEyH0TieN5/EvB7QlZUossgEdRNPHPXRVhy2WicN076NskwDPyB0M1g1jn9Yc22as4JzwzjBQEtdulbZD5ZgjpHElWjxYAP3h1SgCqTVdDjGsn2NPTDZyDj8l8qbkO+/oiU6fTlRgSO7gRfd6j7FqekyHfEHSYVxhRaz3To0tZM2RIkiSDR54L/8KcAAH35hRFFGPVDzg1bQEgEyVluYqKvJ16GmGJNUlmCYqW3876IlHtRnV3WjiVIsNUjcGpv/LUSRBdCIqib6JeXgQsnDwDLhtwIk0ZIPcGuumAoLGY9BhTngBdDz7Nh2UctNi8EUYSOYxTTOtExErUEiT43XK/9L/z7KwEAhokLunxtvQWuqAyAFPirLggoeiNLRZwtxCjFEhOFzZUsQaK9HoKrFaIowH/0C/gPfZrQ+dlhIoivqwYCPjDWInDFI2GefxvYvAHgSsci83u/g/6cSzTnS/EijPxCgv/KHeDji6C4tYLCYoKASHeYZq7w1Png+QzLhtqORIkJEtw2OF/9Fdxv/V+n+rD1BvxHd8L5j7vgeuv/OtzDkOgYFBjdg1h8yWhcdK4DYwZLLrKZ4/qDqQs1RAxPwZZdYflWk5JaT3QM+UNejnUQXG3w7d4Ew5gLweYUB4NT98K//yMlG0hfPg/60bO7bc3pBlc0DP79H8F/YBuYYP8tIKxCcooQeT9ErxNsRk78gWGFAZOBMWdLBU39bjhf+hmMM66Fd/vfAQCGfkOB3JFxz5e/uMjWXNEvWUvYzFwwDAMurxSZ310VfxEsJ7m2ZAuQyh2mfHLoTZEWzrjZYXIrkQRFkMcBqPc5LDAaCL02Nb4vXld+FhxNvb7GVjz8+7dCaDsDtJ1BoKeXlehlkCWoB5GdaUD5kDwlyPScsnyoDEVgLfma8V8flWJS+uVRJdbOIre7EGxSyQHP1ufg//oduN5+DAAQOPI53Jt+j8CR/wIAzAvuhGnWYsreSAJu8EQwpiyIHju8W/+qHBfsjSm/luf9p+B86efw7d0CAODPHIL7g6eVeKTQxTseE8QwjKZXnCyAACBQe7Dd8wuCvQKbbEGBIgflR+lXFhN53fLrkC08qtihaP3P4gZGC5HuMHUVdal6+Gwwwb8ZMXxPVSn28QKjNbFUvj5u/VC1owm3JHakDhWROPQJ3oNRZ9wc4fvD5w8VRXN5/NjyhVRUbe7E0ohzieRgrEERZJfM8nzNbgDB/lcAfF+9rYxls4uVYnlE4rBmKzK+c79y85QRUyyChNbaYFaNCO/HL4BvPgHXG79FoHoHvJ+8GDa44ynyQOyGuf7T+9o9V47ja2yTGiTLPcGYJNq2hIsgMSgmNJYbfZR4wTB3mNBaC2/VG5JYEUPuLBlR5a4yzl4C05yblJ5i7s2PIlDzpWoydUyQ7A6L0ihWtYZolqK+hMiHRBB/8hvNc/59lWd7OX0KEkE9HN3ECrSKFrxon4lPvjmD+hYXBEHEkVobvH4ehTkmTBie3/5ERFxkU7wYtASp4RuOKoUR2dwBMM1ZmlSTVSIEa8mD6fzrNMcEe+Sedwb/AW2ne9+Xb4WuFdbKQ7aIdMQdBiBmkUy+tv1g74KgCPL6eDg9gVB3eF3iSQ7yupXA6KDVQC2CmAREkHfHP+Hb+R+4Nj4cEoaqFHm5CjhXMkaxfqozxrw71ocmE0PnhyxBUawZKutHX7F2CPaG6Fa4QOwYLe+Of8D19mrwjce6bmF9GIoJ6uGYpy3CAXYamt8/hBffOQBACpw26KQPqCHFVrohpwDZHSZ67FKMgwrf7s0AAN3wmTDP+9FZX1tvgxswXvNYaKpB4Pgu6AYn34g0GvLNQj/mQvj3fYjA0S9CT4YXCOyEOwwAdIMnwr//o4jjUqC0GHmCCr2OQ7bFgDaHDw2tbpQE43YYfRJJDsF1P/v6V8gd7Mbl/khLUDTLUviNWM7QEhqPhw6qPld0QyYj48qVmppYmmtkFYTmVrvDgtYiwdUWuXa1JagPuMP4M4fgeuO34PqPRkbFXZrnZEsQVzwS/JlIVypfsxvuphPI/J8/kAs+xdBupgEXTChBUU6oDtA7n59Qmq8OKIpRv4NICsZgVrpluz9cq3kucORzAIhoHEp0DIblkHHlShgmXwG2YAgAwLvztbjniH4PPJ/9PVRMMA5y4LpuxEzpRq662TIxRVDHvg/qBk+E+ZJfRD4h8BC87Vs3ZGtQU5sHYtAyEi2GJyZBEXSm0YF3Pq9RWYJUnwvRrFVhliA2LB1fPTcguea5fsO1ViVVYVHN+XJ2GMOCzcwBAIiu1ojpRZUlCH3AHebbL7VK4mujNN0NWoL0478NGMxgCwYj8/o/IvOGx5VSCaKzGUL9kbO23r4CiaA0wKjn8PPvT8Dw0mwAgNsbwBcHJB/9wEISQalC/pYbrQouk90PbDDFm+g8XL/hME75DkxzbwbQfnC0d/s/pED1DQ/GHSd6ncoNl8sbGOptJhMudjqRIi+jG3RO1PdGVOtHGHKR04ZWdyiDK5r7KhbBekA6hocevCLqBHVdH3XNINmKEFYsMao7io1/exCCxUI18wLa7DA5a8zviQyO1liCer8IYrg477GgIGSz+8Ny/RpkLLwPbEYOWLMVmd/7HXRlMwAAvm/eb9fCSCQHiaA0oV9uBn5zw7lK+rwvWFhxQCEV6ksVprk3w3jedTBMXQTD5CvBFg5VntMPnUpuxy5AyXj0ueLeCAMnv05oPrnvFpOZC8ZgjsioFN02uF7/LTwfvyA97kSKvJpolhTeZWv3vNIC6e/36Bm7EhOUjCVItmwZmABMjCwqGBxtUFm/VMJPrjUmhsWgRBNB6rYZ0TBMuDT0IFonezZYMVofTJMPswaJmpig3i+C1AJcDGt0K7vDGJ0ejM6gcXkxDAP9mDkAGASqt8MfzHgkUkNSImjbtm24/vrrMWPGDIwbNw4XXXQRfve738Fu13aHrqysxFVXXYXx48dj/vz5ePnll6POt27dOsybNw/jx4/HokWLsGPHjogxDocD9957L6ZPn45JkyZh2bJlOHXqVMS4o0ePYunSpZg4cSJmzpyJVatWwePpfX7mgSr3l45jkEeVolMGa8mHYdx8GCdVwDhlIczfuhVsbinA6qAbeV53L69XwhjMQDC2RGNZCEN0tMR8To3sCpNbWqhjVQBAaD4Bvu4Q/Hu3SDfhTsYEyUSrF8W77FFGahkxQLLuHj7ZqmSHIZnssKBgMoAPiSC9CTZP6CYrshwyr1uNjKsfUmLf1JYgURSAaK67dmJPdMOmQTdkcnASlQgKK7bIZkivUQh3iQX6liVI8x5T7bcoiqG9iNHUVlcyBsbpVwMAfF+/EyGiiI6TlAhqa2vDpEmT8OCDD2LdunVYsmQJNmzYgJ/+9KfKmF27dmHFihUoLy/Hs88+i4ULF2LVqlVYv369Zq5169Zh9erVuO6667B27VoMHjwYt9xyCw4cOKAZd/vtt2PLli245557sHr1atTX12PJkiUagWOz2XDjjTfC6XTi8ccfx5133ok333wTK1eu7Mie9GgKVKKHiiR2LWxWATIWPQjLDWvABW+qROqRrTViDBHE1x/R3mTjwDefkObMlcpGsGEiSI3QfLJTbTPU6PqPkgSBzqC0t6lb/zACdfFjmIb0t4JjGbQ6fPC5g/E8SYggOTtNzwRgYoLWBIMJvMpj4mEzwGbmgsstUeKDvJ++LO0rEHTDRXGxtOMOYxgGbH6wgGs0S5CcRZYpWa/VliBR4LW/0z4QE6TOhhM8KiuhEIC8/7GyDQFAX34RYMiAaKuHr+rNrlplnyOpv/wFCxZgwYJQm4Dp06fDYDDgnnvuQV1dHfr164cnn3wS5eXleOihhwAAM2bMQG1tLdasWYNFixaBZVn4fD48/fTTWLx4MZYuXQoAmDZtGioqKvDMM89g9erVAIDdu3fjo48+wtq1azFnzhwAwMiRIzF//ny89tpruPbaawEAr776Kmw2GzZs2IC8PCkGgOM43HHHHVi+fDnKynpPLIe6UWpBDjVN7WoYllUsFUTXwFryITTVRLUEiQEfXK//tt05RJ8bYsALoakGQKi6OmuJLYL4xuOhLKlOWoIAwPStWwGBh+fjF5R12P99Pyw3r4vpbjPqOQwssuDYGTt8bjfMAKAzSnWDRGja6kRDCHaL78e1YbHlY+mYzgy7OyQw3vmqDUP6ncHMscWaxqqeT15E5sL7ItteyCSyJ7JQUjXBDXcxynFBorM1dJ46KBp9IztMXQtJdNuBnOAD9V7EsAQBkjg2Tv0OvJ+8BN8XG6AfM6f9iuhEu3Q6JignJwcAEAgE4PP5sH37dlx++eWaMRUVFWhoaMDevVIaZlVVFex2u0ZQcRyHyy67DJWVlUrgV2VlJaxWK2bPDpmaS0pKMHnyZFRWhgpIbd26FTNnzlQEEABcfPHFMBgMmnG9gYKckCWokFxhRC+AsUh/t/59H0YEfQptdVqLQQyLjeut/4Pz1TvB10qWZNkawxWPBDidJr5LmbupplNtMyJeB8tJ8RxhHdc1hQSjkJslWX6cDinj80wbj6c2fIPb1mxFfWvoxunxBdDYqrWYeEVp3RebQzFTXuhhc4fcXTbBhGff3Iu6Fpd2/wJ+iAEv3O8/FesFxV23NCRYp0iMYwmK4g6LaLraByxBapef3J4HUO8F026WomHst4JZrKJmDqLjdMgGzPM8AoEADh8+jCeffBIXXnghSktLcfjwYfj9fgwbNkwzfvjw4QCA6upqjBs3DtXVkok4fFxZWRmcTifq6upQXFyM6upqDB06NCIgdfjw4fj444+Vx9XV1Vi0aJFmjMFgwKBBg5RrdQadLrXx4xzHav5NhuL8UNZHVqYh5WtLNzqzl4SW7tpLff4A+AEITScgHKyEcew8iHxAql0THlzMB8Cxoka0iH6vUswSAMDqYCgoBcOxQEEJ9Dc9DegMaP3zTRq3jdByEjrZYqTTp+xviQ3LAmJcLXHnzrEE3V8BH8ABz717BMd5KXZn5/56XDFrKARBxB9e/RLHz9jx2x/OQEkwoNon6BBuO6hpDqDe6wWCHxV2UbIY/3d/Pb6t+ixljGYEdr+l7B1jyNAESOv0erDt7ElAJ91CWFFQXmOAkV07HHQ6FpwpA34ADO9TxvAIKxjod0fdo970982ohB7rc4T2QgzuhU4Pvb59Mc7oDBABcCKf1Hu2N+1lKumQCLrwwgtRVydVXr3gggvw2GNSf6W2Nikl1Gq1asbLj+XnbTYbDAYDTCatJSM7W/rG0NraiuLiYthsNmRlZUVc32q1KnPJ84VfM9q4jsCyDHJzu8YdYrV2zp2VkWHosrWlG53dSyLE2d5L8fzLUXdmH1wHP4d4YhdyZ1WgdfvrcHzwghIjYRk3G45vpDorgU9fhK/uKEoWrwJrMMFb14BW1XzGkjLkFWSrjkh/I61CWFyRowmG4sHwAjBnmlP2tySY9PCqHps4Pu7chfnSc0ZGuhn6VB/L2/fWYV9NKwIBAUdOS4LwqQ3f4IdXjceEEYU4wkR+hBtEL3yql+piJDW052gLLu0f+jzUcQxYW6h/ly67AP6GGuVxbp4FrDF+X8K2TDPcAPS60Odkm0kHNwCDUY/c3EwwmRnwADDooIzx8S3QyFufJ+4e9Ya/bycfelcY4Q3tRYCDDQCrNyb0HnQYjBAAWDI4mDvwnu0Ne5lKOiSC1q5dC5fLhcOHD+Opp57CsmXL8Ne/hhoixkolVh+PNkY2hbc3Lt7x8Pk6m9YsCCJsttSWdOc4FlarGTabGzyffJT/jPJ+qDrUgBmji9DSEsOf30fo7F4SIbpzL3WTFwIHP4f7yG40na6F8+AuACFXAZ9VEuyYzsPxlZQi3PDlxzCMmAnfiZAVKGPOEugHT4j7d8Fk5kJ0toB3NMPjlFxQHi+fur+lEXOhO/I1hNZaCF4XXDYbEGduQ/CLuUEWQaIO08YU4fN99ThZ78DJem0F85ozdqx85lM8dtsstLlF5IbNl8s6ISL0ufeTGy7AL9btwcGaFrj0IdHjt7cg4A3dmAO2RskdE8wca23zgNHHr0nj9Uhqy+f1KfvncUrxPf6AiJYWJzw+6b3kc3vQ3GyHe9uLIbdnsMO9yPvRePwYOKu2r1xP/vsWBR6CozlizbEIuEPvAVdzk/KeCDRLwlRk9Qm9B4Vg6QJ7iw2erMTfsz15L7sCq9WckNWrQyJo9GipeeTkyZNRXl6ORYsW4b333lPcXuHWF5vNFlyUVfnX6/XC6/XCaDRGjJMtQlarFbW1tRHXD7f8WK1W5Vw1drs9JUHRgUDXvGF4XujQ3LdUlMPnF2A0cF22tnSjo3tJRNIdeylmFYPJKoBob4TtX/+rDaIFAGuxdMNUBfEGvB6wAQH+5jMAAN2I88CNmgMBgBBl/cYZ1yJwvAqmecvg/MddQMALvlWyaAtgU/eaTTnIWnQfxK83onXbPyF4XHHnzjDpwEBULEFeUYebLhuDumY3jtfFjvv4T2U1xvgiv+QZzBk4J88M2TyWnZ+P/vkZqG1yoTm7HHktUokRIaxPnuh1gTFbIbqlz9KAwIBpZ0/4oNgSAgHlNfKBYMFGMAgEBAjggmP88J05Cu83HyjnM6YssP2Ggz/5Ddy7NsN0/vXRr9MD/77d7z2JwNGdMF/yC+gGndPueHUFcd5tV16PIkR1+oReoxi0jvI+X7u/n2j0xL3sTjrtHBwzZgw4jkNNTQ0GDRoEvV6PI0e0pb0PHz4MAIogkf8Nj9eprq5GZmYm+vXrp4w7evRoRLDk4cOHNeKmrKwsYi6fz4eamppelRkmwzAMjIbOB3ISRE+BYRiY5t4CcHqpq7wQAHQGGKZ8B9ygieBKyyMagcop10KbJILY7H5xr2E452JkVPwabGauUi9HPrejbTPiwRqDRQL98TOfrBkG5LEha8/oYcUw6Dn88tpJuOHiUUr8T1mp1uX/xYEGtHkib2b5lyzHnDEhdyDD6lAWrDZfpZ8C0+yboq5DP+oCQF1pup0UeWnu4OeQum6N3IVeDqwOBmOLfECbSg8pvsUwbj4AIHCsqt3r9SQCR3cCAHxfbmx3rCiKmjIAmuKUQWsnwyXWM052EYcHlxMdo9MiaNeuXeB5HgMGDIDBYMCMGTOwefNmzZiNGzeisLAQ5eXlACQLUlZWFjZt2qSM4Xkemzdvxpw5cxQX1pw5c2Cz2bBtW6grdG1tLaqqqpSUeQCYPXs2tm/fjpaWUEG19957Dz6fTzOOIIiei67/KBjOXag85vqPgnHyFci45GdS1lWYCBIaj8O95c8IHJSSJNic/glfi8mSK1UHb0wpyA4LhzUEYy/aEUFZRga/yg7VfflBhdRgNsOkw4WTSnHPjVNw3fyR+Ol3J+DmBWNw3fyRsGYa4PYGUGfTigrTt24F12842FxtXauyEklAHT7jiijsmPn9R2D69m0wzvwfqXhlkIQadSptONTZYdou9ErFaiEQeePWGcAFW46Izua0LJqYUGZbwAuovsyryxIolbN1sWsEaZDT6Hlf/HFEQiT19efHP/4xxo0bh1GjRsFkMmH//v34y1/+glGjRuFb3/oWAODWW2/F9ddfj5UrV6KiogJVVVVYv349HnjgAbDBbxYGgwHLly/H6tWrkZeXh/Lycqxfvx4nTpxQgqwBYMKECZg7dy7uvvtu3HXXXbBYLFizZg1KS0uxcGHow/Kaa67BSy+9hBUrVmDFihVoamrCww8/jIqKil5pCSKI3op+9AXw768EY8yA6fzFYU9qiwiqLQdMRg50gyYmfB02qxBq+ZCKFPmIawQFRXs3dmvDl2CDrrDWggkYaNJaBIx6DhedK/W1O2+cJPRO1Duwdfdp+MM+wpmgJYcrHQvTRcvB5g0EAAwrkSxBR2ttGsu6bvhMsNn9FCsaY4gfCB35IoP7phZBcs0g2ZKkHhNuCeL0YEwWxQ0ntNWBKxwCURTSplt6IjWOIt4D6grdcuPcZC1BAbIEpYKkRNA555yDTZs2Ye3atRBFEaWlpfje976HpUuXwmCQfoGTJk3CU089hcceewwbNmxAcXExVq5ciauvvloz10033QRRFPHiiy+isbERI0eOxNq1azFq1CjNuEcffRSPPPII7r//fvj9fkyfPh1PPPGEJrPMarXi+eefx6pVq3DbbbfBZDJhwYIFuOOOOzq6LwRBdAOsKQuZ3/9d1Bsgo4+d1WI8//rIDvFx0I+aBf+e90MHOlkxOhqJusN0x7ZDAPCmaxJGll+LgQnMfcX5QwCIKLU1AaoQTCaYzcUwDPRl05Xj/fMzwDCAx8fD5vLDNHURAtU7YJzxfc28TDvZYBHEcYchzB0GPiD9pyZ4Q2dz+oN32yC0noZv9ybwDUeRUXEXkJNY0HEyeHe+BqH5JEzfWpEa8dvO7xeIbBCsKVDJx2+ZEYFcVZrcYSkhqb/8H/7wh/jhD3/Y7rg5c+a064ZiGAY333wzbr755rjjLBYLHnzwQTz4YPzu0UOHDsW6devaXRtBED2bmBaAKEKFMVuR+T+PaSohJwJXMASGKd+Bb+d/pANdYAmSXUvx3CWiKEJoOQkA2OcvxbeKI0uCRCPPasIPLh0D/xEHPCotp3ZnqdFxLPKyTGiyeVDf4sKISRUwTqqIsubkUq7lJquiuit9eLHEoDtMFALacQjtDZvdH3ztAQittQgc+RwA4H57NQzXPJTUetpD9Lnhq3odACA0HlNccZ2aMxER1CDFybL5AyE0nYDoi3SHxWuZoSFoMRLJHZYS0sPeSBBEn0d0NCs/M5m5gDETpnnLkhZAMqwlT/WgC2OCwtwloscBz8cvwLvjnxDdbYBXSmm/8XtzUJRkK5wI61ccd1ZRrjR3fUsc91wMERUTxdWlbpsR5g7jQjFB4ZYg0SMFhLM5xdKQllBzbKH5ZMpjhPgzB0PXTlWrjgQsMnzDMQAAVzoueCAA0e+F55MX4fvmXelYgpYgRo4dIndYSki9DZggCKILkDvEA0Dm934HMAwYXRId18NgMkMiqEtigozRLUHudx9XbsaBU3uksVkFGD64A66fsBtnLEsQIImgfcdb0NAaW1gk7w4LCp0obTMUi568t3xkTJDc+kEOaleaugbhm08B/WL3f4sH33gcgWNVMEy4TGlKGzi9L+LaHUEMfx2CIPUZjLWWYFVuXckY+L9+BxAFBGp2w79HVS4g0cBocoelFLIEEQSRFhgmXAIA0I28AIze1CkBBABs5lmyBAV8inVEcNs01gih8bg0NonMNjUaSxCni+tSka1M8SxBKQmMlt1hrDY7TIyWHRa09MivX91pHgD45pOhab1OuDb9Af6DnyS0NNd/7oOv6nX4VEJDUIks2QrVIcIbwHoj5+KbT8K54QH4D38GUS7jUDRU2WOh+YT2BF1ivSApRT61kCWIIIi0wHDuVeBKxkhNUVMAYwnVWhYDqY+v0LScCHgAQwb4M4ek53IHgDFlhhq+hqW0J4xKBLUnYAqDIqihLbYI0pdNh/e//4auZExi15cbqKrcYYogihYYHRYTxA2Q3EOMpUAaF+YuCzQcg+3LD+BpsyHQfBr8yW/An/wG+pHnh8ac3gfPR3+B6fwboBs8Mbie0DxiW6jgruAMuVQ7ZQkKeLWPXW2AWVvHyf3OGoj2Bni2/Fl6jVmFYE1ZgDET8DrABwWwjPo1xYXcYSmFRBBBEGkBw+mhC940UzKfypLUKatAzPn1SqsP0ecGY8gAX7sfAMAVj4CubDrcm/4ANqc/9KNmtzNbjGuo0qoZS37csRazdPN0eQIxxzAmCyw3PJ6wZYyJWicoLDuMVccESeO40rHgSscqN36GZcFmF0NQWX4AwLdnCxr3bIm7hsCxKoiOJgSOVSkiSDOPUQr2FkUBojNUSy7a71wM+BLLMvSHiaCgoBIczeAbjoArGAzRrq3IzRUOASC5HEWErIAAkHHVPeCCzXzbg1ECo0kEpQISQQRB9Fl0I88HX/MV9EOndMn8jN4E0euEf/9WMIYMJQaEKy2HrmQ0LEuf7Vx/Q9UNuz2BaDJKwsbj4+OOSyrQPJE6QZxsLeIVCw2TmQfjxMu0U+X0D4kXnVEqMJgAoqNJ+leVds7XhToIyGJHdNs16wy3BAltZ+D852+gHzMXpllhNarCrxluCQq6w1yvr4LobIZuyLkR53CFQwEAjCzK3FJtA9OcpcllqSkxQe1bL4XWWsBkkSxQRFRIBBEE0Wcxz70FosB3SWA0IKXwi16nkpYNSMJLvkl2tsGz2mrBtSeCDNLHvccX2xKUNFHqBIkxUuTB+xV3GMNF7jdrDbU94YpHgD/5TUJLEIJZg+q4HKE1FEQvix21FUh9XMb35VuAKMC/d0u7IijSEhQUWkF3W+DYF6EnGRaAGHL9mSyac5msJAPidYkVSxRs9XD+89eA3oSsJc8kd40+BIkggiD6NF0lgAAgY86N8Oz7BELdIQgeOwyj58Aw7bupq4ZsyADXXyowy/UbEXeoyRCyBImi2GkBBkBpjRHNHRbqHSZbLvhQzE+UXm1qdx5jyIBhykL4dr7W7hIUS5BHVXtH/bM76KpSxQNJYzruAo2wBHkcUesF6cdfDMPEy4GAV+lXx1q02W5sVnLZb0yC2WH8acn1mkgxx74MiSCCIIguQl9aDqbf6C6bn2EYZFT8OqGxsggSRcDnF1LShJlhQ64uBVnoyG41xVrEh+JYorjc1GKAMWTAMLEC+tz+YOv3w75bGxckCgEwrA5iwBey9KgsQZqfPXaIfg9EmxSjw2QVQLQ3RgmMTlwURrrDnBDa6iJfU2YO2LCAaUYteji9plRDQiQogkS1dS7gSzj7LBH4usPgG45BP/ai1IjpboREEEEQRB/AqOfAABAhucRSIYJCdYJU2WGK0JFu1poYo6AbiYlmCcpSBXYbzGBYFvoRMwBfS8RY+L2AUae4n4CwpqQqK49ob4DjxZ8oae1c/mAE7I0Q3fZgwHqSBSJVr0N9PcEWKYKiCRy12GNziuPWF4qG7AJtNzBalWknep2AKXUiyPX6Kmktljzoh0xO2bzdAdUJIgiC6AMwDKMIn/aCoxNGCYxW3XCVuB/ZEhQSPIrLKIoLko2R3caaIlt5yCUNBFUVcfB+5XhE3R5VCQTdkMlgsosBkUfg6M7QmAQNGv4jn8P31WbtepzN8G7/R8RYJjM34pjaHcbmdKA0AhdKkRc8dvgPb49a4kFt6YpWxygVhGfzpSMkggiCIPoIpi4TQSpLUEBrCdLE/8hupChFHTXFL32hLutstCrWQTElxwPJKAHKMeJ92PxB0A2fCf3IWdI0hz6NOi4WQmstPO8/BaEpWOgwKPT40/si1gIAbBQRxKjatTAdyNoKFUv0wff5v+DZ8oxUk0gUNeM0IiiFJSDUbjbJrpjekAgiCILoI6Q8Q4wJucPkm7BSqDAoEBiWBYJxI6LsDouSHaZB5aKKbgnywn/wkwjri+h1QuQDoWBgVR0lNm8AzBf/DAzLQjdQytRS9ypTm4I0xR9VCG6b5nF4gUrGkg/jedeHHmfkRMyhzuhjwuKFEkI+n/fDf/gz6cdTe8CrWoIAgKhaa0rrYKkDrdNfA5EIIgiC6Cuk2hKkyayT+4cFLUH/396dx0dV3osf/5zZMtmGLARCwp6YkEBYgkBQBAGpG6kLtWJVvFxKr8Ct1rr21tpbi1Vbf9LauhZqS0VrUeu1KlrrEkAFlbDJIiQEAiEEQpbJOpOZOb8/TuZkNiCBIQnM9/16+YI555kzZ76MmW+e5/s8j98WHt6tM/ThsNDlqNaZizAOytNmVHmfGqInSHW20PrJH4OGeVRHY8cxRcGUMUk/F3Xxrfqmud6ZWmqLXU/M/MbDTrQGT+B2GQFJUdSF12POvRTTBRdhGTv7hNuYWMYVYkhMw5I7I/TrnIy3J6i1wW/VaHfFTu14ewLnPxzWRDh4mmpp+eiFjuueBzPPpDBaCCEixFkbDgNtSMwAeAKGw0BLetxtHb0IJ0iCzBmTMPskLgDGUD1BAXuMebn2fUXbrk8AUKLiMI+YimvPOu06iel6OyUqVuttcrbgaajGmJSOb7eG6nKimIMLidWA5ChwGr8hYQCKwUT09B+EvD+vqAlziJow56RtTkTfaDVgixH3kT207f2M1vV/xTL2qrNSE9T60XP6Vi8AnKVao+4kSZAQQkSIszYcBvpaQWqIniDFaEJt6+g56Mqq1KGGwzyNATPGzNHQ1kLbzo7NUlVnC8b+mVjGX4ditgQtUmiI74fn+AFte4ukdG0YrZ3rwGbMWVOC15Bqf2/GASOIuvhmDAlptO34UO8ROt2NcLtCiUnQEzjva3rqKnEf2aNvzuv88nW/f5twDYf5JUBhvG5PkuEwIYSIEJ3dOqPTfHt0vGsFBdQEae3akwnv0FMXFqgMORwWsPCheUSIvdc8Lm1G3PhrsIy+Mvi67VPVPd49vnymnDvWvkjb9veDr+kdDjNZMCYNQjEY/fZ9O63p9l2kGM2YfKalW/KvCb1vnG8Bc5iGw4JeQpIgIYQQ54qOnqAwJUE+C+Xps4YCZ4eBnhDpNSQnqJUJ+RIWn2Gp9iQjcAsM3xoiY2oWAObcmSe/rk2rC/I0VGvXDKj3ce4qCnqOdzjMt7jZkv9tzLkzsM5cfNLXCyfLyMtAMWJMy8GUMQnziGn6ueir7iHmO7/UkjPvZqthSFbU9p4nv2MyHCaEEOJc0VETFJ7hMEVRtK0zVHfHcJi+TlBwYbS+d9gJaoJCv4YB08Bc3HVVGPtl4Nr3BdWVh0nwadOkWrHOXIz7aClRE29Abaj2m4oeinddIn1qe8DigyFXQg6R4Ckmy6n3GgszY8owYm/6DYo1DkVRMOdcStuujzHY+mFMH4miKBin/SemzAJa3vk17qOl/qt6n4DqduHY+HfU1gasU+f7JXu++7Hp7c+DniBJgoQQIkJYzWEeDgNtaMvdkQSFTBQMJv/Z1KeaIh8grvA+XG0uHJ+9DECCoxKARk8Uy+xXMuxfe1h07UTMGRO110tIPeU1vWv06DOnAldgDrGSc6ieoJ5i8EnyDNE2Ym/6f6D4J2/GAVkQFYvaYsd1+BtInnDSazo+f0Wvq3La+hF14XX6OXflnqD2qqMpfPvQ9RAZDhNCiAgRY9V+721qOcWWC13hTRY8bm16tneqvG9NUGAhdEBPkKqqvPVpGXc//SlbS6qDXkJRDFrvUcCX7X7DEGrUPny5+yhflwUvVngyehLU0oCqeoK3oVBCJGreBK8XJEGBFIMhaGNexWDCPGw8AM3rX6J2/Wu0bn0vaGFFL9ehr/W/O4v/D8cXr+Fpn2Xm2l8c/ATVc9bqjbqLJEFCCBEh4mO0L++G5nAmQd6NWd0d0+MJGA4LTIICHn+5+yhvriujtsHBn9fsxhHQU+XxqKx8bzdlB/0TpGFDU5kxXpv6vupfe2hzhV7kMBTvbDFP7SGa/nonnrojAQ1C9AS11w2daP2f3sgy7tsoMQl4ag5RW/QKLZ++jLP4raB2qseD2qjFV+mj9aQ5t7xNy9uP0/LJctxVe7WGAUsHeE6wXMG5QpIgIYSIEPEx2pd3Qxh7ghTfrTN8167xSXQCp5oH1gR9ufuo/vf6Jief7/RPSEor6vlky2H+enCI3/HE5CSunTKcPrEWqmpbeG/jgc7ft8+WFWprA7QFFP6G2tjU3TE77FxhiO+LdeYiv2POre8GFYKrzbXakKZiJObyO1Fs/QBtfzDXnvUAmLKmEDv311hnLtKXAwgsUj/XSBIkhBARoqMn6AQrIp8OxWc4zDuk5C2Y9goshPZJkFxuD1+XaVPeL8zWZmx9ueuoX/NvDtYBcMSdwOP1szteOiqWGKuJG2dkAvD25weob/Tf4f2Etx2wblCQEIXEql7vdO4kQQCmAdlY8wuJHjZaS+BcjqBtNryz5JS4JAwJA4ib+2tirvtflD79wWjCOv0HWKctwBBtw5wxCaV9iYFzPQmSwmghhIgQ3p6gxpY2PKqKIRwFrXpPkLujuNhg9iuWdQf8vr21tI785MEA7D/SgMPpxhZj5jvTM/nqm2PsOlDLPc98ytQxaTQ53HzwRbn+3Gp38Kajk3L789an+zlS08yBqkZGx0UFtQl0quLmkFtC6IXR585wmFd0wQ0kJsZS8ebTOHZ8RFvJBkyDx+jn1fb1krzrJwEYU4YSe8Oj4HIErYFkiE3EjbaVxsmoLietRX/CU1uBZfw1mIddGL43FQbSEySEEBEiLrp93yk1jMXR3iTIt7g4IEloVf0fv/JJGS63Vr9TY9eSjf5JMfRLiGbksKT24w7eXFfmlwD9/D8msOyujvV/1PZd6RVFoV+i9iVd2xCm/axCrYvjOveGwwJZsqcA4Cr5HNfB7fpxj92bBKX4tVcMhpCLQCoxicCpe4LcR/biKt2Ap+Ygjk9fCi5A72GSBAkhRIQwGQ3ERGkDAOEqjvbOSFI9br0mKLDmpyz5Ejw+E5LcGHj/i3Jcbg91jVpikRiv9d7Muzxb77HyGpOZTOFFQxncP47oKBPm3Jko1njMWVP0Nkk2rWC3tqFzw2GnEmpxQG9P17lUGB3IlJqJOWc6AC0f/AF3VQkA7mP7ALThr07wrsPkaT5FT5BPrZXaXIerZEOX7/lskuEwIYSIIPExZpodrva6oOB9ubpMXwjRZzgsoCdoX3McW5sLmBurfQG6VQOvF2lfuk2tWuKU0D6ElZIQzWP/NZmaBgevfVLCtZdeQEZqHC6fmV/WKbeiXnyz35RwbxJVYw9PEoTHhepu80t4zoeeIICoi76Hx34Ud8UOmt/5NYb4FDy1FQCYh47v1DUMek9QDZ7mOpRoW9AUfaBjq5R2roPbMGdfcmZvIIykJ0gIISJI2KfJ+64TdIKeoPKqBna3pemPnao2hLZ5bzV17T03CT51PNFRJtL7xnL33HFcmBO6ZyLwCzepPQnq0nCY+eR7fQX1Bumbw57bSZBiNBP9rTswDsgGl1NPgAz9MzF0YqFJQC+M9hw/SNNLP6L5tQdDTpfXhyyjbQC4D+/u2GKlF5AkSAghIkjYp8nrNUGhe4Jcbg/7Dtup9cTRMuYGzBO/yzXTcwE4cKSBqtpmABLizyyx0HuCujAcFlP4QPBBn3VwXv/313z6yee07V6Lp7EGT722UnVgT9e5SDFHEX31/Vhn3I7lwusx50zv0vYfhoRUiOroSfTUHqYtxH5rtBeYG9NywBSF2tqAY8OrJ1ywsbvJcJgQQkSQ5PbamSPHm8NyPcWi7fLuqTuCwVtPYuhIEvYfacDp8hAXbSZl4nQUReFyVeXDTQc5bndQVqmtSJzYiRldJ5Oo9wR1Pgky9h1C1JR5ONavbD9gIe62Z2h6+ceozXU0lxQzOvYrWgN2jOgN22aEg2IwYM4sOL3nKgaMCWkdiygC7oodMP4av3Zq+3CYEhWLaeg4XCUbaNv+PqZBozENHHn6Nx8m0hMkhBARZOgAbYp52RF7WK63w60tYGjf/ok+88d3Cvk35VrhbNagBH3avKIoZA5M8LtOwhkmQUnxWnLX6nRjb+pYB6mh2Yn9JOsiKaaO11Wsse2zobTELsccvGkocM6tE3S2GAdk+z12V5UGLS3gHQ7DFIX10u9jaq85cm5b0y33eCqSBAkhRAQZNkCrzSg5VM/mPcdO2rbNdeqNVl8uScClGohqOkLj4fYVm31qgraVant65Q5N9Hve4H7+ixWeaRIUZTGS3lcbntlSUk2Lw8Xbn+3n7qc/5Rcvfkmr0xX6iT7DX4pVi02bSbtOhrkq5FPOxXWCzgZLfiHm0VcQc+1D2grTqhtX+Vb/Ru1JkWK2ohhMRBXcCIoB96Gv/fYq6ymSBAkhRATpnxSj//33b2zX1+nxVVXbzNKVX7Fk2Vp27a854bVaHC7qnSaqPVrvkmnnO9oJY8eijCUV9QCMyejr99xBPknQ4H5xRFm6trN8KCMGa4nWn9fsZsmytbyxdh8ut0ptg4ONO0+Q0Jh9eoJitCSotFb7arQoWhK4U8nyf1Iv7Qnasb+GO59ax6oP9ujrMJ1NiikKa8FcjP2GY86YBEDb3s/82ujDYWYtZgZbP8wjLwOg5d9P4z5aetbv82QkCRJCiAhiUBQKRnbMuNp7SEtSahsclB6up67RweOritl32I7LrfLqRyV4TlDEWnFM20G81hPjf6J9bZgvd1WhqpCeEktyH/+NN32ToKxBCWf6tgAYMcT/OonxUQxM0V5n/fbKkM/xHw6z0dzq4mCTf7nsv5tHEHP9LzoO9NKeoHVbD9PQ3MaHmw7x0aZD3fra5gsuAsB9cBuu8i0dJ/ThsI5//6gLr8OQMgycLTi++kc33mWwLiVBa9asYfHixUybNo2xY8dSWFjIyy+/jMfjn3EWFRVx7bXXkpeXx6xZs1i1alXI661YsYIZM2aQl5fHnDlz2LhxY1CbxsZGHnroISZNmsS4ceO4/fbbqaioCGpXVlbGggULGDt2LJMnT2bp0qW0toZp5VAhhDiPfH92LlPytA0wvT01j63axCMrN7HyvW+oa3Ria59FVn60kU3fhB42O1TdCIDb6j/U5a7cQ+XxJv6xrgyAqWPSgp7bx2f4Ky8j+QzfkWbU8GRGDkviolGpfH92Dg8vmMiS60Zp76OqEY8nRDLnOxwWbWPPwTrq3f5J3aGWWFpi+mlr4cT3RbGEYX2lM1R6uJ7K4036Y4+qsnN/x8KFRVsPd+sMLEPCAExZl4Cq0vLeb3Fuew/w7QnySTYt0VinLwTAfXgXqjM8Rfqno0uzw1588UXS0tK47777SE5OZuPGjTzyyCMcPHiQ+++/H4DNmzezePFirrnmGh544AGKi4tZunQpFouFG264Qb/WihUrWLZsGXfddRe5ubmsXr2ahQsXsnr1arKzO4qt7r77bnbs2MHPfvYz4uLieOqpp5g/fz5vvfUWVqv24bXb7dx2222kpaXx1FNPUVNTw6OPPkpdXR1PPPFEOOIkhBDnDYOiMGp4Euu3V1JyqB57k5NjddovjVtKtI00r5s6nNoGB299up831+3jwuwUv/3AAA5WaUmQpU9f8Bk1M028gV+/spnGljb6xFlCJkEAP503nqO1LeQND08SFGU2cveNY/2ORVtMmIwG2lwequ2t9EvwXxvI98vZEG1jd3ktdT49W/VqLA7MVNvbGHLTE6CqKKF2mO8GbS4Pn+84wpa91fq/09WTh3D91OEcrGqksX3ZA6NBofJ4MwePNjK4f/Bea2eLdcotNNdX4qkqoW3v51hGX9FRGG32r/kyJqRhSBiAp64SV8XOHttTrEtJ0HPPPUdSUpL+uKCggObmZlatWsVdd92FxWLh6aefJjc3l1/96ld6m8rKSn73u98xZ84cDAYDTqeTZ599lnnz5rFgwQIAJk6cSGFhIc899xzLli0DYOvWrXzyySe88MILTJs2DYCsrCxmzZrFP/7xD2666SYA/va3v2G323nzzTf1+zMajdxzzz0sWrSIjIyMMwyTEEKcX4a2F0hXVDeyp32Xdl+jM/pitRh5/4uDVB5v5kBVA0NTbX5tdrTXC9lSUvUk6IuEq0jrM5H6xm0oCjzwvXyizKHrfTLS+pCR1id8byoEg0EhNSmGQ8caOVzdFJQE4TscFh3P4eomGtWONpXGdACq61qD3n93e+XDvXyy2X8k5J3PD/BxcQWtTq1+KT8rhebWNnaX11FxrKlbkyDFFEX0jNtpeuUePDWHtBW2vYXRJmtQe/PoK3B++TqGuL5B57pLl9JZ3wTIKycnB4fDQV1dHU6nkw0bNnD11Vf7tSksLOTYsWPs3LkTgOLiYhoaGpg9e7bexmg0ctVVV1FUVKR34RUVFWGz2Zg6dareLi0tjfz8fIqKOhZlWrt2LZMnT/a7v8svvxyLxeLXTgghhCbZFoVBUXC5VTbu8i8aHjk0kcT4KKKjTIxq39B0y95qvzZVNc0crW3BaFBIGzxQP77xuI2t+7RhmYtGpfoVYveUtL7aPVRWN9HQ7OTj4kNs36fNWlN8hsMwWzlub6XetycobigAx+pD7CXWBY0tbew6UEtz6+kvUhk4m2/a2DQMikKzw6XXbU3JG6BvJutdiLI7KXHJKNZ4UN14jpeHHA7zsoyYRtytT2FMGdrNd9nhjBdL3LRpEwkJCSQnJ1NWVkZbWxvDhw/3a5OZmQlAaWkpo0aNorRUqwYPbJeRkUFTUxNVVVWkpqZSWlrKsGHDgrpgMzMzWb9+vf64tLSUOXPm+LWxWCwMHjxYf60zYTKFt+vTaDT4/SlOn8QyfCSW4XMuxNKEgSRbFNX1rRT71PwM7h/Houvy9J97+dkpbNpzjG2lx/nOdO1nucej8lqR9rM1a1ACcckpeFcd2ldvory9t2LcBSln9PMzXHEc2C+OL3Yd5f8+LeOfn+3Xe00A5l1+Ad7dsowWKzX247g8HT1Bhv6ZsL+RdVsrGZeVgtGgMCC5azVBJYfq+fXLxbQ63eQMSeQnt3Zufy5f9iYn9e3rH10/bTj9E2OYPCqV783KYt3Ww6z6YA/JNivjsvvqyc/RuhY9/t35mTT1H07bga249qzTd5k3WaMxhvm7NBzOKAnavn07b7zxBkuWLMFoNFJfrxXY2Wz+XYbex97zdrsdi8Wi1/R49emjdYvW1dWRmpqK3W4nPj64K89ms+nX8l4v8DVDtTsdBoNCYuLZKYKz2U6+b43oPIll+Egsw6e3xzItJY7q+la85bO/vWsaw9P7+P3iefG4gfzxnzspr2ogOiYKa5SJz7cfZtM3xzCbDNx6VS59M/pi/tZ/smptJR4MOF0ejAaFKfmDiI0+85lUZxrHyyYN5d3PD/glP14r399LfhIoQNTAC3C0HQWMJF17D0aPgyuGTWbN7k84UtPM/zyvbQB746wsbrkiJ+haza1tHK1t0YcaAdwelWd+v15/7V0Hajlqd5A9JHhk5WTKj2mJzYC+scz/dp5+PBG4MbUPU/IHERNlItFmJaN9qYDjdkfQ91d3fCZNoy/h2IGtOH220ejTNxFzQs8XlAc67STo2LFj3HHHHeTl5bFw4UK/c4E9N6GOh2rjHQY7VbuTHQ+8XmfanYzHo2K3h7dL0Wg0YLNFY7e34O6GtRzOZxLL8JFYhs+5EsuE2I71bhQF4iwG6ur8f94Z0DYnrWlw8N6n+5g2Lp1/b9QWRbzswkEMTI6mtrYJMi/FUL4X2hdMzBzYB2erE2friVdrPpVwxTHOYuDh70/i4+JDGA0GZo4fyIebDvH2Z/sB+J/a72JR3GS8oe2PYYu1oKaNxgXQ5mL+VTk88cpm/Xr/+LiEKyYMxBhQIP3717bx5e6jPHBLPrZYC337WDlc3UR1fSvRUUZGDU/my11H+dNbX3P/zfn691Oby0ONvfWkQ4c792m9del9Y7R4B4gxKeB2U1vbRGz7mksVxxqpqWlEUZTu/UwOmkjM1P+gee2f9UP2Jg8GNfi+zxabLbpTvV6nlQQ1NDSwcOFCrFYrzz77LGazlul7e3ICe1/sdnv7Tdn0Px0OBw6Hg6ioqKB23uvYbDYqK4PXdgjs+bHZbPpzA+8zHEXRLtfZ+cC43Z6zdu1II7EMH4ll+PT2WCbbOn7+piREazVCIe532AAbNQ3HWPHOLg4ebdTrgyaO6OfXfvrYdD7dVkldo5P8C1LC9t7DEce+Nis3XJqpP75+6nCumTKUzXuqKd5zjA07q/SlAJLio/xeL3dIIpeNH8ieQ3WUVzXidHk4WNWor0EE2kaxX+4+CsBjLxUD2sKUI9tXys4ZksScqcPZvKeanftrue+Zz5h/VQ5ZgxL4y5rdrN16mP/69ki+OVjHZ19X8q0Jg7h+qvb9ZW928uEmbYhxUL/4U8YiuX0fteZWF/YmJ7HWjt647vpMGkdciqWxBmfxW9rrGsx4euH/C10eoHM4HCxatIjq6mqWL19OYmLH+hCDBw/GbDazb98+v+eUlJQA6AmJ98/Aep3S0lJiY2Pp37+/3q6srCxorYOSkhK/5CYjIyPoWk6nk/LycpkZJoQQJ5CS2DE0knaSOpeM9I4ZXP/68iBOl4eBKXEM7u+/9UWSzcr//udE/uvbI5menx7+Gw4zo8HAhSP6sbAwl4S4jl4x7yazvr43K4v/nT+RrIFaLJ58dQtVtc14PCr1TU4OHGkIek5VTTMfFWvJy8hhSfRLjOGmyy7AoChU1bbw2Kpi3ttYztqt2h5lz7+1g082V+Bs8/D2Zwf41xflvPP5fv7n+Q1U1TSTGB/F9HGnjqvFbCQ6Suvj8N1HrbuZfKe999JVtruUBLlcLu688052797N8uXLSU/3/8ewWCwUFBSwZo3/xmhvv/02KSkp5ObmApCfn098fDzvvvuu3sbtdrNmzRqmTZumdxFOmzYNu93OunXr9HaVlZUUFxfrU+YBpk6dyoYNG6it7Vgo6oMPPsDpdPq1E0II0WFMRl/GXdAXi8nAxNx+J2w3dcwApo9Lx+QzvHDNlKEhyw1sMRYm5fb3a9vbKYrit2p1YHLna3CqVqda1+jk53/6gh/9fj13/X49j/x1k1+7CSP6kdo+vDU0NZ5JOdov99PHpfPbO6aQM0TrQPj7xyVBr+HdQuRvH5XwetE+mh0uBqbEccec0cR1ssbKu9hlQ/Ppz0Y7U8bkwVhn/ZDoq+4549KUs6VLw2EPP/wwH3/8Mffeey+tra1s2bJFP5eZmUlcXBxLlizhlltu4cEHH6SwsJDi4mJWr17Nww8/jKF9/NRisbBo0SKWLVtGUlKSvljiwYMHefLJJ/VrjhkzhksvvZSf/vSnPPDAA8TFxfG73/2O9PR0rrvuOr3d3Llzeemll1i8eDGLFy/m+PHjPPbYYxQWFkpPkBBCnEB0lIkfzhl9ynYxVjO3Xp7Nd6dn8unXWolCflbK2b69bjU01cYXu7ThrOn5A0/YLmdIIv/+StuSwtnmwdnmP8ST1jeWnMGJ3DhTG3qrqm0hLTnGLwmIizazsDCX37yymaqaFjyqSs6QRFISohmdkczojGT+/dUhPt58CI9H5eqLhjJ1dBoGQ+cTifgYC1W1LT3aEwRgHtb1mXDdSVG7sK72jBkzQm5ZAbBy5UomTdI2UCsqKuLJJ5+ktLSU1NRU5s+fz8033+zXXlVVVqxYwapVq6iuriYrK4t7772XgoICv3aNjY08/vjjvP/++7S1tTFp0iR+9rOfBfVClZWVsXTpUjZt2oTVamX27Nncc889QTPQusrt9lBTE95iLpPJQGJiLLW1Tb26XuBcILEMH4ll+Egsw6M749jc6mLFOzsZn53CRaMGnLCdqqqUVtjpnxTNrgPa6EPu0CR27q8hPsZC9uAEDF3o9VBVbTitT6wlrL0lv399G5v3VnPr5dlaL16EfSaTkmI7VRjdpSQoEkkS1LtJLMNHYhk+EsvwkDievj+3F1tfe8kwvn3xsIiLZWeToHNn0FYIIYQQnRLvrQlq6rmaoHOBJEFCCCHEecYWo83Gamjp2Zqg3k6SICGEEOI84+0J6unC6N5OkiAhhBDiPBOv9wTJcNjJSBIkhBBCnGe8iz9W17fi6sVbt/Q0SYKEEEKI88yAvrHERZtxON2UVpzZRuLnM0mChBBCiPOMQVEYNVzbqf6Ntfv4avdR3B6VqppmmlpliMzrtHeRF0IIIUTvNT4rhQ07qth7qJ69r23j06+PsGXPMSxmIwOSY1AUmDomjQuz+xEdZcLl9mBvcpIUYu+085UkQUIIIcR5KD8rhZtmXsC/viznuN3BpvZd7lscLvYdtgNQWmHnnc8O8OMbx/C717ZRebwZgOgoI1dMGkLhRUN76va7hQyHCSGEEOchRVGYNWEQv1l8MbdcnqUfnzyyP0NT48kc2AeAo3UtPLqqWE+AAFocbt5aX8bR2uag655PpCdICCGEOM99a8JgUpLicDraGO+z+e3yt3fy2ddHqG/U1hO67YpskvtY+cfafZRVNrB+eyXXTz1/NyKXJEgIIYSIADMuHBS0d1h+VgqffX0EgGRbFFPHpKEoCv0TY3i9qJSxmSknutx5QYbDhBBCiAg19oK+fGvCIKKjjFxVMETfyT4lIZrbrxnF8DRbD9/h2SU9QUIIIUSEMigKc2dewNyZF/T0rfQI6QkSQgghRESSJEgIIYQQEUmSICGEEEJEJEmChBBCCBGRJAkSQgghRESSJEgIIYQQEUmSICGEEEJEJEmChBBCCBGRJAkSQgghRESSJEgIIYQQEUmSICGEEEJEJEmChBBCCBGRJAkSQgghRESSJEgIIYQQEUlRVVXt6ZvozVRVxeMJf4iMRgNutyfs141EEsvwkViGj8QyPCSO4RNJsTQYFBRFOWU7SYKEEEIIEZFkOEwIIYQQEUmSICGEEEJEJEmChBBCCBGRJAkSQgghRESSJEgIIYQQEUmSICGEEEJEJEmChBBCCBGRJAkSQgghRESSJEgIIYQQEUmSICGEEEJEJEmChBBCCBGRJAkSQgghRESSJEgIIYQQEUmSoG5WVlbGggULGDt2LJMnT2bp0qW0trb29G31GgcOHOChhx7immuuITc3l9mzZ4dsV1RUxLXXXkteXh6zZs1i1apVIdutWLGCGTNmkJeXx5w5c9i4cePZvP1eY82aNSxevJhp06YxduxYCgsLefnll/F4PH7tJI6ntm7dOm655RYKCgoYNWoUM2fO5NFHH6WhocGvncSy65qampg6dSrZ2dls377d75zE8+TeeOMNsrOzg/574okn/NpJHE9OkqBuZLfbue2222hqauKpp57i/vvv55///CcPPvhgT99ar7F3716KiooYMmQIGRkZIdts3ryZxYsXk5ubyx//+Eeuu+46li5dyurVq/3arVixgmXLlnHzzTfzwgsvMGTIEBYuXMg333zTHW+lR7344otYLBbuu+8+nnvuOS677DIeeeQRfvOb3+htJI6dU19fz7hx4/jlL3/JihUrmD9/Pm+++SZ33nmn3kZieXqeeeYZ3G530HGJZ+ctX76cV199Vf/v5ptv1s9JHDtBFd3m+eefV8eMGaMeP35cP/bWW2+pWVlZaklJSQ/eWe/hdrv1v99///3q1VdfHdRmwYIF6ne+8x2/Yw8++KB68cUX6893OBzq+PHj1ccff1xv43K51CuvvFL90Y9+dJbuvvfw/Yx5/epXv1Lz8vJUh8OhqqrE8Uy8+uqralZWlnrkyBFVVSWWp6OkpEQdO3as+sorr6hZWVnqtm3b9HMSz1N7/fXX1aysrJD/r3tJHE9NeoK60dq1a5k8eTJJSUn6scsvvxyLxUJRUVEP3lnvYTCc/CPpdDrZsGEDV199td/xwsJCjh07xs6dOwEoLi6moaHBbzjNaDRy1VVXUVRUhKqq4b/5XsT3M+aVk5ODw+Ggrq5O4niGEhISAHC5XBLL0/TII48wd+5chg0b5ndc4hkeEsfOkSSoG5WWlgYN8VgsFgYPHkxpaWkP3dW5pby8nLa2NoYPH+53PDMzE0CPo/fPwHYZGRk0NTVRVVXVDXfbu2zatImEhASSk5MljqfB7XbjcDjYsWMHTz/9NNOnTyc9PV1ieRree+89du/ezZIlS4LOSTy7Zvbs2eTk5DBz5kyef/55fXhR4tg5pp6+gUhit9ux2WxBx202G/X19T1wR+ceb5wC4+h97D1vt9uxWCxYrVa/dn369AGgrq6O1NTUs327vcb27dt54403WLJkCUajUeJ4GqZPn65/IVxyySU8+eSTgHwmu6qlpYXHHnuMH//4x8TFxQWdl3h2TkpKCj/84Q8ZM2YMiqLw0Ucf8dvf/paqqioeeughiWMnSRLUC6iqiqIoPX0b55QTxcv3eKg23q7dSIr3sWPHuOOOO8jLy2PhwoV+5ySOnffCCy/Q3NxMSUkJzzzzDLfffjsvvviifl5i2TnPPvssycnJXH/99SdtJ/E8uUsuuYRLLrlEfzxlyhSioqL4y1/+wu23364flzienAyHdSObzYbdbg863tDQELKHSATz/nYS2HPmjas3jjabDYfDgcPhCNnOe53zXUNDAwsXLsRqtfLss89iNpsBiePpGDFiBPn5+Xz3u9/lD3/4Axs3buSDDz6QWHZBRUUFf/rTn7jjjjtobGzEbrfT3NwMQHNzM01NTRLPM3DllVfidrvZtWuXxLGTJAnqRhkZGUG1P06nk/Ly8hNOBxf+Bg8ejNlsZt++fX7HS0pKAPQ4ev8MjHdpaSmxsbH079+/G+62ZzkcDhYtWkR1dTXLly8nMTFRPydxPDM5OTkYjUbKy8slll1w6NAh2tra+MEPfsCECROYMGGC3msxb9485s+fL/EME4lj50gS1I2mTp3Khg0bqK2t1Y998MEHOJ1Opk2b1oN3du6wWCwUFBSwZs0av+Nvv/02KSkp5ObmApCfn098fDzvvvuu3sbtdrNmzRqmTZt23nfxulwu7rzzTnbv3s3y5ctJT0/3Oy9xPDObN2/G7XYzcOBAiWUX5OTksHLlSr//fvKTnwDwi1/8gp///OcSzzPw7rvvYjQayc3NlTh2ktQEdaO5c+fy0ksvsXjxYhYvXszx48d57LHHKCwslJ6gdi0tLfpyARUVFTQ2NvLee+8BMHHiRJKSkliyZAm33HILDz74IIWFhRQXF7N69WoefvhhfYq9xWJh0aJFLFu2jKSkJHJzc1m9ejUHDx7UC1rPZw8//DAff/wx9957L62trWzZskU/l5mZSVxcnMSxk/77v/+bUaNGkZ2djdVq1RPL7OxsLrvsMgCJZSfZbDYmTZoU8tzIkSMZOXIkIPHsjAULFlBQUEBWVhYAH374IX//+9+ZN28eKSkpgMSxMxT1fF8EoJcpKytj6dKlbNq0CavVyuzZs7nnnnuCKvMj1aFDh5g5c2bIcytXrtR/gBYVFfHkk09SWlpKamoq8+fP91spFbTCvhUrVrBq1Sqqq6vJysri3nvvpaCg4Ky/j542Y8YMKioqQp6TOHbNCy+8wLvvvkt5eTmqqpKens6sWbNYsGCB3+wmieXp2bhxI/PmzeO1114jLy9PPy7xPLmlS5eybt06jhw5gsfjYejQodxwww3ceuutfr03EseTkyRICCGEEBFJaoKEEEIIEZEkCRJCCCFERJIkSAghhBARSZIgIYQQQkQkSYKEEEIIEZEkCRJCCCFERJIkSAghhBARSZIgIYQQQkQkSYKEEEIIEZEkCRJCCCFERJIkSAghhBAR6f8DUouB/8NdauQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(zip(Y_test,y_pred),columns=['Y_test','y_pred']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "732393ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df.to_csv(\"../result/ANN/btc_NN.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12ff04f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06768fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "0a952188e4bab49300a5758bda39ddc90e91f41f35dfe6ea820e496e515be371"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
