{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5b918ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from datetime import datetime\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n",
    "from keras import losses\n",
    "from keras.constraints import maxnorm\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_validate\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from collections import Counter\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import randint\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, accuracy_score, f1_score, classification_report\n",
    "# import pickle\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, roc_auc_score\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ded3682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535a3fac",
   "metadata": {},
   "source": [
    "### Bitcoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1467391e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66099/2421463472.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  btc['Date'][i]  =  datetime.strptime(btc['Date'][i], '%Y/%m/%d')\n"
     ]
    }
   ],
   "source": [
    "regs = pd.read_csv(\"../Data/train_btc_selected_features.csv\")\n",
    "btc = pd.read_csv(\"../Data/btc_Data.csv\")\n",
    "for i in range(len(btc['Date'])):\n",
    "    btc['Date'][i]  =  datetime.strptime(btc['Date'][i], '%Y/%m/%d')\n",
    "\n",
    "btc = btc.set_index(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d665a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "btcData = btc[regs.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8227b01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66099/751970969.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  btcData['returns'] = btcData['priceUSD'].pct_change()\n"
     ]
    }
   ],
   "source": [
    "btcData['returns'] = btcData['priceUSD'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0204bfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = btcData.drop(columns=['priceUSD'])\n",
    "Data = Data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8a926d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>difficulty14mom</th>\n",
       "      <th>difficulty30mom</th>\n",
       "      <th>difficulty3rsi</th>\n",
       "      <th>difficulty90mom</th>\n",
       "      <th>difficulty90sma</th>\n",
       "      <th>hashrate90std</th>\n",
       "      <th>hashrate90var</th>\n",
       "      <th>median_transaction_fee90rocUSD</th>\n",
       "      <th>median_transaction_feeUSD</th>\n",
       "      <th>mining_profitability</th>\n",
       "      <th>...</th>\n",
       "      <th>price90wmaUSD</th>\n",
       "      <th>sentinusd90emaUSD</th>\n",
       "      <th>sentinusdUSD</th>\n",
       "      <th>top100cap</th>\n",
       "      <th>top100cap14trx</th>\n",
       "      <th>top100cap30rsi</th>\n",
       "      <th>top100cap30trx</th>\n",
       "      <th>top100cap90rsi</th>\n",
       "      <th>top100cap90trx</th>\n",
       "      <th>returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-07-18</th>\n",
       "      <td>1.621380e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.737230e+02</td>\n",
       "      <td>2.055500e+01</td>\n",
       "      <td>8.384389e+08</td>\n",
       "      <td>1.757449e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>401834.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.612000e+03</td>\n",
       "      <td>25.782</td>\n",
       "      <td>0.139</td>\n",
       "      <td>71.191</td>\n",
       "      <td>0.308</td>\n",
       "      <td>79.756</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-19</th>\n",
       "      <td>1.621380e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.737230e+02</td>\n",
       "      <td>2.248600e+01</td>\n",
       "      <td>8.819952e+08</td>\n",
       "      <td>1.944789e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>481473.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.047000e+03</td>\n",
       "      <td>25.685</td>\n",
       "      <td>0.123</td>\n",
       "      <td>68.863</td>\n",
       "      <td>0.300</td>\n",
       "      <td>78.999</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.183196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-20</th>\n",
       "      <td>1.584060e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.735570e+02</td>\n",
       "      <td>2.441400e+01</td>\n",
       "      <td>9.280542e+08</td>\n",
       "      <td>2.153212e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>431831.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.341000e+03</td>\n",
       "      <td>25.602</td>\n",
       "      <td>0.107</td>\n",
       "      <td>66.923</td>\n",
       "      <td>0.292</td>\n",
       "      <td>78.355</td>\n",
       "      <td>0.544</td>\n",
       "      <td>-0.088475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-21</th>\n",
       "      <td>1.580420e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.700790e+02</td>\n",
       "      <td>2.630400e+01</td>\n",
       "      <td>9.761949e+08</td>\n",
       "      <td>2.382391e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>460783.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.122000e+03</td>\n",
       "      <td>25.638</td>\n",
       "      <td>0.092</td>\n",
       "      <td>67.334</td>\n",
       "      <td>0.284</td>\n",
       "      <td>78.431</td>\n",
       "      <td>0.541</td>\n",
       "      <td>-0.020434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-22</th>\n",
       "      <td>1.580420e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.700790e+02</td>\n",
       "      <td>2.819400e+01</td>\n",
       "      <td>1.019474e+09</td>\n",
       "      <td>2.598318e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>334641.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.570000e+03</td>\n",
       "      <td>25.604</td>\n",
       "      <td>0.078</td>\n",
       "      <td>66.526</td>\n",
       "      <td>0.275</td>\n",
       "      <td>78.166</td>\n",
       "      <td>0.537</td>\n",
       "      <td>-0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-24</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.384949e+13</td>\n",
       "      <td>4.612156e+19</td>\n",
       "      <td>5.317997e+38</td>\n",
       "      <td>165.380</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.0577</td>\n",
       "      <td>...</td>\n",
       "      <td>18820.0</td>\n",
       "      <td>28544682257</td>\n",
       "      <td>6.114577e+09</td>\n",
       "      <td>15.236</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>40.118</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>45.140</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.008075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-25</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.394503e+13</td>\n",
       "      <td>4.542600e+19</td>\n",
       "      <td>5.158804e+38</td>\n",
       "      <td>214.464</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.0541</td>\n",
       "      <td>...</td>\n",
       "      <td>18759.0</td>\n",
       "      <td>28076113671</td>\n",
       "      <td>7.224812e+09</td>\n",
       "      <td>15.231</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>39.880</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>45.047</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.006565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-26</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.404058e+13</td>\n",
       "      <td>4.624135e+19</td>\n",
       "      <td>5.345655e+38</td>\n",
       "      <td>255.004</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.0571</td>\n",
       "      <td>...</td>\n",
       "      <td>18701.0</td>\n",
       "      <td>27566714445</td>\n",
       "      <td>4.898449e+09</td>\n",
       "      <td>15.233</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>40.069</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>45.106</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.004001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-27</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.413612e+13</td>\n",
       "      <td>4.576227e+19</td>\n",
       "      <td>5.235464e+38</td>\n",
       "      <td>113.430</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.0618</td>\n",
       "      <td>...</td>\n",
       "      <td>18642.0</td>\n",
       "      <td>27053515674</td>\n",
       "      <td>4.216170e+09</td>\n",
       "      <td>15.259</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>41.833</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>45.656</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.423166e+13</td>\n",
       "      <td>4.552126e+19</td>\n",
       "      <td>5.180463e+38</td>\n",
       "      <td>104.294</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.0592</td>\n",
       "      <td>...</td>\n",
       "      <td>18578.0</td>\n",
       "      <td>26631320968</td>\n",
       "      <td>7.843657e+09</td>\n",
       "      <td>15.219</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>39.965</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>44.957</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.019828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4517 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            difficulty14mom  difficulty30mom  difficulty3rsi  difficulty90mom  \\\n",
       "Date                                                                            \n",
       "2010-07-18     1.621380e+02     1.641620e+02          100.00     1.737230e+02   \n",
       "2010-07-19     1.621380e+02     1.641620e+02          100.00     1.737230e+02   \n",
       "2010-07-20     1.584060e+02     1.641620e+02          100.00     1.735570e+02   \n",
       "2010-07-21     1.580420e+02     1.641620e+02          100.00     1.700790e+02   \n",
       "2010-07-22     1.580420e+02     1.641620e+02          100.00     1.700790e+02   \n",
       "...                     ...              ...             ...              ...   \n",
       "2022-11-24     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-25     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-26     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-27     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-28     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "\n",
       "            difficulty90sma  hashrate90std  hashrate90var  \\\n",
       "Date                                                        \n",
       "2010-07-18     2.055500e+01   8.384389e+08   1.757449e+17   \n",
       "2010-07-19     2.248600e+01   8.819952e+08   1.944789e+17   \n",
       "2010-07-20     2.441400e+01   9.280542e+08   2.153212e+17   \n",
       "2010-07-21     2.630400e+01   9.761949e+08   2.382391e+17   \n",
       "2010-07-22     2.819400e+01   1.019474e+09   2.598318e+17   \n",
       "...                     ...            ...            ...   \n",
       "2022-11-24     3.384949e+13   4.612156e+19   5.317997e+38   \n",
       "2022-11-25     3.394503e+13   4.542600e+19   5.158804e+38   \n",
       "2022-11-26     3.404058e+13   4.624135e+19   5.345655e+38   \n",
       "2022-11-27     3.413612e+13   4.576227e+19   5.235464e+38   \n",
       "2022-11-28     3.423166e+13   4.552126e+19   5.180463e+38   \n",
       "\n",
       "            median_transaction_fee90rocUSD  median_transaction_feeUSD  \\\n",
       "Date                                                                    \n",
       "2010-07-18                           0.000                      0.000   \n",
       "2010-07-19                           0.000                      0.000   \n",
       "2010-07-20                           0.000                      0.000   \n",
       "2010-07-21                           0.000                      0.000   \n",
       "2010-07-22                           0.000                      0.000   \n",
       "...                                    ...                        ...   \n",
       "2022-11-24                         165.380                      0.818   \n",
       "2022-11-25                         214.464                      0.812   \n",
       "2022-11-26                         255.004                      0.809   \n",
       "2022-11-27                         113.430                      0.619   \n",
       "2022-11-28                         104.294                      0.622   \n",
       "\n",
       "            mining_profitability  ...  price90wmaUSD  sentinusd90emaUSD  \\\n",
       "Date                              ...                                     \n",
       "2010-07-18           401834.0000  ...            0.0                  0   \n",
       "2010-07-19           481473.0000  ...            0.0                  0   \n",
       "2010-07-20           431831.0000  ...            0.0                  0   \n",
       "2010-07-21           460783.0000  ...            0.0                  0   \n",
       "2010-07-22           334641.0000  ...            0.0                  0   \n",
       "...                          ...  ...            ...                ...   \n",
       "2022-11-24                0.0577  ...        18820.0        28544682257   \n",
       "2022-11-25                0.0541  ...        18759.0        28076113671   \n",
       "2022-11-26                0.0571  ...        18701.0        27566714445   \n",
       "2022-11-27                0.0618  ...        18642.0        27053515674   \n",
       "2022-11-28                0.0592  ...        18578.0        26631320968   \n",
       "\n",
       "            sentinusdUSD  top100cap  top100cap14trx  top100cap30rsi  \\\n",
       "Date                                                                  \n",
       "2010-07-18  2.612000e+03     25.782           0.139          71.191   \n",
       "2010-07-19  4.047000e+03     25.685           0.123          68.863   \n",
       "2010-07-20  2.341000e+03     25.602           0.107          66.923   \n",
       "2010-07-21  2.122000e+03     25.638           0.092          67.334   \n",
       "2010-07-22  2.570000e+03     25.604           0.078          66.526   \n",
       "...                  ...        ...             ...             ...   \n",
       "2022-11-24  6.114577e+09     15.236          -0.080          40.118   \n",
       "2022-11-25  7.224812e+09     15.231          -0.075          39.880   \n",
       "2022-11-26  4.898449e+09     15.233          -0.070          40.069   \n",
       "2022-11-27  4.216170e+09     15.259          -0.065          41.833   \n",
       "2022-11-28  7.843657e+09     15.219          -0.061          39.965   \n",
       "\n",
       "            top100cap30trx  top100cap90rsi  top100cap90trx   returns  \n",
       "Date                                                                  \n",
       "2010-07-18           0.308          79.756           0.550  0.466667  \n",
       "2010-07-19           0.300          78.999           0.547  0.183196  \n",
       "2010-07-20           0.292          78.355           0.544 -0.088475  \n",
       "2010-07-21           0.284          78.431           0.541 -0.020434  \n",
       "2010-07-22           0.275          78.166           0.537 -0.153846  \n",
       "...                    ...             ...             ...       ...  \n",
       "2022-11-24          -0.042          45.140           0.001  0.008075  \n",
       "2022-11-25          -0.043          45.047           0.001 -0.006565  \n",
       "2022-11-26          -0.043          45.106           0.001  0.004001  \n",
       "2022-11-27          -0.044          45.656           0.000 -0.001147  \n",
       "2022-11-28          -0.044          44.957           0.000 -0.019828  \n",
       "\n",
       "[4517 rows x 32 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5d4f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide X and Y\n",
    "X = Data.iloc[:,0:]\n",
    "#Y = Data['returns']   # 用returns的话就用这一行，然后把下一行comment掉\n",
    "Y = btcData['priceUSD'].shift(-30)[1:] # 反之亦然"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb3275b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>difficulty14mom</th>\n",
       "      <th>difficulty30mom</th>\n",
       "      <th>difficulty3rsi</th>\n",
       "      <th>difficulty90mom</th>\n",
       "      <th>difficulty90sma</th>\n",
       "      <th>hashrate90std</th>\n",
       "      <th>hashrate90var</th>\n",
       "      <th>median_transaction_fee90rocUSD</th>\n",
       "      <th>median_transaction_feeUSD</th>\n",
       "      <th>mining_profitability</th>\n",
       "      <th>...</th>\n",
       "      <th>price90wmaUSD</th>\n",
       "      <th>sentinusd90emaUSD</th>\n",
       "      <th>sentinusdUSD</th>\n",
       "      <th>top100cap</th>\n",
       "      <th>top100cap14trx</th>\n",
       "      <th>top100cap30rsi</th>\n",
       "      <th>top100cap30trx</th>\n",
       "      <th>top100cap90rsi</th>\n",
       "      <th>top100cap90trx</th>\n",
       "      <th>returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-07-18</th>\n",
       "      <td>162.138</td>\n",
       "      <td>164.162</td>\n",
       "      <td>100.0</td>\n",
       "      <td>173.723</td>\n",
       "      <td>20.555</td>\n",
       "      <td>838438881.0</td>\n",
       "      <td>1.757449e+17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>401834.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2612.0</td>\n",
       "      <td>25.782</td>\n",
       "      <td>0.139</td>\n",
       "      <td>71.191</td>\n",
       "      <td>0.308</td>\n",
       "      <td>79.756</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-19</th>\n",
       "      <td>162.138</td>\n",
       "      <td>164.162</td>\n",
       "      <td>100.0</td>\n",
       "      <td>173.723</td>\n",
       "      <td>22.486</td>\n",
       "      <td>881995244.0</td>\n",
       "      <td>1.944789e+17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>481473.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4047.0</td>\n",
       "      <td>25.685</td>\n",
       "      <td>0.123</td>\n",
       "      <td>68.863</td>\n",
       "      <td>0.300</td>\n",
       "      <td>78.999</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.183196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-20</th>\n",
       "      <td>158.406</td>\n",
       "      <td>164.162</td>\n",
       "      <td>100.0</td>\n",
       "      <td>173.557</td>\n",
       "      <td>24.414</td>\n",
       "      <td>928054231.0</td>\n",
       "      <td>2.153212e+17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>431831.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2341.0</td>\n",
       "      <td>25.602</td>\n",
       "      <td>0.107</td>\n",
       "      <td>66.923</td>\n",
       "      <td>0.292</td>\n",
       "      <td>78.355</td>\n",
       "      <td>0.544</td>\n",
       "      <td>-0.088475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            difficulty14mom  difficulty30mom  difficulty3rsi  difficulty90mom  \\\n",
       "Date                                                                            \n",
       "2010-07-18          162.138          164.162           100.0          173.723   \n",
       "2010-07-19          162.138          164.162           100.0          173.723   \n",
       "2010-07-20          158.406          164.162           100.0          173.557   \n",
       "\n",
       "            difficulty90sma  hashrate90std  hashrate90var  \\\n",
       "Date                                                        \n",
       "2010-07-18           20.555    838438881.0   1.757449e+17   \n",
       "2010-07-19           22.486    881995244.0   1.944789e+17   \n",
       "2010-07-20           24.414    928054231.0   2.153212e+17   \n",
       "\n",
       "            median_transaction_fee90rocUSD  median_transaction_feeUSD  \\\n",
       "Date                                                                    \n",
       "2010-07-18                             0.0                        0.0   \n",
       "2010-07-19                             0.0                        0.0   \n",
       "2010-07-20                             0.0                        0.0   \n",
       "\n",
       "            mining_profitability  ...  price90wmaUSD  sentinusd90emaUSD  \\\n",
       "Date                              ...                                     \n",
       "2010-07-18              401834.0  ...            0.0                  0   \n",
       "2010-07-19              481473.0  ...            0.0                  0   \n",
       "2010-07-20              431831.0  ...            0.0                  0   \n",
       "\n",
       "            sentinusdUSD  top100cap  top100cap14trx  top100cap30rsi  \\\n",
       "Date                                                                  \n",
       "2010-07-18        2612.0     25.782           0.139          71.191   \n",
       "2010-07-19        4047.0     25.685           0.123          68.863   \n",
       "2010-07-20        2341.0     25.602           0.107          66.923   \n",
       "\n",
       "            top100cap30trx  top100cap90rsi  top100cap90trx   returns  \n",
       "Date                                                                  \n",
       "2010-07-18           0.308          79.756           0.550  0.466667  \n",
       "2010-07-19           0.300          78.999           0.547  0.183196  \n",
       "2010-07-20           0.292          78.355           0.544 -0.088475  \n",
       "\n",
       "[3 rows x 32 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bd589ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into three data sets\n",
    "X_train = X['2016-01-01':'2019-12-31']\n",
    "X_val = X['2020-01-01':'2021-05-31']\n",
    "X_test = X['2021-06-01':'2023-01-01']\n",
    "\n",
    "Y_train = Y['2016-01-01':'2019-12-31']\n",
    "Y_val = Y['2020-01-01':'2021-05-31']\n",
    "Y_test = Y['2021-06-01':'2023-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0e5e9fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............ (step 1 of 2) Processing mixmax, total=   0.0s\n",
      "[Pipeline] ............ (step 2 of 2) Processing robust, total=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;mixmax&#x27;, MinMaxScaler()), [&#x27;robust&#x27;, RobustScaler()]],\n",
       "         verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;mixmax&#x27;, MinMaxScaler()), [&#x27;robust&#x27;, RobustScaler()]],\n",
       "         verbose=True)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RobustScaler</label><div class=\"sk-toggleable__content\"><pre>RobustScaler()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('mixmax', MinMaxScaler()), ['robust', RobustScaler()]],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators=[]\n",
    "estimators.append(['mixmax',MinMaxScaler()])\n",
    "estimators.append(['robust',RobustScaler()])\n",
    "scale=Pipeline(estimators,verbose=True)\n",
    "scale.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c6fc0859",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=scale.transform(X_train)\n",
    "X_test=scale.transform(X_test)\n",
    "X_val = scale.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c8a84d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8f6916ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "# define neural network model\n",
    "shape=X.shape[1]\n",
    "def sequential_model(initializer='normal', activation='relu', neurons=300, NUM_FEATURES=shape):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_shape=(NUM_FEATURES,), kernel_initializer=initializer, activation=activation))\n",
    "    model.add(Dense(416, activation=activation))\n",
    "    model.add(Dense(32, activation=activation))\n",
    "    model.add(Dense(1, activation=activation))\n",
    "    # Compile model\n",
    "    adam=keras.optimizers.Adam(lr=lr_schedule(0), amsgrad=True)\n",
    "    #sgd=keras.optimizers.SGD(learning_rate=0.08, momentum=0.9, nesterov=False)\n",
    "    model.compile(loss='logcosh', optimizer=adam, metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72db04c9",
   "metadata": {},
   "source": [
    "val_loss is the value of cost function for your cross-validation data and loss is the value of cost function for your training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85653fe",
   "metadata": {},
   "source": [
    "stop training when val_loss is not increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b48b24b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# earlyStopping = EarlyStopping(monitor='val_loss', patience=100,verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "85174a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor=KerasRegressor(build_fn=sequential_model,epochs=5000,verbose=1, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a17646de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Epoch 1/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 5057.5294 - mae: 5058.2230 - val_loss: 20000.7754 - val_mae: 20001.4707\n",
      "Epoch 2/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 3946.7846 - mae: 3947.4778 - val_loss: 8728.7607 - val_mae: 8729.4521\n",
      "Epoch 3/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1891.8334 - mae: 1892.5266 - val_loss: 6528.3071 - val_mae: 6529.0010\n",
      "Epoch 4/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 1390.3296 - mae: 1391.0229 - val_loss: 6666.7656 - val_mae: 6667.4590\n",
      "Epoch 5/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 1401.7638 - mae: 1402.4568 - val_loss: 6456.8608 - val_mae: 6457.5537\n",
      "Epoch 6/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1226.4831 - mae: 1227.1761 - val_loss: 6698.7861 - val_mae: 6699.4800\n",
      "Epoch 7/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1136.1520 - mae: 1136.8451 - val_loss: 6943.2690 - val_mae: 6943.9624\n",
      "Epoch 8/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 1046.7226 - mae: 1047.4158 - val_loss: 6740.1284 - val_mae: 6740.8218\n",
      "Epoch 9/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 1024.3646 - mae: 1025.0573 - val_loss: 6996.4263 - val_mae: 6997.1206\n",
      "Epoch 10/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 937.6453 - mae: 938.3384 - val_loss: 7278.6699 - val_mae: 7279.3613\n",
      "Epoch 11/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 876.9681 - mae: 877.6601 - val_loss: 7496.3140 - val_mae: 7497.0078\n",
      "Epoch 12/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 823.5863 - mae: 824.2768 - val_loss: 7261.2852 - val_mae: 7261.9785\n",
      "Epoch 13/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 828.3225 - mae: 829.0144 - val_loss: 7677.5376 - val_mae: 7678.2300\n",
      "Epoch 14/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 763.9235 - mae: 764.6156 - val_loss: 6971.7329 - val_mae: 6972.4258\n",
      "Epoch 15/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 718.0665 - mae: 718.7580 - val_loss: 7345.4438 - val_mae: 7346.1367\n",
      "Epoch 16/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 759.8942 - mae: 760.5843 - val_loss: 7564.7847 - val_mae: 7565.4780\n",
      "Epoch 17/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 711.0451 - mae: 711.7377 - val_loss: 7441.6250 - val_mae: 7442.3169\n",
      "Epoch 18/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 687.5906 - mae: 688.2829 - val_loss: 7251.4185 - val_mae: 7252.1113\n",
      "Epoch 19/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 671.3780 - mae: 672.0705 - val_loss: 7588.7529 - val_mae: 7589.4443\n",
      "Epoch 20/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 668.6837 - mae: 669.3759 - val_loss: 7550.2441 - val_mae: 7550.9375\n",
      "Epoch 21/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 683.8812 - mae: 684.5735 - val_loss: 7840.0879 - val_mae: 7840.7803\n",
      "Epoch 22/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 588.3904 - mae: 589.0810 - val_loss: 7630.6665 - val_mae: 7631.3589\n",
      "Epoch 23/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 607.5822 - mae: 608.2742 - val_loss: 7645.9009 - val_mae: 7646.5942\n",
      "Epoch 24/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 666.6248 - mae: 667.3165 - val_loss: 7987.6182 - val_mae: 7988.3101\n",
      "Epoch 25/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 602.7082 - mae: 603.3999 - val_loss: 7676.3491 - val_mae: 7677.0420\n",
      "Epoch 26/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 613.2484 - mae: 613.9400 - val_loss: 7411.2920 - val_mae: 7411.9858\n",
      "Epoch 27/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 598.5853 - mae: 599.2776 - val_loss: 7542.5205 - val_mae: 7543.2134\n",
      "Epoch 28/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 566.3794 - mae: 567.0719 - val_loss: 7736.8018 - val_mae: 7737.4956\n",
      "Epoch 29/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 557.4031 - mae: 558.0957 - val_loss: 7616.0112 - val_mae: 7616.7036\n",
      "Epoch 30/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 539.5055 - mae: 540.1975 - val_loss: 7495.1895 - val_mae: 7495.8823\n",
      "Epoch 31/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 510.5790 - mae: 511.2709 - val_loss: 7810.8711 - val_mae: 7811.5640\n",
      "Epoch 32/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 517.0599 - mae: 517.7507 - val_loss: 8208.0088 - val_mae: 8208.7021\n",
      "Epoch 33/5000\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 503.7977 - mae: 504.4890 - val_loss: 8067.6489 - val_mae: 8068.3418\n",
      "Epoch 34/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 475.6212 - mae: 476.3125 - val_loss: 7772.2930 - val_mae: 7772.9858\n",
      "Epoch 35/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 530.3155 - mae: 531.0069 - val_loss: 7755.0703 - val_mae: 7755.7622\n",
      "Epoch 36/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 484.1455 - mae: 484.8366 - val_loss: 8211.6367 - val_mae: 8212.3311\n",
      "Epoch 37/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 487.0862 - mae: 487.7767 - val_loss: 7502.9219 - val_mae: 7503.6147\n",
      "Epoch 38/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 525.4259 - mae: 526.1180 - val_loss: 7676.1128 - val_mae: 7676.8062\n",
      "Epoch 39/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 487.3135 - mae: 488.0047 - val_loss: 8018.0532 - val_mae: 8018.7466\n",
      "Epoch 40/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 493.7295 - mae: 494.4221 - val_loss: 8223.7617 - val_mae: 8224.4551\n",
      "Epoch 41/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 558.0888 - mae: 558.7808 - val_loss: 8137.5522 - val_mae: 8138.2446\n",
      "Epoch 42/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 494.8260 - mae: 495.5156 - val_loss: 8325.3467 - val_mae: 8326.0391\n",
      "Epoch 43/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 457.0196 - mae: 457.7113 - val_loss: 8028.1787 - val_mae: 8028.8721\n",
      "Epoch 44/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 467.8826 - mae: 468.5742 - val_loss: 8289.4844 - val_mae: 8290.1758\n",
      "Epoch 45/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 500.7334 - mae: 501.4241 - val_loss: 8147.6704 - val_mae: 8148.3638\n",
      "Epoch 46/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 452.2384 - mae: 452.9299 - val_loss: 8225.4072 - val_mae: 8226.0996\n",
      "Epoch 47/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 430.0827 - mae: 430.7740 - val_loss: 7630.1396 - val_mae: 7630.8325\n",
      "Epoch 48/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 490.2875 - mae: 490.9776 - val_loss: 8412.8232 - val_mae: 8413.5156\n",
      "Epoch 49/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 430.5557 - mae: 431.2467 - val_loss: 8593.7080 - val_mae: 8594.4014\n",
      "Epoch 50/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 403.0563 - mae: 403.7471 - val_loss: 8641.5566 - val_mae: 8642.2510\n",
      "Epoch 51/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 442.6785 - mae: 443.3693 - val_loss: 8744.6904 - val_mae: 8745.3838\n",
      "Epoch 52/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 434.1515 - mae: 434.8438 - val_loss: 7899.3262 - val_mae: 7900.0200\n",
      "Epoch 53/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 456.8182 - mae: 457.5106 - val_loss: 9005.9141 - val_mae: 9006.6064\n",
      "Epoch 54/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 376.3851 - mae: 377.0752 - val_loss: 8642.9814 - val_mae: 8643.6748\n",
      "Epoch 55/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 429.4869 - mae: 430.1777 - val_loss: 8774.3467 - val_mae: 8775.0391\n",
      "Epoch 56/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 376.4607 - mae: 377.1507 - val_loss: 8713.2148 - val_mae: 8713.9082\n",
      "Epoch 57/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 381.7164 - mae: 382.4076 - val_loss: 8690.3340 - val_mae: 8691.0273\n",
      "Epoch 58/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 419.5709 - mae: 420.2629 - val_loss: 8355.4229 - val_mae: 8356.1152\n",
      "Epoch 59/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 403.1863 - mae: 403.8770 - val_loss: 8818.2500 - val_mae: 8818.9443\n",
      "Epoch 60/5000\n",
      "46/46 [==============================] - 2s 50ms/step - loss: 400.9379 - mae: 401.6296 - val_loss: 9085.3994 - val_mae: 9086.0918\n",
      "Epoch 61/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 393.0107 - mae: 393.7028 - val_loss: 8751.4014 - val_mae: 8752.0957\n",
      "Epoch 62/5000\n",
      "46/46 [==============================] - 2s 42ms/step - loss: 399.0865 - mae: 399.7777 - val_loss: 8827.8594 - val_mae: 8828.5537\n",
      "Epoch 63/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 350.6974 - mae: 351.3887 - val_loss: 8546.1113 - val_mae: 8546.8057\n",
      "Epoch 64/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 387.2756 - mae: 387.9679 - val_loss: 9526.1562 - val_mae: 9526.8496\n",
      "Epoch 65/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 375.7156 - mae: 376.4067 - val_loss: 9079.7979 - val_mae: 9080.4912\n",
      "Epoch 66/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 364.0882 - mae: 364.7787 - val_loss: 8777.2568 - val_mae: 8777.9512\n",
      "Epoch 67/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 373.9068 - mae: 374.5982 - val_loss: 8569.1289 - val_mae: 8569.8232\n",
      "Epoch 68/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 393.5574 - mae: 394.2487 - val_loss: 9084.8516 - val_mae: 9085.5449\n",
      "Epoch 69/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 347.2393 - mae: 347.9314 - val_loss: 8632.0137 - val_mae: 8632.7061\n",
      "Epoch 70/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 346.7475 - mae: 347.4392 - val_loss: 9753.6777 - val_mae: 9754.3721\n",
      "Epoch 71/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 376.3578 - mae: 377.0498 - val_loss: 8881.8086 - val_mae: 8882.5010\n",
      "Epoch 72/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 350.0767 - mae: 350.7671 - val_loss: 9092.6104 - val_mae: 9093.3037\n",
      "Epoch 73/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 356.0891 - mae: 356.7816 - val_loss: 9177.2021 - val_mae: 9177.8955\n",
      "Epoch 74/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 356.8406 - mae: 357.5309 - val_loss: 9375.9229 - val_mae: 9376.6152\n",
      "Epoch 75/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 338.0001 - mae: 338.6918 - val_loss: 9234.0088 - val_mae: 9234.7031\n",
      "Epoch 76/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 336.4804 - mae: 337.1714 - val_loss: 8802.1104 - val_mae: 8802.8027\n",
      "Epoch 77/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 323.0146 - mae: 323.7055 - val_loss: 9794.7959 - val_mae: 9795.4893\n",
      "Epoch 78/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 376.7868 - mae: 377.4784 - val_loss: 9384.3193 - val_mae: 9385.0127\n",
      "Epoch 79/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 372.1877 - mae: 372.8796 - val_loss: 9199.2666 - val_mae: 9199.9590\n",
      "Epoch 80/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 328.2071 - mae: 328.8994 - val_loss: 9499.5732 - val_mae: 9500.2656\n",
      "Epoch 81/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 341.8544 - mae: 342.5455 - val_loss: 9510.3145 - val_mae: 9511.0088\n",
      "Epoch 82/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 317.0778 - mae: 317.7691 - val_loss: 9664.1777 - val_mae: 9664.8721\n",
      "Epoch 83/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 326.2173 - mae: 326.9085 - val_loss: 9877.8066 - val_mae: 9878.4990\n",
      "Epoch 84/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 312.7805 - mae: 313.4706 - val_loss: 9704.3037 - val_mae: 9704.9951\n",
      "Epoch 85/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 326.2565 - mae: 326.9471 - val_loss: 9497.1436 - val_mae: 9497.8359\n",
      "Epoch 86/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 307.3403 - mae: 308.0321 - val_loss: 9309.8291 - val_mae: 9310.5225\n",
      "Epoch 87/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 308.0325 - mae: 308.7238 - val_loss: 9578.2529 - val_mae: 9578.9482\n",
      "Epoch 88/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 314.0993 - mae: 314.7897 - val_loss: 9549.2305 - val_mae: 9549.9238\n",
      "Epoch 89/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 335.8934 - mae: 336.5845 - val_loss: 9381.1445 - val_mae: 9381.8389\n",
      "Epoch 90/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 324.5062 - mae: 325.1972 - val_loss: 9952.0166 - val_mae: 9952.7100\n",
      "Epoch 91/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 302.9579 - mae: 303.6492 - val_loss: 9836.1172 - val_mae: 9836.8096\n",
      "Epoch 92/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 284.9670 - mae: 285.6582 - val_loss: 9358.7002 - val_mae: 9359.3926\n",
      "Epoch 93/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 312.7938 - mae: 313.4848 - val_loss: 9792.8066 - val_mae: 9793.5000\n",
      "Epoch 94/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 315.9520 - mae: 316.6426 - val_loss: 9975.0312 - val_mae: 9975.7227\n",
      "Epoch 95/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 283.1227 - mae: 283.8135 - val_loss: 9899.2275 - val_mae: 9899.9189\n",
      "Epoch 96/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 280.6249 - mae: 281.3144 - val_loss: 9595.7920 - val_mae: 9596.4844\n",
      "Epoch 97/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 284.5900 - mae: 285.2816 - val_loss: 9350.4512 - val_mae: 9351.1455\n",
      "Epoch 98/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 284.2445 - mae: 284.9328 - val_loss: 10016.2549 - val_mae: 10016.9482\n",
      "Epoch 99/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 307.3629 - mae: 308.0526 - val_loss: 9518.1875 - val_mae: 9518.8809\n",
      "Epoch 100/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 301.8552 - mae: 302.5477 - val_loss: 9339.8613 - val_mae: 9340.5547\n",
      "Epoch 101/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 289.2080 - mae: 289.8979 - val_loss: 9941.0723 - val_mae: 9941.7646\n",
      "Epoch 102/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 328.1919 - mae: 328.8824 - val_loss: 9795.4248 - val_mae: 9796.1162\n",
      "Epoch 103/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 345.4054 - mae: 346.0966 - val_loss: 9459.5391 - val_mae: 9460.2334\n",
      "Epoch 104/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 292.1554 - mae: 292.8462 - val_loss: 9814.5771 - val_mae: 9815.2705\n",
      "Epoch 105/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 301.0841 - mae: 301.7763 - val_loss: 10084.9990 - val_mae: 10085.6924\n",
      "Epoch 106/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 301.0731 - mae: 301.7645 - val_loss: 9714.1523 - val_mae: 9714.8467\n",
      "Epoch 107/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 277.6000 - mae: 278.2903 - val_loss: 10149.8965 - val_mae: 10150.5889\n",
      "Epoch 108/5000\n",
      "46/46 [==============================] - 1s 28ms/step - loss: 314.9631 - mae: 315.6554 - val_loss: 9263.2178 - val_mae: 9263.9102\n",
      "Epoch 109/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 311.8192 - mae: 312.5110 - val_loss: 9347.5049 - val_mae: 9348.1973\n",
      "Epoch 110/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 290.5800 - mae: 291.2703 - val_loss: 9438.8301 - val_mae: 9439.5234\n",
      "Epoch 111/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 262.7355 - mae: 263.4267 - val_loss: 9692.3086 - val_mae: 9693.0020\n",
      "Epoch 112/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 292.8428 - mae: 293.5340 - val_loss: 9396.1064 - val_mae: 9396.7998\n",
      "Epoch 113/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 270.5692 - mae: 271.2592 - val_loss: 10147.4707 - val_mae: 10148.1631\n",
      "Epoch 114/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 266.3214 - mae: 267.0113 - val_loss: 9597.5381 - val_mae: 9598.2305\n",
      "Epoch 115/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 281.2798 - mae: 281.9702 - val_loss: 10088.1484 - val_mae: 10088.8408\n",
      "Epoch 116/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 309.4160 - mae: 310.1069 - val_loss: 9187.6025 - val_mae: 9188.2959\n",
      "Epoch 117/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 285.8320 - mae: 286.5230 - val_loss: 9388.0771 - val_mae: 9388.7705\n",
      "Epoch 118/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 296.5196 - mae: 297.2102 - val_loss: 9744.1865 - val_mae: 9744.8799\n",
      "Epoch 119/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 279.7882 - mae: 280.4789 - val_loss: 10025.5596 - val_mae: 10026.2529\n",
      "Epoch 120/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 295.0090 - mae: 295.6997 - val_loss: 9882.1650 - val_mae: 9882.8584\n",
      "Epoch 121/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 289.7950 - mae: 290.4872 - val_loss: 9113.1709 - val_mae: 9113.8643\n",
      "Epoch 122/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 296.7721 - mae: 297.4626 - val_loss: 9648.4121 - val_mae: 9649.1045\n",
      "Epoch 123/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 264.2456 - mae: 264.9356 - val_loss: 9663.7939 - val_mae: 9664.4863\n",
      "Epoch 124/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 277.2312 - mae: 277.9214 - val_loss: 9615.2812 - val_mae: 9615.9746\n",
      "Epoch 125/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 266.2451 - mae: 266.9371 - val_loss: 9589.0234 - val_mae: 9589.7178\n",
      "Epoch 126/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 268.9587 - mae: 269.6494 - val_loss: 9739.8594 - val_mae: 9740.5508\n",
      "Epoch 127/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 259.0018 - mae: 259.6920 - val_loss: 10219.6338 - val_mae: 10220.3271\n",
      "Epoch 128/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 280.7379 - mae: 281.4290 - val_loss: 9710.9297 - val_mae: 9711.6211\n",
      "Epoch 129/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 270.7836 - mae: 271.4754 - val_loss: 9812.6455 - val_mae: 9813.3389\n",
      "Epoch 130/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 274.7042 - mae: 275.3949 - val_loss: 10073.1494 - val_mae: 10073.8447\n",
      "Epoch 131/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 260.9262 - mae: 261.6166 - val_loss: 9656.7344 - val_mae: 9657.4277\n",
      "Epoch 132/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 254.3386 - mae: 255.0294 - val_loss: 9748.0908 - val_mae: 9748.7842\n",
      "Epoch 133/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 276.5602 - mae: 277.2508 - val_loss: 9998.4844 - val_mae: 9999.1777\n",
      "Epoch 134/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 255.0347 - mae: 255.7262 - val_loss: 9658.3848 - val_mae: 9659.0791\n",
      "Epoch 135/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 252.8573 - mae: 253.5458 - val_loss: 9776.4639 - val_mae: 9777.1572\n",
      "Epoch 136/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 250.9440 - mae: 251.6335 - val_loss: 9626.8682 - val_mae: 9627.5615\n",
      "Epoch 137/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 250.2282 - mae: 250.9189 - val_loss: 10046.0820 - val_mae: 10046.7773\n",
      "Epoch 138/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 298.7904 - mae: 299.4815 - val_loss: 9583.4512 - val_mae: 9584.1455\n",
      "Epoch 139/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 256.8297 - mae: 257.5191 - val_loss: 9718.9990 - val_mae: 9719.6914\n",
      "Epoch 140/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 307.0682 - mae: 307.7594 - val_loss: 9877.2930 - val_mae: 9877.9863\n",
      "Epoch 141/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 274.7634 - mae: 275.4539 - val_loss: 9566.5547 - val_mae: 9567.2490\n",
      "Epoch 142/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 270.5854 - mae: 271.2751 - val_loss: 9895.1582 - val_mae: 9895.8496\n",
      "Epoch 143/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 263.7818 - mae: 264.4723 - val_loss: 9979.4102 - val_mae: 9980.1045\n",
      "Epoch 144/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 277.5663 - mae: 278.2573 - val_loss: 9805.5498 - val_mae: 9806.2422\n",
      "Epoch 145/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 264.3066 - mae: 264.9961 - val_loss: 9838.5625 - val_mae: 9839.2568\n",
      "Epoch 146/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 251.7508 - mae: 252.4418 - val_loss: 9569.0859 - val_mae: 9569.7783\n",
      "Epoch 147/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 291.9776 - mae: 292.6678 - val_loss: 9691.1572 - val_mae: 9691.8506\n",
      "Epoch 148/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 257.2395 - mae: 257.9302 - val_loss: 9576.2412 - val_mae: 9576.9346\n",
      "Epoch 149/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 267.0563 - mae: 267.7485 - val_loss: 9407.4844 - val_mae: 9408.1758\n",
      "Epoch 150/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 225.7877 - mae: 226.4779 - val_loss: 9351.3320 - val_mae: 9352.0244\n",
      "Epoch 151/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 252.6868 - mae: 253.3768 - val_loss: 9813.3076 - val_mae: 9814.0000\n",
      "Epoch 152/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 276.3976 - mae: 277.0884 - val_loss: 10115.1738 - val_mae: 10115.8662\n",
      "Epoch 153/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 260.6091 - mae: 261.3004 - val_loss: 9590.3984 - val_mae: 9591.0908\n",
      "Epoch 154/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 245.8882 - mae: 246.5782 - val_loss: 9544.1309 - val_mae: 9544.8242\n",
      "Epoch 155/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 262.2977 - mae: 262.9872 - val_loss: 9788.7158 - val_mae: 9789.4072\n",
      "Epoch 156/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 234.4862 - mae: 235.1760 - val_loss: 9740.1650 - val_mae: 9740.8584\n",
      "Epoch 157/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 247.5181 - mae: 248.2094 - val_loss: 9580.5996 - val_mae: 9581.2930\n",
      "Epoch 158/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 262.7430 - mae: 263.4341 - val_loss: 9699.8174 - val_mae: 9700.5117\n",
      "Epoch 159/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 256.7090 - mae: 257.4004 - val_loss: 9648.5674 - val_mae: 9649.2607\n",
      "Epoch 160/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 256.8566 - mae: 257.5481 - val_loss: 9983.5684 - val_mae: 9984.2607\n",
      "Epoch 161/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 257.6437 - mae: 258.3330 - val_loss: 9287.0869 - val_mae: 9287.7793\n",
      "Epoch 162/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 239.8951 - mae: 240.5835 - val_loss: 9364.0752 - val_mae: 9364.7676\n",
      "Epoch 163/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 262.9675 - mae: 263.6597 - val_loss: 9377.9824 - val_mae: 9378.6748\n",
      "Epoch 164/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 269.0129 - mae: 269.7019 - val_loss: 9400.8857 - val_mae: 9401.5801\n",
      "Epoch 165/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 241.9206 - mae: 242.6122 - val_loss: 9747.8252 - val_mae: 9748.5186\n",
      "Epoch 166/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 248.7840 - mae: 249.4748 - val_loss: 9721.6709 - val_mae: 9722.3633\n",
      "Epoch 167/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 242.2395 - mae: 242.9267 - val_loss: 9860.7324 - val_mae: 9861.4248\n",
      "Epoch 168/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 236.0825 - mae: 236.7732 - val_loss: 9705.2559 - val_mae: 9705.9502\n",
      "Epoch 169/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 255.3901 - mae: 256.0796 - val_loss: 9483.5947 - val_mae: 9484.2871\n",
      "Epoch 170/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 267.9294 - mae: 268.6201 - val_loss: 9541.8594 - val_mae: 9542.5527\n",
      "Epoch 171/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 234.5301 - mae: 235.2198 - val_loss: 9884.5576 - val_mae: 9885.2500\n",
      "Epoch 172/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 262.4075 - mae: 263.0992 - val_loss: 9616.6416 - val_mae: 9617.3350\n",
      "Epoch 173/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 243.6050 - mae: 244.2958 - val_loss: 9704.5518 - val_mae: 9705.2471\n",
      "Epoch 174/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 227.5356 - mae: 228.2244 - val_loss: 9305.0049 - val_mae: 9305.6982\n",
      "Epoch 175/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 241.7622 - mae: 242.4499 - val_loss: 9974.7090 - val_mae: 9975.4004\n",
      "Epoch 176/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 271.4477 - mae: 272.1374 - val_loss: 9472.9238 - val_mae: 9473.6162\n",
      "Epoch 177/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 233.9032 - mae: 234.5927 - val_loss: 9611.7148 - val_mae: 9612.4062\n",
      "Epoch 178/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 238.2195 - mae: 238.9091 - val_loss: 9346.5293 - val_mae: 9347.2217\n",
      "Epoch 179/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 263.0721 - mae: 263.7622 - val_loss: 9574.6416 - val_mae: 9575.3340\n",
      "Epoch 180/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 270.8721 - mae: 271.5593 - val_loss: 9827.7090 - val_mae: 9828.4023\n",
      "Epoch 181/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 251.2335 - mae: 251.9209 - val_loss: 9758.0703 - val_mae: 9758.7627\n",
      "Epoch 182/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 232.7442 - mae: 233.4360 - val_loss: 9667.5801 - val_mae: 9668.2725\n",
      "Epoch 183/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 251.7175 - mae: 252.4075 - val_loss: 9744.8955 - val_mae: 9745.5889\n",
      "Epoch 184/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 238.7123 - mae: 239.4018 - val_loss: 9722.3135 - val_mae: 9723.0068\n",
      "Epoch 185/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 240.9142 - mae: 241.6061 - val_loss: 9687.7793 - val_mae: 9688.4717\n",
      "Epoch 186/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 268.0527 - mae: 268.7422 - val_loss: 9275.3887 - val_mae: 9276.0820\n",
      "Epoch 187/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 239.3759 - mae: 240.0672 - val_loss: 9657.4863 - val_mae: 9658.1787\n",
      "Epoch 188/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 248.1013 - mae: 248.7932 - val_loss: 9583.2031 - val_mae: 9583.8965\n",
      "Epoch 189/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 221.6443 - mae: 222.3338 - val_loss: 9586.7861 - val_mae: 9587.4785\n",
      "Epoch 190/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 239.0703 - mae: 239.7607 - val_loss: 9770.4512 - val_mae: 9771.1465\n",
      "Epoch 191/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 243.1772 - mae: 243.8679 - val_loss: 10095.8623 - val_mae: 10096.5537\n",
      "Epoch 192/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 236.9395 - mae: 237.6260 - val_loss: 9539.0986 - val_mae: 9539.7910\n",
      "Epoch 193/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 251.5295 - mae: 252.2189 - val_loss: 9583.4502 - val_mae: 9584.1436\n",
      "Epoch 194/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 228.7991 - mae: 229.4892 - val_loss: 9635.7451 - val_mae: 9636.4385\n",
      "Epoch 195/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 238.4115 - mae: 239.1007 - val_loss: 9689.4600 - val_mae: 9690.1543\n",
      "Epoch 196/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 241.8170 - mae: 242.5068 - val_loss: 9696.7803 - val_mae: 9697.4736\n",
      "Epoch 197/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 236.1791 - mae: 236.8695 - val_loss: 9826.2744 - val_mae: 9826.9678\n",
      "Epoch 198/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 238.9182 - mae: 239.6074 - val_loss: 9914.7646 - val_mae: 9915.4561\n",
      "Epoch 199/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 216.2875 - mae: 216.9779 - val_loss: 9774.3223 - val_mae: 9775.0146\n",
      "Epoch 200/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 244.8985 - mae: 245.5904 - val_loss: 9696.2461 - val_mae: 9696.9395\n",
      "Epoch 201/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 244.9444 - mae: 245.6347 - val_loss: 10101.6299 - val_mae: 10102.3242\n",
      "Epoch 202/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 235.4037 - mae: 236.0924 - val_loss: 9871.8477 - val_mae: 9872.5410\n",
      "Epoch 203/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 212.8349 - mae: 213.5234 - val_loss: 9643.9639 - val_mae: 9644.6562\n",
      "Epoch 204/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 222.2945 - mae: 222.9852 - val_loss: 9697.1426 - val_mae: 9697.8359\n",
      "Epoch 205/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 211.1369 - mae: 211.8276 - val_loss: 9215.9258 - val_mae: 9216.6191\n",
      "Epoch 206/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 253.1416 - mae: 253.8306 - val_loss: 9777.9990 - val_mae: 9778.6914\n",
      "Epoch 207/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 236.6243 - mae: 237.3150 - val_loss: 9498.3135 - val_mae: 9499.0078\n",
      "Epoch 208/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 212.4427 - mae: 213.1315 - val_loss: 9609.6191 - val_mae: 9610.3115\n",
      "Epoch 209/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 237.3318 - mae: 238.0211 - val_loss: 9504.6445 - val_mae: 9505.3369\n",
      "Epoch 210/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 261.1234 - mae: 261.8128 - val_loss: 9745.5176 - val_mae: 9746.2090\n",
      "Epoch 211/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 219.4811 - mae: 220.1706 - val_loss: 9932.2422 - val_mae: 9932.9346\n",
      "Epoch 212/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 211.0426 - mae: 211.7338 - val_loss: 9207.6641 - val_mae: 9208.3574\n",
      "Epoch 213/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 240.5191 - mae: 241.2093 - val_loss: 9408.5254 - val_mae: 9409.2188\n",
      "Epoch 214/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 254.9462 - mae: 255.6352 - val_loss: 9627.6611 - val_mae: 9628.3545\n",
      "Epoch 215/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 222.2150 - mae: 222.9024 - val_loss: 10143.5020 - val_mae: 10144.1953\n",
      "Epoch 216/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 267.0651 - mae: 267.7540 - val_loss: 10047.1260 - val_mae: 10047.8184\n",
      "Epoch 217/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 247.9130 - mae: 248.6027 - val_loss: 9479.9150 - val_mae: 9480.6064\n",
      "Epoch 218/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 240.5692 - mae: 241.2610 - val_loss: 9850.7295 - val_mae: 9851.4219\n",
      "Epoch 219/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 214.1278 - mae: 214.8163 - val_loss: 10076.2334 - val_mae: 10076.9277\n",
      "Epoch 220/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 233.8934 - mae: 234.5816 - val_loss: 9777.9863 - val_mae: 9778.6797\n",
      "Epoch 221/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 218.5907 - mae: 219.2797 - val_loss: 9693.9277 - val_mae: 9694.6211\n",
      "Epoch 222/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 240.6090 - mae: 241.2996 - val_loss: 9838.5586 - val_mae: 9839.2520\n",
      "Epoch 223/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 240.6540 - mae: 241.3427 - val_loss: 9991.5742 - val_mae: 9992.2666\n",
      "Epoch 224/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 222.2360 - mae: 222.9249 - val_loss: 9686.7656 - val_mae: 9687.4570\n",
      "Epoch 225/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 208.7529 - mae: 209.4424 - val_loss: 9986.5957 - val_mae: 9987.2881\n",
      "Epoch 226/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 221.4412 - mae: 222.1310 - val_loss: 9874.7852 - val_mae: 9875.4795\n",
      "Epoch 227/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 227.8542 - mae: 228.5439 - val_loss: 10200.2500 - val_mae: 10200.9443\n",
      "Epoch 228/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 246.8930 - mae: 247.5832 - val_loss: 10170.4824 - val_mae: 10171.1768\n",
      "Epoch 229/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 244.9620 - mae: 245.6510 - val_loss: 9697.6289 - val_mae: 9698.3223\n",
      "Epoch 230/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 215.7656 - mae: 216.4553 - val_loss: 9823.4629 - val_mae: 9824.1553\n",
      "Epoch 231/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 236.5460 - mae: 237.2374 - val_loss: 10062.4697 - val_mae: 10063.1621\n",
      "Epoch 232/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 220.6731 - mae: 221.3635 - val_loss: 9557.1953 - val_mae: 9557.8887\n",
      "Epoch 233/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 209.4665 - mae: 210.1574 - val_loss: 9585.5703 - val_mae: 9586.2646\n",
      "Epoch 234/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 225.5621 - mae: 226.2530 - val_loss: 10119.0752 - val_mae: 10119.7666\n",
      "Epoch 235/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 240.3841 - mae: 241.0759 - val_loss: 9730.8896 - val_mae: 9731.5830\n",
      "Epoch 236/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 214.4783 - mae: 215.1704 - val_loss: 9691.4902 - val_mae: 9692.1826\n",
      "Epoch 237/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 237.3787 - mae: 238.0693 - val_loss: 9396.0479 - val_mae: 9396.7422\n",
      "Epoch 238/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 241.6219 - mae: 242.3115 - val_loss: 9746.3643 - val_mae: 9747.0557\n",
      "Epoch 239/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 211.0956 - mae: 211.7858 - val_loss: 9366.7344 - val_mae: 9367.4268\n",
      "Epoch 240/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 222.9295 - mae: 223.6186 - val_loss: 9727.7773 - val_mae: 9728.4707\n",
      "Epoch 241/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 217.7851 - mae: 218.4757 - val_loss: 9282.9443 - val_mae: 9283.6387\n",
      "Epoch 242/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 210.8702 - mae: 211.5612 - val_loss: 9766.0957 - val_mae: 9766.7881\n",
      "Epoch 243/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 231.5210 - mae: 232.2106 - val_loss: 9922.1504 - val_mae: 9922.8438\n",
      "Epoch 244/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 215.8438 - mae: 216.5335 - val_loss: 9953.6865 - val_mae: 9954.3789\n",
      "Epoch 245/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 213.6234 - mae: 214.3145 - val_loss: 9796.8408 - val_mae: 9797.5332\n",
      "Epoch 246/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 215.9404 - mae: 216.6300 - val_loss: 9810.2969 - val_mae: 9810.9893\n",
      "Epoch 247/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 209.7920 - mae: 210.4803 - val_loss: 9884.2520 - val_mae: 9884.9453\n",
      "Epoch 248/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 205.3932 - mae: 206.0822 - val_loss: 9541.6055 - val_mae: 9542.2998\n",
      "Epoch 249/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 213.5629 - mae: 214.2499 - val_loss: 9917.4141 - val_mae: 9918.1064\n",
      "Epoch 250/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 252.4968 - mae: 253.1835 - val_loss: 9813.2285 - val_mae: 9813.9209\n",
      "Epoch 251/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 213.6893 - mae: 214.3786 - val_loss: 9762.5918 - val_mae: 9763.2842\n",
      "Epoch 252/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 253.9151 - mae: 254.6045 - val_loss: 9696.0635 - val_mae: 9696.7578\n",
      "Epoch 253/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 196.9350 - mae: 197.6241 - val_loss: 9712.7012 - val_mae: 9713.3936\n",
      "Epoch 254/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 230.7137 - mae: 231.4047 - val_loss: 9175.6270 - val_mae: 9176.3203\n",
      "Epoch 255/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 218.1567 - mae: 218.8461 - val_loss: 9550.0273 - val_mae: 9550.7197\n",
      "Epoch 256/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 210.0765 - mae: 210.7675 - val_loss: 9841.2754 - val_mae: 9841.9678\n",
      "Epoch 257/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 227.6280 - mae: 228.3187 - val_loss: 9819.9121 - val_mae: 9820.6055\n",
      "Epoch 258/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 215.1823 - mae: 215.8723 - val_loss: 9735.6631 - val_mae: 9736.3574\n",
      "Epoch 259/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 190.3258 - mae: 191.0148 - val_loss: 9774.1943 - val_mae: 9774.8867\n",
      "Epoch 260/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 201.5309 - mae: 202.2192 - val_loss: 9654.5918 - val_mae: 9655.2861\n",
      "Epoch 261/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 207.5238 - mae: 208.2131 - val_loss: 9632.8516 - val_mae: 9633.5459\n",
      "Epoch 262/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 221.8784 - mae: 222.5690 - val_loss: 10110.1533 - val_mae: 10110.8467\n",
      "Epoch 263/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 217.6759 - mae: 218.3647 - val_loss: 9700.0859 - val_mae: 9700.7793\n",
      "Epoch 264/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 199.3983 - mae: 200.0861 - val_loss: 9915.6387 - val_mae: 9916.3311\n",
      "Epoch 265/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 213.9519 - mae: 214.6419 - val_loss: 9643.6943 - val_mae: 9644.3877\n",
      "Epoch 266/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 218.0385 - mae: 218.7278 - val_loss: 9989.7314 - val_mae: 9990.4238\n",
      "Epoch 267/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 215.7394 - mae: 216.4303 - val_loss: 9756.8115 - val_mae: 9757.5039\n",
      "Epoch 268/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 200.4427 - mae: 201.1323 - val_loss: 9617.3633 - val_mae: 9618.0557\n",
      "Epoch 269/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 215.4869 - mae: 216.1771 - val_loss: 9855.8545 - val_mae: 9856.5479\n",
      "Epoch 270/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 211.5316 - mae: 212.2207 - val_loss: 9596.8496 - val_mae: 9597.5430\n",
      "Epoch 271/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 210.2581 - mae: 210.9480 - val_loss: 9766.1338 - val_mae: 9766.8262\n",
      "Epoch 272/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 207.1508 - mae: 207.8419 - val_loss: 9471.4883 - val_mae: 9472.1826\n",
      "Epoch 273/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 226.8329 - mae: 227.5219 - val_loss: 9845.5010 - val_mae: 9846.1943\n",
      "Epoch 274/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 216.7734 - mae: 217.4624 - val_loss: 9910.6211 - val_mae: 9911.3154\n",
      "Epoch 275/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 205.9575 - mae: 206.6444 - val_loss: 9814.4512 - val_mae: 9815.1465\n",
      "Epoch 276/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 194.1758 - mae: 194.8656 - val_loss: 9689.2900 - val_mae: 9689.9824\n",
      "Epoch 277/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 211.4731 - mae: 212.1636 - val_loss: 9755.5029 - val_mae: 9756.1953\n",
      "Epoch 278/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 221.3269 - mae: 222.0142 - val_loss: 9012.3984 - val_mae: 9013.0898\n",
      "Epoch 279/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 232.3385 - mae: 233.0286 - val_loss: 9637.6562 - val_mae: 9638.3496\n",
      "Epoch 280/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 208.3607 - mae: 209.0517 - val_loss: 9512.4443 - val_mae: 9513.1367\n",
      "Epoch 281/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 202.5751 - mae: 203.2636 - val_loss: 9580.3008 - val_mae: 9580.9951\n",
      "Epoch 282/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 209.3389 - mae: 210.0301 - val_loss: 9759.4268 - val_mae: 9760.1201\n",
      "Epoch 283/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 207.4510 - mae: 208.1414 - val_loss: 9766.5566 - val_mae: 9767.2500\n",
      "Epoch 284/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 194.4954 - mae: 195.1850 - val_loss: 9646.5557 - val_mae: 9647.2490\n",
      "Epoch 285/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 209.3698 - mae: 210.0594 - val_loss: 9831.4658 - val_mae: 9832.1602\n",
      "Epoch 286/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 209.4629 - mae: 210.1532 - val_loss: 9742.8184 - val_mae: 9743.5117\n",
      "Epoch 287/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 218.6554 - mae: 219.3432 - val_loss: 9930.5664 - val_mae: 9931.2598\n",
      "Epoch 288/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 198.9525 - mae: 199.6433 - val_loss: 9593.8340 - val_mae: 9594.5273\n",
      "Epoch 289/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 203.9967 - mae: 204.6878 - val_loss: 9411.6914 - val_mae: 9412.3838\n",
      "Epoch 290/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 214.3617 - mae: 215.0524 - val_loss: 9769.6260 - val_mae: 9770.3193\n",
      "Epoch 291/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 195.0125 - mae: 195.7032 - val_loss: 9963.8613 - val_mae: 9964.5557\n",
      "Epoch 292/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 218.5683 - mae: 219.2576 - val_loss: 9783.1045 - val_mae: 9783.7988\n",
      "Epoch 293/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 208.2132 - mae: 208.9036 - val_loss: 9204.6562 - val_mae: 9205.3496\n",
      "Epoch 294/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 219.0736 - mae: 219.7650 - val_loss: 9624.4951 - val_mae: 9625.1875\n",
      "Epoch 295/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 198.3789 - mae: 199.0665 - val_loss: 9930.1973 - val_mae: 9930.8896\n",
      "Epoch 296/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 184.7651 - mae: 185.4551 - val_loss: 9859.4531 - val_mae: 9860.1465\n",
      "Epoch 297/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 207.9319 - mae: 208.6241 - val_loss: 10329.1377 - val_mae: 10329.8291\n",
      "Epoch 298/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 231.4936 - mae: 232.1834 - val_loss: 9748.1309 - val_mae: 9748.8252\n",
      "Epoch 299/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 234.8698 - mae: 235.5578 - val_loss: 9500.3701 - val_mae: 9501.0635\n",
      "Epoch 300/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 230.5994 - mae: 231.2891 - val_loss: 9708.7637 - val_mae: 9709.4561\n",
      "Epoch 301/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 199.4951 - mae: 200.1852 - val_loss: 9629.2236 - val_mae: 9629.9160\n",
      "Epoch 302/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 205.3508 - mae: 206.0410 - val_loss: 9189.5859 - val_mae: 9190.2793\n",
      "Epoch 303/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 262.1501 - mae: 262.8413 - val_loss: 9397.9326 - val_mae: 9398.6250\n",
      "Epoch 304/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 198.9731 - mae: 199.6628 - val_loss: 9234.6045 - val_mae: 9235.2979\n",
      "Epoch 305/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 196.4657 - mae: 197.1549 - val_loss: 9714.5498 - val_mae: 9715.2432\n",
      "Epoch 306/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 192.6453 - mae: 193.3342 - val_loss: 9534.2168 - val_mae: 9534.9102\n",
      "Epoch 307/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 218.8962 - mae: 219.5871 - val_loss: 10092.3271 - val_mae: 10093.0205\n",
      "Epoch 308/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 206.1043 - mae: 206.7943 - val_loss: 9872.8223 - val_mae: 9873.5156\n",
      "Epoch 309/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 195.4521 - mae: 196.1419 - val_loss: 9816.1484 - val_mae: 9816.8398\n",
      "Epoch 310/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 234.0987 - mae: 234.7866 - val_loss: 9877.1504 - val_mae: 9877.8457\n",
      "Epoch 311/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 200.3681 - mae: 201.0557 - val_loss: 9631.5918 - val_mae: 9632.2852\n",
      "Epoch 312/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 210.6582 - mae: 211.3465 - val_loss: 9592.9668 - val_mae: 9593.6602\n",
      "Epoch 313/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 206.6372 - mae: 207.3253 - val_loss: 9805.9785 - val_mae: 9806.6719\n",
      "Epoch 314/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 236.3779 - mae: 237.0664 - val_loss: 9751.3721 - val_mae: 9752.0664\n",
      "Epoch 315/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 197.7090 - mae: 198.3992 - val_loss: 9860.4775 - val_mae: 9861.1689\n",
      "Epoch 316/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 196.4392 - mae: 197.1265 - val_loss: 9878.0996 - val_mae: 9878.7939\n",
      "Epoch 317/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 220.4066 - mae: 221.0972 - val_loss: 9511.8213 - val_mae: 9512.5146\n",
      "Epoch 318/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 202.6350 - mae: 203.3234 - val_loss: 9559.8545 - val_mae: 9560.5469\n",
      "Epoch 319/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 192.5406 - mae: 193.2300 - val_loss: 9803.6572 - val_mae: 9804.3506\n",
      "Epoch 320/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 185.2665 - mae: 185.9550 - val_loss: 9871.3408 - val_mae: 9872.0342\n",
      "Epoch 321/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 185.0251 - mae: 185.7159 - val_loss: 9520.7188 - val_mae: 9521.4111\n",
      "Epoch 322/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 206.8307 - mae: 207.5181 - val_loss: 9738.6602 - val_mae: 9739.3506\n",
      "Epoch 323/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 175.1844 - mae: 175.8715 - val_loss: 9932.5361 - val_mae: 9933.2285\n",
      "Epoch 324/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 196.4798 - mae: 197.1693 - val_loss: 9817.0205 - val_mae: 9817.7148\n",
      "Epoch 325/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 190.5220 - mae: 191.2125 - val_loss: 9894.7773 - val_mae: 9895.4707\n",
      "Epoch 326/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 203.2035 - mae: 203.8946 - val_loss: 9850.6338 - val_mae: 9851.3262\n",
      "Epoch 327/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 204.7911 - mae: 205.4819 - val_loss: 9505.6094 - val_mae: 9506.3018\n",
      "Epoch 328/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 194.5764 - mae: 195.2629 - val_loss: 9641.5488 - val_mae: 9642.2432\n",
      "Epoch 329/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 188.6760 - mae: 189.3654 - val_loss: 9726.3252 - val_mae: 9727.0176\n",
      "Epoch 330/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 190.0788 - mae: 190.7697 - val_loss: 9825.5000 - val_mae: 9826.1924\n",
      "Epoch 331/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 195.5760 - mae: 196.2665 - val_loss: 9912.2354 - val_mae: 9912.9268\n",
      "Epoch 332/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 197.3778 - mae: 198.0669 - val_loss: 10029.1914 - val_mae: 10029.8848\n",
      "Epoch 333/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 212.2118 - mae: 212.9011 - val_loss: 9735.8496 - val_mae: 9736.5439\n",
      "Epoch 334/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 203.9362 - mae: 204.6244 - val_loss: 9926.6758 - val_mae: 9927.3691\n",
      "Epoch 335/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 210.7006 - mae: 211.3887 - val_loss: 9534.9883 - val_mae: 9535.6807\n",
      "Epoch 336/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 187.9654 - mae: 188.6564 - val_loss: 9622.7080 - val_mae: 9623.4014\n",
      "Epoch 337/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 205.3258 - mae: 206.0147 - val_loss: 9638.7803 - val_mae: 9639.4727\n",
      "Epoch 338/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 211.0330 - mae: 211.7229 - val_loss: 9577.9092 - val_mae: 9578.6016\n",
      "Epoch 339/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 200.7269 - mae: 201.4145 - val_loss: 9635.9092 - val_mae: 9636.6025\n",
      "Epoch 340/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 206.7426 - mae: 207.4326 - val_loss: 9585.8154 - val_mae: 9586.5098\n",
      "Epoch 341/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 205.6848 - mae: 206.3742 - val_loss: 10027.1641 - val_mae: 10027.8555\n",
      "Epoch 342/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 181.8457 - mae: 182.5334 - val_loss: 9645.4219 - val_mae: 9646.1152\n",
      "Epoch 343/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 210.8621 - mae: 211.5518 - val_loss: 9986.9453 - val_mae: 9987.6387\n",
      "Epoch 344/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 192.6144 - mae: 193.3035 - val_loss: 9875.2988 - val_mae: 9875.9922\n",
      "Epoch 345/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 175.1476 - mae: 175.8353 - val_loss: 9843.9902 - val_mae: 9844.6826\n",
      "Epoch 346/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 216.9501 - mae: 217.6400 - val_loss: 9780.6377 - val_mae: 9781.3320\n",
      "Epoch 347/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 209.7214 - mae: 210.4086 - val_loss: 10240.1309 - val_mae: 10240.8242\n",
      "Epoch 348/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 214.6356 - mae: 215.3251 - val_loss: 9511.0244 - val_mae: 9511.7178\n",
      "Epoch 349/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 192.5716 - mae: 193.2606 - val_loss: 9856.3691 - val_mae: 9857.0625\n",
      "Epoch 350/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 186.8524 - mae: 187.5414 - val_loss: 9575.4531 - val_mae: 9576.1475\n",
      "Epoch 351/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 199.7143 - mae: 200.4037 - val_loss: 9833.3096 - val_mae: 9834.0020\n",
      "Epoch 352/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 188.5327 - mae: 189.2224 - val_loss: 9364.9521 - val_mae: 9365.6475\n",
      "Epoch 353/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 196.4079 - mae: 197.0950 - val_loss: 9753.5635 - val_mae: 9754.2559\n",
      "Epoch 354/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 171.7313 - mae: 172.4220 - val_loss: 9929.5547 - val_mae: 9930.2490\n",
      "Epoch 355/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 187.7975 - mae: 188.4875 - val_loss: 9661.2002 - val_mae: 9661.8936\n",
      "Epoch 356/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 178.6756 - mae: 179.3662 - val_loss: 10217.7793 - val_mae: 10218.4727\n",
      "Epoch 357/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 199.4387 - mae: 200.1303 - val_loss: 9865.9443 - val_mae: 9866.6357\n",
      "Epoch 358/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 183.5515 - mae: 184.2392 - val_loss: 9729.9307 - val_mae: 9730.6230\n",
      "Epoch 359/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 180.8102 - mae: 181.4986 - val_loss: 10231.9775 - val_mae: 10232.6709\n",
      "Epoch 360/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 191.3261 - mae: 192.0173 - val_loss: 9847.6611 - val_mae: 9848.3535\n",
      "Epoch 361/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 186.0600 - mae: 186.7489 - val_loss: 10162.7559 - val_mae: 10163.4482\n",
      "Epoch 362/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 209.1362 - mae: 209.8272 - val_loss: 9811.8896 - val_mae: 9812.5830\n",
      "Epoch 363/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 190.7717 - mae: 191.4596 - val_loss: 9779.2266 - val_mae: 9779.9180\n",
      "Epoch 364/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 178.4823 - mae: 179.1699 - val_loss: 9676.8408 - val_mae: 9677.5342\n",
      "Epoch 365/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 182.1048 - mae: 182.7940 - val_loss: 9662.0127 - val_mae: 9662.7051\n",
      "Epoch 366/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 181.3640 - mae: 182.0509 - val_loss: 9825.4844 - val_mae: 9826.1777\n",
      "Epoch 367/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 191.7713 - mae: 192.4605 - val_loss: 9945.5918 - val_mae: 9946.2852\n",
      "Epoch 368/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 194.9780 - mae: 195.6685 - val_loss: 9980.9834 - val_mae: 9981.6777\n",
      "Epoch 369/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 180.4329 - mae: 181.1233 - val_loss: 10010.1016 - val_mae: 10010.7949\n",
      "Epoch 370/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 200.8813 - mae: 201.5700 - val_loss: 9853.2178 - val_mae: 9853.9102\n",
      "Epoch 371/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 206.2843 - mae: 206.9762 - val_loss: 9848.4658 - val_mae: 9849.1602\n",
      "Epoch 372/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 192.3217 - mae: 193.0123 - val_loss: 9850.4033 - val_mae: 9851.0967\n",
      "Epoch 373/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 184.2559 - mae: 184.9434 - val_loss: 10090.4180 - val_mae: 10091.1123\n",
      "Epoch 374/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 198.1301 - mae: 198.8186 - val_loss: 9946.2119 - val_mae: 9946.9053\n",
      "Epoch 375/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 212.2978 - mae: 212.9881 - val_loss: 10139.5088 - val_mae: 10140.2021\n",
      "Epoch 376/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 188.4469 - mae: 189.1365 - val_loss: 9979.1641 - val_mae: 9979.8584\n",
      "Epoch 377/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 197.5844 - mae: 198.2760 - val_loss: 10006.0625 - val_mae: 10006.7568\n",
      "Epoch 378/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 177.4090 - mae: 178.0976 - val_loss: 9422.1846 - val_mae: 9422.8779\n",
      "Epoch 379/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 195.0574 - mae: 195.7441 - val_loss: 9527.0137 - val_mae: 9527.7051\n",
      "Epoch 380/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 189.6943 - mae: 190.3821 - val_loss: 9800.6904 - val_mae: 9801.3818\n",
      "Epoch 381/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 171.3829 - mae: 172.0707 - val_loss: 9799.6123 - val_mae: 9800.3047\n",
      "Epoch 382/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 184.7480 - mae: 185.4380 - val_loss: 9736.0312 - val_mae: 9736.7236\n",
      "Epoch 383/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 182.9437 - mae: 183.6314 - val_loss: 10138.4814 - val_mae: 10139.1738\n",
      "Epoch 384/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 234.1028 - mae: 234.7905 - val_loss: 9675.2607 - val_mae: 9675.9531\n",
      "Epoch 385/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 183.1495 - mae: 183.8376 - val_loss: 9751.3740 - val_mae: 9752.0684\n",
      "Epoch 386/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 180.7757 - mae: 181.4644 - val_loss: 9663.3994 - val_mae: 9664.0918\n",
      "Epoch 387/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 177.2780 - mae: 177.9667 - val_loss: 9576.2451 - val_mae: 9576.9365\n",
      "Epoch 388/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 213.7982 - mae: 214.4895 - val_loss: 9731.8516 - val_mae: 9732.5459\n",
      "Epoch 389/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 214.1140 - mae: 214.8059 - val_loss: 9894.2959 - val_mae: 9894.9893\n",
      "Epoch 390/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 173.6590 - mae: 174.3487 - val_loss: 9879.9209 - val_mae: 9880.6143\n",
      "Epoch 391/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 202.4817 - mae: 203.1715 - val_loss: 9760.3066 - val_mae: 9761.0000\n",
      "Epoch 392/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 173.5708 - mae: 174.2600 - val_loss: 9831.8164 - val_mae: 9832.5098\n",
      "Epoch 393/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 189.7962 - mae: 190.4871 - val_loss: 9653.9141 - val_mae: 9654.6074\n",
      "Epoch 394/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 187.7217 - mae: 188.4112 - val_loss: 9764.1543 - val_mae: 9764.8486\n",
      "Epoch 395/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 187.2979 - mae: 187.9884 - val_loss: 9793.0469 - val_mae: 9793.7402\n",
      "Epoch 396/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 175.0403 - mae: 175.7305 - val_loss: 10142.1084 - val_mae: 10142.8018\n",
      "Epoch 397/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 190.6427 - mae: 191.3331 - val_loss: 9700.7656 - val_mae: 9701.4590\n",
      "Epoch 398/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 175.6942 - mae: 176.3842 - val_loss: 9622.6611 - val_mae: 9623.3545\n",
      "Epoch 399/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 185.2923 - mae: 185.9804 - val_loss: 9459.1055 - val_mae: 9459.7988\n",
      "Epoch 400/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 181.4892 - mae: 182.1791 - val_loss: 9884.6846 - val_mae: 9885.3789\n",
      "Epoch 401/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 174.9419 - mae: 175.6303 - val_loss: 9967.8252 - val_mae: 9968.5176\n",
      "Epoch 402/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 189.8348 - mae: 190.5239 - val_loss: 9853.8213 - val_mae: 9854.5146\n",
      "Epoch 403/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 189.9558 - mae: 190.6463 - val_loss: 9562.2666 - val_mae: 9562.9600\n",
      "Epoch 404/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 187.1240 - mae: 187.8115 - val_loss: 9660.9453 - val_mae: 9661.6377\n",
      "Epoch 405/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 200.1904 - mae: 200.8791 - val_loss: 9969.8496 - val_mae: 9970.5439\n",
      "Epoch 406/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 208.1164 - mae: 208.8074 - val_loss: 9678.5137 - val_mae: 9679.2061\n",
      "Epoch 407/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 199.1688 - mae: 199.8592 - val_loss: 9729.3428 - val_mae: 9730.0371\n",
      "Epoch 408/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 166.1140 - mae: 166.8027 - val_loss: 9809.4541 - val_mae: 9810.1475\n",
      "Epoch 409/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 162.3214 - mae: 163.0099 - val_loss: 9754.3887 - val_mae: 9755.0811\n",
      "Epoch 410/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 191.0035 - mae: 191.6948 - val_loss: 9690.6377 - val_mae: 9691.3291\n",
      "Epoch 411/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 186.4584 - mae: 187.1469 - val_loss: 10009.3750 - val_mae: 10010.0664\n",
      "Epoch 412/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 171.4644 - mae: 172.1540 - val_loss: 9562.4590 - val_mae: 9563.1514\n",
      "Epoch 413/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 188.1986 - mae: 188.8903 - val_loss: 9973.5791 - val_mae: 9974.2734\n",
      "Epoch 414/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 189.3297 - mae: 190.0201 - val_loss: 9839.0674 - val_mae: 9839.7598\n",
      "Epoch 415/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 190.5095 - mae: 191.1998 - val_loss: 9544.0322 - val_mae: 9544.7246\n",
      "Epoch 416/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 186.5112 - mae: 187.2021 - val_loss: 9768.6650 - val_mae: 9769.3584\n",
      "Epoch 417/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 173.8310 - mae: 174.5182 - val_loss: 9484.5264 - val_mae: 9485.2197\n",
      "Epoch 418/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 178.4047 - mae: 179.0926 - val_loss: 9867.7822 - val_mae: 9868.4766\n",
      "Epoch 419/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 166.5575 - mae: 167.2486 - val_loss: 9721.0000 - val_mae: 9721.6934\n",
      "Epoch 420/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 189.7986 - mae: 190.4854 - val_loss: 9833.7266 - val_mae: 9834.4189\n",
      "Epoch 421/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 177.4053 - mae: 178.0961 - val_loss: 9930.9746 - val_mae: 9931.6670\n",
      "Epoch 422/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 172.5060 - mae: 173.1949 - val_loss: 9396.2451 - val_mae: 9396.9385\n",
      "Epoch 423/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 187.7480 - mae: 188.4378 - val_loss: 9769.1025 - val_mae: 9769.7969\n",
      "Epoch 424/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 153.4413 - mae: 154.1292 - val_loss: 9878.1582 - val_mae: 9878.8516\n",
      "Epoch 425/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 170.2441 - mae: 170.9333 - val_loss: 10212.3584 - val_mae: 10213.0508\n",
      "Epoch 426/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 189.4370 - mae: 190.1272 - val_loss: 9690.8574 - val_mae: 9691.5508\n",
      "Epoch 427/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 171.4534 - mae: 172.1430 - val_loss: 9549.4883 - val_mae: 9550.1816\n",
      "Epoch 428/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 187.0122 - mae: 187.6997 - val_loss: 9661.9346 - val_mae: 9662.6270\n",
      "Epoch 429/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 192.0877 - mae: 192.7766 - val_loss: 9750.8105 - val_mae: 9751.5039\n",
      "Epoch 430/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 183.7756 - mae: 184.4673 - val_loss: 9635.7949 - val_mae: 9636.4873\n",
      "Epoch 431/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 177.3581 - mae: 178.0474 - val_loss: 9748.7471 - val_mae: 9749.4395\n",
      "Epoch 432/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 167.2979 - mae: 167.9872 - val_loss: 9489.6729 - val_mae: 9490.3662\n",
      "Epoch 433/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 172.6711 - mae: 173.3597 - val_loss: 9720.1143 - val_mae: 9720.8057\n",
      "Epoch 434/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 205.2795 - mae: 205.9699 - val_loss: 9524.0156 - val_mae: 9524.7090\n",
      "Epoch 435/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 171.0167 - mae: 171.7065 - val_loss: 9614.2725 - val_mae: 9614.9658\n",
      "Epoch 436/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 177.7600 - mae: 178.4505 - val_loss: 9893.3896 - val_mae: 9894.0820\n",
      "Epoch 437/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 176.1985 - mae: 176.8861 - val_loss: 9963.8311 - val_mae: 9964.5244\n",
      "Epoch 438/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 180.2684 - mae: 180.9570 - val_loss: 9759.8955 - val_mae: 9760.5889\n",
      "Epoch 439/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 163.0557 - mae: 163.7457 - val_loss: 9773.3945 - val_mae: 9774.0889\n",
      "Epoch 440/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 176.8094 - mae: 177.4961 - val_loss: 9697.9912 - val_mae: 9698.6836\n",
      "Epoch 441/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 167.0042 - mae: 167.6951 - val_loss: 9939.0791 - val_mae: 9939.7725\n",
      "Epoch 442/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 182.3738 - mae: 183.0629 - val_loss: 9434.7910 - val_mae: 9435.4834\n",
      "Epoch 443/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 164.0708 - mae: 164.7598 - val_loss: 9774.7793 - val_mae: 9775.4727\n",
      "Epoch 444/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 169.7571 - mae: 170.4443 - val_loss: 9436.0547 - val_mae: 9436.7490\n",
      "Epoch 445/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 178.9708 - mae: 179.6580 - val_loss: 9583.4746 - val_mae: 9584.1680\n",
      "Epoch 446/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 187.2222 - mae: 187.9126 - val_loss: 9720.1406 - val_mae: 9720.8320\n",
      "Epoch 447/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 189.3473 - mae: 190.0358 - val_loss: 9797.9053 - val_mae: 9798.5986\n",
      "Epoch 448/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 168.1939 - mae: 168.8819 - val_loss: 10065.7598 - val_mae: 10066.4531\n",
      "Epoch 449/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 230.7579 - mae: 231.4470 - val_loss: 9709.2637 - val_mae: 9709.9570\n",
      "Epoch 450/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 184.3475 - mae: 185.0377 - val_loss: 9784.5967 - val_mae: 9785.2900\n",
      "Epoch 451/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 184.6534 - mae: 185.3419 - val_loss: 9687.8555 - val_mae: 9688.5479\n",
      "Epoch 452/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 184.1191 - mae: 184.8104 - val_loss: 9715.1748 - val_mae: 9715.8682\n",
      "Epoch 453/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 163.9590 - mae: 164.6461 - val_loss: 9884.2764 - val_mae: 9884.9688\n",
      "Epoch 454/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 168.3778 - mae: 169.0682 - val_loss: 9667.0879 - val_mae: 9667.7803\n",
      "Epoch 455/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 162.8137 - mae: 163.5047 - val_loss: 9643.5117 - val_mae: 9644.2051\n",
      "Epoch 456/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 174.2429 - mae: 174.9297 - val_loss: 9679.1006 - val_mae: 9679.7939\n",
      "Epoch 457/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 167.6297 - mae: 168.3195 - val_loss: 9689.6328 - val_mae: 9690.3252\n",
      "Epoch 458/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 177.9427 - mae: 178.6310 - val_loss: 10020.7500 - val_mae: 10021.4424\n",
      "Epoch 459/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 178.1660 - mae: 178.8552 - val_loss: 9743.5146 - val_mae: 9744.2090\n",
      "Epoch 460/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 160.6938 - mae: 161.3834 - val_loss: 9551.3955 - val_mae: 9552.0889\n",
      "Epoch 461/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 187.0498 - mae: 187.7373 - val_loss: 9945.5840 - val_mae: 9946.2764\n",
      "Epoch 462/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 174.9227 - mae: 175.6117 - val_loss: 9784.1895 - val_mae: 9784.8818\n",
      "Epoch 463/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 168.3216 - mae: 169.0114 - val_loss: 9718.5977 - val_mae: 9719.2910\n",
      "Epoch 464/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 162.4799 - mae: 163.1695 - val_loss: 9911.7002 - val_mae: 9912.3936\n",
      "Epoch 465/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 168.7113 - mae: 169.3976 - val_loss: 9847.0322 - val_mae: 9847.7256\n",
      "Epoch 466/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 180.1894 - mae: 180.8766 - val_loss: 10130.2314 - val_mae: 10130.9258\n",
      "Epoch 467/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 184.7634 - mae: 185.4534 - val_loss: 9175.5459 - val_mae: 9176.2383\n",
      "Epoch 468/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 193.7150 - mae: 194.4045 - val_loss: 9710.0195 - val_mae: 9710.7119\n",
      "Epoch 469/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 183.6363 - mae: 184.3263 - val_loss: 9722.1758 - val_mae: 9722.8691\n",
      "Epoch 470/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 167.3023 - mae: 167.9904 - val_loss: 9631.5244 - val_mae: 9632.2168\n",
      "Epoch 471/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 166.1987 - mae: 166.8876 - val_loss: 9781.2148 - val_mae: 9781.9092\n",
      "Epoch 472/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 166.8273 - mae: 167.5167 - val_loss: 9757.2910 - val_mae: 9757.9834\n",
      "Epoch 473/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 162.6976 - mae: 163.3865 - val_loss: 9798.8916 - val_mae: 9799.5859\n",
      "Epoch 474/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 170.5256 - mae: 171.2158 - val_loss: 9482.0947 - val_mae: 9482.7871\n",
      "Epoch 475/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 224.2824 - mae: 224.9720 - val_loss: 9876.5684 - val_mae: 9877.2607\n",
      "Epoch 476/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 156.8228 - mae: 157.5105 - val_loss: 9609.5664 - val_mae: 9610.2578\n",
      "Epoch 477/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 181.8073 - mae: 182.4952 - val_loss: 9863.3057 - val_mae: 9863.9990\n",
      "Epoch 478/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 168.9697 - mae: 169.6594 - val_loss: 10076.3389 - val_mae: 10077.0322\n",
      "Epoch 479/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 189.5363 - mae: 190.2256 - val_loss: 9709.5410 - val_mae: 9710.2334\n",
      "Epoch 480/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 176.9182 - mae: 177.6092 - val_loss: 9466.0381 - val_mae: 9466.7324\n",
      "Epoch 481/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 177.6632 - mae: 178.3511 - val_loss: 9787.4023 - val_mae: 9788.0947\n",
      "Epoch 482/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 167.7306 - mae: 168.4219 - val_loss: 9884.5684 - val_mae: 9885.2617\n",
      "Epoch 483/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 162.4345 - mae: 163.1229 - val_loss: 10017.5518 - val_mae: 10018.2461\n",
      "Epoch 484/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 177.6366 - mae: 178.3246 - val_loss: 10183.9248 - val_mae: 10184.6162\n",
      "Epoch 485/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 181.4929 - mae: 182.1838 - val_loss: 9827.4297 - val_mae: 9828.1221\n",
      "Epoch 486/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 174.4672 - mae: 175.1581 - val_loss: 9690.4473 - val_mae: 9691.1406\n",
      "Epoch 487/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 155.8609 - mae: 156.5504 - val_loss: 9853.4697 - val_mae: 9854.1621\n",
      "Epoch 488/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 163.2365 - mae: 163.9202 - val_loss: 9874.8613 - val_mae: 9875.5547\n",
      "Epoch 489/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 179.9618 - mae: 180.6480 - val_loss: 9928.2725 - val_mae: 9928.9648\n",
      "Epoch 490/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 167.3949 - mae: 168.0844 - val_loss: 9789.8018 - val_mae: 9790.4951\n",
      "Epoch 491/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 156.5544 - mae: 157.2427 - val_loss: 9902.3955 - val_mae: 9903.0889\n",
      "Epoch 492/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 164.2110 - mae: 164.8992 - val_loss: 10057.3164 - val_mae: 10058.0107\n",
      "Epoch 493/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 165.2831 - mae: 165.9706 - val_loss: 10078.1787 - val_mae: 10078.8740\n",
      "Epoch 494/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 179.0049 - mae: 179.6970 - val_loss: 9925.4961 - val_mae: 9926.1895\n",
      "Epoch 495/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 170.5074 - mae: 171.1996 - val_loss: 9773.1719 - val_mae: 9773.8633\n",
      "Epoch 496/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 185.7278 - mae: 186.4185 - val_loss: 9546.4795 - val_mae: 9547.1719\n",
      "Epoch 497/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 169.9082 - mae: 170.5957 - val_loss: 10276.8496 - val_mae: 10277.5439\n",
      "Epoch 498/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 189.0197 - mae: 189.7070 - val_loss: 9725.0361 - val_mae: 9725.7295\n",
      "Epoch 499/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 160.3181 - mae: 161.0065 - val_loss: 9681.7422 - val_mae: 9682.4346\n",
      "Epoch 500/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 159.2635 - mae: 159.9491 - val_loss: 9900.8398 - val_mae: 9901.5332\n",
      "Epoch 501/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 156.1265 - mae: 156.8181 - val_loss: 10035.8994 - val_mae: 10036.5898\n",
      "Epoch 502/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 176.6375 - mae: 177.3271 - val_loss: 10251.7002 - val_mae: 10252.3936\n",
      "Epoch 503/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 163.6089 - mae: 164.2962 - val_loss: 9829.5889 - val_mae: 9830.2803\n",
      "Epoch 504/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 158.9313 - mae: 159.6164 - val_loss: 9595.7607 - val_mae: 9596.4531\n",
      "Epoch 505/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 185.1251 - mae: 185.8144 - val_loss: 9619.2500 - val_mae: 9619.9443\n",
      "Epoch 506/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 151.8229 - mae: 152.5127 - val_loss: 9988.4395 - val_mae: 9989.1328\n",
      "Epoch 507/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 153.4320 - mae: 154.1201 - val_loss: 9796.9648 - val_mae: 9797.6562\n",
      "Epoch 508/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 157.8375 - mae: 158.5287 - val_loss: 9810.1533 - val_mae: 9810.8477\n",
      "Epoch 509/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 168.3207 - mae: 169.0093 - val_loss: 9967.1914 - val_mae: 9967.8838\n",
      "Epoch 510/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 166.3254 - mae: 167.0130 - val_loss: 9928.6807 - val_mae: 9929.3730\n",
      "Epoch 511/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 162.0017 - mae: 162.6911 - val_loss: 10061.2266 - val_mae: 10061.9189\n",
      "Epoch 512/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 178.4968 - mae: 179.1853 - val_loss: 10098.5791 - val_mae: 10099.2725\n",
      "Epoch 513/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 197.7615 - mae: 198.4527 - val_loss: 9978.2305 - val_mae: 9978.9229\n",
      "Epoch 514/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 147.0865 - mae: 147.7745 - val_loss: 9771.0918 - val_mae: 9771.7852\n",
      "Epoch 515/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 172.8424 - mae: 173.5319 - val_loss: 9958.9854 - val_mae: 9959.6787\n",
      "Epoch 516/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 168.6003 - mae: 169.2889 - val_loss: 9790.8105 - val_mae: 9791.5020\n",
      "Epoch 517/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 159.2711 - mae: 159.9578 - val_loss: 9872.2773 - val_mae: 9872.9707\n",
      "Epoch 518/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 161.5792 - mae: 162.2674 - val_loss: 9917.0859 - val_mae: 9917.7783\n",
      "Epoch 519/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 169.3315 - mae: 170.0207 - val_loss: 9623.2363 - val_mae: 9623.9307\n",
      "Epoch 520/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 160.0063 - mae: 160.6954 - val_loss: 9677.2119 - val_mae: 9677.9043\n",
      "Epoch 521/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 161.3452 - mae: 162.0325 - val_loss: 9815.3691 - val_mae: 9816.0625\n",
      "Epoch 522/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 151.6207 - mae: 152.3080 - val_loss: 9917.2021 - val_mae: 9917.8955\n",
      "Epoch 523/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 161.5053 - mae: 162.1936 - val_loss: 9981.9629 - val_mae: 9982.6562\n",
      "Epoch 524/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 163.7454 - mae: 164.4346 - val_loss: 10049.3379 - val_mae: 10050.0303\n",
      "Epoch 525/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 193.7332 - mae: 194.4193 - val_loss: 9761.2607 - val_mae: 9761.9541\n",
      "Epoch 526/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 166.5651 - mae: 167.2546 - val_loss: 9969.5303 - val_mae: 9970.2246\n",
      "Epoch 527/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 171.3559 - mae: 172.0427 - val_loss: 9890.8535 - val_mae: 9891.5469\n",
      "Epoch 528/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 143.1529 - mae: 143.8425 - val_loss: 9606.2441 - val_mae: 9606.9365\n",
      "Epoch 529/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 158.3128 - mae: 159.0019 - val_loss: 9994.7471 - val_mae: 9995.4404\n",
      "Epoch 530/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 162.3959 - mae: 163.0843 - val_loss: 9884.0693 - val_mae: 9884.7637\n",
      "Epoch 531/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 151.9825 - mae: 152.6669 - val_loss: 9898.0049 - val_mae: 9898.6992\n",
      "Epoch 532/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 157.3817 - mae: 158.0714 - val_loss: 9829.7891 - val_mae: 9830.4834\n",
      "Epoch 533/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 164.4762 - mae: 165.1648 - val_loss: 9674.2461 - val_mae: 9674.9385\n",
      "Epoch 534/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 160.4899 - mae: 161.1770 - val_loss: 9644.5869 - val_mae: 9645.2803\n",
      "Epoch 535/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 156.8743 - mae: 157.5634 - val_loss: 9713.5771 - val_mae: 9714.2705\n",
      "Epoch 536/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 161.5610 - mae: 162.2475 - val_loss: 9655.8076 - val_mae: 9656.5000\n",
      "Epoch 537/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 188.8462 - mae: 189.5358 - val_loss: 9566.0254 - val_mae: 9566.7188\n",
      "Epoch 538/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 198.9361 - mae: 199.6281 - val_loss: 9678.4131 - val_mae: 9679.1055\n",
      "Epoch 539/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 153.5846 - mae: 154.2731 - val_loss: 9944.7236 - val_mae: 9945.4170\n",
      "Epoch 540/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 176.3598 - mae: 177.0487 - val_loss: 9723.8457 - val_mae: 9724.5391\n",
      "Epoch 541/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 164.9305 - mae: 165.6224 - val_loss: 9655.5518 - val_mae: 9656.2451\n",
      "Epoch 542/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 160.8354 - mae: 161.5229 - val_loss: 9598.1816 - val_mae: 9598.8740\n",
      "Epoch 543/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 171.1088 - mae: 171.7947 - val_loss: 9873.9619 - val_mae: 9874.6543\n",
      "Epoch 544/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 161.0776 - mae: 161.7643 - val_loss: 9887.1143 - val_mae: 9887.8066\n",
      "Epoch 545/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 146.4740 - mae: 147.1629 - val_loss: 9758.7754 - val_mae: 9759.4688\n",
      "Epoch 546/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 149.2771 - mae: 149.9639 - val_loss: 9609.3604 - val_mae: 9610.0547\n",
      "Epoch 547/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 165.4127 - mae: 166.1019 - val_loss: 9976.4775 - val_mae: 9977.1709\n",
      "Epoch 548/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 160.5597 - mae: 161.2500 - val_loss: 10050.8916 - val_mae: 10051.5859\n",
      "Epoch 549/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 176.1384 - mae: 176.8297 - val_loss: 9836.7930 - val_mae: 9837.4854\n",
      "Epoch 550/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 175.2591 - mae: 175.9498 - val_loss: 9745.8418 - val_mae: 9746.5361\n",
      "Epoch 551/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 162.3676 - mae: 163.0567 - val_loss: 9758.3135 - val_mae: 9759.0068\n",
      "Epoch 552/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 172.7713 - mae: 173.4616 - val_loss: 10024.6543 - val_mae: 10025.3486\n",
      "Epoch 553/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 169.3909 - mae: 170.0807 - val_loss: 10070.1309 - val_mae: 10070.8232\n",
      "Epoch 554/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 158.0692 - mae: 158.7600 - val_loss: 10233.0977 - val_mae: 10233.7900\n",
      "Epoch 555/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 143.4276 - mae: 144.1165 - val_loss: 9883.5391 - val_mae: 9884.2314\n",
      "Epoch 556/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 159.8428 - mae: 160.5319 - val_loss: 9772.8516 - val_mae: 9773.5459\n",
      "Epoch 557/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 167.3245 - mae: 168.0152 - val_loss: 9801.0430 - val_mae: 9801.7344\n",
      "Epoch 558/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 159.8411 - mae: 160.5302 - val_loss: 9821.2402 - val_mae: 9821.9326\n",
      "Epoch 559/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 146.4210 - mae: 147.1115 - val_loss: 9739.1602 - val_mae: 9739.8516\n",
      "Epoch 560/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 161.8344 - mae: 162.5215 - val_loss: 9540.3848 - val_mae: 9541.0771\n",
      "Epoch 561/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 163.7778 - mae: 164.4677 - val_loss: 10165.6465 - val_mae: 10166.3389\n",
      "Epoch 562/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 167.0036 - mae: 167.6921 - val_loss: 9799.2109 - val_mae: 9799.9023\n",
      "Epoch 563/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 166.2204 - mae: 166.9107 - val_loss: 9798.2949 - val_mae: 9798.9873\n",
      "Epoch 564/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 168.9103 - mae: 169.6005 - val_loss: 9437.1016 - val_mae: 9437.7949\n",
      "Epoch 565/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 177.5956 - mae: 178.2849 - val_loss: 9742.4990 - val_mae: 9743.1914\n",
      "Epoch 566/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 162.1774 - mae: 162.8662 - val_loss: 9957.4375 - val_mae: 9958.1309\n",
      "Epoch 567/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 168.0269 - mae: 168.7159 - val_loss: 10004.6035 - val_mae: 10005.2969\n",
      "Epoch 568/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 170.9337 - mae: 171.6219 - val_loss: 9781.8730 - val_mae: 9782.5664\n",
      "Epoch 569/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 156.8074 - mae: 157.4974 - val_loss: 9676.5361 - val_mae: 9677.2295\n",
      "Epoch 570/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 148.2086 - mae: 148.8993 - val_loss: 9872.4922 - val_mae: 9873.1846\n",
      "Epoch 571/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 151.8588 - mae: 152.5481 - val_loss: 9760.5371 - val_mae: 9761.2295\n",
      "Epoch 572/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 161.6383 - mae: 162.3291 - val_loss: 9850.9443 - val_mae: 9851.6377\n",
      "Epoch 573/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 162.8787 - mae: 163.5687 - val_loss: 9819.7012 - val_mae: 9820.3945\n",
      "Epoch 574/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 146.5083 - mae: 147.1982 - val_loss: 10283.7031 - val_mae: 10284.3965\n",
      "Epoch 575/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 170.9027 - mae: 171.5923 - val_loss: 9879.1338 - val_mae: 9879.8271\n",
      "Epoch 576/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 155.3250 - mae: 156.0137 - val_loss: 10016.1973 - val_mae: 10016.8896\n",
      "Epoch 577/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 162.9381 - mae: 163.6292 - val_loss: 9634.1406 - val_mae: 9634.8330\n",
      "Epoch 578/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 158.0101 - mae: 158.6995 - val_loss: 9897.6230 - val_mae: 9898.3174\n",
      "Epoch 579/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 180.9056 - mae: 181.5937 - val_loss: 9856.4893 - val_mae: 9857.1826\n",
      "Epoch 580/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 175.9034 - mae: 176.5933 - val_loss: 9615.1152 - val_mae: 9615.8076\n",
      "Epoch 581/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 167.0250 - mae: 167.7112 - val_loss: 9968.1943 - val_mae: 9968.8877\n",
      "Epoch 582/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 163.2978 - mae: 163.9876 - val_loss: 10029.3154 - val_mae: 10030.0088\n",
      "Epoch 583/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 169.6568 - mae: 170.3467 - val_loss: 9836.9287 - val_mae: 9837.6211\n",
      "Epoch 584/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 152.8971 - mae: 153.5854 - val_loss: 9798.5986 - val_mae: 9799.2900\n",
      "Epoch 585/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 174.1183 - mae: 174.8103 - val_loss: 9962.0498 - val_mae: 9962.7432\n",
      "Epoch 586/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 163.7691 - mae: 164.4558 - val_loss: 9734.7920 - val_mae: 9735.4854\n",
      "Epoch 587/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 155.5137 - mae: 156.2019 - val_loss: 9838.4600 - val_mae: 9839.1533\n",
      "Epoch 588/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 146.2247 - mae: 146.9125 - val_loss: 9496.3145 - val_mae: 9497.0078\n",
      "Epoch 589/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 156.6783 - mae: 157.3673 - val_loss: 9710.1758 - val_mae: 9710.8701\n",
      "Epoch 590/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 146.4096 - mae: 147.0985 - val_loss: 10086.5303 - val_mae: 10087.2217\n",
      "Epoch 591/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 144.4732 - mae: 145.1632 - val_loss: 9744.8291 - val_mae: 9745.5225\n",
      "Epoch 592/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 171.9044 - mae: 172.5933 - val_loss: 9558.5293 - val_mae: 9559.2217\n",
      "Epoch 593/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 157.9664 - mae: 158.6537 - val_loss: 9965.3818 - val_mae: 9966.0752\n",
      "Epoch 594/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 167.4558 - mae: 168.1417 - val_loss: 9618.1289 - val_mae: 9618.8223\n",
      "Epoch 595/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 165.2253 - mae: 165.9130 - val_loss: 9542.9375 - val_mae: 9543.6299\n",
      "Epoch 596/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 172.2687 - mae: 172.9581 - val_loss: 9698.1260 - val_mae: 9698.8184\n",
      "Epoch 597/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 153.1233 - mae: 153.8082 - val_loss: 9828.0273 - val_mae: 9828.7217\n",
      "Epoch 598/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 154.2223 - mae: 154.9108 - val_loss: 10111.1680 - val_mae: 10111.8604\n",
      "Epoch 599/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 162.9273 - mae: 163.6130 - val_loss: 9937.2764 - val_mae: 9937.9678\n",
      "Epoch 600/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 167.9510 - mae: 168.6388 - val_loss: 9594.4180 - val_mae: 9595.1104\n",
      "Epoch 601/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 140.7363 - mae: 141.4258 - val_loss: 9742.3574 - val_mae: 9743.0518\n",
      "Epoch 602/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 161.4324 - mae: 162.1209 - val_loss: 9763.0947 - val_mae: 9763.7881\n",
      "Epoch 603/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 150.8831 - mae: 151.5745 - val_loss: 9759.2383 - val_mae: 9759.9316\n",
      "Epoch 604/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 174.2471 - mae: 174.9365 - val_loss: 9690.2334 - val_mae: 9690.9268\n",
      "Epoch 605/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 149.0346 - mae: 149.7250 - val_loss: 9961.3262 - val_mae: 9962.0195\n",
      "Epoch 606/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 154.0638 - mae: 154.7529 - val_loss: 9755.1045 - val_mae: 9755.7969\n",
      "Epoch 607/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 148.8022 - mae: 149.4884 - val_loss: 9965.9795 - val_mae: 9966.6719\n",
      "Epoch 608/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 157.8756 - mae: 158.5648 - val_loss: 9834.9023 - val_mae: 9835.5967\n",
      "Epoch 609/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 152.0102 - mae: 152.6997 - val_loss: 9574.9863 - val_mae: 9575.6787\n",
      "Epoch 610/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 179.0295 - mae: 179.7208 - val_loss: 9761.2891 - val_mae: 9761.9805\n",
      "Epoch 611/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 159.1739 - mae: 159.8628 - val_loss: 9782.4980 - val_mae: 9783.1914\n",
      "Epoch 612/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 141.5743 - mae: 142.2619 - val_loss: 9794.9619 - val_mae: 9795.6533\n",
      "Epoch 613/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 140.2000 - mae: 140.8876 - val_loss: 9653.6484 - val_mae: 9654.3398\n",
      "Epoch 614/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 169.4884 - mae: 170.1785 - val_loss: 9830.7842 - val_mae: 9831.4746\n",
      "Epoch 615/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 157.9436 - mae: 158.6338 - val_loss: 10405.1221 - val_mae: 10405.8154\n",
      "Epoch 616/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 161.1754 - mae: 161.8647 - val_loss: 9843.0977 - val_mae: 9843.7910\n",
      "Epoch 617/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 148.5535 - mae: 149.2432 - val_loss: 9866.6729 - val_mae: 9867.3652\n",
      "Epoch 618/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 155.2106 - mae: 155.9011 - val_loss: 9939.8164 - val_mae: 9940.5098\n",
      "Epoch 619/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 149.9649 - mae: 150.6554 - val_loss: 9648.9785 - val_mae: 9649.6709\n",
      "Epoch 620/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 166.6877 - mae: 167.3773 - val_loss: 9701.2148 - val_mae: 9701.9072\n",
      "Epoch 621/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 159.7841 - mae: 160.4717 - val_loss: 9751.0801 - val_mae: 9751.7734\n",
      "Epoch 622/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 147.8566 - mae: 148.5450 - val_loss: 9697.0361 - val_mae: 9697.7295\n",
      "Epoch 623/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 140.5534 - mae: 141.2419 - val_loss: 9687.8447 - val_mae: 9688.5381\n",
      "Epoch 624/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 147.8461 - mae: 148.5350 - val_loss: 9641.4336 - val_mae: 9642.1270\n",
      "Epoch 625/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 151.5209 - mae: 152.2118 - val_loss: 9649.5225 - val_mae: 9650.2168\n",
      "Epoch 626/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 145.5858 - mae: 146.2749 - val_loss: 10022.9277 - val_mae: 10023.6191\n",
      "Epoch 627/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 167.9994 - mae: 168.6885 - val_loss: 9618.9922 - val_mae: 9619.6865\n",
      "Epoch 628/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 184.9676 - mae: 185.6567 - val_loss: 9844.3760 - val_mae: 9845.0693\n",
      "Epoch 629/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 153.7790 - mae: 154.4691 - val_loss: 9982.6025 - val_mae: 9983.2949\n",
      "Epoch 630/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 176.1517 - mae: 176.8433 - val_loss: 9762.6826 - val_mae: 9763.3760\n",
      "Epoch 631/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 150.5635 - mae: 151.2539 - val_loss: 9645.1250 - val_mae: 9645.8174\n",
      "Epoch 632/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 144.8571 - mae: 145.5467 - val_loss: 9779.6055 - val_mae: 9780.2979\n",
      "Epoch 633/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 159.0617 - mae: 159.7515 - val_loss: 9692.2812 - val_mae: 9692.9727\n",
      "Epoch 634/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 141.8764 - mae: 142.5663 - val_loss: 9528.1016 - val_mae: 9528.7939\n",
      "Epoch 635/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 148.8435 - mae: 149.5321 - val_loss: 9769.8652 - val_mae: 9770.5586\n",
      "Epoch 636/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 158.3478 - mae: 159.0379 - val_loss: 9867.5254 - val_mae: 9868.2188\n",
      "Epoch 637/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 150.0086 - mae: 150.6973 - val_loss: 9570.6270 - val_mae: 9571.3193\n",
      "Epoch 638/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 151.2550 - mae: 151.9446 - val_loss: 9572.2529 - val_mae: 9572.9473\n",
      "Epoch 639/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 141.9020 - mae: 142.5891 - val_loss: 9932.1436 - val_mae: 9932.8350\n",
      "Epoch 640/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 150.5582 - mae: 151.2476 - val_loss: 9692.6396 - val_mae: 9693.3320\n",
      "Epoch 641/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 134.8432 - mae: 135.5301 - val_loss: 9749.0498 - val_mae: 9749.7432\n",
      "Epoch 642/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 155.6624 - mae: 156.3528 - val_loss: 9747.6748 - val_mae: 9748.3691\n",
      "Epoch 643/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 154.8870 - mae: 155.5761 - val_loss: 9859.8867 - val_mae: 9860.5801\n",
      "Epoch 644/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 138.6808 - mae: 139.3689 - val_loss: 9707.2773 - val_mae: 9707.9707\n",
      "Epoch 645/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 161.0082 - mae: 161.6949 - val_loss: 10083.9209 - val_mae: 10084.6143\n",
      "Epoch 646/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 153.6352 - mae: 154.3269 - val_loss: 9596.4609 - val_mae: 9597.1533\n",
      "Epoch 647/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 142.0293 - mae: 142.7162 - val_loss: 9774.0879 - val_mae: 9774.7803\n",
      "Epoch 648/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 151.7584 - mae: 152.4484 - val_loss: 9457.0371 - val_mae: 9457.7295\n",
      "Epoch 649/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 173.6366 - mae: 174.3248 - val_loss: 9696.3672 - val_mae: 9697.0596\n",
      "Epoch 650/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 160.9769 - mae: 161.6655 - val_loss: 9765.5879 - val_mae: 9766.2812\n",
      "Epoch 651/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 156.8945 - mae: 157.5781 - val_loss: 9979.9736 - val_mae: 9980.6670\n",
      "Epoch 652/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 173.8408 - mae: 174.5308 - val_loss: 9574.3750 - val_mae: 9575.0674\n",
      "Epoch 653/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 141.4991 - mae: 142.1891 - val_loss: 9749.2539 - val_mae: 9749.9482\n",
      "Epoch 654/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 146.8706 - mae: 147.5537 - val_loss: 9591.4834 - val_mae: 9592.1768\n",
      "Epoch 655/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 160.6223 - mae: 161.3130 - val_loss: 10088.0000 - val_mae: 10088.6924\n",
      "Epoch 656/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 144.9442 - mae: 145.6328 - val_loss: 9665.0156 - val_mae: 9665.7080\n",
      "Epoch 657/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 150.8183 - mae: 151.5064 - val_loss: 10038.7441 - val_mae: 10039.4365\n",
      "Epoch 658/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 158.0890 - mae: 158.7758 - val_loss: 9678.8857 - val_mae: 9679.5791\n",
      "Epoch 659/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 163.2215 - mae: 163.9114 - val_loss: 9832.7764 - val_mae: 9833.4688\n",
      "Epoch 660/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 150.9952 - mae: 151.6823 - val_loss: 10065.3223 - val_mae: 10066.0156\n",
      "Epoch 661/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 149.6723 - mae: 150.3627 - val_loss: 9770.0566 - val_mae: 9770.7500\n",
      "Epoch 662/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 140.5270 - mae: 141.2142 - val_loss: 9544.2070 - val_mae: 9544.8994\n",
      "Epoch 663/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 133.5909 - mae: 134.2775 - val_loss: 10032.0342 - val_mae: 10032.7275\n",
      "Epoch 664/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 153.6216 - mae: 154.3118 - val_loss: 9881.6924 - val_mae: 9882.3848\n",
      "Epoch 665/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 139.3747 - mae: 140.0645 - val_loss: 9862.2129 - val_mae: 9862.9053\n",
      "Epoch 666/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 144.7196 - mae: 145.4088 - val_loss: 9940.2939 - val_mae: 9940.9873\n",
      "Epoch 667/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 163.1294 - mae: 163.8194 - val_loss: 9909.2998 - val_mae: 9909.9922\n",
      "Epoch 668/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 139.7538 - mae: 140.4419 - val_loss: 9835.4434 - val_mae: 9836.1357\n",
      "Epoch 669/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 147.6800 - mae: 148.3642 - val_loss: 9835.8252 - val_mae: 9836.5186\n",
      "Epoch 670/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 136.1591 - mae: 136.8466 - val_loss: 9736.4502 - val_mae: 9737.1445\n",
      "Epoch 671/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 162.8492 - mae: 163.5390 - val_loss: 9797.3242 - val_mae: 9798.0166\n",
      "Epoch 672/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 137.4369 - mae: 138.1249 - val_loss: 9923.7832 - val_mae: 9924.4775\n",
      "Epoch 673/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 164.7695 - mae: 165.4585 - val_loss: 9802.0352 - val_mae: 9802.7275\n",
      "Epoch 674/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 141.9274 - mae: 142.6166 - val_loss: 9870.7324 - val_mae: 9871.4258\n",
      "Epoch 675/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 153.2277 - mae: 153.9160 - val_loss: 9917.5771 - val_mae: 9918.2695\n",
      "Epoch 676/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 140.7724 - mae: 141.4578 - val_loss: 9776.9795 - val_mae: 9777.6719\n",
      "Epoch 677/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 150.5903 - mae: 151.2789 - val_loss: 9888.3164 - val_mae: 9889.0088\n",
      "Epoch 678/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 138.2039 - mae: 138.8931 - val_loss: 9882.7070 - val_mae: 9883.3994\n",
      "Epoch 679/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 161.3408 - mae: 162.0316 - val_loss: 9491.6289 - val_mae: 9492.3213\n",
      "Epoch 680/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 154.4283 - mae: 155.1173 - val_loss: 9793.7891 - val_mae: 9794.4814\n",
      "Epoch 681/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 142.2677 - mae: 142.9555 - val_loss: 9527.5205 - val_mae: 9528.2129\n",
      "Epoch 682/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 157.8853 - mae: 158.5749 - val_loss: 9646.1553 - val_mae: 9646.8486\n",
      "Epoch 683/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 141.3909 - mae: 142.0774 - val_loss: 9862.8418 - val_mae: 9863.5352\n",
      "Epoch 684/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 143.7159 - mae: 144.4044 - val_loss: 10038.6123 - val_mae: 10039.3047\n",
      "Epoch 685/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 158.7068 - mae: 159.3957 - val_loss: 9885.0566 - val_mae: 9885.7500\n",
      "Epoch 686/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 140.4704 - mae: 141.1583 - val_loss: 9788.1465 - val_mae: 9788.8389\n",
      "Epoch 687/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 139.8307 - mae: 140.5167 - val_loss: 9857.4424 - val_mae: 9858.1357\n",
      "Epoch 688/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 169.8783 - mae: 170.5686 - val_loss: 9841.0557 - val_mae: 9841.7500\n",
      "Epoch 689/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 150.8968 - mae: 151.5868 - val_loss: 9696.8652 - val_mae: 9697.5586\n",
      "Epoch 690/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 161.2297 - mae: 161.9182 - val_loss: 10115.4629 - val_mae: 10116.1572\n",
      "Epoch 691/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 147.4504 - mae: 148.1401 - val_loss: 9693.8428 - val_mae: 9694.5361\n",
      "Epoch 692/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 131.2792 - mae: 131.9655 - val_loss: 9982.9541 - val_mae: 9983.6475\n",
      "Epoch 693/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 141.2429 - mae: 141.9318 - val_loss: 9965.0986 - val_mae: 9965.7920\n",
      "Epoch 694/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 134.8427 - mae: 135.5336 - val_loss: 10118.3525 - val_mae: 10119.0449\n",
      "Epoch 695/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 172.6184 - mae: 173.3051 - val_loss: 10075.7188 - val_mae: 10076.4102\n",
      "Epoch 696/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 164.1934 - mae: 164.8825 - val_loss: 9712.3018 - val_mae: 9712.9941\n",
      "Epoch 697/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 151.4813 - mae: 152.1723 - val_loss: 9817.6133 - val_mae: 9818.3057\n",
      "Epoch 698/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 144.1971 - mae: 144.8861 - val_loss: 9823.4492 - val_mae: 9824.1426\n",
      "Epoch 699/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 138.1088 - mae: 138.7985 - val_loss: 9712.5195 - val_mae: 9713.2129\n",
      "Epoch 700/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 130.1849 - mae: 130.8753 - val_loss: 9653.4307 - val_mae: 9654.1240\n",
      "Epoch 701/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 130.5644 - mae: 131.2544 - val_loss: 9886.8906 - val_mae: 9887.5840\n",
      "Epoch 702/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 140.8936 - mae: 141.5827 - val_loss: 9685.7500 - val_mae: 9686.4424\n",
      "Epoch 703/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 127.0684 - mae: 127.7551 - val_loss: 9434.3525 - val_mae: 9435.0449\n",
      "Epoch 704/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 168.3261 - mae: 169.0168 - val_loss: 9425.9297 - val_mae: 9426.6240\n",
      "Epoch 705/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 157.4705 - mae: 158.1597 - val_loss: 9570.8555 - val_mae: 9571.5479\n",
      "Epoch 706/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 137.8980 - mae: 138.5877 - val_loss: 9817.1279 - val_mae: 9817.8203\n",
      "Epoch 707/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 137.2878 - mae: 137.9717 - val_loss: 9767.8643 - val_mae: 9768.5566\n",
      "Epoch 708/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 148.0913 - mae: 148.7792 - val_loss: 9944.9756 - val_mae: 9945.6680\n",
      "Epoch 709/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 146.2679 - mae: 146.9568 - val_loss: 9762.7744 - val_mae: 9763.4688\n",
      "Epoch 710/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 136.1945 - mae: 136.8841 - val_loss: 10008.8691 - val_mae: 10009.5615\n",
      "Epoch 711/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 167.0279 - mae: 167.7133 - val_loss: 9809.1279 - val_mae: 9809.8203\n",
      "Epoch 712/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 154.3971 - mae: 155.0847 - val_loss: 9554.9609 - val_mae: 9555.6543\n",
      "Epoch 713/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 151.7964 - mae: 152.4866 - val_loss: 9866.4072 - val_mae: 9867.1006\n",
      "Epoch 714/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 132.4570 - mae: 133.1444 - val_loss: 9856.3877 - val_mae: 9857.0811\n",
      "Epoch 715/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 135.5840 - mae: 136.2730 - val_loss: 9881.5029 - val_mae: 9882.1943\n",
      "Epoch 716/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 137.2523 - mae: 137.9408 - val_loss: 9821.5234 - val_mae: 9822.2178\n",
      "Epoch 717/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 141.2701 - mae: 141.9565 - val_loss: 9993.0469 - val_mae: 9993.7402\n",
      "Epoch 718/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 145.7154 - mae: 146.4010 - val_loss: 9880.9746 - val_mae: 9881.6680\n",
      "Epoch 719/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 146.0060 - mae: 146.6973 - val_loss: 9897.1943 - val_mae: 9897.8867\n",
      "Epoch 720/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 159.2492 - mae: 159.9366 - val_loss: 9866.5791 - val_mae: 9867.2725\n",
      "Epoch 721/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 158.6107 - mae: 159.2982 - val_loss: 9844.1621 - val_mae: 9844.8545\n",
      "Epoch 722/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 130.7900 - mae: 131.4756 - val_loss: 9889.1758 - val_mae: 9889.8691\n",
      "Epoch 723/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 147.9298 - mae: 148.6190 - val_loss: 9726.8643 - val_mae: 9727.5576\n",
      "Epoch 724/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 162.9031 - mae: 163.5910 - val_loss: 9494.4199 - val_mae: 9495.1113\n",
      "Epoch 725/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 157.1444 - mae: 157.8323 - val_loss: 9866.1328 - val_mae: 9866.8252\n",
      "Epoch 726/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 129.6763 - mae: 130.3606 - val_loss: 9680.6748 - val_mae: 9681.3682\n",
      "Epoch 727/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 142.3749 - mae: 143.0636 - val_loss: 9818.5293 - val_mae: 9819.2227\n",
      "Epoch 728/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 131.0579 - mae: 131.7447 - val_loss: 9815.7139 - val_mae: 9816.4053\n",
      "Epoch 729/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 139.1616 - mae: 139.8508 - val_loss: 9739.6494 - val_mae: 9740.3428\n",
      "Epoch 730/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 128.0022 - mae: 128.6930 - val_loss: 9693.7305 - val_mae: 9694.4238\n",
      "Epoch 731/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 129.1202 - mae: 129.8091 - val_loss: 9917.7822 - val_mae: 9918.4746\n",
      "Epoch 732/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 144.4991 - mae: 145.1889 - val_loss: 9700.6494 - val_mae: 9701.3428\n",
      "Epoch 733/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 124.8003 - mae: 125.4883 - val_loss: 9534.8242 - val_mae: 9535.5176\n",
      "Epoch 734/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 154.9526 - mae: 155.6408 - val_loss: 9746.1963 - val_mae: 9746.8906\n",
      "Epoch 735/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 129.4541 - mae: 130.1404 - val_loss: 9693.5020 - val_mae: 9694.1963\n",
      "Epoch 736/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 141.5543 - mae: 142.2444 - val_loss: 10273.2783 - val_mae: 10273.9697\n",
      "Epoch 737/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 161.1757 - mae: 161.8661 - val_loss: 9687.9932 - val_mae: 9688.6855\n",
      "Epoch 738/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 161.9093 - mae: 162.5989 - val_loss: 10009.6855 - val_mae: 10010.3799\n",
      "Epoch 739/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 149.7795 - mae: 150.4655 - val_loss: 9473.5693 - val_mae: 9474.2627\n",
      "Epoch 740/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 144.8282 - mae: 145.5155 - val_loss: 9774.7080 - val_mae: 9775.4014\n",
      "Epoch 741/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 125.2826 - mae: 125.9690 - val_loss: 9582.7051 - val_mae: 9583.3984\n",
      "Epoch 742/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 134.8704 - mae: 135.5575 - val_loss: 9776.6816 - val_mae: 9777.3750\n",
      "Epoch 743/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 132.2705 - mae: 132.9603 - val_loss: 9855.0215 - val_mae: 9855.7148\n",
      "Epoch 744/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 124.9028 - mae: 125.5905 - val_loss: 9991.6523 - val_mae: 9992.3447\n",
      "Epoch 745/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 142.2715 - mae: 142.9595 - val_loss: 9984.1680 - val_mae: 9984.8623\n",
      "Epoch 746/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 143.3018 - mae: 143.9880 - val_loss: 9905.4082 - val_mae: 9906.1016\n",
      "Epoch 747/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 136.8685 - mae: 137.5564 - val_loss: 9582.7891 - val_mae: 9583.4814\n",
      "Epoch 748/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 147.1238 - mae: 147.8118 - val_loss: 9766.9531 - val_mae: 9767.6484\n",
      "Epoch 749/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 138.9741 - mae: 139.6639 - val_loss: 9571.7129 - val_mae: 9572.4072\n",
      "Epoch 750/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 141.9492 - mae: 142.6383 - val_loss: 9759.8350 - val_mae: 9760.5283\n",
      "Epoch 751/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 124.4935 - mae: 125.1845 - val_loss: 9842.8066 - val_mae: 9843.5000\n",
      "Epoch 752/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 141.8908 - mae: 142.5785 - val_loss: 9901.2148 - val_mae: 9901.9082\n",
      "Epoch 753/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 148.8873 - mae: 149.5758 - val_loss: 9826.7373 - val_mae: 9827.4307\n",
      "Epoch 754/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 132.2141 - mae: 132.9014 - val_loss: 10006.8818 - val_mae: 10007.5752\n",
      "Epoch 755/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 131.8378 - mae: 132.5275 - val_loss: 9874.0762 - val_mae: 9874.7705\n",
      "Epoch 756/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 138.6968 - mae: 139.3841 - val_loss: 9959.3457 - val_mae: 9960.0400\n",
      "Epoch 757/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 134.7048 - mae: 135.3946 - val_loss: 9964.9697 - val_mae: 9965.6621\n",
      "Epoch 758/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 151.2237 - mae: 151.9130 - val_loss: 10065.7148 - val_mae: 10066.4072\n",
      "Epoch 759/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 140.7829 - mae: 141.4689 - val_loss: 9687.1016 - val_mae: 9687.7949\n",
      "Epoch 760/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 143.9198 - mae: 144.6092 - val_loss: 9875.2207 - val_mae: 9875.9141\n",
      "Epoch 761/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 137.1293 - mae: 137.8173 - val_loss: 9855.1260 - val_mae: 9855.8193\n",
      "Epoch 762/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 141.0220 - mae: 141.7093 - val_loss: 9889.8379 - val_mae: 9890.5303\n",
      "Epoch 763/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 138.3255 - mae: 139.0168 - val_loss: 9806.4883 - val_mae: 9807.1816\n",
      "Epoch 764/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 139.8678 - mae: 140.5571 - val_loss: 9850.6719 - val_mae: 9851.3643\n",
      "Epoch 765/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 150.0073 - mae: 150.6957 - val_loss: 9644.3516 - val_mae: 9645.0459\n",
      "Epoch 766/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 133.3175 - mae: 134.0052 - val_loss: 10089.8027 - val_mae: 10090.4951\n",
      "Epoch 767/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 138.5126 - mae: 139.2038 - val_loss: 9801.3457 - val_mae: 9802.0400\n",
      "Epoch 768/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 148.2043 - mae: 148.8936 - val_loss: 9762.0225 - val_mae: 9762.7139\n",
      "Epoch 769/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 133.3814 - mae: 134.0708 - val_loss: 9750.8291 - val_mae: 9751.5225\n",
      "Epoch 770/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 126.5506 - mae: 127.2394 - val_loss: 9769.8604 - val_mae: 9770.5537\n",
      "Epoch 771/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 139.7904 - mae: 140.4771 - val_loss: 9975.5625 - val_mae: 9976.2559\n",
      "Epoch 772/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 161.9070 - mae: 162.5989 - val_loss: 9595.4365 - val_mae: 9596.1299\n",
      "Epoch 773/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 142.4000 - mae: 143.0907 - val_loss: 9648.0010 - val_mae: 9648.6943\n",
      "Epoch 774/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 136.5938 - mae: 137.2805 - val_loss: 9676.2490 - val_mae: 9676.9434\n",
      "Epoch 775/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 136.3006 - mae: 136.9902 - val_loss: 9851.1084 - val_mae: 9851.8027\n",
      "Epoch 776/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 147.0310 - mae: 147.7172 - val_loss: 9686.3652 - val_mae: 9687.0576\n",
      "Epoch 777/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 131.1611 - mae: 131.8509 - val_loss: 10044.5371 - val_mae: 10045.2295\n",
      "Epoch 778/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 164.4076 - mae: 165.0979 - val_loss: 9836.2041 - val_mae: 9836.8984\n",
      "Epoch 779/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 141.9462 - mae: 142.6336 - val_loss: 9600.5176 - val_mae: 9601.2109\n",
      "Epoch 780/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 134.5516 - mae: 135.2403 - val_loss: 9773.3398 - val_mae: 9774.0303\n",
      "Epoch 781/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 149.6446 - mae: 150.3342 - val_loss: 9864.0293 - val_mae: 9864.7227\n",
      "Epoch 782/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 154.3619 - mae: 155.0503 - val_loss: 9627.8516 - val_mae: 9628.5459\n",
      "Epoch 783/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 128.5404 - mae: 129.2276 - val_loss: 9961.6914 - val_mae: 9962.3848\n",
      "Epoch 784/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 141.5136 - mae: 142.1999 - val_loss: 9884.0742 - val_mae: 9884.7656\n",
      "Epoch 785/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 138.9773 - mae: 139.6665 - val_loss: 9909.9873 - val_mae: 9910.6797\n",
      "Epoch 786/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 139.2271 - mae: 139.9143 - val_loss: 9970.6650 - val_mae: 9971.3584\n",
      "Epoch 787/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 127.0712 - mae: 127.7610 - val_loss: 9843.1592 - val_mae: 9843.8506\n",
      "Epoch 788/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 132.5154 - mae: 133.2019 - val_loss: 9627.0049 - val_mae: 9627.6973\n",
      "Epoch 789/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 128.8743 - mae: 129.5602 - val_loss: 9683.5391 - val_mae: 9684.2314\n",
      "Epoch 790/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 135.6714 - mae: 136.3585 - val_loss: 9826.8789 - val_mae: 9827.5723\n",
      "Epoch 791/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 128.9505 - mae: 129.6390 - val_loss: 9722.9609 - val_mae: 9723.6533\n",
      "Epoch 792/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 132.7386 - mae: 133.4247 - val_loss: 9924.2861 - val_mae: 9924.9795\n",
      "Epoch 793/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 143.4191 - mae: 144.1090 - val_loss: 9629.0576 - val_mae: 9629.7510\n",
      "Epoch 794/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 130.9053 - mae: 131.5950 - val_loss: 9850.9453 - val_mae: 9851.6377\n",
      "Epoch 795/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 129.2739 - mae: 129.9619 - val_loss: 9854.4453 - val_mae: 9855.1367\n",
      "Epoch 796/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 130.1869 - mae: 130.8705 - val_loss: 9667.8105 - val_mae: 9668.5039\n",
      "Epoch 797/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 154.5610 - mae: 155.2499 - val_loss: 9770.9365 - val_mae: 9771.6299\n",
      "Epoch 798/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 149.6417 - mae: 150.3297 - val_loss: 9840.3975 - val_mae: 9841.0918\n",
      "Epoch 799/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 139.3498 - mae: 140.0378 - val_loss: 9828.9365 - val_mae: 9829.6299\n",
      "Epoch 800/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 139.4319 - mae: 140.1196 - val_loss: 9856.7109 - val_mae: 9857.4043\n",
      "Epoch 801/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 129.9022 - mae: 130.5892 - val_loss: 9959.4277 - val_mae: 9960.1211\n",
      "Epoch 802/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 131.9204 - mae: 132.6078 - val_loss: 9858.6172 - val_mae: 9859.3096\n",
      "Epoch 803/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 139.0657 - mae: 139.7554 - val_loss: 9863.4082 - val_mae: 9864.1016\n",
      "Epoch 804/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 137.0910 - mae: 137.7820 - val_loss: 9765.3750 - val_mae: 9766.0684\n",
      "Epoch 805/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 131.9583 - mae: 132.6444 - val_loss: 9651.7139 - val_mae: 9652.4062\n",
      "Epoch 806/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 139.1748 - mae: 139.8616 - val_loss: 9774.0791 - val_mae: 9774.7715\n",
      "Epoch 807/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 143.5714 - mae: 144.2599 - val_loss: 9799.1641 - val_mae: 9799.8564\n",
      "Epoch 808/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 146.9721 - mae: 147.6615 - val_loss: 9897.3174 - val_mae: 9898.0117\n",
      "Epoch 809/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 131.4853 - mae: 132.1713 - val_loss: 9688.9463 - val_mae: 9689.6387\n",
      "Epoch 810/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 137.3755 - mae: 138.0661 - val_loss: 9871.5547 - val_mae: 9872.2490\n",
      "Epoch 811/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 122.2186 - mae: 122.9048 - val_loss: 9987.7969 - val_mae: 9988.4912\n",
      "Epoch 812/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 138.1791 - mae: 138.8689 - val_loss: 9955.4502 - val_mae: 9956.1445\n",
      "Epoch 813/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 122.0208 - mae: 122.7068 - val_loss: 9761.0371 - val_mae: 9761.7314\n",
      "Epoch 814/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 128.4027 - mae: 129.0878 - val_loss: 10138.8760 - val_mae: 10139.5693\n",
      "Epoch 815/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 155.1235 - mae: 155.8144 - val_loss: 9818.7373 - val_mae: 9819.4297\n",
      "Epoch 816/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 138.9846 - mae: 139.6708 - val_loss: 10047.6895 - val_mae: 10048.3818\n",
      "Epoch 817/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 146.8130 - mae: 147.5010 - val_loss: 9763.1572 - val_mae: 9763.8496\n",
      "Epoch 818/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 136.3728 - mae: 137.0612 - val_loss: 9945.5322 - val_mae: 9946.2266\n",
      "Epoch 819/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 136.1341 - mae: 136.8244 - val_loss: 9820.2051 - val_mae: 9820.8984\n",
      "Epoch 820/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 130.5851 - mae: 131.2750 - val_loss: 9850.5742 - val_mae: 9851.2676\n",
      "Epoch 821/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 125.5698 - mae: 126.2579 - val_loss: 9938.9561 - val_mae: 9939.6504\n",
      "Epoch 822/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 127.8329 - mae: 128.5213 - val_loss: 9768.7051 - val_mae: 9769.4004\n",
      "Epoch 823/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 167.6311 - mae: 168.3211 - val_loss: 9878.6367 - val_mae: 9879.3301\n",
      "Epoch 824/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 144.0584 - mae: 144.7483 - val_loss: 9980.4990 - val_mae: 9981.1934\n",
      "Epoch 825/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 136.3529 - mae: 137.0407 - val_loss: 9705.4766 - val_mae: 9706.1699\n",
      "Epoch 826/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 142.4956 - mae: 143.1834 - val_loss: 9512.8779 - val_mae: 9513.5713\n",
      "Epoch 827/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 148.0851 - mae: 148.7706 - val_loss: 9832.0430 - val_mae: 9832.7373\n",
      "Epoch 828/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 125.9478 - mae: 126.6349 - val_loss: 9965.6865 - val_mae: 9966.3789\n",
      "Epoch 829/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 135.0008 - mae: 135.6909 - val_loss: 9832.8066 - val_mae: 9833.5010\n",
      "Epoch 830/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 128.9892 - mae: 129.6781 - val_loss: 9942.4834 - val_mae: 9943.1768\n",
      "Epoch 831/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 130.4693 - mae: 131.1574 - val_loss: 9904.9873 - val_mae: 9905.6807\n",
      "Epoch 832/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 122.7873 - mae: 123.4767 - val_loss: 10048.2686 - val_mae: 10048.9609\n",
      "Epoch 833/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 132.5854 - mae: 133.2717 - val_loss: 9909.6963 - val_mae: 9910.3896\n",
      "Epoch 834/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 137.0099 - mae: 137.6996 - val_loss: 9603.0107 - val_mae: 9603.7051\n",
      "Epoch 835/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 139.8697 - mae: 140.5568 - val_loss: 9907.4570 - val_mae: 9908.1504\n",
      "Epoch 836/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 137.4808 - mae: 138.1683 - val_loss: 9927.8320 - val_mae: 9928.5234\n",
      "Epoch 837/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 135.4561 - mae: 136.1422 - val_loss: 9989.6650 - val_mae: 9990.3564\n",
      "Epoch 838/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 126.1027 - mae: 126.7892 - val_loss: 9730.6230 - val_mae: 9731.3145\n",
      "Epoch 839/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 140.9588 - mae: 141.6492 - val_loss: 9809.5801 - val_mae: 9810.2725\n",
      "Epoch 840/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 136.9563 - mae: 137.6463 - val_loss: 10047.0732 - val_mae: 10047.7656\n",
      "Epoch 841/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 149.7567 - mae: 150.4439 - val_loss: 9661.0771 - val_mae: 9661.7695\n",
      "Epoch 842/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 132.1294 - mae: 132.8191 - val_loss: 10001.7959 - val_mae: 10002.4902\n",
      "Epoch 843/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 127.4372 - mae: 128.1218 - val_loss: 10015.0596 - val_mae: 10015.7529\n",
      "Epoch 844/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 140.7155 - mae: 141.4016 - val_loss: 9797.5596 - val_mae: 9798.2520\n",
      "Epoch 845/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 133.1681 - mae: 133.8569 - val_loss: 9899.6201 - val_mae: 9900.3125\n",
      "Epoch 846/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 129.8533 - mae: 130.5387 - val_loss: 10184.7617 - val_mae: 10185.4561\n",
      "Epoch 847/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 136.0136 - mae: 136.7006 - val_loss: 9947.6758 - val_mae: 9948.3682\n",
      "Epoch 848/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 136.8289 - mae: 137.5174 - val_loss: 9957.8379 - val_mae: 9958.5303\n",
      "Epoch 849/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 147.1201 - mae: 147.8096 - val_loss: 9569.6572 - val_mae: 9570.3506\n",
      "Epoch 850/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 141.8086 - mae: 142.4996 - val_loss: 9742.6992 - val_mae: 9743.3936\n",
      "Epoch 851/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 132.0883 - mae: 132.7790 - val_loss: 9966.6572 - val_mae: 9967.3496\n",
      "Epoch 852/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 138.1931 - mae: 138.8805 - val_loss: 9722.6055 - val_mae: 9723.2988\n",
      "Epoch 853/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 121.9030 - mae: 122.5894 - val_loss: 9824.9404 - val_mae: 9825.6318\n",
      "Epoch 854/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 132.4482 - mae: 133.1350 - val_loss: 9737.0166 - val_mae: 9737.7090\n",
      "Epoch 855/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 138.2501 - mae: 138.9395 - val_loss: 9626.7080 - val_mae: 9627.4014\n",
      "Epoch 856/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 133.0619 - mae: 133.7494 - val_loss: 9926.6279 - val_mae: 9927.3203\n",
      "Epoch 857/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 145.3477 - mae: 146.0377 - val_loss: 9963.3906 - val_mae: 9964.0840\n",
      "Epoch 858/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 146.5218 - mae: 147.2067 - val_loss: 9629.5088 - val_mae: 9630.2031\n",
      "Epoch 859/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 138.0529 - mae: 138.7415 - val_loss: 10137.1562 - val_mae: 10137.8496\n",
      "Epoch 860/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 138.7995 - mae: 139.4835 - val_loss: 9965.4277 - val_mae: 9966.1191\n",
      "Epoch 861/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 145.0054 - mae: 145.6953 - val_loss: 9706.5684 - val_mae: 9707.2617\n",
      "Epoch 862/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 148.7837 - mae: 149.4732 - val_loss: 9929.5889 - val_mae: 9930.2822\n",
      "Epoch 863/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 128.5933 - mae: 129.2825 - val_loss: 9906.3271 - val_mae: 9907.0205\n",
      "Epoch 864/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 129.0624 - mae: 129.7494 - val_loss: 9824.1797 - val_mae: 9824.8730\n",
      "Epoch 865/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 127.0781 - mae: 127.7645 - val_loss: 9998.6084 - val_mae: 9999.3008\n",
      "Epoch 866/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 122.8086 - mae: 123.4974 - val_loss: 9852.0293 - val_mae: 9852.7236\n",
      "Epoch 867/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 129.6430 - mae: 130.3325 - val_loss: 9584.8730 - val_mae: 9585.5674\n",
      "Epoch 868/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 122.3929 - mae: 123.0795 - val_loss: 9946.5566 - val_mae: 9947.2500\n",
      "Epoch 869/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 119.7167 - mae: 120.4041 - val_loss: 9538.5723 - val_mae: 9539.2656\n",
      "Epoch 870/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 153.5761 - mae: 154.2631 - val_loss: 9724.2373 - val_mae: 9724.9307\n",
      "Epoch 871/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 132.4691 - mae: 133.1534 - val_loss: 9877.8135 - val_mae: 9878.5068\n",
      "Epoch 872/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 132.0674 - mae: 132.7547 - val_loss: 9611.0439 - val_mae: 9611.7373\n",
      "Epoch 873/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 124.7304 - mae: 125.4161 - val_loss: 9871.8711 - val_mae: 9872.5645\n",
      "Epoch 874/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 131.2878 - mae: 131.9766 - val_loss: 9895.3584 - val_mae: 9896.0508\n",
      "Epoch 875/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 124.7564 - mae: 125.4459 - val_loss: 9834.9727 - val_mae: 9835.6650\n",
      "Epoch 876/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 127.6122 - mae: 128.2987 - val_loss: 9923.5117 - val_mae: 9924.2051\n",
      "Epoch 877/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 123.2735 - mae: 123.9593 - val_loss: 9974.4502 - val_mae: 9975.1455\n",
      "Epoch 878/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 123.0822 - mae: 123.7712 - val_loss: 9897.2754 - val_mae: 9897.9678\n",
      "Epoch 879/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 123.7479 - mae: 124.4343 - val_loss: 9685.0527 - val_mae: 9685.7480\n",
      "Epoch 880/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 136.9828 - mae: 137.6727 - val_loss: 9879.5684 - val_mae: 9880.2627\n",
      "Epoch 881/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 136.7674 - mae: 137.4573 - val_loss: 9900.6533 - val_mae: 9901.3467\n",
      "Epoch 882/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 144.0325 - mae: 144.7208 - val_loss: 9974.8008 - val_mae: 9975.4922\n",
      "Epoch 883/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 128.5823 - mae: 129.2686 - val_loss: 9998.3525 - val_mae: 9999.0449\n",
      "Epoch 884/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 132.6448 - mae: 133.3323 - val_loss: 9890.5713 - val_mae: 9891.2637\n",
      "Epoch 885/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 126.3242 - mae: 127.0103 - val_loss: 9817.8252 - val_mae: 9818.5186\n",
      "Epoch 886/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 127.4335 - mae: 128.1207 - val_loss: 9756.3770 - val_mae: 9757.0684\n",
      "Epoch 887/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 138.8645 - mae: 139.5526 - val_loss: 9958.7432 - val_mae: 9959.4355\n",
      "Epoch 888/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 131.7335 - mae: 132.4234 - val_loss: 10365.3643 - val_mae: 10366.0566\n",
      "Epoch 889/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 150.4215 - mae: 151.1107 - val_loss: 9981.0020 - val_mae: 9981.6943\n",
      "Epoch 890/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 132.5306 - mae: 133.2196 - val_loss: 10023.7637 - val_mae: 10024.4561\n",
      "Epoch 891/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 132.2385 - mae: 132.9304 - val_loss: 10077.3281 - val_mae: 10078.0195\n",
      "Epoch 892/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 133.2151 - mae: 133.9024 - val_loss: 9866.3701 - val_mae: 9867.0615\n",
      "Epoch 893/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 132.0283 - mae: 132.7158 - val_loss: 9830.5811 - val_mae: 9831.2734\n",
      "Epoch 894/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 124.0799 - mae: 124.7675 - val_loss: 10126.4238 - val_mae: 10127.1182\n",
      "Epoch 895/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 153.2232 - mae: 153.9116 - val_loss: 9984.5127 - val_mae: 9985.2051\n",
      "Epoch 896/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 123.9670 - mae: 124.6537 - val_loss: 9953.9336 - val_mae: 9954.6250\n",
      "Epoch 897/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 143.9806 - mae: 144.6720 - val_loss: 9727.5195 - val_mae: 9728.2129\n",
      "Epoch 898/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 127.4413 - mae: 128.1295 - val_loss: 9974.0000 - val_mae: 9974.6924\n",
      "Epoch 899/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 129.2688 - mae: 129.9586 - val_loss: 9894.4531 - val_mae: 9895.1484\n",
      "Epoch 900/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 125.4096 - mae: 126.0997 - val_loss: 9789.6416 - val_mae: 9790.3350\n",
      "Epoch 901/5000\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 120.8687 - mae: 121.5579 - val_loss: 10026.5732 - val_mae: 10027.2646\n",
      "Epoch 902/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 130.9441 - mae: 131.6327 - val_loss: 9555.0801 - val_mae: 9555.7734\n",
      "Epoch 903/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 149.6663 - mae: 150.3561 - val_loss: 9693.3740 - val_mae: 9694.0674\n",
      "Epoch 904/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 121.7626 - mae: 122.4521 - val_loss: 9704.5225 - val_mae: 9705.2158\n",
      "Epoch 905/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 116.4810 - mae: 117.1657 - val_loss: 9960.6162 - val_mae: 9961.3105\n",
      "Epoch 906/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 122.6458 - mae: 123.3329 - val_loss: 9928.8623 - val_mae: 9929.5557\n",
      "Epoch 907/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 156.5375 - mae: 157.2280 - val_loss: 9500.9502 - val_mae: 9501.6416\n",
      "Epoch 908/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 132.3366 - mae: 133.0265 - val_loss: 9970.0967 - val_mae: 9970.7891\n",
      "Epoch 909/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 146.3991 - mae: 147.0885 - val_loss: 9702.2451 - val_mae: 9702.9365\n",
      "Epoch 910/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 125.1250 - mae: 125.8147 - val_loss: 9925.9248 - val_mae: 9926.6182\n",
      "Epoch 911/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 132.0897 - mae: 132.7766 - val_loss: 9573.0869 - val_mae: 9573.7803\n",
      "Epoch 912/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 130.0825 - mae: 130.7684 - val_loss: 9884.8789 - val_mae: 9885.5713\n",
      "Epoch 913/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 123.8542 - mae: 124.5431 - val_loss: 9743.3057 - val_mae: 9743.9990\n",
      "Epoch 914/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 125.4550 - mae: 126.1432 - val_loss: 9905.5703 - val_mae: 9906.2627\n",
      "Epoch 915/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 130.5959 - mae: 131.2820 - val_loss: 9542.1270 - val_mae: 9542.8184\n",
      "Epoch 916/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 155.4643 - mae: 156.1530 - val_loss: 9650.7041 - val_mae: 9651.3984\n",
      "Epoch 917/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 128.2453 - mae: 128.9356 - val_loss: 9890.2178 - val_mae: 9890.9102\n",
      "Epoch 918/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 115.7908 - mae: 116.4759 - val_loss: 9709.9795 - val_mae: 9710.6719\n",
      "Epoch 919/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 117.6365 - mae: 118.3237 - val_loss: 9893.0215 - val_mae: 9893.7158\n",
      "Epoch 920/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 142.7000 - mae: 143.3878 - val_loss: 9791.0654 - val_mae: 9791.7588\n",
      "Epoch 921/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 125.8864 - mae: 126.5760 - val_loss: 9905.8848 - val_mae: 9906.5781\n",
      "Epoch 922/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 119.8844 - mae: 120.5719 - val_loss: 9871.1406 - val_mae: 9871.8350\n",
      "Epoch 923/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 124.2651 - mae: 124.9533 - val_loss: 10031.1904 - val_mae: 10031.8828\n",
      "Epoch 924/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 135.0339 - mae: 135.7192 - val_loss: 9805.4893 - val_mae: 9806.1816\n",
      "Epoch 925/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 130.8378 - mae: 131.5249 - val_loss: 9963.2832 - val_mae: 9963.9756\n",
      "Epoch 926/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 136.8526 - mae: 137.5427 - val_loss: 9963.3955 - val_mae: 9964.0889\n",
      "Epoch 927/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 132.4410 - mae: 133.1307 - val_loss: 9835.9902 - val_mae: 9836.6836\n",
      "Epoch 928/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 117.0989 - mae: 117.7870 - val_loss: 9642.4346 - val_mae: 9643.1279\n",
      "Epoch 929/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 138.7270 - mae: 139.4117 - val_loss: 9945.2783 - val_mae: 9945.9697\n",
      "Epoch 930/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 142.0997 - mae: 142.7875 - val_loss: 9659.6816 - val_mae: 9660.3740\n",
      "Epoch 931/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 132.6363 - mae: 133.3210 - val_loss: 9555.3604 - val_mae: 9556.0537\n",
      "Epoch 932/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 138.8879 - mae: 139.5745 - val_loss: 9972.8613 - val_mae: 9973.5537\n",
      "Epoch 933/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 118.5419 - mae: 119.2288 - val_loss: 9748.1777 - val_mae: 9748.8711\n",
      "Epoch 934/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 118.3709 - mae: 119.0579 - val_loss: 9894.5303 - val_mae: 9895.2236\n",
      "Epoch 935/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 139.9457 - mae: 140.6352 - val_loss: 9811.0928 - val_mae: 9811.7842\n",
      "Epoch 936/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 124.4370 - mae: 125.1269 - val_loss: 9812.5986 - val_mae: 9813.2920\n",
      "Epoch 937/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 133.1661 - mae: 133.8557 - val_loss: 9612.8398 - val_mae: 9613.5322\n",
      "Epoch 938/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 143.8691 - mae: 144.5573 - val_loss: 9697.2393 - val_mae: 9697.9316\n",
      "Epoch 939/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 129.9203 - mae: 130.6094 - val_loss: 9709.8164 - val_mae: 9710.5107\n",
      "Epoch 940/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 137.3843 - mae: 138.0712 - val_loss: 10054.0342 - val_mae: 10054.7266\n",
      "Epoch 941/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 152.7611 - mae: 153.4525 - val_loss: 9880.4033 - val_mae: 9881.0957\n",
      "Epoch 942/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 122.4811 - mae: 123.1689 - val_loss: 9841.0332 - val_mae: 9841.7256\n",
      "Epoch 943/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 125.2581 - mae: 125.9449 - val_loss: 9857.7959 - val_mae: 9858.4893\n",
      "Epoch 944/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 129.7415 - mae: 130.4299 - val_loss: 9918.6133 - val_mae: 9919.3066\n",
      "Epoch 945/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 122.1551 - mae: 122.8443 - val_loss: 9769.4150 - val_mae: 9770.1084\n",
      "Epoch 946/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 120.0861 - mae: 120.7753 - val_loss: 9747.7256 - val_mae: 9748.4180\n",
      "Epoch 947/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 123.3136 - mae: 123.9995 - val_loss: 9842.1963 - val_mae: 9842.8877\n",
      "Epoch 948/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 126.2552 - mae: 126.9432 - val_loss: 10039.0928 - val_mae: 10039.7861\n",
      "Epoch 949/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 159.0856 - mae: 159.7739 - val_loss: 10154.2334 - val_mae: 10154.9268\n",
      "Epoch 950/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 132.8932 - mae: 133.5796 - val_loss: 9688.7285 - val_mae: 9689.4209\n",
      "Epoch 951/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 121.6054 - mae: 122.2896 - val_loss: 9709.7148 - val_mae: 9710.4082\n",
      "Epoch 952/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 121.3626 - mae: 122.0492 - val_loss: 9813.1484 - val_mae: 9813.8418\n",
      "Epoch 953/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 128.6019 - mae: 129.2860 - val_loss: 9924.4648 - val_mae: 9925.1582\n",
      "Epoch 954/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 138.6247 - mae: 139.3128 - val_loss: 9809.1006 - val_mae: 9809.7949\n",
      "Epoch 955/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 139.1359 - mae: 139.8254 - val_loss: 9628.6455 - val_mae: 9629.3389\n",
      "Epoch 956/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 126.6182 - mae: 127.3063 - val_loss: 9912.8105 - val_mae: 9913.5039\n",
      "Epoch 957/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 118.0192 - mae: 118.7060 - val_loss: 9792.0078 - val_mae: 9792.7012\n",
      "Epoch 958/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 112.5849 - mae: 113.2706 - val_loss: 9773.3525 - val_mae: 9774.0479\n",
      "Epoch 959/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 117.8867 - mae: 118.5739 - val_loss: 9880.9893 - val_mae: 9881.6836\n",
      "Epoch 960/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 121.2067 - mae: 121.8955 - val_loss: 9698.0693 - val_mae: 9698.7607\n",
      "Epoch 961/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 136.7072 - mae: 137.3941 - val_loss: 9858.1621 - val_mae: 9858.8564\n",
      "Epoch 962/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 121.9293 - mae: 122.6138 - val_loss: 9790.8662 - val_mae: 9791.5586\n",
      "Epoch 963/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 124.0266 - mae: 124.7146 - val_loss: 9882.9424 - val_mae: 9883.6367\n",
      "Epoch 964/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 123.2105 - mae: 123.8962 - val_loss: 10045.6514 - val_mae: 10046.3457\n",
      "Epoch 965/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 139.0579 - mae: 139.7471 - val_loss: 9888.7041 - val_mae: 9889.3975\n",
      "Epoch 966/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 134.0070 - mae: 134.6935 - val_loss: 9865.0283 - val_mae: 9865.7227\n",
      "Epoch 967/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 117.3291 - mae: 118.0186 - val_loss: 9885.2002 - val_mae: 9885.8926\n",
      "Epoch 968/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 122.8438 - mae: 123.5327 - val_loss: 10017.3291 - val_mae: 10018.0215\n",
      "Epoch 969/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 128.2087 - mae: 128.8962 - val_loss: 9902.1914 - val_mae: 9902.8838\n",
      "Epoch 970/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 123.3118 - mae: 123.9972 - val_loss: 10087.7705 - val_mae: 10088.4629\n",
      "Epoch 971/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 133.7135 - mae: 134.4012 - val_loss: 9758.4766 - val_mae: 9759.1689\n",
      "Epoch 972/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 127.6623 - mae: 128.3482 - val_loss: 10029.3184 - val_mae: 10030.0098\n",
      "Epoch 973/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 124.2635 - mae: 124.9496 - val_loss: 9815.2051 - val_mae: 9815.8965\n",
      "Epoch 974/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 113.9554 - mae: 114.6377 - val_loss: 9849.4131 - val_mae: 9850.1055\n",
      "Epoch 975/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 123.9821 - mae: 124.6683 - val_loss: 9726.2402 - val_mae: 9726.9326\n",
      "Epoch 976/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 118.8867 - mae: 119.5741 - val_loss: 9942.8525 - val_mae: 9943.5479\n",
      "Epoch 977/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 135.5205 - mae: 136.2100 - val_loss: 9697.3008 - val_mae: 9697.9922\n",
      "Epoch 978/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 119.4960 - mae: 120.1808 - val_loss: 9987.6025 - val_mae: 9988.2949\n",
      "Epoch 979/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 118.6351 - mae: 119.3239 - val_loss: 9710.8086 - val_mae: 9711.5029\n",
      "Epoch 980/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 131.8159 - mae: 132.5034 - val_loss: 9887.5938 - val_mae: 9888.2861\n",
      "Epoch 981/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 128.6278 - mae: 129.3179 - val_loss: 9688.1230 - val_mae: 9688.8164\n",
      "Epoch 982/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 123.5374 - mae: 124.2221 - val_loss: 9979.8525 - val_mae: 9980.5469\n",
      "Epoch 983/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 122.6778 - mae: 123.3673 - val_loss: 9807.6533 - val_mae: 9808.3467\n",
      "Epoch 984/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 114.8611 - mae: 115.5468 - val_loss: 10079.8828 - val_mae: 10080.5752\n",
      "Epoch 985/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 145.8623 - mae: 146.5520 - val_loss: 10021.2412 - val_mae: 10021.9355\n",
      "Epoch 986/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 126.2290 - mae: 126.9187 - val_loss: 10115.8369 - val_mae: 10116.5293\n",
      "Epoch 987/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 137.5548 - mae: 138.2431 - val_loss: 9687.7725 - val_mae: 9688.4639\n",
      "Epoch 988/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 143.2798 - mae: 143.9694 - val_loss: 9572.0918 - val_mae: 9572.7842\n",
      "Epoch 989/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 137.5496 - mae: 138.2380 - val_loss: 9659.7891 - val_mae: 9660.4824\n",
      "Epoch 990/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 117.6140 - mae: 118.2984 - val_loss: 9778.9609 - val_mae: 9779.6533\n",
      "Epoch 991/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 118.1794 - mae: 118.8660 - val_loss: 9940.4248 - val_mae: 9941.1182\n",
      "Epoch 992/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 121.6831 - mae: 122.3691 - val_loss: 9710.1377 - val_mae: 9710.8311\n",
      "Epoch 993/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 128.5147 - mae: 129.2024 - val_loss: 9581.2285 - val_mae: 9581.9209\n",
      "Epoch 994/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 134.0460 - mae: 134.7359 - val_loss: 9812.3613 - val_mae: 9813.0557\n",
      "Epoch 995/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 121.3112 - mae: 121.9977 - val_loss: 9813.6963 - val_mae: 9814.3896\n",
      "Epoch 996/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 116.7910 - mae: 117.4793 - val_loss: 9548.6953 - val_mae: 9549.3877\n",
      "Epoch 997/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 117.4871 - mae: 118.1744 - val_loss: 9711.6035 - val_mae: 9712.2969\n",
      "Epoch 998/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 119.4302 - mae: 120.1207 - val_loss: 9813.1699 - val_mae: 9813.8623\n",
      "Epoch 999/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 114.6842 - mae: 115.3720 - val_loss: 9778.0244 - val_mae: 9778.7178\n",
      "Epoch 1000/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 116.7676 - mae: 117.4529 - val_loss: 9665.2754 - val_mae: 9665.9688\n",
      "Epoch 1001/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 128.3520 - mae: 129.0375 - val_loss: 9936.3008 - val_mae: 9936.9941\n",
      "Epoch 1002/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 129.6735 - mae: 130.3591 - val_loss: 9739.5273 - val_mae: 9740.2207\n",
      "Epoch 1003/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 123.8690 - mae: 124.5580 - val_loss: 9724.8613 - val_mae: 9725.5547\n",
      "Epoch 1004/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 125.9621 - mae: 126.6513 - val_loss: 10025.2686 - val_mae: 10025.9619\n",
      "Epoch 1005/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 126.1355 - mae: 126.8229 - val_loss: 9779.8340 - val_mae: 9780.5283\n",
      "Epoch 1006/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 129.2652 - mae: 129.9552 - val_loss: 9639.2900 - val_mae: 9639.9834\n",
      "Epoch 1007/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 120.7681 - mae: 121.4561 - val_loss: 9774.4297 - val_mae: 9775.1221\n",
      "Epoch 1008/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 124.6252 - mae: 125.3106 - val_loss: 9793.6484 - val_mae: 9794.3408\n",
      "Epoch 1009/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 135.3519 - mae: 136.0413 - val_loss: 9635.3555 - val_mae: 9636.0488\n",
      "Epoch 1010/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 131.3914 - mae: 132.0809 - val_loss: 9916.5703 - val_mae: 9917.2627\n",
      "Epoch 1011/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 119.8083 - mae: 120.4959 - val_loss: 9534.8389 - val_mae: 9535.5322\n",
      "Epoch 1012/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 117.8473 - mae: 118.5333 - val_loss: 9878.5127 - val_mae: 9879.2051\n",
      "Epoch 1013/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 119.7244 - mae: 120.4128 - val_loss: 9621.7051 - val_mae: 9622.3984\n",
      "Epoch 1014/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 124.5545 - mae: 125.2445 - val_loss: 9948.4209 - val_mae: 9949.1123\n",
      "Epoch 1015/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 120.5348 - mae: 121.2205 - val_loss: 9823.1680 - val_mae: 9823.8584\n",
      "Epoch 1016/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 119.4537 - mae: 120.1394 - val_loss: 10007.8516 - val_mae: 10008.5430\n",
      "Epoch 1017/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 112.9790 - mae: 113.6662 - val_loss: 9846.6777 - val_mae: 9847.3701\n",
      "Epoch 1018/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 129.2600 - mae: 129.9475 - val_loss: 9379.9375 - val_mae: 9380.6299\n",
      "Epoch 1019/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 138.2804 - mae: 138.9681 - val_loss: 9867.0479 - val_mae: 9867.7412\n",
      "Epoch 1020/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 129.8636 - mae: 130.5535 - val_loss: 9667.1357 - val_mae: 9667.8291\n",
      "Epoch 1021/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 123.6463 - mae: 124.3330 - val_loss: 9659.8750 - val_mae: 9660.5693\n",
      "Epoch 1022/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 112.1742 - mae: 112.8625 - val_loss: 9683.5869 - val_mae: 9684.2793\n",
      "Epoch 1023/5000\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 121.6033 - mae: 122.2916 - val_loss: 9743.6396 - val_mae: 9744.3330\n",
      "Epoch 1024/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 123.6650 - mae: 124.3511 - val_loss: 9963.1465 - val_mae: 9963.8398\n",
      "Epoch 1025/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 114.6331 - mae: 115.3201 - val_loss: 9744.9766 - val_mae: 9745.6699\n",
      "Epoch 1026/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 124.7540 - mae: 125.4434 - val_loss: 9898.0859 - val_mae: 9898.7803\n",
      "Epoch 1027/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 125.9361 - mae: 126.6256 - val_loss: 9881.5029 - val_mae: 9882.1963\n",
      "Epoch 1028/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 121.0436 - mae: 121.7298 - val_loss: 9798.3652 - val_mae: 9799.0576\n",
      "Epoch 1029/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 114.5243 - mae: 115.2096 - val_loss: 9981.6230 - val_mae: 9982.3164\n",
      "Epoch 1030/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 121.7543 - mae: 122.4404 - val_loss: 9734.7158 - val_mae: 9735.4092\n",
      "Epoch 1031/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 121.4059 - mae: 122.0918 - val_loss: 9870.8975 - val_mae: 9871.5898\n",
      "Epoch 1032/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 121.2428 - mae: 121.9301 - val_loss: 9780.5537 - val_mae: 9781.2480\n",
      "Epoch 1033/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 104.8969 - mae: 105.5820 - val_loss: 9809.5811 - val_mae: 9810.2725\n",
      "Epoch 1034/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 125.6527 - mae: 126.3397 - val_loss: 9944.9805 - val_mae: 9945.6738\n",
      "Epoch 1035/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 122.2636 - mae: 122.9496 - val_loss: 9939.2314 - val_mae: 9939.9248\n",
      "Epoch 1036/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 122.8448 - mae: 123.5337 - val_loss: 9943.0732 - val_mae: 9943.7656\n",
      "Epoch 1037/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 119.7725 - mae: 120.4580 - val_loss: 9677.1816 - val_mae: 9677.8740\n",
      "Epoch 1038/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 115.8648 - mae: 116.5531 - val_loss: 9750.6846 - val_mae: 9751.3789\n",
      "Epoch 1039/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 110.5809 - mae: 111.2638 - val_loss: 9835.9736 - val_mae: 9836.6660\n",
      "Epoch 1040/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 136.8035 - mae: 137.4901 - val_loss: 9951.2129 - val_mae: 9951.9043\n",
      "Epoch 1041/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 124.4853 - mae: 125.1679 - val_loss: 9668.6152 - val_mae: 9669.3086\n",
      "Epoch 1042/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 113.8601 - mae: 114.5458 - val_loss: 9707.9883 - val_mae: 9708.6826\n",
      "Epoch 1043/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 135.6882 - mae: 136.3760 - val_loss: 9800.6533 - val_mae: 9801.3477\n",
      "Epoch 1044/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 125.7183 - mae: 126.4090 - val_loss: 10073.2979 - val_mae: 10073.9912\n",
      "Epoch 1045/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 153.8381 - mae: 154.5277 - val_loss: 9799.6377 - val_mae: 9800.3311\n",
      "Epoch 1046/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 119.2041 - mae: 119.8915 - val_loss: 9696.7354 - val_mae: 9697.4297\n",
      "Epoch 1047/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 124.4153 - mae: 125.1038 - val_loss: 9855.8770 - val_mae: 9856.5693\n",
      "Epoch 1048/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 122.7266 - mae: 123.4162 - val_loss: 10189.4121 - val_mae: 10190.1055\n",
      "Epoch 1049/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 145.3283 - mae: 146.0172 - val_loss: 9985.7900 - val_mae: 9986.4824\n",
      "Epoch 1050/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 126.2112 - mae: 126.9005 - val_loss: 9886.1523 - val_mae: 9886.8467\n",
      "Epoch 1051/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 117.1676 - mae: 117.8547 - val_loss: 9759.7285 - val_mae: 9760.4219\n",
      "Epoch 1052/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 130.9612 - mae: 131.6472 - val_loss: 9632.6699 - val_mae: 9633.3633\n",
      "Epoch 1053/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 126.4127 - mae: 127.0985 - val_loss: 9726.4502 - val_mae: 9727.1436\n",
      "Epoch 1054/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 119.2591 - mae: 119.9481 - val_loss: 9698.5234 - val_mae: 9699.2178\n",
      "Epoch 1055/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 119.2928 - mae: 119.9794 - val_loss: 9877.6982 - val_mae: 9878.3906\n",
      "Epoch 1056/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 131.7119 - mae: 132.4018 - val_loss: 9801.3428 - val_mae: 9802.0371\n",
      "Epoch 1057/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 131.3342 - mae: 132.0222 - val_loss: 9768.3604 - val_mae: 9769.0527\n",
      "Epoch 1058/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 112.7655 - mae: 113.4519 - val_loss: 10214.7676 - val_mae: 10215.4609\n",
      "Epoch 1059/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 150.2285 - mae: 150.9161 - val_loss: 10107.5752 - val_mae: 10108.2676\n",
      "Epoch 1060/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 134.9382 - mae: 135.6251 - val_loss: 9726.6797 - val_mae: 9727.3740\n",
      "Epoch 1061/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 142.4818 - mae: 143.1705 - val_loss: 9830.4854 - val_mae: 9831.1787\n",
      "Epoch 1062/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 135.9845 - mae: 136.6735 - val_loss: 9897.7549 - val_mae: 9898.4492\n",
      "Epoch 1063/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 112.8917 - mae: 113.5782 - val_loss: 9752.7822 - val_mae: 9753.4746\n",
      "Epoch 1064/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 115.2439 - mae: 115.9300 - val_loss: 9763.9531 - val_mae: 9764.6455\n",
      "Epoch 1065/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 116.8477 - mae: 117.5349 - val_loss: 9752.4150 - val_mae: 9753.1094\n",
      "Epoch 1066/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 117.3525 - mae: 118.0392 - val_loss: 9914.1367 - val_mae: 9914.8301\n",
      "Epoch 1067/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 128.1723 - mae: 128.8610 - val_loss: 9625.7129 - val_mae: 9626.4043\n",
      "Epoch 1068/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 117.4696 - mae: 118.1583 - val_loss: 9738.6484 - val_mae: 9739.3428\n",
      "Epoch 1069/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 121.5710 - mae: 122.2616 - val_loss: 9844.3184 - val_mae: 9845.0107\n",
      "Epoch 1070/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 117.8730 - mae: 118.5592 - val_loss: 9792.3096 - val_mae: 9793.0029\n",
      "Epoch 1071/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 116.9500 - mae: 117.6409 - val_loss: 9541.3438 - val_mae: 9542.0371\n",
      "Epoch 1072/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 128.3178 - mae: 129.0070 - val_loss: 9846.6182 - val_mae: 9847.3105\n",
      "Epoch 1073/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 116.1864 - mae: 116.8772 - val_loss: 9747.0605 - val_mae: 9747.7529\n",
      "Epoch 1074/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 127.8702 - mae: 128.5584 - val_loss: 9845.8037 - val_mae: 9846.4961\n",
      "Epoch 1075/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 104.2306 - mae: 104.9115 - val_loss: 9978.1895 - val_mae: 9978.8809\n",
      "Epoch 1076/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 112.7041 - mae: 113.3912 - val_loss: 9615.4502 - val_mae: 9616.1436\n",
      "Epoch 1077/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 127.1927 - mae: 127.8787 - val_loss: 9981.4658 - val_mae: 9982.1602\n",
      "Epoch 1078/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 121.8082 - mae: 122.4986 - val_loss: 9858.8896 - val_mae: 9859.5811\n",
      "Epoch 1079/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 114.8297 - mae: 115.5158 - val_loss: 9853.9834 - val_mae: 9854.6768\n",
      "Epoch 1080/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 117.3975 - mae: 118.0856 - val_loss: 9876.1602 - val_mae: 9876.8545\n",
      "Epoch 1081/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 117.3538 - mae: 118.0406 - val_loss: 9939.5742 - val_mae: 9940.2666\n",
      "Epoch 1082/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 131.8150 - mae: 132.5061 - val_loss: 9859.4502 - val_mae: 9860.1416\n",
      "Epoch 1083/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 117.0801 - mae: 117.7655 - val_loss: 9815.3291 - val_mae: 9816.0225\n",
      "Epoch 1084/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 108.6437 - mae: 109.3288 - val_loss: 9581.7100 - val_mae: 9582.4023\n",
      "Epoch 1085/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 136.9432 - mae: 137.6302 - val_loss: 9464.2373 - val_mae: 9464.9316\n",
      "Epoch 1086/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 109.6594 - mae: 110.3467 - val_loss: 9573.3057 - val_mae: 9573.9990\n",
      "Epoch 1087/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 127.2212 - mae: 127.9095 - val_loss: 9511.1914 - val_mae: 9511.8848\n",
      "Epoch 1088/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 127.4921 - mae: 128.1792 - val_loss: 9652.3633 - val_mae: 9653.0566\n",
      "Epoch 1089/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 109.4333 - mae: 110.1217 - val_loss: 9481.8467 - val_mae: 9482.5410\n",
      "Epoch 1090/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 136.7555 - mae: 137.4459 - val_loss: 9756.0352 - val_mae: 9756.7275\n",
      "Epoch 1091/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 132.9443 - mae: 133.6293 - val_loss: 9934.1328 - val_mae: 9934.8262\n",
      "Epoch 1092/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 143.5959 - mae: 144.2853 - val_loss: 9977.3750 - val_mae: 9978.0674\n",
      "Epoch 1093/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 126.3017 - mae: 126.9900 - val_loss: 9715.9082 - val_mae: 9716.5996\n",
      "Epoch 1094/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 120.2951 - mae: 120.9815 - val_loss: 9666.7832 - val_mae: 9667.4775\n",
      "Epoch 1095/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 121.0958 - mae: 121.7777 - val_loss: 9668.2979 - val_mae: 9668.9912\n",
      "Epoch 1096/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 122.6222 - mae: 123.3101 - val_loss: 9667.1055 - val_mae: 9667.7979\n",
      "Epoch 1097/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 113.0710 - mae: 113.7575 - val_loss: 9776.2441 - val_mae: 9776.9385\n",
      "Epoch 1098/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 115.5362 - mae: 116.2233 - val_loss: 9528.7695 - val_mae: 9529.4629\n",
      "Epoch 1099/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 112.7754 - mae: 113.4620 - val_loss: 9769.3857 - val_mae: 9770.0771\n",
      "Epoch 1100/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 115.2798 - mae: 115.9653 - val_loss: 9709.2900 - val_mae: 9709.9824\n",
      "Epoch 1101/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 122.5900 - mae: 123.2767 - val_loss: 9626.3320 - val_mae: 9627.0244\n",
      "Epoch 1102/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 114.6507 - mae: 115.3385 - val_loss: 9761.2559 - val_mae: 9761.9502\n",
      "Epoch 1103/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 107.2113 - mae: 107.8931 - val_loss: 9966.9111 - val_mae: 9967.6045\n",
      "Epoch 1104/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 110.3106 - mae: 110.9993 - val_loss: 9799.5303 - val_mae: 9800.2227\n",
      "Epoch 1105/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 114.9788 - mae: 115.6657 - val_loss: 9645.1602 - val_mae: 9645.8525\n",
      "Epoch 1106/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 120.1814 - mae: 120.8706 - val_loss: 9427.3027 - val_mae: 9427.9961\n",
      "Epoch 1107/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 117.1286 - mae: 117.8193 - val_loss: 9762.1514 - val_mae: 9762.8467\n",
      "Epoch 1108/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 122.5021 - mae: 123.1906 - val_loss: 9572.3477 - val_mae: 9573.0420\n",
      "Epoch 1109/5000\n",
      "46/46 [==============================] - 1s 29ms/step - loss: 113.0082 - mae: 113.6967 - val_loss: 9658.7705 - val_mae: 9659.4629\n",
      "Epoch 1110/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 129.0979 - mae: 129.7855 - val_loss: 9839.8623 - val_mae: 9840.5537\n",
      "Epoch 1111/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 113.3317 - mae: 114.0197 - val_loss: 9982.1494 - val_mae: 9982.8428\n",
      "Epoch 1112/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 135.1752 - mae: 135.8636 - val_loss: 9569.7988 - val_mae: 9570.4912\n",
      "Epoch 1113/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 134.9535 - mae: 135.6440 - val_loss: 9884.1182 - val_mae: 9884.8125\n",
      "Epoch 1114/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 117.2274 - mae: 117.9154 - val_loss: 9781.5596 - val_mae: 9782.2520\n",
      "Epoch 1115/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 111.9144 - mae: 112.5992 - val_loss: 9695.5020 - val_mae: 9696.1943\n",
      "Epoch 1116/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 134.7493 - mae: 135.4385 - val_loss: 9624.9971 - val_mae: 9625.6904\n",
      "Epoch 1117/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 108.9455 - mae: 109.6345 - val_loss: 9741.8545 - val_mae: 9742.5498\n",
      "Epoch 1118/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 113.8561 - mae: 114.5433 - val_loss: 9871.5068 - val_mae: 9872.2002\n",
      "Epoch 1119/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 121.0450 - mae: 121.7331 - val_loss: 9583.2764 - val_mae: 9583.9678\n",
      "Epoch 1120/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 114.1556 - mae: 114.8408 - val_loss: 9715.9619 - val_mae: 9716.6553\n",
      "Epoch 1121/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 114.1416 - mae: 114.8315 - val_loss: 9784.2568 - val_mae: 9784.9502\n",
      "Epoch 1122/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 115.1993 - mae: 115.8869 - val_loss: 9794.8027 - val_mae: 9795.4971\n",
      "Epoch 1123/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 107.6307 - mae: 108.3134 - val_loss: 9769.1143 - val_mae: 9769.8076\n",
      "Epoch 1124/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 117.2890 - mae: 117.9773 - val_loss: 9715.7236 - val_mae: 9716.4170\n",
      "Epoch 1125/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 112.2376 - mae: 112.9270 - val_loss: 9514.7168 - val_mae: 9515.4111\n",
      "Epoch 1126/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 114.2908 - mae: 114.9775 - val_loss: 9908.2881 - val_mae: 9908.9824\n",
      "Epoch 1127/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 116.2909 - mae: 116.9779 - val_loss: 9335.0244 - val_mae: 9335.7178\n",
      "Epoch 1128/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 126.3373 - mae: 127.0239 - val_loss: 10023.3945 - val_mae: 10024.0879\n",
      "Epoch 1129/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 148.7203 - mae: 149.4091 - val_loss: 9838.4873 - val_mae: 9839.1807\n",
      "Epoch 1130/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 107.7293 - mae: 108.4131 - val_loss: 9656.5000 - val_mae: 9657.1934\n",
      "Epoch 1131/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 124.4224 - mae: 125.1126 - val_loss: 9833.1035 - val_mae: 9833.7959\n",
      "Epoch 1132/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 123.6685 - mae: 124.3569 - val_loss: 9661.5449 - val_mae: 9662.2393\n",
      "Epoch 1133/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 122.1219 - mae: 122.8116 - val_loss: 9940.9102 - val_mae: 9941.6035\n",
      "Epoch 1134/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 118.3557 - mae: 119.0404 - val_loss: 9794.3467 - val_mae: 9795.0391\n",
      "Epoch 1135/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 107.9932 - mae: 108.6801 - val_loss: 9792.6064 - val_mae: 9793.2998\n",
      "Epoch 1136/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 106.4328 - mae: 107.1204 - val_loss: 9902.5176 - val_mae: 9903.2090\n",
      "Epoch 1137/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 111.6258 - mae: 112.3133 - val_loss: 9705.1162 - val_mae: 9705.8086\n",
      "Epoch 1138/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 118.0372 - mae: 118.7223 - val_loss: 9891.9453 - val_mae: 9892.6396\n",
      "Epoch 1139/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 101.8937 - mae: 102.5821 - val_loss: 9849.2520 - val_mae: 9849.9473\n",
      "Epoch 1140/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 122.1981 - mae: 122.8842 - val_loss: 9798.7158 - val_mae: 9799.4102\n",
      "Epoch 1141/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 104.8144 - mae: 105.5015 - val_loss: 9866.0117 - val_mae: 9866.7051\n",
      "Epoch 1142/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 122.0525 - mae: 122.7403 - val_loss: 9888.8311 - val_mae: 9889.5225\n",
      "Epoch 1143/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 109.4244 - mae: 110.1113 - val_loss: 9537.9883 - val_mae: 9538.6816\n",
      "Epoch 1144/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 128.5237 - mae: 129.2116 - val_loss: 9988.7646 - val_mae: 9989.4580\n",
      "Epoch 1145/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 123.8712 - mae: 124.5575 - val_loss: 9678.4092 - val_mae: 9679.1025\n",
      "Epoch 1146/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 124.9437 - mae: 125.6328 - val_loss: 9625.6475 - val_mae: 9626.3389\n",
      "Epoch 1147/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 116.0877 - mae: 116.7745 - val_loss: 9718.4844 - val_mae: 9719.1748\n",
      "Epoch 1148/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 113.1166 - mae: 113.8032 - val_loss: 9653.5693 - val_mae: 9654.2617\n",
      "Epoch 1149/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 105.0403 - mae: 105.7277 - val_loss: 9787.5127 - val_mae: 9788.2051\n",
      "Epoch 1150/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 109.0957 - mae: 109.7824 - val_loss: 9823.2627 - val_mae: 9823.9561\n",
      "Epoch 1151/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 116.2430 - mae: 116.9272 - val_loss: 10092.2344 - val_mae: 10092.9277\n",
      "Epoch 1152/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 114.7316 - mae: 115.4189 - val_loss: 9838.4893 - val_mae: 9839.1826\n",
      "Epoch 1153/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 140.8851 - mae: 141.5753 - val_loss: 9696.5215 - val_mae: 9697.2148\n",
      "Epoch 1154/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 142.8785 - mae: 143.5650 - val_loss: 9802.0254 - val_mae: 9802.7178\n",
      "Epoch 1155/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 116.3400 - mae: 117.0288 - val_loss: 9858.0723 - val_mae: 9858.7656\n",
      "Epoch 1156/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 99.1872 - mae: 99.8744 - val_loss: 9633.3125 - val_mae: 9634.0049\n",
      "Epoch 1157/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 111.6041 - mae: 112.2895 - val_loss: 9765.3652 - val_mae: 9766.0576\n",
      "Epoch 1158/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 125.2905 - mae: 125.9784 - val_loss: 9538.9609 - val_mae: 9539.6553\n",
      "Epoch 1159/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 124.8292 - mae: 125.5168 - val_loss: 9737.5127 - val_mae: 9738.2041\n",
      "Epoch 1160/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 112.5046 - mae: 113.1874 - val_loss: 9644.8809 - val_mae: 9645.5732\n",
      "Epoch 1161/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 110.9730 - mae: 111.6628 - val_loss: 9905.9883 - val_mae: 9906.6807\n",
      "Epoch 1162/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 105.3073 - mae: 105.9890 - val_loss: 9810.2422 - val_mae: 9810.9336\n",
      "Epoch 1163/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 125.6851 - mae: 126.3737 - val_loss: 9481.3623 - val_mae: 9482.0557\n",
      "Epoch 1164/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 110.3954 - mae: 111.0807 - val_loss: 9668.4229 - val_mae: 9669.1152\n",
      "Epoch 1165/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 126.3902 - mae: 127.0761 - val_loss: 10121.1055 - val_mae: 10121.7998\n",
      "Epoch 1166/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 125.3060 - mae: 125.9967 - val_loss: 9669.1357 - val_mae: 9669.8301\n",
      "Epoch 1167/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 109.7712 - mae: 110.4571 - val_loss: 9681.0391 - val_mae: 9681.7314\n",
      "Epoch 1168/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 117.1391 - mae: 117.8257 - val_loss: 9615.9775 - val_mae: 9616.6709\n",
      "Epoch 1169/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 106.5323 - mae: 107.2193 - val_loss: 9561.7666 - val_mae: 9562.4600\n",
      "Epoch 1170/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 127.0814 - mae: 127.7687 - val_loss: 9740.3281 - val_mae: 9741.0205\n",
      "Epoch 1171/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 123.3550 - mae: 124.0407 - val_loss: 9548.7373 - val_mae: 9549.4307\n",
      "Epoch 1172/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 122.2554 - mae: 122.9434 - val_loss: 9823.1543 - val_mae: 9823.8486\n",
      "Epoch 1173/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 120.6200 - mae: 121.3067 - val_loss: 9550.7793 - val_mae: 9551.4707\n",
      "Epoch 1174/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 110.8390 - mae: 111.5216 - val_loss: 9723.3828 - val_mae: 9724.0771\n",
      "Epoch 1175/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 113.4254 - mae: 114.1109 - val_loss: 9918.9668 - val_mae: 9919.6602\n",
      "Epoch 1176/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 111.6042 - mae: 112.2931 - val_loss: 9830.4648 - val_mae: 9831.1572\n",
      "Epoch 1177/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 110.9135 - mae: 111.6040 - val_loss: 9850.9824 - val_mae: 9851.6748\n",
      "Epoch 1178/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 99.5092 - mae: 100.1972 - val_loss: 9798.0850 - val_mae: 9798.7764\n",
      "Epoch 1179/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 114.9122 - mae: 115.5999 - val_loss: 9706.0107 - val_mae: 9706.7041\n",
      "Epoch 1180/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 106.5112 - mae: 107.1974 - val_loss: 9807.4834 - val_mae: 9808.1768\n",
      "Epoch 1181/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 121.8393 - mae: 122.5223 - val_loss: 9736.6572 - val_mae: 9737.3516\n",
      "Epoch 1182/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 105.5515 - mae: 106.2393 - val_loss: 9502.1699 - val_mae: 9502.8623\n",
      "Epoch 1183/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 133.0080 - mae: 133.6926 - val_loss: 9686.7695 - val_mae: 9687.4629\n",
      "Epoch 1184/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 101.6189 - mae: 102.3072 - val_loss: 9677.7178 - val_mae: 9678.4092\n",
      "Epoch 1185/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 121.0325 - mae: 121.7183 - val_loss: 9772.8018 - val_mae: 9773.4951\n",
      "Epoch 1186/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 117.2448 - mae: 117.9317 - val_loss: 9823.3877 - val_mae: 9824.0811\n",
      "Epoch 1187/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 112.9699 - mae: 113.6580 - val_loss: 9913.2314 - val_mae: 9913.9238\n",
      "Epoch 1188/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 134.5187 - mae: 135.2053 - val_loss: 9763.0518 - val_mae: 9763.7461\n",
      "Epoch 1189/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 123.3488 - mae: 124.0379 - val_loss: 9766.9971 - val_mae: 9767.6904\n",
      "Epoch 1190/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 112.6486 - mae: 113.3390 - val_loss: 9778.6387 - val_mae: 9779.3301\n",
      "Epoch 1191/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 110.6437 - mae: 111.3308 - val_loss: 9939.2119 - val_mae: 9939.9053\n",
      "Epoch 1192/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 120.2595 - mae: 120.9502 - val_loss: 9603.4102 - val_mae: 9604.1035\n",
      "Epoch 1193/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 114.3958 - mae: 115.0847 - val_loss: 9687.4404 - val_mae: 9688.1338\n",
      "Epoch 1194/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 106.9437 - mae: 107.6320 - val_loss: 9815.1670 - val_mae: 9815.8594\n",
      "Epoch 1195/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 119.3100 - mae: 119.9918 - val_loss: 9569.8564 - val_mae: 9570.5498\n",
      "Epoch 1196/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 118.2035 - mae: 118.8910 - val_loss: 9785.8740 - val_mae: 9786.5674\n",
      "Epoch 1197/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 102.7916 - mae: 103.4758 - val_loss: 9816.5059 - val_mae: 9817.1992\n",
      "Epoch 1198/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 105.1674 - mae: 105.8536 - val_loss: 9446.9326 - val_mae: 9447.6240\n",
      "Epoch 1199/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 139.7519 - mae: 140.4440 - val_loss: 9695.8652 - val_mae: 9696.5566\n",
      "Epoch 1200/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 129.8080 - mae: 130.4973 - val_loss: 9618.6045 - val_mae: 9619.2969\n",
      "Epoch 1201/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 108.1479 - mae: 108.8337 - val_loss: 9621.0752 - val_mae: 9621.7695\n",
      "Epoch 1202/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 119.4981 - mae: 120.1876 - val_loss: 9941.7666 - val_mae: 9942.4580\n",
      "Epoch 1203/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 110.0810 - mae: 110.7703 - val_loss: 9663.3594 - val_mae: 9664.0527\n",
      "Epoch 1204/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 135.6400 - mae: 136.3311 - val_loss: 9867.8691 - val_mae: 9868.5625\n",
      "Epoch 1205/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 118.2527 - mae: 118.9388 - val_loss: 9898.4473 - val_mae: 9899.1416\n",
      "Epoch 1206/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 116.2814 - mae: 116.9706 - val_loss: 10008.8369 - val_mae: 10009.5293\n",
      "Epoch 1207/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 106.8405 - mae: 107.5290 - val_loss: 9923.5010 - val_mae: 9924.1943\n",
      "Epoch 1208/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 114.3136 - mae: 115.0012 - val_loss: 9706.3135 - val_mae: 9707.0059\n",
      "Epoch 1209/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 136.2607 - mae: 136.9498 - val_loss: 9714.0420 - val_mae: 9714.7344\n",
      "Epoch 1210/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 110.2299 - mae: 110.9170 - val_loss: 9831.9189 - val_mae: 9832.6133\n",
      "Epoch 1211/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 121.3595 - mae: 122.0451 - val_loss: 9956.2529 - val_mae: 9956.9473\n",
      "Epoch 1212/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 120.6802 - mae: 121.3699 - val_loss: 9718.0869 - val_mae: 9718.7793\n",
      "Epoch 1213/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 107.7290 - mae: 108.4146 - val_loss: 9705.1963 - val_mae: 9705.8896\n",
      "Epoch 1214/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 119.5450 - mae: 120.2330 - val_loss: 9780.1572 - val_mae: 9780.8496\n",
      "Epoch 1215/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 129.2469 - mae: 129.9371 - val_loss: 10019.5742 - val_mae: 10020.2686\n",
      "Epoch 1216/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 132.0026 - mae: 132.6903 - val_loss: 9704.0928 - val_mae: 9704.7852\n",
      "Epoch 1217/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 110.7483 - mae: 111.4338 - val_loss: 9650.9424 - val_mae: 9651.6367\n",
      "Epoch 1218/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 114.3103 - mae: 114.9981 - val_loss: 10125.6641 - val_mae: 10126.3574\n",
      "Epoch 1219/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 123.5238 - mae: 124.2126 - val_loss: 9708.0000 - val_mae: 9708.6934\n",
      "Epoch 1220/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 109.6201 - mae: 110.3060 - val_loss: 9662.8105 - val_mae: 9663.5039\n",
      "Epoch 1221/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 104.2452 - mae: 104.9325 - val_loss: 9991.7383 - val_mae: 9992.4307\n",
      "Epoch 1222/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 122.0566 - mae: 122.7439 - val_loss: 9740.0684 - val_mae: 9740.7607\n",
      "Epoch 1223/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 98.6936 - mae: 99.3801 - val_loss: 9922.1680 - val_mae: 9922.8604\n",
      "Epoch 1224/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 115.1069 - mae: 115.7925 - val_loss: 9949.8604 - val_mae: 9950.5547\n",
      "Epoch 1225/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 115.3063 - mae: 115.9895 - val_loss: 9689.8799 - val_mae: 9690.5732\n",
      "Epoch 1226/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 103.2249 - mae: 103.9104 - val_loss: 9534.1064 - val_mae: 9534.7998\n",
      "Epoch 1227/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 100.0700 - mae: 100.7561 - val_loss: 9690.4053 - val_mae: 9691.0986\n",
      "Epoch 1228/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 104.4487 - mae: 105.1351 - val_loss: 9745.6709 - val_mae: 9746.3643\n",
      "Epoch 1229/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 104.2114 - mae: 104.8985 - val_loss: 9778.6455 - val_mae: 9779.3389\n",
      "Epoch 1230/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 110.3955 - mae: 111.0842 - val_loss: 9512.8350 - val_mae: 9513.5273\n",
      "Epoch 1231/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 117.7237 - mae: 118.4130 - val_loss: 9747.9170 - val_mae: 9748.6104\n",
      "Epoch 1232/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 116.3527 - mae: 117.0367 - val_loss: 9708.6514 - val_mae: 9709.3447\n",
      "Epoch 1233/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 123.9373 - mae: 124.6241 - val_loss: 9838.2119 - val_mae: 9838.9043\n",
      "Epoch 1234/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 113.8707 - mae: 114.5577 - val_loss: 9756.1338 - val_mae: 9756.8262\n",
      "Epoch 1235/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 122.8994 - mae: 123.5857 - val_loss: 9663.6748 - val_mae: 9664.3662\n",
      "Epoch 1236/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 115.9000 - mae: 116.5887 - val_loss: 9814.0957 - val_mae: 9814.7891\n",
      "Epoch 1237/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 111.9653 - mae: 112.6501 - val_loss: 9787.2822 - val_mae: 9787.9746\n",
      "Epoch 1238/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 120.7200 - mae: 121.4067 - val_loss: 9763.9912 - val_mae: 9764.6846\n",
      "Epoch 1239/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 116.1779 - mae: 116.8678 - val_loss: 9726.5176 - val_mae: 9727.2100\n",
      "Epoch 1240/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 104.0414 - mae: 104.7272 - val_loss: 9859.3564 - val_mae: 9860.0508\n",
      "Epoch 1241/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 95.5482 - mae: 96.2338 - val_loss: 9608.0586 - val_mae: 9608.7510\n",
      "Epoch 1242/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 111.4574 - mae: 112.1475 - val_loss: 9825.2861 - val_mae: 9825.9795\n",
      "Epoch 1243/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 101.0438 - mae: 101.7318 - val_loss: 9647.6846 - val_mae: 9648.3770\n",
      "Epoch 1244/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 114.6649 - mae: 115.3513 - val_loss: 9763.5342 - val_mae: 9764.2285\n",
      "Epoch 1245/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 101.0778 - mae: 101.7644 - val_loss: 9546.0674 - val_mae: 9546.7598\n",
      "Epoch 1246/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 103.6598 - mae: 104.3451 - val_loss: 9655.5146 - val_mae: 9656.2070\n",
      "Epoch 1247/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 131.2793 - mae: 131.9683 - val_loss: 9761.4346 - val_mae: 9762.1289\n",
      "Epoch 1248/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 111.8269 - mae: 112.5147 - val_loss: 9686.4287 - val_mae: 9687.1221\n",
      "Epoch 1249/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 108.0683 - mae: 108.7543 - val_loss: 9850.8135 - val_mae: 9851.5059\n",
      "Epoch 1250/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 113.2622 - mae: 113.9521 - val_loss: 9657.9414 - val_mae: 9658.6348\n",
      "Epoch 1251/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 121.0861 - mae: 121.7750 - val_loss: 9965.2344 - val_mae: 9965.9258\n",
      "Epoch 1252/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 114.2516 - mae: 114.9394 - val_loss: 9791.7666 - val_mae: 9792.4590\n",
      "Epoch 1253/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 112.2133 - mae: 112.9006 - val_loss: 9947.7178 - val_mae: 9948.4111\n",
      "Epoch 1254/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 108.4245 - mae: 109.1139 - val_loss: 9726.8438 - val_mae: 9727.5361\n",
      "Epoch 1255/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 107.1912 - mae: 107.8758 - val_loss: 9716.0908 - val_mae: 9716.7832\n",
      "Epoch 1256/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 109.2518 - mae: 109.9400 - val_loss: 9722.2002 - val_mae: 9722.8926\n",
      "Epoch 1257/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 98.9166 - mae: 99.6017 - val_loss: 9558.5947 - val_mae: 9559.2881\n",
      "Epoch 1258/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 114.8098 - mae: 115.4985 - val_loss: 9800.5303 - val_mae: 9801.2236\n",
      "Epoch 1259/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 104.3472 - mae: 105.0356 - val_loss: 9683.8418 - val_mae: 9684.5361\n",
      "Epoch 1260/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 117.3395 - mae: 118.0246 - val_loss: 9882.4531 - val_mae: 9883.1484\n",
      "Epoch 1261/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 126.0507 - mae: 126.7384 - val_loss: 9557.4893 - val_mae: 9558.1816\n",
      "Epoch 1262/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 107.6806 - mae: 108.3661 - val_loss: 9874.9443 - val_mae: 9875.6377\n",
      "Epoch 1263/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 120.0383 - mae: 120.7269 - val_loss: 9645.7861 - val_mae: 9646.4795\n",
      "Epoch 1264/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 104.0917 - mae: 104.7765 - val_loss: 9996.5859 - val_mae: 9997.2803\n",
      "Epoch 1265/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 116.5899 - mae: 117.2792 - val_loss: 9770.7354 - val_mae: 9771.4287\n",
      "Epoch 1266/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 118.4595 - mae: 119.1482 - val_loss: 9838.1982 - val_mae: 9838.8896\n",
      "Epoch 1267/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 105.7928 - mae: 106.4799 - val_loss: 9799.8379 - val_mae: 9800.5322\n",
      "Epoch 1268/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 118.5867 - mae: 119.2747 - val_loss: 9763.8115 - val_mae: 9764.5049\n",
      "Epoch 1269/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 108.3925 - mae: 109.0788 - val_loss: 9803.8799 - val_mae: 9804.5732\n",
      "Epoch 1270/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 107.4792 - mae: 108.1673 - val_loss: 9780.1973 - val_mae: 9780.8896\n",
      "Epoch 1271/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 104.1079 - mae: 104.7932 - val_loss: 9529.3496 - val_mae: 9530.0420\n",
      "Epoch 1272/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 114.8507 - mae: 115.5373 - val_loss: 9681.6650 - val_mae: 9682.3594\n",
      "Epoch 1273/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 98.7340 - mae: 99.4226 - val_loss: 9624.1201 - val_mae: 9624.8135\n",
      "Epoch 1274/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 100.9138 - mae: 101.6017 - val_loss: 9555.8789 - val_mae: 9556.5713\n",
      "Epoch 1275/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 135.1202 - mae: 135.8082 - val_loss: 9913.3691 - val_mae: 9914.0635\n",
      "Epoch 1276/5000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 103.6930 - mae: 104.3821 - val_loss: 9654.1680 - val_mae: 9654.8613\n",
      "Epoch 1277/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 124.1029 - mae: 124.7897 - val_loss: 9565.6279 - val_mae: 9566.3203\n",
      "Epoch 1278/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 114.3547 - mae: 115.0412 - val_loss: 9784.9121 - val_mae: 9785.6055\n",
      "Epoch 1279/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 109.9119 - mae: 110.6009 - val_loss: 9843.0908 - val_mae: 9843.7832\n",
      "Epoch 1280/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 120.9602 - mae: 121.6461 - val_loss: 9691.1816 - val_mae: 9691.8760\n",
      "Epoch 1281/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 119.5859 - mae: 120.2716 - val_loss: 9936.7852 - val_mae: 9937.4775\n",
      "Epoch 1282/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 113.5414 - mae: 114.2281 - val_loss: 9697.4424 - val_mae: 9698.1367\n",
      "Epoch 1283/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 101.0987 - mae: 101.7860 - val_loss: 9616.2412 - val_mae: 9616.9336\n",
      "Epoch 1284/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 109.6696 - mae: 110.3574 - val_loss: 9404.6885 - val_mae: 9405.3809\n",
      "Epoch 1285/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 126.7185 - mae: 127.4056 - val_loss: 9414.2568 - val_mae: 9414.9502\n",
      "Epoch 1286/5000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 137.6708 - mae: 138.3574 - val_loss: 9864.5273 - val_mae: 9865.2207\n",
      "Epoch 1287/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 102.6635 - mae: 103.3460 - val_loss: 9886.0586 - val_mae: 9886.7529\n",
      "Epoch 1288/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 101.3649 - mae: 102.0530 - val_loss: 9956.7539 - val_mae: 9957.4482\n",
      "Epoch 1289/5000\n",
      "46/46 [==============================] - 1s 30ms/step - loss: 112.5261 - mae: 113.2138 - val_loss: 9701.0400 - val_mae: 9701.7324\n",
      "Epoch 1290/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 124.3458 - mae: 125.0326 - val_loss: 9720.5049 - val_mae: 9721.1982\n",
      "Epoch 1291/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 120.5960 - mae: 121.2873 - val_loss: 9793.2900 - val_mae: 9793.9824\n",
      "Epoch 1292/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 117.2681 - mae: 117.9557 - val_loss: 9879.6025 - val_mae: 9880.2959\n",
      "Epoch 1293/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 113.5534 - mae: 114.2342 - val_loss: 9649.7656 - val_mae: 9650.4590\n",
      "Epoch 1294/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 112.9919 - mae: 113.6780 - val_loss: 9438.1484 - val_mae: 9438.8408\n",
      "Epoch 1295/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 117.0129 - mae: 117.7012 - val_loss: 9854.0742 - val_mae: 9854.7666\n",
      "Epoch 1296/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 101.6401 - mae: 102.3244 - val_loss: 10015.1846 - val_mae: 10015.8779\n",
      "Epoch 1297/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 115.6577 - mae: 116.3470 - val_loss: 9713.4453 - val_mae: 9714.1387\n",
      "Epoch 1298/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 99.7547 - mae: 100.4407 - val_loss: 9762.4160 - val_mae: 9763.1094\n",
      "Epoch 1299/5000\n",
      " 6/46 [==>...........................] - ETA: 1s - loss: 113.3837 - mae: 114.0736"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_66099/112544627.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "regressor.fit(X_train,Y_train,validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2dd3a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf8814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred=regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0f090b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 127us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9966428561796283"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for check\n",
    "y_train_pred=regressor.predict(X_train)\n",
    "r2_score(Y_train, y_train_pred) #training score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e757ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred = regressor.predict(y_val)\n",
    "r2=r2_score(y_test,y_pred) #validation score/ r^2\n",
    "print(f'r2:{r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ef1c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2_oos\n",
    "def r2_oos(ret, pred):\n",
    "    sum_of_sq_res = np.nansum(np.power((ret-pred), 2))\n",
    "    sum_of_sq_total = np.nansum(np.power(ret, 2))\n",
    "    \n",
    "    return 1-sum_of_sq_res/sum_of_sq_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8969a6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_oos = r2_oos(y_test, y_pred)\n",
    "print(f'r2_oos:{r2_oos}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b87143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9293.30533975775"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mae=mean_absolute_error(y_test,y_pred) #mae\n",
    "print(f'mae:{mae}')\n",
    "\n",
    "rmse=np.sqrt(mean_squared_error(Y_test,y_pred)) #rmse\n",
    "print(f'rmse:{rmse}')\n",
    "\n",
    "mape=mean_absolute_percentage_error(y_test,y_pred) #mape\n",
    "print(f'mape:{mape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb36caf",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5641115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546/546 [==============================] - 0s 83us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-06-01</th>\n",
       "      <td>33731.0</td>\n",
       "      <td>34394.320312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-02</th>\n",
       "      <td>33285.0</td>\n",
       "      <td>34062.855469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-03</th>\n",
       "      <td>34298.0</td>\n",
       "      <td>33482.195312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-04</th>\n",
       "      <td>35271.0</td>\n",
       "      <td>34105.074219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-05</th>\n",
       "      <td>34100.0</td>\n",
       "      <td>33992.710938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>24159.792969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>23844.636719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>23678.773438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>23188.109375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>22679.246094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>546 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             y_test        y_pred\n",
       "Date                             \n",
       "2021-06-01  33731.0  34394.320312\n",
       "2021-06-02  33285.0  34062.855469\n",
       "2021-06-03  34298.0  33482.195312\n",
       "2021-06-04  35271.0  34105.074219\n",
       "2021-06-05  34100.0  33992.710938\n",
       "...             ...           ...\n",
       "2022-11-24      NaN  24159.792969\n",
       "2022-11-25      NaN  23844.636719\n",
       "2022-11-26      NaN  23678.773438\n",
       "2022-11-27      NaN  23188.109375\n",
       "2022-11-28      NaN  22679.246094\n",
       "\n",
       "[546 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pre_df = pd.DataFrame(zip(Y_test,y_pred),columns=['y_test','y_pred'])\n",
    "pre_df.index = Y_test.index\n",
    "pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4bd2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df['pred_returns'] = pre_df['y_pred'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b94aa95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>pred_returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-06-01</th>\n",
       "      <td>33731.0</td>\n",
       "      <td>34394.320312</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-02</th>\n",
       "      <td>33285.0</td>\n",
       "      <td>34062.855469</td>\n",
       "      <td>-0.009637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-03</th>\n",
       "      <td>34298.0</td>\n",
       "      <td>33482.195312</td>\n",
       "      <td>-0.017047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-04</th>\n",
       "      <td>35271.0</td>\n",
       "      <td>34105.074219</td>\n",
       "      <td>0.018603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-05</th>\n",
       "      <td>34100.0</td>\n",
       "      <td>33992.710938</td>\n",
       "      <td>-0.003295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>24159.792969</td>\n",
       "      <td>-0.012123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>23844.636719</td>\n",
       "      <td>-0.013045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>23678.773438</td>\n",
       "      <td>-0.006956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>23188.109375</td>\n",
       "      <td>-0.020722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>22679.246094</td>\n",
       "      <td>-0.021945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>546 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             y_test        y_pred  pred_returns\n",
       "Date                                           \n",
       "2021-06-01  33731.0  34394.320312           NaN\n",
       "2021-06-02  33285.0  34062.855469     -0.009637\n",
       "2021-06-03  34298.0  33482.195312     -0.017047\n",
       "2021-06-04  35271.0  34105.074219      0.018603\n",
       "2021-06-05  34100.0  33992.710938     -0.003295\n",
       "...             ...           ...           ...\n",
       "2022-11-24      NaN  24159.792969     -0.012123\n",
       "2022-11-25      NaN  23844.636719     -0.013045\n",
       "2022-11-26      NaN  23678.773438     -0.006956\n",
       "2022-11-27      NaN  23188.109375     -0.020722\n",
       "2022-11-28      NaN  22679.246094     -0.021945\n",
       "\n",
       "[546 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9674c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(zip(Y_test,y_pred),columns=['y_test','y_pred']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732393ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df.to_csv(\"../result/ANN/btc_NN.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12ff04f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "0a952188e4bab49300a5758bda39ddc90e91f41f35dfe6ea820e496e515be371"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
