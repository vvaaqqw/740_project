{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5b918ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from datetime import datetime\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n",
    "from keras import losses\n",
    "from keras.constraints import maxnorm\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_validate\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from collections import Counter\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import randint\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, accuracy_score, f1_score, classification_report\n",
    "# import pickle\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, roc_auc_score\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ded3682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535a3fac",
   "metadata": {},
   "source": [
    "### Bitcoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1467391e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66099/2421463472.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  btc['Date'][i]  =  datetime.strptime(btc['Date'][i], '%Y/%m/%d')\n"
     ]
    }
   ],
   "source": [
    "regs = pd.read_csv(\"../Data/train_btc_selected_features.csv\")\n",
    "btc = pd.read_csv(\"../Data/btc_Data.csv\")\n",
    "for i in range(len(btc['Date'])):\n",
    "    btc['Date'][i]  =  datetime.strptime(btc['Date'][i], '%Y/%m/%d')\n",
    "\n",
    "btc = btc.set_index(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0d665a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "btcData = btc[regs.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8227b01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66099/751970969.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  btcData['returns'] = btcData['priceUSD'].pct_change()\n"
     ]
    }
   ],
   "source": [
    "btcData['returns'] = btcData['priceUSD'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0204bfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = btcData.drop(columns=['priceUSD'])\n",
    "Data = Data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8a926d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>difficulty14mom</th>\n",
       "      <th>difficulty30mom</th>\n",
       "      <th>difficulty3rsi</th>\n",
       "      <th>difficulty90mom</th>\n",
       "      <th>difficulty90sma</th>\n",
       "      <th>hashrate90std</th>\n",
       "      <th>hashrate90var</th>\n",
       "      <th>median_transaction_fee90rocUSD</th>\n",
       "      <th>median_transaction_feeUSD</th>\n",
       "      <th>mining_profitability</th>\n",
       "      <th>...</th>\n",
       "      <th>price90wmaUSD</th>\n",
       "      <th>sentinusd90emaUSD</th>\n",
       "      <th>sentinusdUSD</th>\n",
       "      <th>top100cap</th>\n",
       "      <th>top100cap14trx</th>\n",
       "      <th>top100cap30rsi</th>\n",
       "      <th>top100cap30trx</th>\n",
       "      <th>top100cap90rsi</th>\n",
       "      <th>top100cap90trx</th>\n",
       "      <th>returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-07-18</th>\n",
       "      <td>1.621380e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.737230e+02</td>\n",
       "      <td>2.055500e+01</td>\n",
       "      <td>8.384389e+08</td>\n",
       "      <td>1.757449e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>401834.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.612000e+03</td>\n",
       "      <td>25.782</td>\n",
       "      <td>0.139</td>\n",
       "      <td>71.191</td>\n",
       "      <td>0.308</td>\n",
       "      <td>79.756</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-19</th>\n",
       "      <td>1.621380e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.737230e+02</td>\n",
       "      <td>2.248600e+01</td>\n",
       "      <td>8.819952e+08</td>\n",
       "      <td>1.944789e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>481473.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.047000e+03</td>\n",
       "      <td>25.685</td>\n",
       "      <td>0.123</td>\n",
       "      <td>68.863</td>\n",
       "      <td>0.300</td>\n",
       "      <td>78.999</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.183196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-20</th>\n",
       "      <td>1.584060e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.735570e+02</td>\n",
       "      <td>2.441400e+01</td>\n",
       "      <td>9.280542e+08</td>\n",
       "      <td>2.153212e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>431831.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.341000e+03</td>\n",
       "      <td>25.602</td>\n",
       "      <td>0.107</td>\n",
       "      <td>66.923</td>\n",
       "      <td>0.292</td>\n",
       "      <td>78.355</td>\n",
       "      <td>0.544</td>\n",
       "      <td>-0.088475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-21</th>\n",
       "      <td>1.580420e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.700790e+02</td>\n",
       "      <td>2.630400e+01</td>\n",
       "      <td>9.761949e+08</td>\n",
       "      <td>2.382391e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>460783.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.122000e+03</td>\n",
       "      <td>25.638</td>\n",
       "      <td>0.092</td>\n",
       "      <td>67.334</td>\n",
       "      <td>0.284</td>\n",
       "      <td>78.431</td>\n",
       "      <td>0.541</td>\n",
       "      <td>-0.020434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-22</th>\n",
       "      <td>1.580420e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.700790e+02</td>\n",
       "      <td>2.819400e+01</td>\n",
       "      <td>1.019474e+09</td>\n",
       "      <td>2.598318e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>334641.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.570000e+03</td>\n",
       "      <td>25.604</td>\n",
       "      <td>0.078</td>\n",
       "      <td>66.526</td>\n",
       "      <td>0.275</td>\n",
       "      <td>78.166</td>\n",
       "      <td>0.537</td>\n",
       "      <td>-0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-24</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.384949e+13</td>\n",
       "      <td>4.612156e+19</td>\n",
       "      <td>5.317997e+38</td>\n",
       "      <td>165.380</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.0577</td>\n",
       "      <td>...</td>\n",
       "      <td>18820.0</td>\n",
       "      <td>28544682257</td>\n",
       "      <td>6.114577e+09</td>\n",
       "      <td>15.236</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>40.118</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>45.140</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.008075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-25</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.394503e+13</td>\n",
       "      <td>4.542600e+19</td>\n",
       "      <td>5.158804e+38</td>\n",
       "      <td>214.464</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.0541</td>\n",
       "      <td>...</td>\n",
       "      <td>18759.0</td>\n",
       "      <td>28076113671</td>\n",
       "      <td>7.224812e+09</td>\n",
       "      <td>15.231</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>39.880</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>45.047</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.006565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-26</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.404058e+13</td>\n",
       "      <td>4.624135e+19</td>\n",
       "      <td>5.345655e+38</td>\n",
       "      <td>255.004</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.0571</td>\n",
       "      <td>...</td>\n",
       "      <td>18701.0</td>\n",
       "      <td>27566714445</td>\n",
       "      <td>4.898449e+09</td>\n",
       "      <td>15.233</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>40.069</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>45.106</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.004001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-27</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.413612e+13</td>\n",
       "      <td>4.576227e+19</td>\n",
       "      <td>5.235464e+38</td>\n",
       "      <td>113.430</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.0618</td>\n",
       "      <td>...</td>\n",
       "      <td>18642.0</td>\n",
       "      <td>27053515674</td>\n",
       "      <td>4.216170e+09</td>\n",
       "      <td>15.259</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>41.833</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>45.656</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.423166e+13</td>\n",
       "      <td>4.552126e+19</td>\n",
       "      <td>5.180463e+38</td>\n",
       "      <td>104.294</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.0592</td>\n",
       "      <td>...</td>\n",
       "      <td>18578.0</td>\n",
       "      <td>26631320968</td>\n",
       "      <td>7.843657e+09</td>\n",
       "      <td>15.219</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>39.965</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>44.957</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.019828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4517 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            difficulty14mom  difficulty30mom  difficulty3rsi  difficulty90mom  \\\n",
       "Date                                                                            \n",
       "2010-07-18     1.621380e+02     1.641620e+02          100.00     1.737230e+02   \n",
       "2010-07-19     1.621380e+02     1.641620e+02          100.00     1.737230e+02   \n",
       "2010-07-20     1.584060e+02     1.641620e+02          100.00     1.735570e+02   \n",
       "2010-07-21     1.580420e+02     1.641620e+02          100.00     1.700790e+02   \n",
       "2010-07-22     1.580420e+02     1.641620e+02          100.00     1.700790e+02   \n",
       "...                     ...              ...             ...              ...   \n",
       "2022-11-24     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-25     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-26     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-27     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-28     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "\n",
       "            difficulty90sma  hashrate90std  hashrate90var  \\\n",
       "Date                                                        \n",
       "2010-07-18     2.055500e+01   8.384389e+08   1.757449e+17   \n",
       "2010-07-19     2.248600e+01   8.819952e+08   1.944789e+17   \n",
       "2010-07-20     2.441400e+01   9.280542e+08   2.153212e+17   \n",
       "2010-07-21     2.630400e+01   9.761949e+08   2.382391e+17   \n",
       "2010-07-22     2.819400e+01   1.019474e+09   2.598318e+17   \n",
       "...                     ...            ...            ...   \n",
       "2022-11-24     3.384949e+13   4.612156e+19   5.317997e+38   \n",
       "2022-11-25     3.394503e+13   4.542600e+19   5.158804e+38   \n",
       "2022-11-26     3.404058e+13   4.624135e+19   5.345655e+38   \n",
       "2022-11-27     3.413612e+13   4.576227e+19   5.235464e+38   \n",
       "2022-11-28     3.423166e+13   4.552126e+19   5.180463e+38   \n",
       "\n",
       "            median_transaction_fee90rocUSD  median_transaction_feeUSD  \\\n",
       "Date                                                                    \n",
       "2010-07-18                           0.000                      0.000   \n",
       "2010-07-19                           0.000                      0.000   \n",
       "2010-07-20                           0.000                      0.000   \n",
       "2010-07-21                           0.000                      0.000   \n",
       "2010-07-22                           0.000                      0.000   \n",
       "...                                    ...                        ...   \n",
       "2022-11-24                         165.380                      0.818   \n",
       "2022-11-25                         214.464                      0.812   \n",
       "2022-11-26                         255.004                      0.809   \n",
       "2022-11-27                         113.430                      0.619   \n",
       "2022-11-28                         104.294                      0.622   \n",
       "\n",
       "            mining_profitability  ...  price90wmaUSD  sentinusd90emaUSD  \\\n",
       "Date                              ...                                     \n",
       "2010-07-18           401834.0000  ...            0.0                  0   \n",
       "2010-07-19           481473.0000  ...            0.0                  0   \n",
       "2010-07-20           431831.0000  ...            0.0                  0   \n",
       "2010-07-21           460783.0000  ...            0.0                  0   \n",
       "2010-07-22           334641.0000  ...            0.0                  0   \n",
       "...                          ...  ...            ...                ...   \n",
       "2022-11-24                0.0577  ...        18820.0        28544682257   \n",
       "2022-11-25                0.0541  ...        18759.0        28076113671   \n",
       "2022-11-26                0.0571  ...        18701.0        27566714445   \n",
       "2022-11-27                0.0618  ...        18642.0        27053515674   \n",
       "2022-11-28                0.0592  ...        18578.0        26631320968   \n",
       "\n",
       "            sentinusdUSD  top100cap  top100cap14trx  top100cap30rsi  \\\n",
       "Date                                                                  \n",
       "2010-07-18  2.612000e+03     25.782           0.139          71.191   \n",
       "2010-07-19  4.047000e+03     25.685           0.123          68.863   \n",
       "2010-07-20  2.341000e+03     25.602           0.107          66.923   \n",
       "2010-07-21  2.122000e+03     25.638           0.092          67.334   \n",
       "2010-07-22  2.570000e+03     25.604           0.078          66.526   \n",
       "...                  ...        ...             ...             ...   \n",
       "2022-11-24  6.114577e+09     15.236          -0.080          40.118   \n",
       "2022-11-25  7.224812e+09     15.231          -0.075          39.880   \n",
       "2022-11-26  4.898449e+09     15.233          -0.070          40.069   \n",
       "2022-11-27  4.216170e+09     15.259          -0.065          41.833   \n",
       "2022-11-28  7.843657e+09     15.219          -0.061          39.965   \n",
       "\n",
       "            top100cap30trx  top100cap90rsi  top100cap90trx   returns  \n",
       "Date                                                                  \n",
       "2010-07-18           0.308          79.756           0.550  0.466667  \n",
       "2010-07-19           0.300          78.999           0.547  0.183196  \n",
       "2010-07-20           0.292          78.355           0.544 -0.088475  \n",
       "2010-07-21           0.284          78.431           0.541 -0.020434  \n",
       "2010-07-22           0.275          78.166           0.537 -0.153846  \n",
       "...                    ...             ...             ...       ...  \n",
       "2022-11-24          -0.042          45.140           0.001  0.008075  \n",
       "2022-11-25          -0.043          45.047           0.001 -0.006565  \n",
       "2022-11-26          -0.043          45.106           0.001  0.004001  \n",
       "2022-11-27          -0.044          45.656           0.000 -0.001147  \n",
       "2022-11-28          -0.044          44.957           0.000 -0.019828  \n",
       "\n",
       "[4517 rows x 32 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b5d4f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide X and Y\n",
    "X = Data.iloc[:,0:]\n",
    "#Y = Data['returns']   # 用returns的话就用这一行，然后把下一行comment掉\n",
    "Y = btcData['priceUSD'].shift(-30)[1:] # 反之亦然"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eb3275b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>difficulty14mom</th>\n",
       "      <th>difficulty30mom</th>\n",
       "      <th>difficulty3rsi</th>\n",
       "      <th>difficulty90mom</th>\n",
       "      <th>difficulty90sma</th>\n",
       "      <th>hashrate90std</th>\n",
       "      <th>hashrate90var</th>\n",
       "      <th>median_transaction_fee90rocUSD</th>\n",
       "      <th>median_transaction_feeUSD</th>\n",
       "      <th>mining_profitability</th>\n",
       "      <th>...</th>\n",
       "      <th>price90wmaUSD</th>\n",
       "      <th>sentinusd90emaUSD</th>\n",
       "      <th>sentinusdUSD</th>\n",
       "      <th>top100cap</th>\n",
       "      <th>top100cap14trx</th>\n",
       "      <th>top100cap30rsi</th>\n",
       "      <th>top100cap30trx</th>\n",
       "      <th>top100cap90rsi</th>\n",
       "      <th>top100cap90trx</th>\n",
       "      <th>returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-07-18</th>\n",
       "      <td>162.138</td>\n",
       "      <td>164.162</td>\n",
       "      <td>100.0</td>\n",
       "      <td>173.723</td>\n",
       "      <td>20.555</td>\n",
       "      <td>838438881.0</td>\n",
       "      <td>1.757449e+17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>401834.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2612.0</td>\n",
       "      <td>25.782</td>\n",
       "      <td>0.139</td>\n",
       "      <td>71.191</td>\n",
       "      <td>0.308</td>\n",
       "      <td>79.756</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-19</th>\n",
       "      <td>162.138</td>\n",
       "      <td>164.162</td>\n",
       "      <td>100.0</td>\n",
       "      <td>173.723</td>\n",
       "      <td>22.486</td>\n",
       "      <td>881995244.0</td>\n",
       "      <td>1.944789e+17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>481473.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4047.0</td>\n",
       "      <td>25.685</td>\n",
       "      <td>0.123</td>\n",
       "      <td>68.863</td>\n",
       "      <td>0.300</td>\n",
       "      <td>78.999</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.183196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-20</th>\n",
       "      <td>158.406</td>\n",
       "      <td>164.162</td>\n",
       "      <td>100.0</td>\n",
       "      <td>173.557</td>\n",
       "      <td>24.414</td>\n",
       "      <td>928054231.0</td>\n",
       "      <td>2.153212e+17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>431831.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2341.0</td>\n",
       "      <td>25.602</td>\n",
       "      <td>0.107</td>\n",
       "      <td>66.923</td>\n",
       "      <td>0.292</td>\n",
       "      <td>78.355</td>\n",
       "      <td>0.544</td>\n",
       "      <td>-0.088475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            difficulty14mom  difficulty30mom  difficulty3rsi  difficulty90mom  \\\n",
       "Date                                                                            \n",
       "2010-07-18          162.138          164.162           100.0          173.723   \n",
       "2010-07-19          162.138          164.162           100.0          173.723   \n",
       "2010-07-20          158.406          164.162           100.0          173.557   \n",
       "\n",
       "            difficulty90sma  hashrate90std  hashrate90var  \\\n",
       "Date                                                        \n",
       "2010-07-18           20.555    838438881.0   1.757449e+17   \n",
       "2010-07-19           22.486    881995244.0   1.944789e+17   \n",
       "2010-07-20           24.414    928054231.0   2.153212e+17   \n",
       "\n",
       "            median_transaction_fee90rocUSD  median_transaction_feeUSD  \\\n",
       "Date                                                                    \n",
       "2010-07-18                             0.0                        0.0   \n",
       "2010-07-19                             0.0                        0.0   \n",
       "2010-07-20                             0.0                        0.0   \n",
       "\n",
       "            mining_profitability  ...  price90wmaUSD  sentinusd90emaUSD  \\\n",
       "Date                              ...                                     \n",
       "2010-07-18              401834.0  ...            0.0                  0   \n",
       "2010-07-19              481473.0  ...            0.0                  0   \n",
       "2010-07-20              431831.0  ...            0.0                  0   \n",
       "\n",
       "            sentinusdUSD  top100cap  top100cap14trx  top100cap30rsi  \\\n",
       "Date                                                                  \n",
       "2010-07-18        2612.0     25.782           0.139          71.191   \n",
       "2010-07-19        4047.0     25.685           0.123          68.863   \n",
       "2010-07-20        2341.0     25.602           0.107          66.923   \n",
       "\n",
       "            top100cap30trx  top100cap90rsi  top100cap90trx   returns  \n",
       "Date                                                                  \n",
       "2010-07-18           0.308          79.756           0.550  0.466667  \n",
       "2010-07-19           0.300          78.999           0.547  0.183196  \n",
       "2010-07-20           0.292          78.355           0.544 -0.088475  \n",
       "\n",
       "[3 rows x 32 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bd589ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into three data sets\n",
    "X_train = X['2016-01-01':'2019-12-31']\n",
    "X_val = X['2020-01-01':'2021-05-31']\n",
    "X_test = X['2021-06-01':'2023-01-01']\n",
    "\n",
    "Y_train = Y['2016-01-01':'2019-12-31']\n",
    "Y_val = Y['2020-01-01':'2021-05-31']\n",
    "Y_test = Y['2021-06-01':'2023-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0e5e9fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............ (step 1 of 2) Processing mixmax, total=   0.0s\n",
      "[Pipeline] ............ (step 2 of 2) Processing robust, total=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;mixmax&#x27;, MinMaxScaler()), [&#x27;robust&#x27;, RobustScaler()]],\n",
       "         verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;mixmax&#x27;, MinMaxScaler()), [&#x27;robust&#x27;, RobustScaler()]],\n",
       "         verbose=True)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RobustScaler</label><div class=\"sk-toggleable__content\"><pre>RobustScaler()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('mixmax', MinMaxScaler()), ['robust', RobustScaler()]],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators=[]\n",
    "estimators.append(['mixmax',MinMaxScaler()])\n",
    "estimators.append(['robust',RobustScaler()])\n",
    "scale=Pipeline(estimators,verbose=True)\n",
    "scale.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c6fc0859",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=scale.transform(X_train)\n",
    "X_test=scale.transform(X_test)\n",
    "X_val = scale.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c8a84d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8f6916ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "# define neural network model\n",
    "shape=X.shape[1]\n",
    "def sequential_model(initializer='normal', activation='relu', neurons=300, NUM_FEATURES=shape):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_shape=(NUM_FEATURES,), kernel_initializer=initializer, activation=activation))\n",
    "    model.add(Dense(416, activation=activation))\n",
    "    model.add(Dense(32, activation=activation))\n",
    "    model.add(Dense(1, activation=activation))\n",
    "    # Compile model\n",
    "    adam=keras.optimizers.Adam(lr=lr_schedule(0), amsgrad=True)\n",
    "    #sgd=keras.optimizers.SGD(learning_rate=0.08, momentum=0.9, nesterov=False)\n",
    "    model.compile(loss='logcosh', optimizer=adam, metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72db04c9",
   "metadata": {},
   "source": [
    "val_loss is the value of cost function for your cross-validation data and loss is the value of cost function for your training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85653fe",
   "metadata": {},
   "source": [
    "stop training when val_loss is not increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b48b24b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# earlyStopping = EarlyStopping(monitor='val_loss', patience=100,verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "85174a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor=KerasRegressor(build_fn=sequential_model,epochs=5000,verbose=1, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a17646de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Epoch 1/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 4893.7255 - mae: 4894.4191 - val_loss: 20535.8086 - val_mae: 20536.5000\n",
      "Epoch 2/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 4055.1974 - mae: 4055.8900 - val_loss: 11409.8750 - val_mae: 11410.5664\n",
      "Epoch 3/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 2088.3460 - mae: 2089.0379 - val_loss: 6276.2676 - val_mae: 6276.9604\n",
      "Epoch 4/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1526.0194 - mae: 1526.7127 - val_loss: 6377.0200 - val_mae: 6377.7134\n",
      "Epoch 5/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 1409.3777 - mae: 1410.0705 - val_loss: 6541.8247 - val_mae: 6542.5181\n",
      "Epoch 6/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 1217.3922 - mae: 1218.0854 - val_loss: 6568.6631 - val_mae: 6569.3569\n",
      "Epoch 7/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1217.5789 - mae: 1218.2717 - val_loss: 6566.2144 - val_mae: 6566.9077\n",
      "Epoch 8/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1097.8337 - mae: 1098.5263 - val_loss: 6847.6392 - val_mae: 6848.3325\n",
      "Epoch 9/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 1041.2536 - mae: 1041.9461 - val_loss: 7184.8755 - val_mae: 7185.5688\n",
      "Epoch 10/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 998.1669 - mae: 998.8600 - val_loss: 6607.3467 - val_mae: 6608.0396\n",
      "Epoch 11/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 935.4486 - mae: 936.1400 - val_loss: 7184.8950 - val_mae: 7185.5869\n",
      "Epoch 12/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 918.8190 - mae: 919.5119 - val_loss: 6830.3169 - val_mae: 6831.0107\n",
      "Epoch 13/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 877.2813 - mae: 877.9735 - val_loss: 7466.0630 - val_mae: 7466.7563\n",
      "Epoch 14/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 841.2927 - mae: 841.9854 - val_loss: 7476.8179 - val_mae: 7477.5103\n",
      "Epoch 15/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 731.6226 - mae: 732.3154 - val_loss: 7593.4507 - val_mae: 7594.1440\n",
      "Epoch 16/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 750.7377 - mae: 751.4294 - val_loss: 7716.7090 - val_mae: 7717.4014\n",
      "Epoch 17/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 727.9609 - mae: 728.6526 - val_loss: 7307.1567 - val_mae: 7307.8496\n",
      "Epoch 18/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 729.0375 - mae: 729.7294 - val_loss: 7733.0078 - val_mae: 7733.7007\n",
      "Epoch 19/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 729.4671 - mae: 730.1592 - val_loss: 7561.3936 - val_mae: 7562.0864\n",
      "Epoch 20/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 709.8028 - mae: 710.4958 - val_loss: 7944.9829 - val_mae: 7945.6758\n",
      "Epoch 21/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 672.2637 - mae: 672.9557 - val_loss: 7583.0190 - val_mae: 7583.7124\n",
      "Epoch 22/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 633.7441 - mae: 634.4366 - val_loss: 7583.0752 - val_mae: 7583.7686\n",
      "Epoch 23/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 600.7603 - mae: 601.4530 - val_loss: 7927.7954 - val_mae: 7928.4883\n",
      "Epoch 24/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 655.4933 - mae: 656.1853 - val_loss: 7304.0962 - val_mae: 7304.7900\n",
      "Epoch 25/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 611.4230 - mae: 612.1155 - val_loss: 7488.5698 - val_mae: 7489.2632\n",
      "Epoch 26/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 604.3965 - mae: 605.0885 - val_loss: 7507.5020 - val_mae: 7508.1948\n",
      "Epoch 27/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 562.6448 - mae: 563.3360 - val_loss: 8026.3696 - val_mae: 8027.0610\n",
      "Epoch 28/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 600.1855 - mae: 600.8774 - val_loss: 7716.2378 - val_mae: 7716.9316\n",
      "Epoch 29/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 584.7066 - mae: 585.3991 - val_loss: 8029.8105 - val_mae: 8030.5034\n",
      "Epoch 30/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 543.6477 - mae: 544.3389 - val_loss: 7987.3760 - val_mae: 7988.0693\n",
      "Epoch 31/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 580.0633 - mae: 580.7548 - val_loss: 7934.2290 - val_mae: 7934.9224\n",
      "Epoch 32/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 525.6111 - mae: 526.3010 - val_loss: 7791.5786 - val_mae: 7792.2725\n",
      "Epoch 33/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 563.1484 - mae: 563.8401 - val_loss: 7935.3286 - val_mae: 7936.0220\n",
      "Epoch 34/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 579.8278 - mae: 580.5192 - val_loss: 7990.8447 - val_mae: 7991.5386\n",
      "Epoch 35/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 550.7082 - mae: 551.3991 - val_loss: 7585.1113 - val_mae: 7585.8042\n",
      "Epoch 36/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 530.8789 - mae: 531.5706 - val_loss: 7622.5952 - val_mae: 7623.2881\n",
      "Epoch 37/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 486.2204 - mae: 486.9123 - val_loss: 7668.3491 - val_mae: 7669.0420\n",
      "Epoch 38/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 553.6299 - mae: 554.3221 - val_loss: 8229.5928 - val_mae: 8230.2852\n",
      "Epoch 39/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 518.9129 - mae: 519.6046 - val_loss: 7763.1240 - val_mae: 7763.8159\n",
      "Epoch 40/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 468.3645 - mae: 469.0568 - val_loss: 8410.1084 - val_mae: 8410.7998\n",
      "Epoch 41/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 513.2800 - mae: 513.9706 - val_loss: 7941.4756 - val_mae: 7942.1689\n",
      "Epoch 42/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 480.7278 - mae: 481.4192 - val_loss: 7521.3784 - val_mae: 7522.0723\n",
      "Epoch 43/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 474.5852 - mae: 475.2752 - val_loss: 7755.7334 - val_mae: 7756.4272\n",
      "Epoch 44/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 491.5680 - mae: 492.2598 - val_loss: 8277.2627 - val_mae: 8277.9561\n",
      "Epoch 45/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 497.8759 - mae: 498.5683 - val_loss: 8045.7246 - val_mae: 8046.4175\n",
      "Epoch 46/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 463.7046 - mae: 464.3971 - val_loss: 8339.0176 - val_mae: 8339.7119\n",
      "Epoch 47/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 445.3553 - mae: 446.0468 - val_loss: 7855.1865 - val_mae: 7855.8799\n",
      "Epoch 48/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 479.9339 - mae: 480.6259 - val_loss: 7678.6416 - val_mae: 7679.3350\n",
      "Epoch 49/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 485.0723 - mae: 485.7626 - val_loss: 7558.4136 - val_mae: 7559.1060\n",
      "Epoch 50/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 520.1968 - mae: 520.8888 - val_loss: 8350.6328 - val_mae: 8351.3252\n",
      "Epoch 51/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 464.2313 - mae: 464.9222 - val_loss: 8313.2500 - val_mae: 8313.9443\n",
      "Epoch 52/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 443.1941 - mae: 443.8851 - val_loss: 8473.0283 - val_mae: 8473.7207\n",
      "Epoch 53/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 429.8134 - mae: 430.5049 - val_loss: 7874.5806 - val_mae: 7875.2749\n",
      "Epoch 54/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 481.4103 - mae: 482.1019 - val_loss: 8728.8799 - val_mae: 8729.5732\n",
      "Epoch 55/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 472.1032 - mae: 472.7947 - val_loss: 7827.8301 - val_mae: 7828.5229\n",
      "Epoch 56/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 459.8884 - mae: 460.5792 - val_loss: 7955.1045 - val_mae: 7955.7974\n",
      "Epoch 57/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 475.5250 - mae: 476.2174 - val_loss: 8313.7686 - val_mae: 8314.4609\n",
      "Epoch 58/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 447.3698 - mae: 448.0603 - val_loss: 8559.5215 - val_mae: 8560.2139\n",
      "Epoch 59/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 419.3085 - mae: 420.0002 - val_loss: 8368.8242 - val_mae: 8369.5176\n",
      "Epoch 60/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 441.6639 - mae: 442.3556 - val_loss: 8488.1934 - val_mae: 8488.8867\n",
      "Epoch 61/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 430.4743 - mae: 431.1646 - val_loss: 8648.1562 - val_mae: 8648.8496\n",
      "Epoch 62/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 399.1790 - mae: 399.8713 - val_loss: 8351.1045 - val_mae: 8351.7969\n",
      "Epoch 63/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 387.5239 - mae: 388.2147 - val_loss: 8086.6670 - val_mae: 8087.3604\n",
      "Epoch 64/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 402.5034 - mae: 403.1947 - val_loss: 7870.2231 - val_mae: 7870.9170\n",
      "Epoch 65/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 471.0468 - mae: 471.7373 - val_loss: 8621.4863 - val_mae: 8622.1787\n",
      "Epoch 66/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 412.7580 - mae: 413.4495 - val_loss: 8816.7754 - val_mae: 8817.4688\n",
      "Epoch 67/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 373.1014 - mae: 373.7923 - val_loss: 8985.4199 - val_mae: 8986.1143\n",
      "Epoch 68/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 378.6697 - mae: 379.3605 - val_loss: 8295.4697 - val_mae: 8296.1631\n",
      "Epoch 69/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 393.0212 - mae: 393.7135 - val_loss: 8821.0205 - val_mae: 8821.7139\n",
      "Epoch 70/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 377.0420 - mae: 377.7311 - val_loss: 9100.6377 - val_mae: 9101.3311\n",
      "Epoch 71/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 371.8936 - mae: 372.5856 - val_loss: 8370.4521 - val_mae: 8371.1465\n",
      "Epoch 72/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 375.4100 - mae: 376.0988 - val_loss: 8625.5859 - val_mae: 8626.2783\n",
      "Epoch 73/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 373.7685 - mae: 374.4600 - val_loss: 8842.4639 - val_mae: 8843.1582\n",
      "Epoch 74/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 397.5156 - mae: 398.2058 - val_loss: 8780.0723 - val_mae: 8780.7666\n",
      "Epoch 75/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 371.9199 - mae: 372.6091 - val_loss: 8633.1230 - val_mae: 8633.8174\n",
      "Epoch 76/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 351.8574 - mae: 352.5484 - val_loss: 8688.3584 - val_mae: 8689.0518\n",
      "Epoch 77/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 384.7711 - mae: 385.4622 - val_loss: 8476.4648 - val_mae: 8477.1572\n",
      "Epoch 78/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 385.0468 - mae: 385.7340 - val_loss: 8654.5967 - val_mae: 8655.2900\n",
      "Epoch 79/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 359.1831 - mae: 359.8738 - val_loss: 8921.9014 - val_mae: 8922.5938\n",
      "Epoch 80/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 361.1122 - mae: 361.8041 - val_loss: 8118.4644 - val_mae: 8119.1587\n",
      "Epoch 81/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 382.9942 - mae: 383.6851 - val_loss: 9156.5713 - val_mae: 9157.2656\n",
      "Epoch 82/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 382.5017 - mae: 383.1932 - val_loss: 9081.2617 - val_mae: 9081.9551\n",
      "Epoch 83/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 347.9791 - mae: 348.6708 - val_loss: 9308.1006 - val_mae: 9308.7930\n",
      "Epoch 84/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 407.2693 - mae: 407.9597 - val_loss: 9039.0439 - val_mae: 9039.7383\n",
      "Epoch 85/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 343.9382 - mae: 344.6292 - val_loss: 9048.5928 - val_mae: 9049.2852\n",
      "Epoch 86/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 385.7053 - mae: 386.3978 - val_loss: 8857.4004 - val_mae: 8858.0928\n",
      "Epoch 87/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 342.3508 - mae: 343.0423 - val_loss: 8934.6221 - val_mae: 8935.3154\n",
      "Epoch 88/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 351.6486 - mae: 352.3395 - val_loss: 8636.3105 - val_mae: 8637.0049\n",
      "Epoch 89/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 339.4807 - mae: 340.1729 - val_loss: 8381.9941 - val_mae: 8382.6875\n",
      "Epoch 90/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 405.2971 - mae: 405.9893 - val_loss: 9282.0674 - val_mae: 9282.7607\n",
      "Epoch 91/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 335.7736 - mae: 336.4647 - val_loss: 9197.6934 - val_mae: 9198.3867\n",
      "Epoch 92/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 360.4820 - mae: 361.1727 - val_loss: 9054.1699 - val_mae: 9054.8633\n",
      "Epoch 93/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 351.1356 - mae: 351.8268 - val_loss: 9109.6260 - val_mae: 9110.3193\n",
      "Epoch 94/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 323.7515 - mae: 324.4419 - val_loss: 9647.6143 - val_mae: 9648.3076\n",
      "Epoch 95/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 342.5365 - mae: 343.2277 - val_loss: 8826.3975 - val_mae: 8827.0898\n",
      "Epoch 96/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 330.5449 - mae: 331.2357 - val_loss: 9303.6504 - val_mae: 9304.3418\n",
      "Epoch 97/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 355.1388 - mae: 355.8281 - val_loss: 8958.2627 - val_mae: 8958.9561\n",
      "Epoch 98/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 326.5648 - mae: 327.2569 - val_loss: 9567.1895 - val_mae: 9567.8828\n",
      "Epoch 99/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 333.0646 - mae: 333.7559 - val_loss: 9392.9346 - val_mae: 9393.6270\n",
      "Epoch 100/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 338.0316 - mae: 338.7219 - val_loss: 9086.1592 - val_mae: 9086.8516\n",
      "Epoch 101/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 329.9654 - mae: 330.6562 - val_loss: 9254.2510 - val_mae: 9254.9434\n",
      "Epoch 102/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 318.2935 - mae: 318.9813 - val_loss: 9284.5664 - val_mae: 9285.2588\n",
      "Epoch 103/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 321.5183 - mae: 322.2083 - val_loss: 9382.5439 - val_mae: 9383.2383\n",
      "Epoch 104/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 323.2112 - mae: 323.9036 - val_loss: 8688.7266 - val_mae: 8689.4209\n",
      "Epoch 105/5000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 300.6338 - mae: 301.3229 - val_loss: 9400.2500 - val_mae: 9400.9443\n",
      "Epoch 106/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 332.3728 - mae: 333.0645 - val_loss: 9505.7871 - val_mae: 9506.4795\n",
      "Epoch 107/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 302.1103 - mae: 302.8018 - val_loss: 9243.0518 - val_mae: 9243.7451\n",
      "Epoch 108/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 309.1745 - mae: 309.8651 - val_loss: 9143.7949 - val_mae: 9144.4893\n",
      "Epoch 109/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 297.8897 - mae: 298.5804 - val_loss: 8965.1230 - val_mae: 8965.8164\n",
      "Epoch 110/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 331.4912 - mae: 332.1825 - val_loss: 8966.5088 - val_mae: 8967.2021\n",
      "Epoch 111/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 306.9937 - mae: 307.6836 - val_loss: 8861.6475 - val_mae: 8862.3389\n",
      "Epoch 112/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 302.2612 - mae: 302.9520 - val_loss: 9183.7598 - val_mae: 9184.4531\n",
      "Epoch 113/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 296.0269 - mae: 296.7140 - val_loss: 9296.4883 - val_mae: 9297.1826\n",
      "Epoch 114/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 267.2937 - mae: 267.9833 - val_loss: 9030.6758 - val_mae: 9031.3691\n",
      "Epoch 115/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 278.9055 - mae: 279.5952 - val_loss: 9247.6592 - val_mae: 9248.3516\n",
      "Epoch 116/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 293.0342 - mae: 293.7250 - val_loss: 9298.3398 - val_mae: 9299.0342\n",
      "Epoch 117/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 294.1970 - mae: 294.8881 - val_loss: 8724.2920 - val_mae: 8724.9863\n",
      "Epoch 118/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 318.0713 - mae: 318.7611 - val_loss: 9594.0752 - val_mae: 9594.7686\n",
      "Epoch 119/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 306.5765 - mae: 307.2675 - val_loss: 8885.2266 - val_mae: 8885.9199\n",
      "Epoch 120/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 288.7955 - mae: 289.4868 - val_loss: 9158.6484 - val_mae: 9159.3428\n",
      "Epoch 121/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 299.0601 - mae: 299.7485 - val_loss: 9139.2402 - val_mae: 9139.9326\n",
      "Epoch 122/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 281.8311 - mae: 282.5233 - val_loss: 9348.4121 - val_mae: 9349.1045\n",
      "Epoch 123/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 286.5962 - mae: 287.2863 - val_loss: 9485.3125 - val_mae: 9486.0049\n",
      "Epoch 124/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 305.2288 - mae: 305.9204 - val_loss: 8954.7012 - val_mae: 8955.3945\n",
      "Epoch 125/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 296.2805 - mae: 296.9696 - val_loss: 8804.9473 - val_mae: 8805.6406\n",
      "Epoch 126/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 300.2036 - mae: 300.8926 - val_loss: 9508.0986 - val_mae: 9508.7910\n",
      "Epoch 127/5000\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 282.6763 - mae: 283.3680 - val_loss: 8683.3789 - val_mae: 8684.0723\n",
      "Epoch 128/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 303.3560 - mae: 304.0462 - val_loss: 9527.5410 - val_mae: 9528.2334\n",
      "Epoch 129/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 309.3897 - mae: 310.0800 - val_loss: 9144.6064 - val_mae: 9145.3018\n",
      "Epoch 130/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 294.9503 - mae: 295.6395 - val_loss: 9374.2812 - val_mae: 9374.9746\n",
      "Epoch 131/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 274.9436 - mae: 275.6341 - val_loss: 8984.7764 - val_mae: 8985.4697\n",
      "Epoch 132/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 255.5208 - mae: 256.2098 - val_loss: 9307.1514 - val_mae: 9307.8457\n",
      "Epoch 133/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 267.2641 - mae: 267.9551 - val_loss: 9300.2910 - val_mae: 9300.9844\n",
      "Epoch 134/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 283.6121 - mae: 284.3026 - val_loss: 9244.3789 - val_mae: 9245.0713\n",
      "Epoch 135/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 287.3756 - mae: 288.0653 - val_loss: 9029.5156 - val_mae: 9030.2090\n",
      "Epoch 136/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 258.2987 - mae: 258.9884 - val_loss: 9135.1855 - val_mae: 9135.8789\n",
      "Epoch 137/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 284.0104 - mae: 284.7012 - val_loss: 9511.7627 - val_mae: 9512.4570\n",
      "Epoch 138/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 294.5975 - mae: 295.2878 - val_loss: 9143.6211 - val_mae: 9144.3154\n",
      "Epoch 139/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 269.7837 - mae: 270.4711 - val_loss: 8867.8223 - val_mae: 8868.5156\n",
      "Epoch 140/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 270.7868 - mae: 271.4755 - val_loss: 8637.6826 - val_mae: 8638.3750\n",
      "Epoch 141/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 279.0730 - mae: 279.7621 - val_loss: 9263.4521 - val_mae: 9264.1475\n",
      "Epoch 142/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 270.6361 - mae: 271.3252 - val_loss: 9212.9395 - val_mae: 9213.6309\n",
      "Epoch 143/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 245.7977 - mae: 246.4892 - val_loss: 9352.7480 - val_mae: 9353.4404\n",
      "Epoch 144/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 275.9004 - mae: 276.5926 - val_loss: 9053.7178 - val_mae: 9054.4111\n",
      "Epoch 145/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 279.8054 - mae: 280.4948 - val_loss: 9104.1338 - val_mae: 9104.8271\n",
      "Epoch 146/5000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 249.3156 - mae: 250.0025 - val_loss: 8965.9434 - val_mae: 8966.6357\n",
      "Epoch 147/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 251.8553 - mae: 252.5437 - val_loss: 9039.5342 - val_mae: 9040.2285\n",
      "Epoch 148/5000\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 256.6284 - mae: 257.3188 - val_loss: 8965.4277 - val_mae: 8966.1201\n",
      "Epoch 149/5000\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 259.3255 - mae: 260.0169 - val_loss: 9654.3223 - val_mae: 9655.0156\n",
      "Epoch 150/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 289.1470 - mae: 289.8394 - val_loss: 9017.5664 - val_mae: 9018.2578\n",
      "Epoch 151/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 262.8473 - mae: 263.5381 - val_loss: 8965.8682 - val_mae: 8966.5615\n",
      "Epoch 152/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 271.4801 - mae: 272.1701 - val_loss: 8614.5674 - val_mae: 8615.2607\n",
      "Epoch 153/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 293.4073 - mae: 294.0961 - val_loss: 9341.0762 - val_mae: 9341.7705\n",
      "Epoch 154/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 291.3427 - mae: 292.0341 - val_loss: 9015.1250 - val_mae: 9015.8184\n",
      "Epoch 155/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 264.4995 - mae: 265.1885 - val_loss: 9258.7744 - val_mae: 9259.4668\n",
      "Epoch 156/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 267.4614 - mae: 268.1529 - val_loss: 8956.1045 - val_mae: 8956.7979\n",
      "Epoch 157/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 278.3817 - mae: 279.0722 - val_loss: 9107.5459 - val_mae: 9108.2393\n",
      "Epoch 158/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 261.3022 - mae: 261.9943 - val_loss: 9126.8174 - val_mae: 9127.5098\n",
      "Epoch 159/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 237.6117 - mae: 238.3008 - val_loss: 9040.8770 - val_mae: 9041.5713\n",
      "Epoch 160/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 250.6668 - mae: 251.3564 - val_loss: 9298.3154 - val_mae: 9299.0078\n",
      "Epoch 161/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 256.3098 - mae: 257.0012 - val_loss: 9013.0283 - val_mae: 9013.7217\n",
      "Epoch 162/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 280.1720 - mae: 280.8628 - val_loss: 9017.3135 - val_mae: 9018.0068\n",
      "Epoch 163/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 284.1447 - mae: 284.8340 - val_loss: 9231.1699 - val_mae: 9231.8633\n",
      "Epoch 164/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 285.2214 - mae: 285.9126 - val_loss: 8682.2988 - val_mae: 8682.9922\n",
      "Epoch 165/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 283.3009 - mae: 283.9911 - val_loss: 8890.8535 - val_mae: 8891.5479\n",
      "Epoch 166/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 251.3196 - mae: 252.0101 - val_loss: 9518.8369 - val_mae: 9519.5293\n",
      "Epoch 167/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 282.1079 - mae: 282.7973 - val_loss: 8890.0430 - val_mae: 8890.7344\n",
      "Epoch 168/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 260.1729 - mae: 260.8645 - val_loss: 8739.2725 - val_mae: 8739.9648\n",
      "Epoch 169/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 261.4289 - mae: 262.1195 - val_loss: 8505.6758 - val_mae: 8506.3701\n",
      "Epoch 170/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 307.2999 - mae: 307.9901 - val_loss: 9135.3281 - val_mae: 9136.0225\n",
      "Epoch 171/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 258.7066 - mae: 259.3975 - val_loss: 9162.2139 - val_mae: 9162.9072\n",
      "Epoch 172/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 266.6946 - mae: 267.3843 - val_loss: 9381.7656 - val_mae: 9382.4590\n",
      "Epoch 173/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 270.5517 - mae: 271.2414 - val_loss: 9486.0332 - val_mae: 9486.7266\n",
      "Epoch 174/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 266.4636 - mae: 267.1533 - val_loss: 9381.4229 - val_mae: 9382.1162\n",
      "Epoch 175/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 241.4242 - mae: 242.1137 - val_loss: 9167.2773 - val_mae: 9167.9707\n",
      "Epoch 176/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 244.5667 - mae: 245.2581 - val_loss: 9284.6475 - val_mae: 9285.3408\n",
      "Epoch 177/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 225.5077 - mae: 226.1963 - val_loss: 9541.5605 - val_mae: 9542.2539\n",
      "Epoch 178/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 247.2091 - mae: 247.8985 - val_loss: 9071.0312 - val_mae: 9071.7246\n",
      "Epoch 179/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 241.3431 - mae: 242.0350 - val_loss: 9222.1270 - val_mae: 9222.8203\n",
      "Epoch 180/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 267.7738 - mae: 268.4631 - val_loss: 8893.9707 - val_mae: 8894.6631\n",
      "Epoch 181/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 250.9492 - mae: 251.6406 - val_loss: 9381.6631 - val_mae: 9382.3574\n",
      "Epoch 182/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 246.0764 - mae: 246.7676 - val_loss: 9357.3262 - val_mae: 9358.0195\n",
      "Epoch 183/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 250.2674 - mae: 250.9583 - val_loss: 8816.1807 - val_mae: 8816.8740\n",
      "Epoch 184/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 244.4060 - mae: 245.0945 - val_loss: 9101.2930 - val_mae: 9101.9863\n",
      "Epoch 185/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 237.4694 - mae: 238.1559 - val_loss: 8952.2197 - val_mae: 8952.9131\n",
      "Epoch 186/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 228.7340 - mae: 229.4236 - val_loss: 9202.1553 - val_mae: 9202.8496\n",
      "Epoch 187/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 254.2244 - mae: 254.9132 - val_loss: 9138.0732 - val_mae: 9138.7656\n",
      "Epoch 188/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 249.3187 - mae: 250.0102 - val_loss: 9510.8750 - val_mae: 9511.5674\n",
      "Epoch 189/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 249.3621 - mae: 250.0529 - val_loss: 8851.1475 - val_mae: 8851.8418\n",
      "Epoch 190/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 264.7327 - mae: 265.4233 - val_loss: 9421.4805 - val_mae: 9422.1729\n",
      "Epoch 191/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 244.4124 - mae: 245.1028 - val_loss: 9742.6914 - val_mae: 9743.3857\n",
      "Epoch 192/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 270.0575 - mae: 270.7476 - val_loss: 9733.8027 - val_mae: 9734.4961\n",
      "Epoch 193/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 246.5642 - mae: 247.2542 - val_loss: 9464.4111 - val_mae: 9465.1055\n",
      "Epoch 194/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 244.8997 - mae: 245.5904 - val_loss: 9245.3789 - val_mae: 9246.0723\n",
      "Epoch 195/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 256.3096 - mae: 256.9963 - val_loss: 8805.2227 - val_mae: 8805.9150\n",
      "Epoch 196/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 239.5507 - mae: 240.2401 - val_loss: 8770.1934 - val_mae: 8770.8867\n",
      "Epoch 197/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 247.5194 - mae: 248.2101 - val_loss: 8830.2285 - val_mae: 8830.9219\n",
      "Epoch 198/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 285.3137 - mae: 286.0039 - val_loss: 8793.1777 - val_mae: 8793.8711\n",
      "Epoch 199/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 224.0731 - mae: 224.7654 - val_loss: 9279.2646 - val_mae: 9279.9561\n",
      "Epoch 200/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 227.3538 - mae: 228.0429 - val_loss: 9355.4570 - val_mae: 9356.1514\n",
      "Epoch 201/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 240.0850 - mae: 240.7764 - val_loss: 9077.1914 - val_mae: 9077.8838\n",
      "Epoch 202/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 236.2740 - mae: 236.9658 - val_loss: 9061.6494 - val_mae: 9062.3428\n",
      "Epoch 203/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 262.6662 - mae: 263.3568 - val_loss: 9212.9902 - val_mae: 9213.6836\n",
      "Epoch 204/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 225.1710 - mae: 225.8610 - val_loss: 8830.9004 - val_mae: 8831.5938\n",
      "Epoch 205/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 244.0040 - mae: 244.6923 - val_loss: 9420.1738 - val_mae: 9420.8662\n",
      "Epoch 206/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 272.9155 - mae: 273.6062 - val_loss: 8748.4170 - val_mae: 8749.1094\n",
      "Epoch 207/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 259.6911 - mae: 260.3803 - val_loss: 9012.6494 - val_mae: 9013.3438\n",
      "Epoch 208/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 231.0760 - mae: 231.7665 - val_loss: 9202.0430 - val_mae: 9202.7373\n",
      "Epoch 209/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 238.4579 - mae: 239.1473 - val_loss: 9135.3213 - val_mae: 9136.0137\n",
      "Epoch 210/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 228.1326 - mae: 228.8226 - val_loss: 9016.7969 - val_mae: 9017.4912\n",
      "Epoch 211/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 249.1547 - mae: 249.8457 - val_loss: 8933.7275 - val_mae: 8934.4180\n",
      "Epoch 212/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 246.9636 - mae: 247.6539 - val_loss: 9123.8584 - val_mae: 9124.5518\n",
      "Epoch 213/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 274.5586 - mae: 275.2482 - val_loss: 8777.1104 - val_mae: 8777.8027\n",
      "Epoch 214/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 238.9249 - mae: 239.6119 - val_loss: 9226.9619 - val_mae: 9227.6553\n",
      "Epoch 215/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 243.1168 - mae: 243.8053 - val_loss: 9387.7129 - val_mae: 9388.4053\n",
      "Epoch 216/5000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 238.8342 - mae: 239.5238 - val_loss: 9168.2969 - val_mae: 9168.9902\n",
      "Epoch 217/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 230.0819 - mae: 230.7715 - val_loss: 9318.4639 - val_mae: 9319.1562\n",
      "Epoch 218/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 269.2664 - mae: 269.9562 - val_loss: 9434.4854 - val_mae: 9435.1787\n",
      "Epoch 219/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 244.9331 - mae: 245.6232 - val_loss: 9127.8008 - val_mae: 9128.4912\n",
      "Epoch 220/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 250.7889 - mae: 251.4787 - val_loss: 9385.7109 - val_mae: 9386.4053\n",
      "Epoch 221/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 231.1711 - mae: 231.8588 - val_loss: 8933.8955 - val_mae: 8934.5889\n",
      "Epoch 222/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 225.1787 - mae: 225.8670 - val_loss: 9232.5430 - val_mae: 9233.2373\n",
      "Epoch 223/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 253.7980 - mae: 254.4883 - val_loss: 9247.4102 - val_mae: 9248.1025\n",
      "Epoch 224/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 224.9803 - mae: 225.6683 - val_loss: 8972.7119 - val_mae: 8973.4053\n",
      "Epoch 225/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 229.3407 - mae: 230.0282 - val_loss: 9254.3252 - val_mae: 9255.0176\n",
      "Epoch 226/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 234.8669 - mae: 235.5570 - val_loss: 9004.7500 - val_mae: 9005.4443\n",
      "Epoch 227/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 227.8649 - mae: 228.5560 - val_loss: 9562.5674 - val_mae: 9563.2598\n",
      "Epoch 228/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 250.1343 - mae: 250.8249 - val_loss: 9120.5869 - val_mae: 9121.2793\n",
      "Epoch 229/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 233.9365 - mae: 234.6252 - val_loss: 9074.4736 - val_mae: 9075.1670\n",
      "Epoch 230/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 220.6592 - mae: 221.3459 - val_loss: 9242.1328 - val_mae: 9242.8262\n",
      "Epoch 231/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 240.6600 - mae: 241.3476 - val_loss: 9185.1807 - val_mae: 9185.8740\n",
      "Epoch 232/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 236.9229 - mae: 237.6107 - val_loss: 9289.6719 - val_mae: 9290.3652\n",
      "Epoch 233/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 252.2569 - mae: 252.9470 - val_loss: 8863.7588 - val_mae: 8864.4541\n",
      "Epoch 234/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 251.8988 - mae: 252.5891 - val_loss: 9395.0039 - val_mae: 9395.6973\n",
      "Epoch 235/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 218.9386 - mae: 219.6283 - val_loss: 9138.4795 - val_mae: 9139.1738\n",
      "Epoch 236/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 235.1848 - mae: 235.8745 - val_loss: 9202.5283 - val_mae: 9203.2217\n",
      "Epoch 237/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 240.8785 - mae: 241.5679 - val_loss: 9215.1865 - val_mae: 9215.8809\n",
      "Epoch 238/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 238.4702 - mae: 239.1592 - val_loss: 9220.0498 - val_mae: 9220.7441\n",
      "Epoch 239/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 216.1031 - mae: 216.7918 - val_loss: 9077.8291 - val_mae: 9078.5225\n",
      "Epoch 240/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 219.0093 - mae: 219.6995 - val_loss: 9268.1240 - val_mae: 9268.8164\n",
      "Epoch 241/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 231.0424 - mae: 231.7293 - val_loss: 9007.4111 - val_mae: 9008.1045\n",
      "Epoch 242/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 252.5777 - mae: 253.2640 - val_loss: 9410.0234 - val_mae: 9410.7168\n",
      "Epoch 243/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 226.7151 - mae: 227.4059 - val_loss: 9696.5947 - val_mae: 9697.2891\n",
      "Epoch 244/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 231.0503 - mae: 231.7413 - val_loss: 9156.3408 - val_mae: 9157.0352\n",
      "Epoch 245/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 228.4227 - mae: 229.1126 - val_loss: 9176.4775 - val_mae: 9177.1709\n",
      "Epoch 246/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 233.0764 - mae: 233.7652 - val_loss: 9099.7754 - val_mae: 9100.4697\n",
      "Epoch 247/5000\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 202.8145 - mae: 203.5033 - val_loss: 9358.0117 - val_mae: 9358.7041\n",
      "Epoch 248/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 226.5680 - mae: 227.2590 - val_loss: 9355.6416 - val_mae: 9356.3359\n",
      "Epoch 249/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 228.2403 - mae: 228.9302 - val_loss: 9330.1709 - val_mae: 9330.8633\n",
      "Epoch 250/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 222.9056 - mae: 223.5952 - val_loss: 8988.6504 - val_mae: 8989.3438\n",
      "Epoch 251/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 228.9049 - mae: 229.5924 - val_loss: 9292.6885 - val_mae: 9293.3818\n",
      "Epoch 252/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 224.1146 - mae: 224.8057 - val_loss: 9172.2930 - val_mae: 9172.9854\n",
      "Epoch 253/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 243.0473 - mae: 243.7368 - val_loss: 9178.5986 - val_mae: 9179.2920\n",
      "Epoch 254/5000\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 228.5260 - mae: 229.2162 - val_loss: 9303.0840 - val_mae: 9303.7754\n",
      "Epoch 255/5000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 277.4524 - mae: 278.1420 - val_loss: 9048.2900 - val_mae: 9048.9834\n",
      "Epoch 256/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 232.7656 - mae: 233.4526 - val_loss: 9307.5537 - val_mae: 9308.2480\n",
      "Epoch 257/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 220.2323 - mae: 220.9209 - val_loss: 9345.9346 - val_mae: 9346.6279\n",
      "Epoch 258/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 221.4841 - mae: 222.1736 - val_loss: 9517.7080 - val_mae: 9518.4014\n",
      "Epoch 259/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 240.4816 - mae: 241.1723 - val_loss: 9220.7715 - val_mae: 9221.4629\n",
      "Epoch 260/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 217.7099 - mae: 218.4003 - val_loss: 9084.0000 - val_mae: 9084.6934\n",
      "Epoch 261/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 228.3010 - mae: 228.9920 - val_loss: 9401.4990 - val_mae: 9402.1904\n",
      "Epoch 262/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 238.5038 - mae: 239.1952 - val_loss: 9426.0957 - val_mae: 9426.7881\n",
      "Epoch 263/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 217.7933 - mae: 218.4820 - val_loss: 9466.7686 - val_mae: 9467.4619\n",
      "Epoch 264/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 206.9182 - mae: 207.6078 - val_loss: 9207.5410 - val_mae: 9208.2354\n",
      "Epoch 265/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 217.4648 - mae: 218.1544 - val_loss: 9057.0557 - val_mae: 9057.7500\n",
      "Epoch 266/5000\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 228.3141 - mae: 229.0045 - val_loss: 8946.7510 - val_mae: 8947.4443\n",
      "Epoch 267/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 228.9138 - mae: 229.6019 - val_loss: 9320.1553 - val_mae: 9320.8496\n",
      "Epoch 268/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 244.8057 - mae: 245.4971 - val_loss: 8985.7363 - val_mae: 8986.4297\n",
      "Epoch 269/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 209.5744 - mae: 210.2635 - val_loss: 9354.0244 - val_mae: 9354.7178\n",
      "Epoch 270/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 215.5021 - mae: 216.1923 - val_loss: 9057.5479 - val_mae: 9058.2412\n",
      "Epoch 271/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 231.8525 - mae: 232.5437 - val_loss: 9116.1953 - val_mae: 9116.8877\n",
      "Epoch 272/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 226.2267 - mae: 226.9165 - val_loss: 9054.3418 - val_mae: 9055.0342\n",
      "Epoch 273/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 244.8542 - mae: 245.5421 - val_loss: 9451.7861 - val_mae: 9452.4814\n",
      "Epoch 274/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 255.8105 - mae: 256.5015 - val_loss: 9355.8965 - val_mae: 9356.5898\n",
      "Epoch 275/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 214.7192 - mae: 215.4067 - val_loss: 9281.2070 - val_mae: 9281.9014\n",
      "Epoch 276/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 223.7661 - mae: 224.4550 - val_loss: 9384.5010 - val_mae: 9385.1953\n",
      "Epoch 277/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 229.1519 - mae: 229.8413 - val_loss: 8995.5820 - val_mae: 8996.2734\n",
      "Epoch 278/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 205.3300 - mae: 206.0168 - val_loss: 9225.6387 - val_mae: 9226.3311\n",
      "Epoch 279/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 207.0896 - mae: 207.7806 - val_loss: 9581.0244 - val_mae: 9581.7158\n",
      "Epoch 280/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 227.2802 - mae: 227.9712 - val_loss: 9107.1719 - val_mae: 9107.8652\n",
      "Epoch 281/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 248.3071 - mae: 248.9985 - val_loss: 8770.9199 - val_mae: 8771.6123\n",
      "Epoch 282/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 213.7058 - mae: 214.3938 - val_loss: 9134.3516 - val_mae: 9135.0459\n",
      "Epoch 283/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 210.3160 - mae: 211.0053 - val_loss: 9310.5137 - val_mae: 9311.2061\n",
      "Epoch 284/5000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 226.3133 - mae: 227.0036 - val_loss: 9112.6025 - val_mae: 9113.2959\n",
      "Epoch 285/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 208.5288 - mae: 209.2164 - val_loss: 8980.8467 - val_mae: 8981.5391\n",
      "Epoch 286/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 221.0535 - mae: 221.7439 - val_loss: 8908.6875 - val_mae: 8909.3799\n",
      "Epoch 287/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 215.2853 - mae: 215.9742 - val_loss: 8704.1045 - val_mae: 8704.7979\n",
      "Epoch 288/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 228.7878 - mae: 229.4789 - val_loss: 8860.8555 - val_mae: 8861.5488\n",
      "Epoch 289/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 238.6939 - mae: 239.3849 - val_loss: 9285.0049 - val_mae: 9285.6973\n",
      "Epoch 290/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 201.7440 - mae: 202.4289 - val_loss: 9172.2324 - val_mae: 9172.9258\n",
      "Epoch 291/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 210.2414 - mae: 210.9290 - val_loss: 9460.8906 - val_mae: 9461.5830\n",
      "Epoch 292/5000\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 207.1220 - mae: 207.8108 - val_loss: 8787.9062 - val_mae: 8788.6006\n",
      "Epoch 293/5000\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 246.3568 - mae: 247.0454 - val_loss: 9179.2715 - val_mae: 9179.9658\n",
      "Epoch 294/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 207.4739 - mae: 208.1625 - val_loss: 8899.1211 - val_mae: 8899.8145\n",
      "Epoch 295/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 223.9023 - mae: 224.5918 - val_loss: 9221.6279 - val_mae: 9222.3203\n",
      "Epoch 296/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 224.0593 - mae: 224.7498 - val_loss: 8929.3701 - val_mae: 8930.0645\n",
      "Epoch 297/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 208.7141 - mae: 209.4045 - val_loss: 9531.5186 - val_mae: 9532.2119\n",
      "Epoch 298/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 206.4404 - mae: 207.1258 - val_loss: 9009.0664 - val_mae: 9009.7607\n",
      "Epoch 299/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 214.3890 - mae: 215.0775 - val_loss: 8929.0840 - val_mae: 8929.7773\n",
      "Epoch 300/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 204.7982 - mae: 205.4871 - val_loss: 8990.1572 - val_mae: 8990.8516\n",
      "Epoch 301/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 221.3791 - mae: 222.0678 - val_loss: 8957.6104 - val_mae: 8958.3047\n",
      "Epoch 302/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 214.0231 - mae: 214.7139 - val_loss: 9039.8486 - val_mae: 9040.5430\n",
      "Epoch 303/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 226.0938 - mae: 226.7838 - val_loss: 9135.3174 - val_mae: 9136.0088\n",
      "Epoch 304/5000\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 201.9075 - mae: 202.5950 - val_loss: 9233.7578 - val_mae: 9234.4512\n",
      "Epoch 305/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 207.0669 - mae: 207.7562 - val_loss: 9359.2920 - val_mae: 9359.9863\n",
      "Epoch 306/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 216.8652 - mae: 217.5517 - val_loss: 8997.0938 - val_mae: 8997.7881\n",
      "Epoch 307/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 226.1199 - mae: 226.8097 - val_loss: 9215.8125 - val_mae: 9216.5059\n",
      "Epoch 308/5000\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 202.9760 - mae: 203.6634 - val_loss: 9154.9912 - val_mae: 9155.6855\n",
      "Epoch 309/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 210.6509 - mae: 211.3380 - val_loss: 8516.7568 - val_mae: 8517.4502\n",
      "Epoch 310/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 231.6640 - mae: 232.3519 - val_loss: 9217.4229 - val_mae: 9218.1162\n",
      "Epoch 311/5000\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 219.6737 - mae: 220.3628 - val_loss: 9167.8584 - val_mae: 9168.5518\n",
      "Epoch 312/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 202.1704 - mae: 202.8613 - val_loss: 9050.4404 - val_mae: 9051.1338\n",
      "Epoch 313/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 209.2293 - mae: 209.9186 - val_loss: 9561.1934 - val_mae: 9561.8867\n",
      "Epoch 314/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 217.9093 - mae: 218.5999 - val_loss: 9474.3613 - val_mae: 9475.0557\n",
      "Epoch 315/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 218.5980 - mae: 219.2866 - val_loss: 9013.2480 - val_mae: 9013.9404\n",
      "Epoch 316/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 197.2458 - mae: 197.9352 - val_loss: 8922.6768 - val_mae: 8923.3691\n",
      "Epoch 317/5000\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 198.1803 - mae: 198.8714 - val_loss: 9382.8398 - val_mae: 9383.5332\n",
      "Epoch 318/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 230.8383 - mae: 231.5227 - val_loss: 8667.6367 - val_mae: 8668.3301\n",
      "Epoch 319/5000\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 228.8263 - mae: 229.5091 - val_loss: 8831.5459 - val_mae: 8832.2393\n",
      "Epoch 320/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 210.9594 - mae: 211.6490 - val_loss: 8685.5850 - val_mae: 8686.2803\n",
      "Epoch 321/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 283.3036 - mae: 283.9936 - val_loss: 9333.0820 - val_mae: 9333.7754\n",
      "Epoch 322/5000\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 206.4253 - mae: 207.1125 - val_loss: 9133.2988 - val_mae: 9133.9941\n",
      "Epoch 323/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 193.9363 - mae: 194.6276 - val_loss: 8987.3477 - val_mae: 8988.0410\n",
      "Epoch 324/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 198.2191 - mae: 198.9084 - val_loss: 9525.6406 - val_mae: 9526.3340\n",
      "Epoch 325/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 209.7736 - mae: 210.4644 - val_loss: 8828.9980 - val_mae: 8829.6924\n",
      "Epoch 326/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 182.5634 - mae: 183.2522 - val_loss: 9339.3076 - val_mae: 9340.0010\n",
      "Epoch 327/5000\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 217.1100 - mae: 217.7990 - val_loss: 9313.4736 - val_mae: 9314.1650\n",
      "Epoch 328/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 206.3915 - mae: 207.0779 - val_loss: 8960.4434 - val_mae: 8961.1367\n",
      "Epoch 329/5000\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 205.9088 - mae: 206.5969 - val_loss: 9068.8740 - val_mae: 9069.5684\n",
      "Epoch 330/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 203.9927 - mae: 204.6831 - val_loss: 8995.7061 - val_mae: 8996.3994\n",
      "Epoch 331/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 215.6940 - mae: 216.3836 - val_loss: 9040.8057 - val_mae: 9041.5000\n",
      "Epoch 332/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 216.7803 - mae: 217.4706 - val_loss: 8969.8906 - val_mae: 8970.5830\n",
      "Epoch 333/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 182.2763 - mae: 182.9618 - val_loss: 9145.6543 - val_mae: 9146.3477\n",
      "Epoch 334/5000\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 205.6473 - mae: 206.3381 - val_loss: 9049.4697 - val_mae: 9050.1621\n",
      "Epoch 335/5000\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 211.0727 - mae: 211.7634 - val_loss: 8834.7520 - val_mae: 8835.4473\n",
      "Epoch 336/5000\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 207.8075 - mae: 208.4971 - val_loss: 8547.8701 - val_mae: 8548.5625\n",
      "Epoch 337/5000\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 209.9031 - mae: 210.5928 - val_loss: 9110.2822 - val_mae: 9110.9727\n",
      "Epoch 338/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 211.6123 - mae: 212.3000 - val_loss: 9282.2334 - val_mae: 9282.9268\n",
      "Epoch 339/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 211.4499 - mae: 212.1370 - val_loss: 9025.6113 - val_mae: 9026.3047\n",
      "Epoch 340/5000\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 192.0227 - mae: 192.7129 - val_loss: 9215.5967 - val_mae: 9216.2891\n",
      "Epoch 341/5000\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 204.3115 - mae: 204.9993 - val_loss: 9513.3652 - val_mae: 9514.0576\n",
      "Epoch 342/5000\n",
      "12/46 [======>.......................] - ETA: 0s - loss: 188.1850 - mae: 188.8753"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_66099/112544627.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "regressor.fit(X_train,Y_train,validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2dd3a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf8814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred=regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0f090b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 127us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9966428561796283"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for check\n",
    "y_train_pred=regressor.predict(X_train)\n",
    "r2_score(Y_train, y_train_pred) #training score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e757ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2=r2_score(y_test,y_pred) #score/ r^2\n",
    "print(f'r2:{r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ef1c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2_oos\n",
    "def r2_oos(ret, pred):\n",
    "    sum_of_sq_res = np.nansum(np.power((ret-pred), 2))\n",
    "    sum_of_sq_total = np.nansum(np.power(ret, 2))\n",
    "    \n",
    "    return 1-sum_of_sq_res/sum_of_sq_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8969a6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_oos = r2_oos(y_test, y_pred)\n",
    "print(f'r2_oos:{r2_oos}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b87143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9293.30533975775"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mae=mean_absolute_error(y_test,y_pred) #mae\n",
    "print(f'mae:{mae}')\n",
    "\n",
    "rmse=np.sqrt(mean_squared_error(Y_test,y_pred)) #rmse\n",
    "print(f'rmse:{rmse}')\n",
    "\n",
    "mape=mean_absolute_percentage_error(y_test,y_pred) #mape\n",
    "print(f'mape:{mape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb36caf",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5641115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546/546 [==============================] - 0s 83us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-06-01</th>\n",
       "      <td>33731.0</td>\n",
       "      <td>34394.320312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-02</th>\n",
       "      <td>33285.0</td>\n",
       "      <td>34062.855469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-03</th>\n",
       "      <td>34298.0</td>\n",
       "      <td>33482.195312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-04</th>\n",
       "      <td>35271.0</td>\n",
       "      <td>34105.074219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-05</th>\n",
       "      <td>34100.0</td>\n",
       "      <td>33992.710938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>24159.792969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>23844.636719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>23678.773438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>23188.109375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>22679.246094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>546 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             y_test        y_pred\n",
       "Date                             \n",
       "2021-06-01  33731.0  34394.320312\n",
       "2021-06-02  33285.0  34062.855469\n",
       "2021-06-03  34298.0  33482.195312\n",
       "2021-06-04  35271.0  34105.074219\n",
       "2021-06-05  34100.0  33992.710938\n",
       "...             ...           ...\n",
       "2022-11-24      NaN  24159.792969\n",
       "2022-11-25      NaN  23844.636719\n",
       "2022-11-26      NaN  23678.773438\n",
       "2022-11-27      NaN  23188.109375\n",
       "2022-11-28      NaN  22679.246094\n",
       "\n",
       "[546 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pre_df = pd.DataFrame(zip(Y_test,y_pred),columns=['y_test','y_pred'])\n",
    "pre_df.index = Y_test.index\n",
    "pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4bd2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df['pred_returns'] = pre_df['y_pred'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b94aa95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>pred_returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-06-01</th>\n",
       "      <td>33731.0</td>\n",
       "      <td>34394.320312</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-02</th>\n",
       "      <td>33285.0</td>\n",
       "      <td>34062.855469</td>\n",
       "      <td>-0.009637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-03</th>\n",
       "      <td>34298.0</td>\n",
       "      <td>33482.195312</td>\n",
       "      <td>-0.017047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-04</th>\n",
       "      <td>35271.0</td>\n",
       "      <td>34105.074219</td>\n",
       "      <td>0.018603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-05</th>\n",
       "      <td>34100.0</td>\n",
       "      <td>33992.710938</td>\n",
       "      <td>-0.003295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>24159.792969</td>\n",
       "      <td>-0.012123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>23844.636719</td>\n",
       "      <td>-0.013045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>23678.773438</td>\n",
       "      <td>-0.006956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>23188.109375</td>\n",
       "      <td>-0.020722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>22679.246094</td>\n",
       "      <td>-0.021945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>546 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             y_test        y_pred  pred_returns\n",
       "Date                                           \n",
       "2021-06-01  33731.0  34394.320312           NaN\n",
       "2021-06-02  33285.0  34062.855469     -0.009637\n",
       "2021-06-03  34298.0  33482.195312     -0.017047\n",
       "2021-06-04  35271.0  34105.074219      0.018603\n",
       "2021-06-05  34100.0  33992.710938     -0.003295\n",
       "...             ...           ...           ...\n",
       "2022-11-24      NaN  24159.792969     -0.012123\n",
       "2022-11-25      NaN  23844.636719     -0.013045\n",
       "2022-11-26      NaN  23678.773438     -0.006956\n",
       "2022-11-27      NaN  23188.109375     -0.020722\n",
       "2022-11-28      NaN  22679.246094     -0.021945\n",
       "\n",
       "[546 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9674c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(zip(Y_test,y_pred),columns=['y_test','y_pred']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732393ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df.to_csv(\"../result/ANN/btc_NN.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12ff04f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "0a952188e4bab49300a5758bda39ddc90e91f41f35dfe6ea820e496e515be371"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
