{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5b918ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/liuxuyang/opt/anaconda3/envs/mytensorflow/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from datetime import datetime\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n",
    "from keras import losses\n",
    "from keras.constraints import maxnorm\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_validate\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from collections import Counter\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import randint\n",
    "import shap\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, accuracy_score, f1_score, classification_report\n",
    "import pickle\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from commons import mean_absolute_percentage_error\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, roc_auc_score\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import Audio\n",
    "#sound_file = 'beep.wav'\n",
    "from keras.callbacks import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3451bb",
   "metadata": {},
   "source": [
    "### Etherium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a39f422c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "regs = pd.read_csv(\"train_eth_selected_features.csv\")\n",
    "btc = pd.read_csv(\"etherium_Data.csv\")\n",
    "for i in range(len(btc['Date'])):\n",
    "    btc['Date'][i]  =  datetime.strptime(btc['Date'][i], '%Y/%m/%d')\n",
    "\n",
    "btc = btc.set_index(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10218810",
   "metadata": {},
   "outputs": [],
   "source": [
    "btcData = btc[regs.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51fa6903",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "btcData['returns'] = btcData['priceUSD'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0a609ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = btcData.drop(columns=['priceUSD'])\n",
    "Data = Data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f582b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activeaddresses90roc</th>\n",
       "      <th>confirmationtime</th>\n",
       "      <th>confirmationtime90roc</th>\n",
       "      <th>difficulty90roc</th>\n",
       "      <th>fee_to_reward90rocUSD</th>\n",
       "      <th>fee_to_rewardUSD</th>\n",
       "      <th>hashrate90roc</th>\n",
       "      <th>median_transaction_fee90rocUSD</th>\n",
       "      <th>mediantransactionvalue90rocUSD</th>\n",
       "      <th>mediantransactionvalueUSD</th>\n",
       "      <th>...</th>\n",
       "      <th>sentinusd90rocUSD</th>\n",
       "      <th>sentinusdUSD</th>\n",
       "      <th>size</th>\n",
       "      <th>size90roc</th>\n",
       "      <th>transactionfees90rocUSD</th>\n",
       "      <th>transactionfeesUSD</th>\n",
       "      <th>transactions90roc</th>\n",
       "      <th>transactionvalue90rocUSD</th>\n",
       "      <th>transactionvalueUSD</th>\n",
       "      <th>returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-02</th>\n",
       "      <td>68.156</td>\n",
       "      <td>0.283</td>\n",
       "      <td>-4.102</td>\n",
       "      <td>31.289</td>\n",
       "      <td>92.095</td>\n",
       "      <td>0.0684</td>\n",
       "      <td>42.295</td>\n",
       "      <td>43.084</td>\n",
       "      <td>64.299</td>\n",
       "      <td>1.348</td>\n",
       "      <td>...</td>\n",
       "      <td>33.650</td>\n",
       "      <td>250428</td>\n",
       "      <td>953.560</td>\n",
       "      <td>22.907</td>\n",
       "      <td>112.289</td>\n",
       "      <td>0.00204</td>\n",
       "      <td>38.345</td>\n",
       "      <td>-3.394</td>\n",
       "      <td>27.327</td>\n",
       "      <td>0.006250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-03</th>\n",
       "      <td>67.716</td>\n",
       "      <td>0.280</td>\n",
       "      <td>-2.435</td>\n",
       "      <td>32.266</td>\n",
       "      <td>60.229</td>\n",
       "      <td>0.0679</td>\n",
       "      <td>41.299</td>\n",
       "      <td>48.484</td>\n",
       "      <td>6.051</td>\n",
       "      <td>1.320</td>\n",
       "      <td>...</td>\n",
       "      <td>235.901</td>\n",
       "      <td>363640</td>\n",
       "      <td>955.046</td>\n",
       "      <td>23.805</td>\n",
       "      <td>70.505</td>\n",
       "      <td>0.00206</td>\n",
       "      <td>47.444</td>\n",
       "      <td>127.816</td>\n",
       "      <td>39.278</td>\n",
       "      <td>0.015528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-04</th>\n",
       "      <td>71.308</td>\n",
       "      <td>0.283</td>\n",
       "      <td>-5.691</td>\n",
       "      <td>32.784</td>\n",
       "      <td>94.471</td>\n",
       "      <td>0.0751</td>\n",
       "      <td>46.895</td>\n",
       "      <td>56.364</td>\n",
       "      <td>32.177</td>\n",
       "      <td>1.238</td>\n",
       "      <td>...</td>\n",
       "      <td>-52.432</td>\n",
       "      <td>149848</td>\n",
       "      <td>981.314</td>\n",
       "      <td>25.645</td>\n",
       "      <td>127.247</td>\n",
       "      <td>0.00219</td>\n",
       "      <td>46.702</td>\n",
       "      <td>-67.575</td>\n",
       "      <td>15.817</td>\n",
       "      <td>-0.009174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-05</th>\n",
       "      <td>71.818</td>\n",
       "      <td>0.281</td>\n",
       "      <td>-2.380</td>\n",
       "      <td>38.812</td>\n",
       "      <td>358.045</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>49.564</td>\n",
       "      <td>359.484</td>\n",
       "      <td>-100.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>415.046</td>\n",
       "      <td>1010627</td>\n",
       "      <td>1396.000</td>\n",
       "      <td>72.720</td>\n",
       "      <td>231.035</td>\n",
       "      <td>0.00358</td>\n",
       "      <td>130.726</td>\n",
       "      <td>123.229</td>\n",
       "      <td>61.511</td>\n",
       "      <td>0.003086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-06</th>\n",
       "      <td>87.024</td>\n",
       "      <td>0.283</td>\n",
       "      <td>1.218</td>\n",
       "      <td>40.900</td>\n",
       "      <td>434.941</td>\n",
       "      <td>0.2310</td>\n",
       "      <td>47.170</td>\n",
       "      <td>360.138</td>\n",
       "      <td>-100.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>35.258</td>\n",
       "      <td>255146</td>\n",
       "      <td>1553.000</td>\n",
       "      <td>96.838</td>\n",
       "      <td>264.918</td>\n",
       "      <td>0.00380</td>\n",
       "      <td>146.520</td>\n",
       "      <td>-45.133</td>\n",
       "      <td>14.944</td>\n",
       "      <td>-0.001026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-25</th>\n",
       "      <td>-1.334</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-12.856</td>\n",
       "      <td>-100.000</td>\n",
       "      <td>-15.546</td>\n",
       "      <td>2.1180</td>\n",
       "      <td>-100.000</td>\n",
       "      <td>6.214</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-23.152</td>\n",
       "      <td>1634051131</td>\n",
       "      <td>68250.000</td>\n",
       "      <td>-21.055</td>\n",
       "      <td>11.363</td>\n",
       "      <td>1.70400</td>\n",
       "      <td>-4.659</td>\n",
       "      <td>-19.397</td>\n",
       "      <td>1635.000</td>\n",
       "      <td>-0.007506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-26</th>\n",
       "      <td>-6.811</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-12.851</td>\n",
       "      <td>-100.000</td>\n",
       "      <td>-18.774</td>\n",
       "      <td>2.0300</td>\n",
       "      <td>-100.000</td>\n",
       "      <td>9.807</td>\n",
       "      <td>-100.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-46.738</td>\n",
       "      <td>1167917265</td>\n",
       "      <td>71240.000</td>\n",
       "      <td>-20.897</td>\n",
       "      <td>15.063</td>\n",
       "      <td>1.78600</td>\n",
       "      <td>-8.786</td>\n",
       "      <td>-41.608</td>\n",
       "      <td>1249.000</td>\n",
       "      <td>0.021008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-27</th>\n",
       "      <td>-13.483</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-11.622</td>\n",
       "      <td>-100.000</td>\n",
       "      <td>-38.242</td>\n",
       "      <td>1.9200</td>\n",
       "      <td>-100.000</td>\n",
       "      <td>-38.760</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-68.602</td>\n",
       "      <td>1061082115</td>\n",
       "      <td>68460.000</td>\n",
       "      <td>-27.495</td>\n",
       "      <td>-28.381</td>\n",
       "      <td>1.82900</td>\n",
       "      <td>-18.636</td>\n",
       "      <td>-61.410</td>\n",
       "      <td>1204.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28</th>\n",
       "      <td>8.872</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-13.436</td>\n",
       "      <td>-100.000</td>\n",
       "      <td>-28.444</td>\n",
       "      <td>2.5500</td>\n",
       "      <td>-100.000</td>\n",
       "      <td>-47.021</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-23.837</td>\n",
       "      <td>2304917567</td>\n",
       "      <td>73284.000</td>\n",
       "      <td>-25.807</td>\n",
       "      <td>-40.170</td>\n",
       "      <td>1.96400</td>\n",
       "      <td>3.399</td>\n",
       "      <td>-26.341</td>\n",
       "      <td>2119.000</td>\n",
       "      <td>-0.035391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-29</th>\n",
       "      <td>0.660</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-12.612</td>\n",
       "      <td>-100.000</td>\n",
       "      <td>-31.422</td>\n",
       "      <td>2.4030</td>\n",
       "      <td>-100.000</td>\n",
       "      <td>-53.937</td>\n",
       "      <td>-100.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-29.484</td>\n",
       "      <td>2080626561</td>\n",
       "      <td>76431.000</td>\n",
       "      <td>-14.948</td>\n",
       "      <td>-52.778</td>\n",
       "      <td>1.76600</td>\n",
       "      <td>1.184</td>\n",
       "      <td>-30.309</td>\n",
       "      <td>1986.000</td>\n",
       "      <td>0.029010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2524 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            activeaddresses90roc  confirmationtime  confirmationtime90roc  \\\n",
       "Date                                                                        \n",
       "2016-01-02                68.156             0.283                 -4.102   \n",
       "2016-01-03                67.716             0.280                 -2.435   \n",
       "2016-01-04                71.308             0.283                 -5.691   \n",
       "2016-01-05                71.818             0.281                 -2.380   \n",
       "2016-01-06                87.024             0.283                  1.218   \n",
       "...                          ...               ...                    ...   \n",
       "2022-11-25                -1.334             0.201                -12.856   \n",
       "2022-11-26                -6.811             0.201                -12.851   \n",
       "2022-11-27               -13.483             0.201                -11.622   \n",
       "2022-11-28                 8.872             0.201                -13.436   \n",
       "2022-11-29                 0.660             0.201                -12.612   \n",
       "\n",
       "            difficulty90roc  fee_to_reward90rocUSD  fee_to_rewardUSD  \\\n",
       "Date                                                                   \n",
       "2016-01-02           31.289                 92.095            0.0684   \n",
       "2016-01-03           32.266                 60.229            0.0679   \n",
       "2016-01-04           32.784                 94.471            0.0751   \n",
       "2016-01-05           38.812                358.045            0.2060   \n",
       "2016-01-06           40.900                434.941            0.2310   \n",
       "...                     ...                    ...               ...   \n",
       "2022-11-25         -100.000                -15.546            2.1180   \n",
       "2022-11-26         -100.000                -18.774            2.0300   \n",
       "2022-11-27         -100.000                -38.242            1.9200   \n",
       "2022-11-28         -100.000                -28.444            2.5500   \n",
       "2022-11-29         -100.000                -31.422            2.4030   \n",
       "\n",
       "            hashrate90roc  median_transaction_fee90rocUSD  \\\n",
       "Date                                                        \n",
       "2016-01-02         42.295                          43.084   \n",
       "2016-01-03         41.299                          48.484   \n",
       "2016-01-04         46.895                          56.364   \n",
       "2016-01-05         49.564                         359.484   \n",
       "2016-01-06         47.170                         360.138   \n",
       "...                   ...                             ...   \n",
       "2022-11-25       -100.000                           6.214   \n",
       "2022-11-26       -100.000                           9.807   \n",
       "2022-11-27       -100.000                         -38.760   \n",
       "2022-11-28       -100.000                         -47.021   \n",
       "2022-11-29       -100.000                         -53.937   \n",
       "\n",
       "            mediantransactionvalue90rocUSD  mediantransactionvalueUSD  ...  \\\n",
       "Date                                                                   ...   \n",
       "2016-01-02                          64.299                      1.348  ...   \n",
       "2016-01-03                           6.051                      1.320  ...   \n",
       "2016-01-04                          32.177                      1.238  ...   \n",
       "2016-01-05                        -100.000                      0.000  ...   \n",
       "2016-01-06                        -100.000                      0.000  ...   \n",
       "...                                    ...                        ...  ...   \n",
       "2022-11-25                           0.000                      0.000  ...   \n",
       "2022-11-26                        -100.000                      0.000  ...   \n",
       "2022-11-27                           0.000                      0.000  ...   \n",
       "2022-11-28                           0.000                      0.000  ...   \n",
       "2022-11-29                        -100.000                      0.000  ...   \n",
       "\n",
       "            sentinusd90rocUSD  sentinusdUSD       size  size90roc  \\\n",
       "Date                                                                \n",
       "2016-01-02             33.650        250428    953.560     22.907   \n",
       "2016-01-03            235.901        363640    955.046     23.805   \n",
       "2016-01-04            -52.432        149848    981.314     25.645   \n",
       "2016-01-05            415.046       1010627   1396.000     72.720   \n",
       "2016-01-06             35.258        255146   1553.000     96.838   \n",
       "...                       ...           ...        ...        ...   \n",
       "2022-11-25            -23.152    1634051131  68250.000    -21.055   \n",
       "2022-11-26            -46.738    1167917265  71240.000    -20.897   \n",
       "2022-11-27            -68.602    1061082115  68460.000    -27.495   \n",
       "2022-11-28            -23.837    2304917567  73284.000    -25.807   \n",
       "2022-11-29            -29.484    2080626561  76431.000    -14.948   \n",
       "\n",
       "            transactionfees90rocUSD  transactionfeesUSD  transactions90roc  \\\n",
       "Date                                                                         \n",
       "2016-01-02                  112.289             0.00204             38.345   \n",
       "2016-01-03                   70.505             0.00206             47.444   \n",
       "2016-01-04                  127.247             0.00219             46.702   \n",
       "2016-01-05                  231.035             0.00358            130.726   \n",
       "2016-01-06                  264.918             0.00380            146.520   \n",
       "...                             ...                 ...                ...   \n",
       "2022-11-25                   11.363             1.70400             -4.659   \n",
       "2022-11-26                   15.063             1.78600             -8.786   \n",
       "2022-11-27                  -28.381             1.82900            -18.636   \n",
       "2022-11-28                  -40.170             1.96400              3.399   \n",
       "2022-11-29                  -52.778             1.76600              1.184   \n",
       "\n",
       "            transactionvalue90rocUSD  transactionvalueUSD   returns  \n",
       "Date                                                                 \n",
       "2016-01-02                    -3.394               27.327  0.006250  \n",
       "2016-01-03                   127.816               39.278  0.015528  \n",
       "2016-01-04                   -67.575               15.817 -0.009174  \n",
       "2016-01-05                   123.229               61.511  0.003086  \n",
       "2016-01-06                   -45.133               14.944 -0.001026  \n",
       "...                              ...                  ...       ...  \n",
       "2022-11-25                   -19.397             1635.000 -0.007506  \n",
       "2022-11-26                   -41.608             1249.000  0.021008  \n",
       "2022-11-27                   -61.410             1204.000  0.000000  \n",
       "2022-11-28                   -26.341             2119.000 -0.035391  \n",
       "2022-11-29                   -30.309             1986.000  0.029010  \n",
       "\n",
       "[2524 rows x 23 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "562e66d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide X and Y\n",
    "X = Data.iloc[:,0:]\n",
    "#Y = Data['returns']   # 用returns的话就用这一行，然后把下一行comment掉\n",
    "Y = btcData['priceUSD'].shift(-30)[1:] # 反之亦然"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f636fe25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activeaddresses90roc</th>\n",
       "      <th>confirmationtime</th>\n",
       "      <th>confirmationtime90roc</th>\n",
       "      <th>difficulty90roc</th>\n",
       "      <th>fee_to_reward90rocUSD</th>\n",
       "      <th>fee_to_rewardUSD</th>\n",
       "      <th>hashrate90roc</th>\n",
       "      <th>median_transaction_fee90rocUSD</th>\n",
       "      <th>mediantransactionvalue90rocUSD</th>\n",
       "      <th>mediantransactionvalueUSD</th>\n",
       "      <th>...</th>\n",
       "      <th>sentinusd90rocUSD</th>\n",
       "      <th>sentinusdUSD</th>\n",
       "      <th>size</th>\n",
       "      <th>size90roc</th>\n",
       "      <th>transactionfees90rocUSD</th>\n",
       "      <th>transactionfeesUSD</th>\n",
       "      <th>transactions90roc</th>\n",
       "      <th>transactionvalue90rocUSD</th>\n",
       "      <th>transactionvalueUSD</th>\n",
       "      <th>returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-02</th>\n",
       "      <td>68.156</td>\n",
       "      <td>0.283</td>\n",
       "      <td>-4.102</td>\n",
       "      <td>31.289</td>\n",
       "      <td>92.095</td>\n",
       "      <td>0.0684</td>\n",
       "      <td>42.295</td>\n",
       "      <td>43.084</td>\n",
       "      <td>64.299</td>\n",
       "      <td>1.348</td>\n",
       "      <td>...</td>\n",
       "      <td>33.650</td>\n",
       "      <td>250428</td>\n",
       "      <td>953.560</td>\n",
       "      <td>22.907</td>\n",
       "      <td>112.289</td>\n",
       "      <td>0.00204</td>\n",
       "      <td>38.345</td>\n",
       "      <td>-3.394</td>\n",
       "      <td>27.327</td>\n",
       "      <td>0.006250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-03</th>\n",
       "      <td>67.716</td>\n",
       "      <td>0.280</td>\n",
       "      <td>-2.435</td>\n",
       "      <td>32.266</td>\n",
       "      <td>60.229</td>\n",
       "      <td>0.0679</td>\n",
       "      <td>41.299</td>\n",
       "      <td>48.484</td>\n",
       "      <td>6.051</td>\n",
       "      <td>1.320</td>\n",
       "      <td>...</td>\n",
       "      <td>235.901</td>\n",
       "      <td>363640</td>\n",
       "      <td>955.046</td>\n",
       "      <td>23.805</td>\n",
       "      <td>70.505</td>\n",
       "      <td>0.00206</td>\n",
       "      <td>47.444</td>\n",
       "      <td>127.816</td>\n",
       "      <td>39.278</td>\n",
       "      <td>0.015528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-04</th>\n",
       "      <td>71.308</td>\n",
       "      <td>0.283</td>\n",
       "      <td>-5.691</td>\n",
       "      <td>32.784</td>\n",
       "      <td>94.471</td>\n",
       "      <td>0.0751</td>\n",
       "      <td>46.895</td>\n",
       "      <td>56.364</td>\n",
       "      <td>32.177</td>\n",
       "      <td>1.238</td>\n",
       "      <td>...</td>\n",
       "      <td>-52.432</td>\n",
       "      <td>149848</td>\n",
       "      <td>981.314</td>\n",
       "      <td>25.645</td>\n",
       "      <td>127.247</td>\n",
       "      <td>0.00219</td>\n",
       "      <td>46.702</td>\n",
       "      <td>-67.575</td>\n",
       "      <td>15.817</td>\n",
       "      <td>-0.009174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            activeaddresses90roc  confirmationtime  confirmationtime90roc  \\\n",
       "Date                                                                        \n",
       "2016-01-02                68.156             0.283                 -4.102   \n",
       "2016-01-03                67.716             0.280                 -2.435   \n",
       "2016-01-04                71.308             0.283                 -5.691   \n",
       "\n",
       "            difficulty90roc  fee_to_reward90rocUSD  fee_to_rewardUSD  \\\n",
       "Date                                                                   \n",
       "2016-01-02           31.289                 92.095            0.0684   \n",
       "2016-01-03           32.266                 60.229            0.0679   \n",
       "2016-01-04           32.784                 94.471            0.0751   \n",
       "\n",
       "            hashrate90roc  median_transaction_fee90rocUSD  \\\n",
       "Date                                                        \n",
       "2016-01-02         42.295                          43.084   \n",
       "2016-01-03         41.299                          48.484   \n",
       "2016-01-04         46.895                          56.364   \n",
       "\n",
       "            mediantransactionvalue90rocUSD  mediantransactionvalueUSD  ...  \\\n",
       "Date                                                                   ...   \n",
       "2016-01-02                          64.299                      1.348  ...   \n",
       "2016-01-03                           6.051                      1.320  ...   \n",
       "2016-01-04                          32.177                      1.238  ...   \n",
       "\n",
       "            sentinusd90rocUSD  sentinusdUSD     size  size90roc  \\\n",
       "Date                                                              \n",
       "2016-01-02             33.650        250428  953.560     22.907   \n",
       "2016-01-03            235.901        363640  955.046     23.805   \n",
       "2016-01-04            -52.432        149848  981.314     25.645   \n",
       "\n",
       "            transactionfees90rocUSD  transactionfeesUSD  transactions90roc  \\\n",
       "Date                                                                         \n",
       "2016-01-02                  112.289             0.00204             38.345   \n",
       "2016-01-03                   70.505             0.00206             47.444   \n",
       "2016-01-04                  127.247             0.00219             46.702   \n",
       "\n",
       "            transactionvalue90rocUSD  transactionvalueUSD   returns  \n",
       "Date                                                                 \n",
       "2016-01-02                    -3.394               27.327  0.006250  \n",
       "2016-01-03                   127.816               39.278  0.015528  \n",
       "2016-01-04                   -67.575               15.817 -0.009174  \n",
       "\n",
       "[3 rows x 23 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c30c0ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into three data sets\n",
    "X_train = X['2016-01-01':'2019-12-31']\n",
    "X_val = X['2020-01-01':'2021-05-31']\n",
    "X_test = X['2021-06-01':'2023-01-01']\n",
    "\n",
    "Y_train = Y['2016-01-01':'2019-12-31']\n",
    "Y_val = Y['2020-01-01':'2021-05-31']\n",
    "Y_test = Y['2021-06-01':'2023-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4fb8665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............ (step 1 of 2) Processing mixmax, total=   0.0s\n",
      "[Pipeline] ............ (step 2 of 2) Processing robust, total=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('mixmax', MinMaxScaler()), ['robust', RobustScaler()]],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators=[]\n",
    "estimators.append(['mixmax',MinMaxScaler()])\n",
    "estimators.append(['robust',RobustScaler()])\n",
    "scale=Pipeline(estimators,verbose=True)\n",
    "scale.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b722f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=scale.transform(X_train)\n",
    "X_test=scale.transform(X_test)\n",
    "X_val = scale.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8553e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3ab9dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "# define neural network model\n",
    "shape=X.shape[1]\n",
    "def sequential_model(initializer='normal', activation='relu', neurons=200, NUM_FEATURES=shape):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_shape=(NUM_FEATURES,), kernel_initializer=initializer, activation=activation))\n",
    "    model.add(Dense(neurons, activation=activation))\n",
    "    model.add(Dense(neurons, activation=activation))\n",
    "    model.add(Dense(1, activation=activation))\n",
    "    # Compile model\n",
    "    adam=keras.optimizers.adam(lr=lr_schedule(0), amsgrad=True)\n",
    "    #sgd=keras.optimizers.SGD(learning_rate=0.08, momentum=0.9, nesterov=False)\n",
    "    model.compile(loss='logcosh', optimizer=adam, metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61afd6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mcp_save = ModelCheckpoint('trained_models/ANN_reg_seven_new.hdf5', save_best_only=True, monitor='val_loss', mode='auto')\n",
    "#earlyStopping = EarlyStopping(monitor='val_loss', patience=100,verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1641aae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor=KerasRegressor(build_fn=sequential_model,epochs=1000,verbose=1, shuffle=True,use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f3b66a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 20:32:29.348150: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-30 20:32:29.350466: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Train on 1460 samples, validate on 517 samples\n",
      "Epoch 1/1000\n",
      "1460/1460 [==============================] - 1s 680us/step - loss: 196.0952 - mae: 196.7837 - val_loss: 391.8425 - val_mae: 392.5353\n",
      "Epoch 2/1000\n",
      "1460/1460 [==============================] - 0s 217us/step - loss: 98.9023 - mae: 99.5834 - val_loss: 962.4407 - val_mae: 963.1322\n",
      "Epoch 3/1000\n",
      "1460/1460 [==============================] - 0s 216us/step - loss: 76.2706 - mae: 76.9508 - val_loss: 1489.8495 - val_mae: 1490.5420\n",
      "Epoch 4/1000\n",
      "1460/1460 [==============================] - 0s 214us/step - loss: 66.8127 - mae: 67.4886 - val_loss: 1540.8121 - val_mae: 1541.5035\n",
      "Epoch 5/1000\n",
      "1460/1460 [==============================] - 0s 223us/step - loss: 62.5915 - mae: 63.2647 - val_loss: 1520.3272 - val_mae: 1521.0190\n",
      "Epoch 6/1000\n",
      "1460/1460 [==============================] - 0s 232us/step - loss: 61.2234 - mae: 61.8951 - val_loss: 1650.6043 - val_mae: 1651.2966\n",
      "Epoch 7/1000\n",
      "1460/1460 [==============================] - 0s 221us/step - loss: 59.6952 - mae: 60.3634 - val_loss: 1592.6036 - val_mae: 1593.2941\n",
      "Epoch 8/1000\n",
      "1460/1460 [==============================] - 0s 223us/step - loss: 56.8659 - mae: 57.5339 - val_loss: 1566.7501 - val_mae: 1567.4391\n",
      "Epoch 9/1000\n",
      "1460/1460 [==============================] - 0s 219us/step - loss: 54.8424 - mae: 55.5063 - val_loss: 1445.3750 - val_mae: 1446.0665\n",
      "Epoch 10/1000\n",
      "1460/1460 [==============================] - 0s 216us/step - loss: 52.4369 - mae: 53.0944 - val_loss: 1453.4674 - val_mae: 1454.1597\n",
      "Epoch 11/1000\n",
      "1460/1460 [==============================] - 0s 228us/step - loss: 50.8952 - mae: 51.5471 - val_loss: 1286.1684 - val_mae: 1286.8611\n",
      "Epoch 12/1000\n",
      "1460/1460 [==============================] - 0s 218us/step - loss: 51.2820 - mae: 51.9457 - val_loss: 1295.8578 - val_mae: 1296.5494\n",
      "Epoch 13/1000\n",
      "1460/1460 [==============================] - 0s 214us/step - loss: 48.1795 - mae: 48.8383 - val_loss: 1195.8447 - val_mae: 1196.5374\n",
      "Epoch 14/1000\n",
      "1460/1460 [==============================] - 0s 219us/step - loss: 46.5970 - mae: 47.2549 - val_loss: 1116.2001 - val_mae: 1116.8925\n",
      "Epoch 15/1000\n",
      "1460/1460 [==============================] - 0s 219us/step - loss: 44.1714 - mae: 44.8223 - val_loss: 920.1977 - val_mae: 920.8901\n",
      "Epoch 16/1000\n",
      "1460/1460 [==============================] - 0s 219us/step - loss: 44.2696 - mae: 44.9284 - val_loss: 777.8310 - val_mae: 778.5228\n",
      "Epoch 17/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 43.0075 - mae: 43.6585 - val_loss: 741.6347 - val_mae: 742.3240\n",
      "Epoch 18/1000\n",
      "1460/1460 [==============================] - 0s 227us/step - loss: 42.1839 - mae: 42.8377 - val_loss: 759.3487 - val_mae: 760.0375\n",
      "Epoch 19/1000\n",
      "1460/1460 [==============================] - 0s 251us/step - loss: 40.8880 - mae: 41.5385 - val_loss: 710.1838 - val_mae: 710.8727\n",
      "Epoch 20/1000\n",
      "1460/1460 [==============================] - 0s 219us/step - loss: 39.0329 - mae: 39.6836 - val_loss: 570.3836 - val_mae: 571.0754\n",
      "Epoch 21/1000\n",
      "1460/1460 [==============================] - 0s 229us/step - loss: 36.2270 - mae: 36.8785 - val_loss: 558.5697 - val_mae: 559.2621\n",
      "Epoch 22/1000\n",
      "1460/1460 [==============================] - 0s 219us/step - loss: 35.1989 - mae: 35.8419 - val_loss: 500.5386 - val_mae: 501.2311\n",
      "Epoch 23/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 35.8618 - mae: 36.5152 - val_loss: 414.6924 - val_mae: 415.3818\n",
      "Epoch 24/1000\n",
      "1460/1460 [==============================] - 0s 223us/step - loss: 34.2674 - mae: 34.9169 - val_loss: 420.1534 - val_mae: 420.8444\n",
      "Epoch 25/1000\n",
      "1460/1460 [==============================] - 0s 216us/step - loss: 32.3821 - mae: 33.0311 - val_loss: 395.7959 - val_mae: 396.4879\n",
      "Epoch 26/1000\n",
      "1460/1460 [==============================] - 0s 216us/step - loss: 31.4759 - mae: 32.1259 - val_loss: 375.9008 - val_mae: 376.5923\n",
      "Epoch 27/1000\n",
      "1460/1460 [==============================] - 0s 251us/step - loss: 31.4394 - mae: 32.0831 - val_loss: 378.9162 - val_mae: 379.6073\n",
      "Epoch 28/1000\n",
      "1460/1460 [==============================] - 0s 318us/step - loss: 31.6162 - mae: 32.2636 - val_loss: 376.6004 - val_mae: 377.2922\n",
      "Epoch 29/1000\n",
      "1460/1460 [==============================] - 0s 240us/step - loss: 30.5481 - mae: 31.1951 - val_loss: 382.1187 - val_mae: 382.8104\n",
      "Epoch 30/1000\n",
      "1460/1460 [==============================] - 0s 286us/step - loss: 28.6960 - mae: 29.3369 - val_loss: 375.0049 - val_mae: 375.6949\n",
      "Epoch 31/1000\n",
      "1460/1460 [==============================] - 0s 224us/step - loss: 27.9969 - mae: 28.6382 - val_loss: 454.0478 - val_mae: 454.7389\n",
      "Epoch 32/1000\n",
      "1460/1460 [==============================] - 0s 234us/step - loss: 31.4991 - mae: 32.1455 - val_loss: 430.6848 - val_mae: 431.3764\n",
      "Epoch 33/1000\n",
      "1460/1460 [==============================] - 0s 239us/step - loss: 27.6399 - mae: 28.2845 - val_loss: 358.5568 - val_mae: 359.2480\n",
      "Epoch 34/1000\n",
      "1460/1460 [==============================] - 0s 217us/step - loss: 27.2859 - mae: 27.9299 - val_loss: 422.3870 - val_mae: 423.0791\n",
      "Epoch 35/1000\n",
      "1460/1460 [==============================] - 0s 204us/step - loss: 26.7725 - mae: 27.4155 - val_loss: 478.4832 - val_mae: 479.1757\n",
      "Epoch 36/1000\n",
      "1460/1460 [==============================] - 0s 223us/step - loss: 25.9003 - mae: 26.5385 - val_loss: 407.9995 - val_mae: 408.6917\n",
      "Epoch 37/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 24.5846 - mae: 25.2241 - val_loss: 405.6821 - val_mae: 406.3731\n",
      "Epoch 38/1000\n",
      "1460/1460 [==============================] - 0s 219us/step - loss: 25.1797 - mae: 25.8200 - val_loss: 471.2883 - val_mae: 471.9808\n",
      "Epoch 39/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 24.0671 - mae: 24.7026 - val_loss: 432.8365 - val_mae: 433.5289\n",
      "Epoch 40/1000\n",
      "1460/1460 [==============================] - 0s 250us/step - loss: 25.5228 - mae: 26.1606 - val_loss: 487.7415 - val_mae: 488.4315\n",
      "Epoch 41/1000\n",
      "1460/1460 [==============================] - 0s 204us/step - loss: 23.5289 - mae: 24.1604 - val_loss: 399.4703 - val_mae: 400.1618\n",
      "Epoch 42/1000\n",
      "1460/1460 [==============================] - 0s 205us/step - loss: 23.2241 - mae: 23.8658 - val_loss: 416.5429 - val_mae: 417.2350\n",
      "Epoch 43/1000\n",
      "1460/1460 [==============================] - 0s 200us/step - loss: 23.7422 - mae: 24.3824 - val_loss: 412.1965 - val_mae: 412.8896\n",
      "Epoch 44/1000\n",
      "1460/1460 [==============================] - 0s 220us/step - loss: 23.7404 - mae: 24.3743 - val_loss: 457.6718 - val_mae: 458.3646\n",
      "Epoch 45/1000\n",
      "1460/1460 [==============================] - 0s 223us/step - loss: 22.6229 - mae: 23.2518 - val_loss: 390.2273 - val_mae: 390.9196\n",
      "Epoch 46/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 23.8361 - mae: 24.4672 - val_loss: 469.8783 - val_mae: 470.5712\n",
      "Epoch 47/1000\n",
      "1460/1460 [==============================] - 0s 203us/step - loss: 22.1461 - mae: 22.7767 - val_loss: 392.4937 - val_mae: 393.1859\n",
      "Epoch 48/1000\n",
      "1460/1460 [==============================] - 0s 200us/step - loss: 23.3788 - mae: 24.0094 - val_loss: 380.9402 - val_mae: 381.6318\n",
      "Epoch 49/1000\n",
      "1460/1460 [==============================] - 0s 205us/step - loss: 22.3154 - mae: 22.9452 - val_loss: 441.4100 - val_mae: 442.1015\n",
      "Epoch 50/1000\n",
      "1460/1460 [==============================] - 0s 212us/step - loss: 21.4949 - mae: 22.1264 - val_loss: 533.3240 - val_mae: 534.0168\n",
      "Epoch 51/1000\n",
      "1460/1460 [==============================] - 0s 205us/step - loss: 21.4447 - mae: 22.0752 - val_loss: 424.6518 - val_mae: 425.3444\n",
      "Epoch 52/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 20.0323 - mae: 20.6570 - val_loss: 452.5789 - val_mae: 453.2720\n",
      "Epoch 53/1000\n",
      "1460/1460 [==============================] - 0s 216us/step - loss: 21.0461 - mae: 21.6738 - val_loss: 434.2160 - val_mae: 434.9087\n",
      "Epoch 54/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 20.3778 - mae: 21.0091 - val_loss: 432.5087 - val_mae: 433.2015\n",
      "Epoch 55/1000\n",
      "1460/1460 [==============================] - 0s 216us/step - loss: 20.8000 - mae: 21.4249 - val_loss: 448.1074 - val_mae: 448.7985\n",
      "Epoch 56/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 22.0023 - mae: 22.6291 - val_loss: 396.0296 - val_mae: 396.7217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/1000\n",
      "1460/1460 [==============================] - 0s 202us/step - loss: 20.0258 - mae: 20.6497 - val_loss: 468.4362 - val_mae: 469.1283\n",
      "Epoch 58/1000\n",
      "1460/1460 [==============================] - 0s 215us/step - loss: 19.8272 - mae: 20.4513 - val_loss: 415.0322 - val_mae: 415.7242\n",
      "Epoch 59/1000\n",
      "1460/1460 [==============================] - 0s 246us/step - loss: 18.9912 - mae: 19.6120 - val_loss: 466.4093 - val_mae: 467.1024\n",
      "Epoch 60/1000\n",
      "1460/1460 [==============================] - 0s 228us/step - loss: 18.9809 - mae: 19.6064 - val_loss: 440.2298 - val_mae: 440.9229\n",
      "Epoch 61/1000\n",
      "1460/1460 [==============================] - 0s 223us/step - loss: 20.4211 - mae: 21.0499 - val_loss: 433.0326 - val_mae: 433.7257\n",
      "Epoch 62/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 18.6573 - mae: 19.2791 - val_loss: 415.1548 - val_mae: 415.8452\n",
      "Epoch 63/1000\n",
      "1460/1460 [==============================] - 0s 217us/step - loss: 19.0400 - mae: 19.6582 - val_loss: 430.3283 - val_mae: 431.0202\n",
      "Epoch 64/1000\n",
      "1460/1460 [==============================] - 0s 234us/step - loss: 17.6755 - mae: 18.2949 - val_loss: 414.9394 - val_mae: 415.6325\n",
      "Epoch 65/1000\n",
      "1460/1460 [==============================] - 0s 219us/step - loss: 18.0048 - mae: 18.6260 - val_loss: 474.3241 - val_mae: 475.0151\n",
      "Epoch 66/1000\n",
      "1460/1460 [==============================] - 0s 212us/step - loss: 18.2469 - mae: 18.8703 - val_loss: 466.2961 - val_mae: 466.9891\n",
      "Epoch 67/1000\n",
      "1460/1460 [==============================] - 0s 216us/step - loss: 18.0088 - mae: 18.6299 - val_loss: 478.8718 - val_mae: 479.5622\n",
      "Epoch 68/1000\n",
      "1460/1460 [==============================] - 0s 250us/step - loss: 20.2404 - mae: 20.8672 - val_loss: 444.4364 - val_mae: 445.1284\n",
      "Epoch 69/1000\n",
      "1460/1460 [==============================] - 0s 235us/step - loss: 18.6640 - mae: 19.2921 - val_loss: 418.3224 - val_mae: 419.0153\n",
      "Epoch 70/1000\n",
      "1460/1460 [==============================] - 0s 202us/step - loss: 16.7682 - mae: 17.3848 - val_loss: 497.9032 - val_mae: 498.5963\n",
      "Epoch 71/1000\n",
      "1460/1460 [==============================] - 0s 205us/step - loss: 20.3063 - mae: 20.9351 - val_loss: 438.2351 - val_mae: 438.9282\n",
      "Epoch 72/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 17.0786 - mae: 17.6936 - val_loss: 418.9345 - val_mae: 419.6275\n",
      "Epoch 73/1000\n",
      "1460/1460 [==============================] - 0s 200us/step - loss: 18.3172 - mae: 18.9351 - val_loss: 395.3979 - val_mae: 396.0902\n",
      "Epoch 74/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 17.2983 - mae: 17.9174 - val_loss: 449.3535 - val_mae: 450.0443\n",
      "Epoch 75/1000\n",
      "1460/1460 [==============================] - 0s 203us/step - loss: 17.8517 - mae: 18.4700 - val_loss: 426.0665 - val_mae: 426.7594\n",
      "Epoch 76/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 17.4355 - mae: 18.0507 - val_loss: 441.2152 - val_mae: 441.9079\n",
      "Epoch 77/1000\n",
      "1460/1460 [==============================] - 0s 215us/step - loss: 17.9856 - mae: 18.6092 - val_loss: 443.8830 - val_mae: 444.5759\n",
      "Epoch 78/1000\n",
      "1460/1460 [==============================] - 0s 252us/step - loss: 16.8888 - mae: 17.5012 - val_loss: 447.6539 - val_mae: 448.3470\n",
      "Epoch 79/1000\n",
      "1460/1460 [==============================] - 1s 349us/step - loss: 18.9370 - mae: 19.5528 - val_loss: 426.7100 - val_mae: 427.4022\n",
      "Epoch 80/1000\n",
      "1460/1460 [==============================] - 0s 251us/step - loss: 18.3920 - mae: 19.0123 - val_loss: 491.2192 - val_mae: 491.9120\n",
      "Epoch 81/1000\n",
      "1460/1460 [==============================] - 0s 250us/step - loss: 18.3253 - mae: 18.9414 - val_loss: 452.1579 - val_mae: 452.8511\n",
      "Epoch 82/1000\n",
      "1460/1460 [==============================] - 0s 257us/step - loss: 17.7998 - mae: 18.4227 - val_loss: 471.0213 - val_mae: 471.7144\n",
      "Epoch 83/1000\n",
      "1460/1460 [==============================] - 0s 255us/step - loss: 17.5805 - mae: 18.2048 - val_loss: 444.2525 - val_mae: 444.9455\n",
      "Epoch 84/1000\n",
      "1460/1460 [==============================] - 0s 217us/step - loss: 17.0335 - mae: 17.6510 - val_loss: 434.9355 - val_mae: 435.6282\n",
      "Epoch 85/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 16.4699 - mae: 17.0842 - val_loss: 485.2635 - val_mae: 485.9567\n",
      "Epoch 86/1000\n",
      "1460/1460 [==============================] - 0s 240us/step - loss: 15.3155 - mae: 15.9329 - val_loss: 469.6268 - val_mae: 470.3197\n",
      "Epoch 87/1000\n",
      "1460/1460 [==============================] - 0s 323us/step - loss: 15.5013 - mae: 16.1153 - val_loss: 441.4503 - val_mae: 442.1434\n",
      "Epoch 88/1000\n",
      "1460/1460 [==============================] - 0s 240us/step - loss: 15.9513 - mae: 16.5713 - val_loss: 446.8322 - val_mae: 447.5253\n",
      "Epoch 89/1000\n",
      "1460/1460 [==============================] - 0s 239us/step - loss: 16.1856 - mae: 16.8086 - val_loss: 405.3464 - val_mae: 406.0395\n",
      "Epoch 90/1000\n",
      "1460/1460 [==============================] - 0s 247us/step - loss: 17.5015 - mae: 18.1206 - val_loss: 430.6872 - val_mae: 431.3800\n",
      "Epoch 91/1000\n",
      "1460/1460 [==============================] - 0s 217us/step - loss: 16.0721 - mae: 16.6807 - val_loss: 422.9229 - val_mae: 423.6152\n",
      "Epoch 92/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 17.5294 - mae: 18.1476 - val_loss: 453.4865 - val_mae: 454.1790\n",
      "Epoch 93/1000\n",
      "1460/1460 [==============================] - 0s 221us/step - loss: 16.9416 - mae: 17.5624 - val_loss: 386.4306 - val_mae: 387.1231\n",
      "Epoch 94/1000\n",
      "1460/1460 [==============================] - 0s 233us/step - loss: 16.1313 - mae: 16.7453 - val_loss: 407.2241 - val_mae: 407.9167\n",
      "Epoch 95/1000\n",
      "1460/1460 [==============================] - 0s 222us/step - loss: 16.3156 - mae: 16.9297 - val_loss: 428.6757 - val_mae: 429.3675\n",
      "Epoch 96/1000\n",
      "1460/1460 [==============================] - 0s 233us/step - loss: 16.9151 - mae: 17.5289 - val_loss: 430.7167 - val_mae: 431.4099\n",
      "Epoch 97/1000\n",
      "1460/1460 [==============================] - 0s 231us/step - loss: 15.6203 - mae: 16.2363 - val_loss: 442.1522 - val_mae: 442.8443\n",
      "Epoch 98/1000\n",
      "1460/1460 [==============================] - 0s 241us/step - loss: 15.3628 - mae: 15.9809 - val_loss: 424.0502 - val_mae: 424.7433\n",
      "Epoch 99/1000\n",
      "1460/1460 [==============================] - 0s 224us/step - loss: 15.7647 - mae: 16.3761 - val_loss: 452.2066 - val_mae: 452.8984\n",
      "Epoch 100/1000\n",
      "1460/1460 [==============================] - 0s 218us/step - loss: 16.4253 - mae: 17.0429 - val_loss: 416.6655 - val_mae: 417.3571\n",
      "Epoch 101/1000\n",
      "1460/1460 [==============================] - 0s 216us/step - loss: 16.1599 - mae: 16.7784 - val_loss: 403.7879 - val_mae: 404.4808\n",
      "Epoch 102/1000\n",
      "1460/1460 [==============================] - 0s 223us/step - loss: 15.4641 - mae: 16.0756 - val_loss: 477.1685 - val_mae: 477.8613\n",
      "Epoch 103/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 16.0252 - mae: 16.6372 - val_loss: 436.4799 - val_mae: 437.1721\n",
      "Epoch 104/1000\n",
      "1460/1460 [==============================] - 0s 265us/step - loss: 17.4474 - mae: 18.0596 - val_loss: 423.5961 - val_mae: 424.2882\n",
      "Epoch 105/1000\n",
      "1460/1460 [==============================] - 0s 262us/step - loss: 14.9224 - mae: 15.5316 - val_loss: 470.8202 - val_mae: 471.5128\n",
      "Epoch 106/1000\n",
      "1460/1460 [==============================] - 0s 218us/step - loss: 16.4081 - mae: 17.0202 - val_loss: 430.4890 - val_mae: 431.1815\n",
      "Epoch 107/1000\n",
      "1460/1460 [==============================] - 0s 242us/step - loss: 17.5170 - mae: 18.1396 - val_loss: 415.1426 - val_mae: 415.8339\n",
      "Epoch 108/1000\n",
      "1460/1460 [==============================] - 0s 227us/step - loss: 17.5184 - mae: 18.1376 - val_loss: 439.2322 - val_mae: 439.9254\n",
      "Epoch 109/1000\n",
      "1460/1460 [==============================] - 0s 239us/step - loss: 14.8942 - mae: 15.5038 - val_loss: 411.0332 - val_mae: 411.7263\n",
      "Epoch 110/1000\n",
      "1460/1460 [==============================] - 0s 218us/step - loss: 15.5675 - mae: 16.1799 - val_loss: 411.8338 - val_mae: 412.5268\n",
      "Epoch 111/1000\n",
      "1460/1460 [==============================] - 0s 230us/step - loss: 14.5872 - mae: 15.1978 - val_loss: 483.5532 - val_mae: 484.2461\n",
      "Epoch 112/1000\n",
      "1460/1460 [==============================] - 0s 220us/step - loss: 15.3943 - mae: 16.0148 - val_loss: 387.5517 - val_mae: 388.2429\n",
      "Epoch 113/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460/1460 [==============================] - 0s 226us/step - loss: 15.2899 - mae: 15.9045 - val_loss: 375.7082 - val_mae: 376.4002\n",
      "Epoch 114/1000\n",
      "1460/1460 [==============================] - 0s 220us/step - loss: 19.6112 - mae: 20.2313 - val_loss: 422.4143 - val_mae: 423.1074\n",
      "Epoch 115/1000\n",
      "1460/1460 [==============================] - 0s 230us/step - loss: 15.3414 - mae: 15.9536 - val_loss: 441.4498 - val_mae: 442.1414\n",
      "Epoch 116/1000\n",
      "1460/1460 [==============================] - 0s 223us/step - loss: 15.5374 - mae: 16.1484 - val_loss: 395.5746 - val_mae: 396.2648\n",
      "Epoch 117/1000\n",
      "1460/1460 [==============================] - 0s 202us/step - loss: 15.8325 - mae: 16.4458 - val_loss: 432.5842 - val_mae: 433.2773\n",
      "Epoch 118/1000\n",
      "1460/1460 [==============================] - 0s 201us/step - loss: 14.2682 - mae: 14.8703 - val_loss: 432.4113 - val_mae: 433.1029\n",
      "Epoch 119/1000\n",
      "1460/1460 [==============================] - 0s 201us/step - loss: 14.5492 - mae: 15.1564 - val_loss: 463.8126 - val_mae: 464.5042\n",
      "Epoch 120/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 13.8816 - mae: 14.4881 - val_loss: 488.5049 - val_mae: 489.1969\n",
      "Epoch 121/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 14.4484 - mae: 15.0588 - val_loss: 500.7908 - val_mae: 501.4835\n",
      "Epoch 122/1000\n",
      "1460/1460 [==============================] - 0s 214us/step - loss: 15.3269 - mae: 15.9424 - val_loss: 424.3851 - val_mae: 425.0780\n",
      "Epoch 123/1000\n",
      "1460/1460 [==============================] - 0s 201us/step - loss: 14.1980 - mae: 14.8061 - val_loss: 442.2176 - val_mae: 442.9107\n",
      "Epoch 124/1000\n",
      "1460/1460 [==============================] - 0s 201us/step - loss: 13.2676 - mae: 13.8784 - val_loss: 478.3104 - val_mae: 479.0029\n",
      "Epoch 125/1000\n",
      "1460/1460 [==============================] - 0s 201us/step - loss: 15.2385 - mae: 15.8535 - val_loss: 424.6782 - val_mae: 425.3706\n",
      "Epoch 126/1000\n",
      "1460/1460 [==============================] - 0s 205us/step - loss: 14.2486 - mae: 14.8615 - val_loss: 401.0835 - val_mae: 401.7756\n",
      "Epoch 127/1000\n",
      "1460/1460 [==============================] - 0s 214us/step - loss: 14.5115 - mae: 15.1257 - val_loss: 424.9860 - val_mae: 425.6769\n",
      "Epoch 128/1000\n",
      "1460/1460 [==============================] - 0s 255us/step - loss: 14.5514 - mae: 15.1682 - val_loss: 451.7099 - val_mae: 452.4021\n",
      "Epoch 129/1000\n",
      "1460/1460 [==============================] - 0s 232us/step - loss: 13.9418 - mae: 14.5467 - val_loss: 449.1025 - val_mae: 449.7954\n",
      "Epoch 130/1000\n",
      "1460/1460 [==============================] - 0s 222us/step - loss: 12.8212 - mae: 13.4340 - val_loss: 425.0353 - val_mae: 425.7273\n",
      "Epoch 131/1000\n",
      "1460/1460 [==============================] - 0s 230us/step - loss: 13.5063 - mae: 14.1123 - val_loss: 440.3836 - val_mae: 441.0762\n",
      "Epoch 132/1000\n",
      "1460/1460 [==============================] - 0s 278us/step - loss: 15.0247 - mae: 15.6345 - val_loss: 418.8238 - val_mae: 419.5161\n",
      "Epoch 133/1000\n",
      "1460/1460 [==============================] - 0s 222us/step - loss: 13.8150 - mae: 14.4328 - val_loss: 406.2887 - val_mae: 406.9818\n",
      "Epoch 134/1000\n",
      "1460/1460 [==============================] - 0s 204us/step - loss: 13.2769 - mae: 13.8813 - val_loss: 443.0391 - val_mae: 443.7322\n",
      "Epoch 135/1000\n",
      "1460/1460 [==============================] - 0s 202us/step - loss: 14.0606 - mae: 14.6718 - val_loss: 413.2259 - val_mae: 413.9179\n",
      "Epoch 136/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 13.4988 - mae: 14.1030 - val_loss: 440.9433 - val_mae: 441.6364\n",
      "Epoch 137/1000\n",
      "1460/1460 [==============================] - 0s 203us/step - loss: 13.4446 - mae: 14.0544 - val_loss: 407.4939 - val_mae: 408.1859\n",
      "Epoch 138/1000\n",
      "1460/1460 [==============================] - 0s 200us/step - loss: 13.5491 - mae: 14.1580 - val_loss: 446.3405 - val_mae: 447.0330\n",
      "Epoch 139/1000\n",
      "1460/1460 [==============================] - 0s 200us/step - loss: 13.6688 - mae: 14.2700 - val_loss: 409.5040 - val_mae: 410.1972\n",
      "Epoch 140/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 13.4841 - mae: 14.0967 - val_loss: 447.6665 - val_mae: 448.3592\n",
      "Epoch 141/1000\n",
      "1460/1460 [==============================] - 0s 233us/step - loss: 12.6345 - mae: 13.2387 - val_loss: 437.8537 - val_mae: 438.5468\n",
      "Epoch 142/1000\n",
      "1460/1460 [==============================] - 0s 203us/step - loss: 12.8492 - mae: 13.4529 - val_loss: 512.8828 - val_mae: 513.5758\n",
      "Epoch 143/1000\n",
      "1460/1460 [==============================] - 0s 226us/step - loss: 13.1101 - mae: 13.7089 - val_loss: 456.7901 - val_mae: 457.4814\n",
      "Epoch 144/1000\n",
      "1460/1460 [==============================] - 0s 214us/step - loss: 12.2604 - mae: 12.8618 - val_loss: 419.8690 - val_mae: 420.5621\n",
      "Epoch 145/1000\n",
      "1460/1460 [==============================] - 0s 214us/step - loss: 13.2583 - mae: 13.8593 - val_loss: 454.1805 - val_mae: 454.8734\n",
      "Epoch 146/1000\n",
      "1460/1460 [==============================] - 0s 220us/step - loss: 14.6487 - mae: 15.2659 - val_loss: 461.5675 - val_mae: 462.2602\n",
      "Epoch 147/1000\n",
      "1460/1460 [==============================] - 0s 223us/step - loss: 13.2963 - mae: 13.9032 - val_loss: 431.6502 - val_mae: 432.3427\n",
      "Epoch 148/1000\n",
      "1460/1460 [==============================] - 0s 216us/step - loss: 13.0369 - mae: 13.6426 - val_loss: 481.6850 - val_mae: 482.3773\n",
      "Epoch 149/1000\n",
      "1460/1460 [==============================] - 0s 202us/step - loss: 12.6668 - mae: 13.2664 - val_loss: 403.1052 - val_mae: 403.7976\n",
      "Epoch 150/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 12.7076 - mae: 13.3139 - val_loss: 435.5687 - val_mae: 436.2617\n",
      "Epoch 151/1000\n",
      "1460/1460 [==============================] - 0s 222us/step - loss: 12.9139 - mae: 13.5187 - val_loss: 463.4061 - val_mae: 464.0992\n",
      "Epoch 152/1000\n",
      "1460/1460 [==============================] - 0s 228us/step - loss: 13.0212 - mae: 13.6260 - val_loss: 484.3880 - val_mae: 485.0808\n",
      "Epoch 153/1000\n",
      "1460/1460 [==============================] - 0s 217us/step - loss: 12.9167 - mae: 13.5205 - val_loss: 470.1154 - val_mae: 470.8083\n",
      "Epoch 154/1000\n",
      "1460/1460 [==============================] - 0s 206us/step - loss: 12.1780 - mae: 12.7811 - val_loss: 436.2303 - val_mae: 436.9231\n",
      "Epoch 155/1000\n",
      "1460/1460 [==============================] - 0s 237us/step - loss: 13.1741 - mae: 13.7750 - val_loss: 429.8469 - val_mae: 430.5379\n",
      "Epoch 156/1000\n",
      "1460/1460 [==============================] - 0s 219us/step - loss: 13.3047 - mae: 13.9110 - val_loss: 454.6432 - val_mae: 455.3349\n",
      "Epoch 157/1000\n",
      "1460/1460 [==============================] - 0s 201us/step - loss: 13.5221 - mae: 14.1280 - val_loss: 420.0165 - val_mae: 420.7090\n",
      "Epoch 158/1000\n",
      "1460/1460 [==============================] - 0s 205us/step - loss: 12.3094 - mae: 12.9128 - val_loss: 484.5449 - val_mae: 485.2367\n",
      "Epoch 159/1000\n",
      "1460/1460 [==============================] - 0s 239us/step - loss: 11.4633 - mae: 12.0623 - val_loss: 466.5148 - val_mae: 467.2079\n",
      "Epoch 160/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 11.7929 - mae: 12.3948 - val_loss: 424.7935 - val_mae: 425.4861\n",
      "Epoch 161/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 12.5864 - mae: 13.1885 - val_loss: 398.2787 - val_mae: 398.9715\n",
      "Epoch 162/1000\n",
      "1460/1460 [==============================] - 0s 237us/step - loss: 13.1083 - mae: 13.7121 - val_loss: 421.2070 - val_mae: 421.8998\n",
      "Epoch 163/1000\n",
      "1460/1460 [==============================] - 0s 218us/step - loss: 12.7101 - mae: 13.3101 - val_loss: 417.0996 - val_mae: 417.7926\n",
      "Epoch 164/1000\n",
      "1460/1460 [==============================] - 0s 222us/step - loss: 13.0927 - mae: 13.7085 - val_loss: 414.1215 - val_mae: 414.8138\n",
      "Epoch 165/1000\n",
      "1460/1460 [==============================] - 0s 240us/step - loss: 11.6172 - mae: 12.2209 - val_loss: 431.6124 - val_mae: 432.3035\n",
      "Epoch 166/1000\n",
      "1460/1460 [==============================] - 0s 212us/step - loss: 12.4539 - mae: 13.0625 - val_loss: 462.1043 - val_mae: 462.7975\n",
      "Epoch 167/1000\n",
      "1460/1460 [==============================] - 0s 231us/step - loss: 12.3154 - mae: 12.9129 - val_loss: 421.3911 - val_mae: 422.0842\n",
      "Epoch 168/1000\n",
      "1460/1460 [==============================] - 0s 241us/step - loss: 11.7995 - mae: 12.3953 - val_loss: 493.0678 - val_mae: 493.7605\n",
      "Epoch 169/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460/1460 [==============================] - 0s 228us/step - loss: 13.0521 - mae: 13.6516 - val_loss: 437.8159 - val_mae: 438.5052\n",
      "Epoch 170/1000\n",
      "1460/1460 [==============================] - 0s 256us/step - loss: 12.2750 - mae: 12.8771 - val_loss: 407.0189 - val_mae: 407.7119\n",
      "Epoch 171/1000\n",
      "1460/1460 [==============================] - 0s 234us/step - loss: 11.8914 - mae: 12.4861 - val_loss: 450.9821 - val_mae: 451.6750\n",
      "Epoch 172/1000\n",
      "1460/1460 [==============================] - 0s 259us/step - loss: 11.7512 - mae: 12.3452 - val_loss: 467.1685 - val_mae: 467.8611\n",
      "Epoch 173/1000\n",
      "1460/1460 [==============================] - 0s 230us/step - loss: 12.2783 - mae: 12.8801 - val_loss: 454.2108 - val_mae: 454.9036\n",
      "Epoch 174/1000\n",
      "1460/1460 [==============================] - 0s 233us/step - loss: 13.9286 - mae: 14.5373 - val_loss: 429.5999 - val_mae: 430.2927\n",
      "Epoch 175/1000\n",
      "1460/1460 [==============================] - 0s 226us/step - loss: 11.5844 - mae: 12.1831 - val_loss: 466.4275 - val_mae: 467.1188\n",
      "Epoch 176/1000\n",
      "1460/1460 [==============================] - 0s 242us/step - loss: 11.4175 - mae: 12.0222 - val_loss: 476.8099 - val_mae: 477.5020\n",
      "Epoch 177/1000\n",
      "1460/1460 [==============================] - 0s 236us/step - loss: 11.3460 - mae: 11.9505 - val_loss: 437.8942 - val_mae: 438.5873\n",
      "Epoch 178/1000\n",
      "1460/1460 [==============================] - 0s 229us/step - loss: 11.5473 - mae: 12.1461 - val_loss: 429.0216 - val_mae: 429.7145\n",
      "Epoch 179/1000\n",
      "1460/1460 [==============================] - 0s 255us/step - loss: 11.3561 - mae: 11.9604 - val_loss: 422.0940 - val_mae: 422.7850\n",
      "Epoch 180/1000\n",
      "1460/1460 [==============================] - 0s 268us/step - loss: 12.6562 - mae: 13.2604 - val_loss: 428.3841 - val_mae: 429.0772\n",
      "Epoch 181/1000\n",
      "1460/1460 [==============================] - 0s 238us/step - loss: 12.1265 - mae: 12.7214 - val_loss: 434.2528 - val_mae: 434.9455\n",
      "Epoch 182/1000\n",
      "1460/1460 [==============================] - 0s 246us/step - loss: 12.2819 - mae: 12.8866 - val_loss: 414.6772 - val_mae: 415.3698\n",
      "Epoch 183/1000\n",
      "1460/1460 [==============================] - 0s 231us/step - loss: 12.5376 - mae: 13.1415 - val_loss: 427.6375 - val_mae: 428.3292\n",
      "Epoch 184/1000\n",
      "1460/1460 [==============================] - 0s 223us/step - loss: 12.8365 - mae: 13.4420 - val_loss: 501.6707 - val_mae: 502.3638\n",
      "Epoch 185/1000\n",
      "1460/1460 [==============================] - 0s 231us/step - loss: 11.6537 - mae: 12.2582 - val_loss: 457.4818 - val_mae: 458.1748\n",
      "Epoch 186/1000\n",
      "1460/1460 [==============================] - 0s 237us/step - loss: 11.3843 - mae: 11.9933 - val_loss: 432.0314 - val_mae: 432.7234\n",
      "Epoch 187/1000\n",
      "1460/1460 [==============================] - 0s 225us/step - loss: 10.2236 - mae: 10.8097 - val_loss: 400.4783 - val_mae: 401.1693\n",
      "Epoch 188/1000\n",
      "1460/1460 [==============================] - 0s 232us/step - loss: 13.9400 - mae: 14.5402 - val_loss: 421.5340 - val_mae: 422.2261\n",
      "Epoch 189/1000\n",
      "1460/1460 [==============================] - 0s 234us/step - loss: 13.3681 - mae: 13.9838 - val_loss: 402.6967 - val_mae: 403.3884\n",
      "Epoch 190/1000\n",
      "1460/1460 [==============================] - 0s 231us/step - loss: 11.3065 - mae: 11.9085 - val_loss: 476.7196 - val_mae: 477.4127\n",
      "Epoch 191/1000\n",
      "1460/1460 [==============================] - 0s 228us/step - loss: 12.9385 - mae: 13.5356 - val_loss: 482.2441 - val_mae: 482.9373\n",
      "Epoch 192/1000\n",
      "1460/1460 [==============================] - 0s 225us/step - loss: 11.6613 - mae: 12.2522 - val_loss: 409.4397 - val_mae: 410.1323\n",
      "Epoch 193/1000\n",
      "1460/1460 [==============================] - 0s 228us/step - loss: 10.3536 - mae: 10.9438 - val_loss: 469.8348 - val_mae: 470.5272\n",
      "Epoch 194/1000\n",
      "1460/1460 [==============================] - 0s 228us/step - loss: 10.7427 - mae: 11.3326 - val_loss: 436.8298 - val_mae: 437.5214\n",
      "Epoch 195/1000\n",
      "1460/1460 [==============================] - 0s 222us/step - loss: 11.0869 - mae: 11.6847 - val_loss: 439.2133 - val_mae: 439.9064\n",
      "Epoch 196/1000\n",
      "1460/1460 [==============================] - 0s 226us/step - loss: 11.1264 - mae: 11.7176 - val_loss: 401.6848 - val_mae: 402.3779\n",
      "Epoch 197/1000\n",
      "1460/1460 [==============================] - 0s 232us/step - loss: 11.3910 - mae: 11.9852 - val_loss: 454.0215 - val_mae: 454.7124\n",
      "Epoch 198/1000\n",
      "1460/1460 [==============================] - 0s 239us/step - loss: 10.6510 - mae: 11.2469 - val_loss: 411.9135 - val_mae: 412.6056\n",
      "Epoch 199/1000\n",
      "1460/1460 [==============================] - 0s 226us/step - loss: 12.9564 - mae: 13.5595 - val_loss: 424.7012 - val_mae: 425.3944\n",
      "Epoch 200/1000\n",
      "1460/1460 [==============================] - 0s 216us/step - loss: 11.6258 - mae: 12.2367 - val_loss: 485.7294 - val_mae: 486.4222\n",
      "Epoch 201/1000\n",
      "1460/1460 [==============================] - 0s 233us/step - loss: 11.5881 - mae: 12.1886 - val_loss: 462.3638 - val_mae: 463.0563\n",
      "Epoch 202/1000\n",
      "1460/1460 [==============================] - 0s 221us/step - loss: 12.8440 - mae: 13.4520 - val_loss: 464.6091 - val_mae: 465.3002\n",
      "Epoch 203/1000\n",
      "1460/1460 [==============================] - 0s 235us/step - loss: 11.8722 - mae: 12.4763 - val_loss: 466.6915 - val_mae: 467.3841\n",
      "Epoch 204/1000\n",
      "1460/1460 [==============================] - 0s 237us/step - loss: 10.4910 - mae: 11.0756 - val_loss: 473.5794 - val_mae: 474.2694\n",
      "Epoch 205/1000\n",
      "1460/1460 [==============================] - 0s 224us/step - loss: 10.5478 - mae: 11.1440 - val_loss: 436.0254 - val_mae: 436.7162\n",
      "Epoch 206/1000\n",
      "1460/1460 [==============================] - 0s 255us/step - loss: 12.0403 - mae: 12.6350 - val_loss: 440.1239 - val_mae: 440.8168\n",
      "Epoch 207/1000\n",
      "1460/1460 [==============================] - 0s 220us/step - loss: 11.7363 - mae: 12.3426 - val_loss: 485.2135 - val_mae: 485.9063\n",
      "Epoch 208/1000\n",
      "1460/1460 [==============================] - 0s 215us/step - loss: 10.8679 - mae: 11.4607 - val_loss: 443.2860 - val_mae: 443.9789\n",
      "Epoch 209/1000\n",
      "1460/1460 [==============================] - 0s 227us/step - loss: 10.4393 - mae: 11.0330 - val_loss: 423.8867 - val_mae: 424.5787\n",
      "Epoch 210/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 10.5652 - mae: 11.1631 - val_loss: 460.9699 - val_mae: 461.6620\n",
      "Epoch 211/1000\n",
      "1460/1460 [==============================] - 0s 225us/step - loss: 10.5398 - mae: 11.1343 - val_loss: 473.6564 - val_mae: 474.3495\n",
      "Epoch 212/1000\n",
      "1460/1460 [==============================] - 0s 288us/step - loss: 10.8397 - mae: 11.4366 - val_loss: 459.7716 - val_mae: 460.4641\n",
      "Epoch 213/1000\n",
      "1460/1460 [==============================] - 0s 255us/step - loss: 10.9470 - mae: 11.5427 - val_loss: 391.1301 - val_mae: 391.8232\n",
      "Epoch 214/1000\n",
      "1460/1460 [==============================] - 0s 342us/step - loss: 10.9165 - mae: 11.5159 - val_loss: 460.9144 - val_mae: 461.6052\n",
      "Epoch 215/1000\n",
      "1460/1460 [==============================] - 0s 289us/step - loss: 10.7280 - mae: 11.3225 - val_loss: 440.6028 - val_mae: 441.2944\n",
      "Epoch 216/1000\n",
      "1460/1460 [==============================] - 0s 255us/step - loss: 10.7095 - mae: 11.2993 - val_loss: 464.3825 - val_mae: 465.0749\n",
      "Epoch 217/1000\n",
      "1460/1460 [==============================] - 0s 249us/step - loss: 9.8197 - mae: 10.4039 - val_loss: 474.7612 - val_mae: 475.4543\n",
      "Epoch 218/1000\n",
      "1460/1460 [==============================] - 0s 236us/step - loss: 10.0660 - mae: 10.6477 - val_loss: 450.7707 - val_mae: 451.4630\n",
      "Epoch 219/1000\n",
      "1460/1460 [==============================] - 0s 241us/step - loss: 10.1648 - mae: 10.7492 - val_loss: 477.5072 - val_mae: 478.1978\n",
      "Epoch 220/1000\n",
      "1460/1460 [==============================] - 0s 293us/step - loss: 10.3355 - mae: 10.9344 - val_loss: 448.0763 - val_mae: 448.7680\n",
      "Epoch 221/1000\n",
      "1460/1460 [==============================] - 0s 246us/step - loss: 10.9927 - mae: 11.5816 - val_loss: 437.6257 - val_mae: 438.3188\n",
      "Epoch 222/1000\n",
      "1460/1460 [==============================] - 0s 278us/step - loss: 10.0431 - mae: 10.6340 - val_loss: 474.6425 - val_mae: 475.3334\n",
      "Epoch 223/1000\n",
      "1460/1460 [==============================] - 3s 2ms/step - loss: 9.8355 - mae: 10.4291 - val_loss: 495.8503 - val_mae: 496.5435\n",
      "Epoch 224/1000\n",
      "1460/1460 [==============================] - 0s 228us/step - loss: 10.1759 - mae: 10.7679 - val_loss: 456.4972 - val_mae: 457.1880\n",
      "Epoch 225/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460/1460 [==============================] - 0s 251us/step - loss: 10.0784 - mae: 10.6696 - val_loss: 466.5003 - val_mae: 467.1933\n",
      "Epoch 226/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 9.9146 - mae: 10.5040 - val_loss: 442.4989 - val_mae: 443.1919\n",
      "Epoch 227/1000\n",
      "1460/1460 [==============================] - 0s 214us/step - loss: 9.6549 - mae: 10.2404 - val_loss: 457.2556 - val_mae: 457.9480\n",
      "Epoch 228/1000\n",
      "1460/1460 [==============================] - 0s 229us/step - loss: 9.5905 - mae: 10.1715 - val_loss: 465.0633 - val_mae: 465.7557\n",
      "Epoch 229/1000\n",
      "1460/1460 [==============================] - 0s 246us/step - loss: 10.7152 - mae: 11.3037 - val_loss: 448.9283 - val_mae: 449.6210\n",
      "Epoch 230/1000\n",
      "1460/1460 [==============================] - 0s 247us/step - loss: 10.6087 - mae: 11.2024 - val_loss: 458.5509 - val_mae: 459.2439\n",
      "Epoch 231/1000\n",
      "1460/1460 [==============================] - 1s 409us/step - loss: 9.8133 - mae: 10.4048 - val_loss: 459.4992 - val_mae: 460.1895\n",
      "Epoch 232/1000\n",
      "1460/1460 [==============================] - 0s 221us/step - loss: 10.5995 - mae: 11.2023 - val_loss: 450.2322 - val_mae: 450.9246\n",
      "Epoch 233/1000\n",
      "1460/1460 [==============================] - 0s 223us/step - loss: 10.2836 - mae: 10.8651 - val_loss: 483.4634 - val_mae: 484.1566\n",
      "Epoch 234/1000\n",
      "1460/1460 [==============================] - 0s 267us/step - loss: 10.6993 - mae: 11.2871 - val_loss: 444.4772 - val_mae: 445.1698\n",
      "Epoch 235/1000\n",
      "1460/1460 [==============================] - 0s 225us/step - loss: 9.6349 - mae: 10.2378 - val_loss: 469.3832 - val_mae: 470.0758\n",
      "Epoch 236/1000\n",
      "1460/1460 [==============================] - 0s 227us/step - loss: 9.8174 - mae: 10.4117 - val_loss: 469.6443 - val_mae: 470.3371\n",
      "Epoch 237/1000\n",
      "1460/1460 [==============================] - 0s 226us/step - loss: 9.8387 - mae: 10.4284 - val_loss: 508.0071 - val_mae: 508.7002\n",
      "Epoch 238/1000\n",
      "1460/1460 [==============================] - 0s 259us/step - loss: 9.8827 - mae: 10.4675 - val_loss: 465.1352 - val_mae: 465.8277\n",
      "Epoch 239/1000\n",
      "1460/1460 [==============================] - 0s 231us/step - loss: 9.6642 - mae: 10.2644 - val_loss: 460.9151 - val_mae: 461.6053\n",
      "Epoch 240/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 9.4010 - mae: 9.9852 - val_loss: 490.2389 - val_mae: 490.9306\n",
      "Epoch 241/1000\n",
      "1460/1460 [==============================] - 0s 297us/step - loss: 9.6577 - mae: 10.2328 - val_loss: 480.4972 - val_mae: 481.1876\n",
      "Epoch 242/1000\n",
      "1460/1460 [==============================] - 0s 296us/step - loss: 10.1946 - mae: 10.7861 - val_loss: 438.8932 - val_mae: 439.5859\n",
      "Epoch 243/1000\n",
      "1460/1460 [==============================] - 0s 220us/step - loss: 9.8823 - mae: 10.4738 - val_loss: 469.5650 - val_mae: 470.2559\n",
      "Epoch 244/1000\n",
      "1460/1460 [==============================] - 0s 205us/step - loss: 10.5468 - mae: 11.1276 - val_loss: 457.9306 - val_mae: 458.6227\n",
      "Epoch 245/1000\n",
      "1460/1460 [==============================] - 0s 203us/step - loss: 9.6108 - mae: 10.1926 - val_loss: 432.4275 - val_mae: 433.1199\n",
      "Epoch 246/1000\n",
      "1460/1460 [==============================] - 0s 224us/step - loss: 10.3001 - mae: 10.8744 - val_loss: 449.7045 - val_mae: 450.3976\n",
      "Epoch 247/1000\n",
      "1460/1460 [==============================] - 0s 215us/step - loss: 9.8911 - mae: 10.4750 - val_loss: 469.1161 - val_mae: 469.8091\n",
      "Epoch 248/1000\n",
      "1460/1460 [==============================] - 0s 232us/step - loss: 9.8419 - mae: 10.4332 - val_loss: 462.8642 - val_mae: 463.5560\n",
      "Epoch 249/1000\n",
      "1460/1460 [==============================] - 0s 217us/step - loss: 9.6930 - mae: 10.2854 - val_loss: 465.7988 - val_mae: 466.4909\n",
      "Epoch 250/1000\n",
      "1460/1460 [==============================] - 0s 231us/step - loss: 9.8048 - mae: 10.4042 - val_loss: 524.3929 - val_mae: 525.0861\n",
      "Epoch 251/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 10.1241 - mae: 10.7148 - val_loss: 439.6927 - val_mae: 440.3830\n",
      "Epoch 252/1000\n",
      "1460/1460 [==============================] - 0s 220us/step - loss: 9.2367 - mae: 9.8071 - val_loss: 427.3780 - val_mae: 428.0706\n",
      "Epoch 253/1000\n",
      "1460/1460 [==============================] - 0s 235us/step - loss: 10.3254 - mae: 10.9070 - val_loss: 444.8241 - val_mae: 445.5172\n",
      "Epoch 254/1000\n",
      "1460/1460 [==============================] - 0s 263us/step - loss: 9.6289 - mae: 10.2131 - val_loss: 440.4510 - val_mae: 441.1440\n",
      "Epoch 255/1000\n",
      "1460/1460 [==============================] - 0s 278us/step - loss: 9.9957 - mae: 10.5748 - val_loss: 432.9061 - val_mae: 433.5984\n",
      "Epoch 256/1000\n",
      "1460/1460 [==============================] - 0s 224us/step - loss: 10.0033 - mae: 10.5830 - val_loss: 465.0238 - val_mae: 465.7169\n",
      "Epoch 257/1000\n",
      "1460/1460 [==============================] - 0s 233us/step - loss: 9.9883 - mae: 10.5716 - val_loss: 518.9591 - val_mae: 519.6517\n",
      "Epoch 258/1000\n",
      "1460/1460 [==============================] - 0s 264us/step - loss: 9.9037 - mae: 10.4916 - val_loss: 516.8376 - val_mae: 517.5295\n",
      "Epoch 259/1000\n",
      "1460/1460 [==============================] - 0s 258us/step - loss: 9.7912 - mae: 10.3909 - val_loss: 473.2329 - val_mae: 473.9257\n",
      "Epoch 260/1000\n",
      "1460/1460 [==============================] - 0s 233us/step - loss: 9.1172 - mae: 9.6969 - val_loss: 419.4099 - val_mae: 420.1012\n",
      "Epoch 261/1000\n",
      "1460/1460 [==============================] - 0s 234us/step - loss: 9.4896 - mae: 10.0755 - val_loss: 452.1347 - val_mae: 452.8275\n",
      "Epoch 262/1000\n",
      "1460/1460 [==============================] - 0s 235us/step - loss: 9.1085 - mae: 9.6886 - val_loss: 444.0533 - val_mae: 444.7458\n",
      "Epoch 263/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 9.9826 - mae: 10.5658 - val_loss: 471.2560 - val_mae: 471.9483\n",
      "Epoch 264/1000\n",
      "1460/1460 [==============================] - 0s 231us/step - loss: 9.3410 - mae: 9.9182 - val_loss: 502.9869 - val_mae: 503.6791\n",
      "Epoch 265/1000\n",
      "1460/1460 [==============================] - 0s 216us/step - loss: 8.9096 - mae: 9.4963 - val_loss: 519.6258 - val_mae: 520.3178\n",
      "Epoch 266/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 9.1733 - mae: 9.7529 - val_loss: 459.2547 - val_mae: 459.9479\n",
      "Epoch 267/1000\n",
      "1460/1460 [==============================] - 0s 214us/step - loss: 8.3395 - mae: 8.9157 - val_loss: 496.7721 - val_mae: 497.4634\n",
      "Epoch 268/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 9.3977 - mae: 9.9893 - val_loss: 482.9444 - val_mae: 483.6357\n",
      "Epoch 269/1000\n",
      "1460/1460 [==============================] - 0s 215us/step - loss: 10.2533 - mae: 10.8470 - val_loss: 477.3482 - val_mae: 478.0412\n",
      "Epoch 270/1000\n",
      "1460/1460 [==============================] - 0s 214us/step - loss: 8.7689 - mae: 9.3473 - val_loss: 493.5796 - val_mae: 494.2727\n",
      "Epoch 271/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 9.2889 - mae: 9.8744 - val_loss: 506.2849 - val_mae: 506.9768\n",
      "Epoch 272/1000\n",
      "1460/1460 [==============================] - 0s 212us/step - loss: 9.4230 - mae: 10.0095 - val_loss: 459.5133 - val_mae: 460.2054\n",
      "Epoch 273/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 9.4699 - mae: 10.0551 - val_loss: 495.6350 - val_mae: 496.3280\n",
      "Epoch 274/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 8.6087 - mae: 9.1910 - val_loss: 508.9628 - val_mae: 509.6558\n",
      "Epoch 275/1000\n",
      "1460/1460 [==============================] - 0s 217us/step - loss: 9.3928 - mae: 9.9703 - val_loss: 492.0673 - val_mae: 492.7603\n",
      "Epoch 276/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 9.5206 - mae: 10.1207 - val_loss: 454.2517 - val_mae: 454.9433\n",
      "Epoch 277/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 9.9174 - mae: 10.5058 - val_loss: 449.4508 - val_mae: 450.1435\n",
      "Epoch 278/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 9.0362 - mae: 9.6112 - val_loss: 460.9877 - val_mae: 461.6797\n",
      "Epoch 279/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 9.4114 - mae: 9.9951 - val_loss: 513.6972 - val_mae: 514.3885\n",
      "Epoch 280/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 8.8889 - mae: 9.4694 - val_loss: 474.5683 - val_mae: 475.2609\n",
      "Epoch 281/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460/1460 [==============================] - 0s 209us/step - loss: 8.6984 - mae: 9.2810 - val_loss: 490.0085 - val_mae: 490.7015\n",
      "Epoch 282/1000\n",
      "1460/1460 [==============================] - 0s 205us/step - loss: 9.0864 - mae: 9.6613 - val_loss: 491.6849 - val_mae: 492.3770\n",
      "Epoch 283/1000\n",
      "1460/1460 [==============================] - 0s 214us/step - loss: 9.2779 - mae: 9.8635 - val_loss: 501.4242 - val_mae: 502.1167\n",
      "Epoch 284/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 8.3711 - mae: 8.9543 - val_loss: 467.1377 - val_mae: 467.8297\n",
      "Epoch 285/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 8.2055 - mae: 8.7853 - val_loss: 485.8226 - val_mae: 486.5157\n",
      "Epoch 286/1000\n",
      "1460/1460 [==============================] - 0s 204us/step - loss: 8.2510 - mae: 8.8321 - val_loss: 485.6477 - val_mae: 486.3376\n",
      "Epoch 287/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 9.2387 - mae: 9.8199 - val_loss: 505.9497 - val_mae: 506.6413\n",
      "Epoch 288/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 8.6349 - mae: 9.2132 - val_loss: 482.8384 - val_mae: 483.5301\n",
      "Epoch 289/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 8.5488 - mae: 9.1284 - val_loss: 507.9166 - val_mae: 508.6095\n",
      "Epoch 290/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 8.9311 - mae: 9.5216 - val_loss: 517.0017 - val_mae: 517.6934\n",
      "Epoch 291/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 9.4177 - mae: 10.0067 - val_loss: 462.5377 - val_mae: 463.2296\n",
      "Epoch 292/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 9.6703 - mae: 10.2500 - val_loss: 438.4814 - val_mae: 439.1744\n",
      "Epoch 293/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 9.3070 - mae: 9.8943 - val_loss: 503.9244 - val_mae: 504.6165\n",
      "Epoch 294/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 8.4669 - mae: 9.0429 - val_loss: 491.0809 - val_mae: 491.7732\n",
      "Epoch 295/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 8.5226 - mae: 9.0939 - val_loss: 501.8904 - val_mae: 502.5828\n",
      "Epoch 296/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 9.2644 - mae: 9.8502 - val_loss: 512.6538 - val_mae: 513.3465\n",
      "Epoch 297/1000\n",
      "1460/1460 [==============================] - 0s 233us/step - loss: 10.6785 - mae: 11.2715 - val_loss: 519.3644 - val_mae: 520.0573\n",
      "Epoch 298/1000\n",
      "1460/1460 [==============================] - 0s 229us/step - loss: 8.3163 - mae: 8.8978 - val_loss: 464.4819 - val_mae: 465.1743\n",
      "Epoch 299/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 7.9647 - mae: 8.5365 - val_loss: 494.0837 - val_mae: 494.7764\n",
      "Epoch 300/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 9.1197 - mae: 9.7051 - val_loss: 516.6564 - val_mae: 517.3491\n",
      "Epoch 301/1000\n",
      "1460/1460 [==============================] - 0s 212us/step - loss: 8.5452 - mae: 9.1156 - val_loss: 514.6908 - val_mae: 515.3838\n",
      "Epoch 302/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 8.6526 - mae: 9.2344 - val_loss: 523.4949 - val_mae: 524.1869\n",
      "Epoch 303/1000\n",
      "1460/1460 [==============================] - 0s 212us/step - loss: 8.9471 - mae: 9.5171 - val_loss: 473.1542 - val_mae: 473.8451\n",
      "Epoch 304/1000\n",
      "1460/1460 [==============================] - 0s 212us/step - loss: 8.7543 - mae: 9.3276 - val_loss: 475.6107 - val_mae: 476.3039\n",
      "Epoch 305/1000\n",
      "1460/1460 [==============================] - 0s 212us/step - loss: 8.0868 - mae: 8.6560 - val_loss: 467.6511 - val_mae: 468.3439\n",
      "Epoch 306/1000\n",
      "1460/1460 [==============================] - 0s 212us/step - loss: 8.4864 - mae: 9.0588 - val_loss: 524.3516 - val_mae: 525.0448\n",
      "Epoch 307/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 8.4898 - mae: 9.0823 - val_loss: 467.9791 - val_mae: 468.6718\n",
      "Epoch 308/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 8.8476 - mae: 9.4284 - val_loss: 469.4407 - val_mae: 470.1325\n",
      "Epoch 309/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 7.9937 - mae: 8.5761 - val_loss: 477.5619 - val_mae: 478.2527\n",
      "Epoch 310/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 8.5434 - mae: 9.1155 - val_loss: 450.0632 - val_mae: 450.7544\n",
      "Epoch 311/1000\n",
      "1460/1460 [==============================] - 0s 215us/step - loss: 8.4286 - mae: 9.0049 - val_loss: 480.0144 - val_mae: 480.7063\n",
      "Epoch 312/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 7.5830 - mae: 8.1522 - val_loss: 486.5518 - val_mae: 487.2448\n",
      "Epoch 313/1000\n",
      "1460/1460 [==============================] - 0s 215us/step - loss: 8.5124 - mae: 9.0805 - val_loss: 493.0032 - val_mae: 493.6961\n",
      "Epoch 314/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 7.8420 - mae: 8.4101 - val_loss: 496.5167 - val_mae: 497.2080\n",
      "Epoch 315/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 7.6329 - mae: 8.2030 - val_loss: 460.0416 - val_mae: 460.7345\n",
      "Epoch 316/1000\n",
      "1460/1460 [==============================] - 0s 216us/step - loss: 7.5330 - mae: 8.1051 - val_loss: 482.4754 - val_mae: 483.1685\n",
      "Epoch 317/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 7.8952 - mae: 8.4638 - val_loss: 499.6111 - val_mae: 500.3041\n",
      "Epoch 318/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 8.2999 - mae: 8.8600 - val_loss: 498.1501 - val_mae: 498.8413\n",
      "Epoch 319/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 10.0189 - mae: 10.5941 - val_loss: 483.7639 - val_mae: 484.4569\n",
      "Epoch 320/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 7.6072 - mae: 8.1677 - val_loss: 507.1518 - val_mae: 507.8443\n",
      "Epoch 321/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 8.1163 - mae: 8.6855 - val_loss: 509.5924 - val_mae: 510.2849\n",
      "Epoch 322/1000\n",
      "1460/1460 [==============================] - 0s 212us/step - loss: 10.0924 - mae: 10.6708 - val_loss: 481.1372 - val_mae: 481.8279\n",
      "Epoch 323/1000\n",
      "1460/1460 [==============================] - 0s 216us/step - loss: 8.6516 - mae: 9.2305 - val_loss: 486.6432 - val_mae: 487.3353\n",
      "Epoch 324/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 9.3046 - mae: 9.8785 - val_loss: 534.0239 - val_mae: 534.7152\n",
      "Epoch 325/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 8.9408 - mae: 9.5139 - val_loss: 436.5993 - val_mae: 437.2907\n",
      "Epoch 326/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 8.1812 - mae: 8.7611 - val_loss: 515.8098 - val_mae: 516.5020\n",
      "Epoch 327/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 7.9406 - mae: 8.5175 - val_loss: 500.7215 - val_mae: 501.4123\n",
      "Epoch 328/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 7.9667 - mae: 8.5371 - val_loss: 541.6877 - val_mae: 542.3806\n",
      "Epoch 329/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 8.6090 - mae: 9.1815 - val_loss: 543.4547 - val_mae: 544.1475\n",
      "Epoch 330/1000\n",
      "1460/1460 [==============================] - 0s 229us/step - loss: 7.8142 - mae: 8.3907 - val_loss: 524.5882 - val_mae: 525.2804\n",
      "Epoch 331/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 7.7695 - mae: 8.3294 - val_loss: 515.1729 - val_mae: 515.8658\n",
      "Epoch 332/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 8.4760 - mae: 9.0528 - val_loss: 530.3163 - val_mae: 531.0085\n",
      "Epoch 333/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 7.5113 - mae: 8.0743 - val_loss: 496.5283 - val_mae: 497.2215\n",
      "Epoch 334/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 7.8009 - mae: 8.3731 - val_loss: 524.1395 - val_mae: 524.8298\n",
      "Epoch 335/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 8.5254 - mae: 9.1028 - val_loss: 493.5803 - val_mae: 494.2731\n",
      "Epoch 336/1000\n",
      "1460/1460 [==============================] - 0s 220us/step - loss: 8.1595 - mae: 8.7378 - val_loss: 483.0272 - val_mae: 483.7196\n",
      "Epoch 337/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 8.3022 - mae: 8.8769 - val_loss: 516.1236 - val_mae: 516.8156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 338/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 8.1257 - mae: 8.6901 - val_loss: 440.1553 - val_mae: 440.8454\n",
      "Epoch 339/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 8.7976 - mae: 9.3770 - val_loss: 479.7632 - val_mae: 480.4549\n",
      "Epoch 340/1000\n",
      "1460/1460 [==============================] - 0s 212us/step - loss: 7.4802 - mae: 8.0443 - val_loss: 493.4466 - val_mae: 494.1384\n",
      "Epoch 341/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 8.2579 - mae: 8.8270 - val_loss: 494.8182 - val_mae: 495.5106\n",
      "Epoch 342/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 7.5516 - mae: 8.1293 - val_loss: 472.3830 - val_mae: 473.0740\n",
      "Epoch 343/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 7.8420 - mae: 8.4038 - val_loss: 490.0684 - val_mae: 490.7589\n",
      "Epoch 344/1000\n",
      "1460/1460 [==============================] - 0s 216us/step - loss: 7.9026 - mae: 8.4829 - val_loss: 487.4447 - val_mae: 488.1376\n",
      "Epoch 345/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 7.4351 - mae: 8.0037 - val_loss: 493.0547 - val_mae: 493.7478\n",
      "Epoch 346/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 7.2507 - mae: 7.8109 - val_loss: 478.1870 - val_mae: 478.8797\n",
      "Epoch 347/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 8.4155 - mae: 8.9831 - val_loss: 505.5724 - val_mae: 506.2649\n",
      "Epoch 348/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 7.7981 - mae: 8.3596 - val_loss: 528.3128 - val_mae: 529.0037\n",
      "Epoch 349/1000\n",
      "1460/1460 [==============================] - 0s 228us/step - loss: 8.1410 - mae: 8.7100 - val_loss: 503.9066 - val_mae: 504.5995\n",
      "Epoch 350/1000\n",
      "1460/1460 [==============================] - 0s 221us/step - loss: 7.9145 - mae: 8.4899 - val_loss: 476.8794 - val_mae: 477.5717\n",
      "Epoch 351/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 7.2893 - mae: 7.8619 - val_loss: 466.6468 - val_mae: 467.3381\n",
      "Epoch 352/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 7.0833 - mae: 7.6537 - val_loss: 483.5487 - val_mae: 484.2418\n",
      "Epoch 353/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 7.2951 - mae: 7.8651 - val_loss: 520.2803 - val_mae: 520.9698\n",
      "Epoch 354/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 7.7001 - mae: 8.2641 - val_loss: 520.5236 - val_mae: 521.2166\n",
      "Epoch 355/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 8.9440 - mae: 9.5187 - val_loss: 514.9836 - val_mae: 515.6765\n",
      "Epoch 356/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 8.9548 - mae: 9.5341 - val_loss: 520.0774 - val_mae: 520.7703\n",
      "Epoch 357/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 7.8297 - mae: 8.4034 - val_loss: 482.4928 - val_mae: 483.1844\n",
      "Epoch 358/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 8.5645 - mae: 9.1447 - val_loss: 481.8585 - val_mae: 482.5504\n",
      "Epoch 359/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 7.2814 - mae: 7.8535 - val_loss: 488.1033 - val_mae: 488.7958\n",
      "Epoch 360/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 7.4994 - mae: 8.0751 - val_loss: 484.4079 - val_mae: 485.1005\n",
      "Epoch 361/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 7.7943 - mae: 8.3606 - val_loss: 485.9001 - val_mae: 486.5925\n",
      "Epoch 362/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 7.1798 - mae: 7.7446 - val_loss: 480.4603 - val_mae: 481.1512\n",
      "Epoch 363/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 8.9640 - mae: 9.5503 - val_loss: 441.9662 - val_mae: 442.6580\n",
      "Epoch 364/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 9.5781 - mae: 10.1612 - val_loss: 422.2397 - val_mae: 422.9328\n",
      "Epoch 365/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 8.1813 - mae: 8.7526 - val_loss: 495.6741 - val_mae: 496.3664\n",
      "Epoch 366/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 8.0047 - mae: 8.5688 - val_loss: 437.9993 - val_mae: 438.6920\n",
      "Epoch 367/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 8.9302 - mae: 9.5084 - val_loss: 546.6457 - val_mae: 547.3378\n",
      "Epoch 368/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 8.4910 - mae: 9.0646 - val_loss: 468.1068 - val_mae: 468.7989\n",
      "Epoch 369/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 7.3440 - mae: 7.9103 - val_loss: 509.2511 - val_mae: 509.9429\n",
      "Epoch 370/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 6.5363 - mae: 7.1070 - val_loss: 535.0323 - val_mae: 535.7252\n",
      "Epoch 371/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 7.5246 - mae: 8.0960 - val_loss: 473.0829 - val_mae: 473.7741\n",
      "Epoch 372/1000\n",
      "1460/1460 [==============================] - 0s 214us/step - loss: 8.5536 - mae: 9.1182 - val_loss: 441.9515 - val_mae: 442.6441\n",
      "Epoch 373/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 8.4392 - mae: 9.0033 - val_loss: 443.1977 - val_mae: 443.8903\n",
      "Epoch 374/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 7.3869 - mae: 7.9499 - val_loss: 482.0895 - val_mae: 482.7814\n",
      "Epoch 375/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 7.0008 - mae: 7.5795 - val_loss: 500.4124 - val_mae: 501.1050\n",
      "Epoch 376/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 6.5286 - mae: 7.0948 - val_loss: 541.2128 - val_mae: 541.9053\n",
      "Epoch 377/1000\n",
      "1460/1460 [==============================] - 0s 206us/step - loss: 6.6815 - mae: 7.2345 - val_loss: 504.1637 - val_mae: 504.8556\n",
      "Epoch 378/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 7.0186 - mae: 7.5950 - val_loss: 497.3827 - val_mae: 498.0748\n",
      "Epoch 379/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 7.3923 - mae: 7.9584 - val_loss: 479.7911 - val_mae: 480.4819\n",
      "Epoch 380/1000\n",
      "1460/1460 [==============================] - 0s 205us/step - loss: 6.9025 - mae: 7.4603 - val_loss: 540.7159 - val_mae: 541.4077\n",
      "Epoch 381/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 7.0887 - mae: 7.6452 - val_loss: 513.2825 - val_mae: 513.9754\n",
      "Epoch 382/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 7.5348 - mae: 8.0904 - val_loss: 500.9088 - val_mae: 501.6008\n",
      "Epoch 383/1000\n",
      "1460/1460 [==============================] - 0s 212us/step - loss: 8.9982 - mae: 9.5661 - val_loss: 445.5616 - val_mae: 446.2538\n",
      "Epoch 384/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 7.0580 - mae: 7.6188 - val_loss: 496.9644 - val_mae: 497.6562\n",
      "Epoch 385/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 6.7543 - mae: 7.3122 - val_loss: 507.9247 - val_mae: 508.6165\n",
      "Epoch 386/1000\n",
      "1460/1460 [==============================] - 0s 206us/step - loss: 7.4950 - mae: 8.0643 - val_loss: 449.5094 - val_mae: 450.2023\n",
      "Epoch 387/1000\n",
      "1460/1460 [==============================] - 0s 206us/step - loss: 7.5967 - mae: 8.1626 - val_loss: 504.0950 - val_mae: 504.7865\n",
      "Epoch 388/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 7.9711 - mae: 8.5301 - val_loss: 476.2195 - val_mae: 476.9112\n",
      "Epoch 389/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 8.2570 - mae: 8.8372 - val_loss: 460.8338 - val_mae: 461.5249\n",
      "Epoch 390/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 7.3079 - mae: 7.8821 - val_loss: 447.5828 - val_mae: 448.2747\n",
      "Epoch 391/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 7.5655 - mae: 8.1312 - val_loss: 498.2571 - val_mae: 498.9499\n",
      "Epoch 392/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 6.8914 - mae: 7.4505 - val_loss: 552.1570 - val_mae: 552.8488\n",
      "Epoch 393/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 8.2853 - mae: 8.8456 - val_loss: 560.5666 - val_mae: 561.2596\n",
      "Epoch 394/1000\n",
      "1460/1460 [==============================] - 0s 220us/step - loss: 7.8526 - mae: 8.4336 - val_loss: 536.1821 - val_mae: 536.8745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 395/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 9.3189 - mae: 9.8889 - val_loss: 478.0183 - val_mae: 478.7106\n",
      "Epoch 396/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 7.2679 - mae: 7.8383 - val_loss: 487.5990 - val_mae: 488.2894\n",
      "Epoch 397/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 7.5922 - mae: 8.1607 - val_loss: 524.2016 - val_mae: 524.8925\n",
      "Epoch 398/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 6.6764 - mae: 7.2337 - val_loss: 464.9392 - val_mae: 465.6320\n",
      "Epoch 399/1000\n",
      "1460/1460 [==============================] - 0s 222us/step - loss: 7.6574 - mae: 8.2197 - val_loss: 494.9149 - val_mae: 495.6080\n",
      "Epoch 400/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 6.9862 - mae: 7.5496 - val_loss: 497.6950 - val_mae: 498.3880\n",
      "Epoch 401/1000\n",
      "1460/1460 [==============================] - 0s 232us/step - loss: 7.0653 - mae: 7.6269 - val_loss: 501.5322 - val_mae: 502.2248\n",
      "Epoch 402/1000\n",
      "1460/1460 [==============================] - 0s 222us/step - loss: 6.6894 - mae: 7.2575 - val_loss: 484.8405 - val_mae: 485.5325\n",
      "Epoch 403/1000\n",
      "1460/1460 [==============================] - 0s 206us/step - loss: 7.2741 - mae: 7.8467 - val_loss: 469.3979 - val_mae: 470.0904\n",
      "Epoch 404/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 7.2285 - mae: 7.8093 - val_loss: 477.2191 - val_mae: 477.9107\n",
      "Epoch 405/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 6.8152 - mae: 7.3698 - val_loss: 487.0780 - val_mae: 487.7696\n",
      "Epoch 406/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 7.7471 - mae: 8.3147 - val_loss: 464.5845 - val_mae: 465.2760\n",
      "Epoch 407/1000\n",
      "1460/1460 [==============================] - 0s 214us/step - loss: 7.5061 - mae: 8.0655 - val_loss: 470.2036 - val_mae: 470.8964\n",
      "Epoch 408/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 7.0783 - mae: 7.6438 - val_loss: 463.5350 - val_mae: 464.2268\n",
      "Epoch 409/1000\n",
      "1460/1460 [==============================] - 0s 214us/step - loss: 7.3578 - mae: 7.9158 - val_loss: 457.2352 - val_mae: 457.9278\n",
      "Epoch 410/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 6.4854 - mae: 7.0466 - val_loss: 503.7637 - val_mae: 504.4547\n",
      "Epoch 411/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 7.3620 - mae: 7.9176 - val_loss: 518.9611 - val_mae: 519.6528\n",
      "Epoch 412/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 7.2227 - mae: 7.7831 - val_loss: 459.2449 - val_mae: 459.9377\n",
      "Epoch 413/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 6.5184 - mae: 7.0726 - val_loss: 537.6770 - val_mae: 538.3697\n",
      "Epoch 414/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 7.3061 - mae: 7.8609 - val_loss: 504.0166 - val_mae: 504.7094\n",
      "Epoch 415/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 6.7219 - mae: 7.2827 - val_loss: 445.4032 - val_mae: 446.0953\n",
      "Epoch 416/1000\n",
      "1460/1460 [==============================] - 0s 217us/step - loss: 6.8367 - mae: 7.3961 - val_loss: 444.0027 - val_mae: 444.6950\n",
      "Epoch 417/1000\n",
      "1460/1460 [==============================] - 0s 218us/step - loss: 6.3691 - mae: 6.9223 - val_loss: 451.0636 - val_mae: 451.7556\n",
      "Epoch 418/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 7.3318 - mae: 7.9076 - val_loss: 495.8824 - val_mae: 496.5730\n",
      "Epoch 419/1000\n",
      "1460/1460 [==============================] - 0s 203us/step - loss: 7.6101 - mae: 8.1778 - val_loss: 475.1009 - val_mae: 475.7939\n",
      "Epoch 420/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 7.0248 - mae: 7.5878 - val_loss: 509.9869 - val_mae: 510.6784\n",
      "Epoch 421/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 6.8862 - mae: 7.4420 - val_loss: 454.7974 - val_mae: 455.4896\n",
      "Epoch 422/1000\n",
      "1460/1460 [==============================] - 0s 205us/step - loss: 6.9016 - mae: 7.4503 - val_loss: 516.9017 - val_mae: 517.5944\n",
      "Epoch 423/1000\n",
      "1460/1460 [==============================] - 0s 206us/step - loss: 6.8616 - mae: 7.4211 - val_loss: 476.5296 - val_mae: 477.2227\n",
      "Epoch 424/1000\n",
      "1460/1460 [==============================] - 0s 204us/step - loss: 7.0717 - mae: 7.6348 - val_loss: 468.0196 - val_mae: 468.7114\n",
      "Epoch 425/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 6.6864 - mae: 7.2396 - val_loss: 457.6314 - val_mae: 458.3233\n",
      "Epoch 426/1000\n",
      "1460/1460 [==============================] - 0s 205us/step - loss: 7.0272 - mae: 7.5751 - val_loss: 495.0515 - val_mae: 495.7438\n",
      "Epoch 427/1000\n",
      "1460/1460 [==============================] - 0s 204us/step - loss: 5.8190 - mae: 6.3668 - val_loss: 520.4182 - val_mae: 521.1109\n",
      "Epoch 428/1000\n",
      "1460/1460 [==============================] - 0s 204us/step - loss: 6.1842 - mae: 6.7396 - val_loss: 501.5931 - val_mae: 502.2853\n",
      "Epoch 429/1000\n",
      "1460/1460 [==============================] - 0s 204us/step - loss: 6.6215 - mae: 7.1777 - val_loss: 468.8698 - val_mae: 469.5600\n",
      "Epoch 430/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 7.2798 - mae: 7.8485 - val_loss: 512.9585 - val_mae: 513.6512\n",
      "Epoch 431/1000\n",
      "1460/1460 [==============================] - 0s 212us/step - loss: 6.7153 - mae: 7.2690 - val_loss: 460.4714 - val_mae: 461.1611\n",
      "Epoch 432/1000\n",
      "1460/1460 [==============================] - 0s 204us/step - loss: 6.1564 - mae: 6.7154 - val_loss: 487.8492 - val_mae: 488.5403\n",
      "Epoch 433/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 6.2061 - mae: 6.7637 - val_loss: 481.4743 - val_mae: 482.1668\n",
      "Epoch 434/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 7.6526 - mae: 8.2216 - val_loss: 472.1294 - val_mae: 472.8215\n",
      "Epoch 435/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 8.8348 - mae: 9.4007 - val_loss: 520.5919 - val_mae: 521.2819\n",
      "Epoch 436/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 6.7343 - mae: 7.2882 - val_loss: 535.1034 - val_mae: 535.7953\n",
      "Epoch 437/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 7.3416 - mae: 7.9127 - val_loss: 494.5991 - val_mae: 495.2915\n",
      "Epoch 438/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 6.5105 - mae: 7.0747 - val_loss: 441.3142 - val_mae: 442.0067\n",
      "Epoch 439/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 6.2307 - mae: 6.7974 - val_loss: 491.0065 - val_mae: 491.6984\n",
      "Epoch 440/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 6.2713 - mae: 6.8216 - val_loss: 512.4350 - val_mae: 513.1274\n",
      "Epoch 441/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 7.7273 - mae: 8.2927 - val_loss: 438.5331 - val_mae: 439.2234\n",
      "Epoch 442/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 7.2389 - mae: 7.7962 - val_loss: 463.4060 - val_mae: 464.0982\n",
      "Epoch 443/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 6.8215 - mae: 7.3755 - val_loss: 506.7571 - val_mae: 507.4487\n",
      "Epoch 444/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 6.0959 - mae: 6.6468 - val_loss: 496.6285 - val_mae: 497.3203\n",
      "Epoch 445/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 7.5217 - mae: 8.0782 - val_loss: 544.1685 - val_mae: 544.8616\n",
      "Epoch 446/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 6.7925 - mae: 7.3597 - val_loss: 439.6741 - val_mae: 440.3651\n",
      "Epoch 447/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 8.0318 - mae: 8.6086 - val_loss: 507.9189 - val_mae: 508.6112\n",
      "Epoch 448/1000\n",
      "1460/1460 [==============================] - 1s 397us/step - loss: 6.2093 - mae: 6.7647 - val_loss: 504.8298 - val_mae: 505.5226\n",
      "Epoch 449/1000\n",
      "1460/1460 [==============================] - 1s 657us/step - loss: 6.9195 - mae: 7.4850 - val_loss: 513.0283 - val_mae: 513.7206\n",
      "Epoch 450/1000\n",
      "1460/1460 [==============================] - 1s 800us/step - loss: 5.9711 - mae: 6.5311 - val_loss: 493.4839 - val_mae: 494.1770\n",
      "Epoch 451/1000\n",
      "1460/1460 [==============================] - 1s 549us/step - loss: 5.9044 - mae: 6.4513 - val_loss: 509.8322 - val_mae: 510.5227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 452/1000\n",
      "1460/1460 [==============================] - 1s 532us/step - loss: 6.9790 - mae: 7.5310 - val_loss: 466.9601 - val_mae: 467.6531\n",
      "Epoch 453/1000\n",
      "1460/1460 [==============================] - 1s 516us/step - loss: 7.0226 - mae: 7.5815 - val_loss: 506.6861 - val_mae: 507.3788\n",
      "Epoch 454/1000\n",
      "1460/1460 [==============================] - 0s 293us/step - loss: 6.1114 - mae: 6.6629 - val_loss: 461.3295 - val_mae: 462.0207\n",
      "Epoch 455/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 6.7920 - mae: 7.3504 - val_loss: 488.1771 - val_mae: 488.8698\n",
      "Epoch 456/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 7.3604 - mae: 7.9395 - val_loss: 492.6311 - val_mae: 493.3242\n",
      "Epoch 457/1000\n",
      "1460/1460 [==============================] - 0s 220us/step - loss: 6.3145 - mae: 6.8725 - val_loss: 480.4117 - val_mae: 481.1039\n",
      "Epoch 458/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 5.9582 - mae: 6.5128 - val_loss: 498.1596 - val_mae: 498.8490\n",
      "Epoch 459/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 7.2094 - mae: 7.7700 - val_loss: 512.4350 - val_mae: 513.1271\n",
      "Epoch 460/1000\n",
      "1460/1460 [==============================] - 0s 327us/step - loss: 6.5754 - mae: 7.1215 - val_loss: 485.8325 - val_mae: 486.5238\n",
      "Epoch 461/1000\n",
      "1460/1460 [==============================] - 1s 496us/step - loss: 7.0558 - mae: 7.6260 - val_loss: 499.7820 - val_mae: 500.4749\n",
      "Epoch 462/1000\n",
      "1460/1460 [==============================] - 1s 490us/step - loss: 6.6242 - mae: 7.1776 - val_loss: 499.6316 - val_mae: 500.3246\n",
      "Epoch 463/1000\n",
      "1460/1460 [==============================] - 1s 493us/step - loss: 6.3165 - mae: 6.8636 - val_loss: 500.7067 - val_mae: 501.3989\n",
      "Epoch 464/1000\n",
      "1460/1460 [==============================] - 1s 497us/step - loss: 7.4540 - mae: 8.0028 - val_loss: 491.3923 - val_mae: 492.0854\n",
      "Epoch 465/1000\n",
      "1460/1460 [==============================] - 1s 502us/step - loss: 8.1682 - mae: 8.7295 - val_loss: 469.7964 - val_mae: 470.4883\n",
      "Epoch 466/1000\n",
      "1460/1460 [==============================] - 1s 491us/step - loss: 6.5956 - mae: 7.1379 - val_loss: 506.3087 - val_mae: 507.0010\n",
      "Epoch 467/1000\n",
      "1460/1460 [==============================] - 1s 362us/step - loss: 5.9753 - mae: 6.5180 - val_loss: 486.1433 - val_mae: 486.8340\n",
      "Epoch 468/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 6.1786 - mae: 6.7408 - val_loss: 482.9754 - val_mae: 483.6677\n",
      "Epoch 469/1000\n",
      "1460/1460 [==============================] - 0s 215us/step - loss: 6.3107 - mae: 6.8567 - val_loss: 529.1074 - val_mae: 529.7994\n",
      "Epoch 470/1000\n",
      "1460/1460 [==============================] - 0s 223us/step - loss: 6.9404 - mae: 7.4925 - val_loss: 492.8844 - val_mae: 493.5760\n",
      "Epoch 471/1000\n",
      "1460/1460 [==============================] - 0s 214us/step - loss: 6.0445 - mae: 6.5836 - val_loss: 481.3135 - val_mae: 482.0051\n",
      "Epoch 472/1000\n",
      "1460/1460 [==============================] - 0s 277us/step - loss: 5.8122 - mae: 6.3552 - val_loss: 540.2646 - val_mae: 540.9575\n",
      "Epoch 473/1000\n",
      "1460/1460 [==============================] - 0s 223us/step - loss: 6.2004 - mae: 6.7463 - val_loss: 493.8335 - val_mae: 494.5262\n",
      "Epoch 474/1000\n",
      "1460/1460 [==============================] - 0s 218us/step - loss: 6.4057 - mae: 6.9509 - val_loss: 505.8988 - val_mae: 506.5919\n",
      "Epoch 475/1000\n",
      "1460/1460 [==============================] - 0s 214us/step - loss: 5.7332 - mae: 6.2887 - val_loss: 493.3063 - val_mae: 493.9986\n",
      "Epoch 476/1000\n",
      "1460/1460 [==============================] - 0s 224us/step - loss: 6.0187 - mae: 6.5623 - val_loss: 487.2312 - val_mae: 487.9232\n",
      "Epoch 477/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 6.1380 - mae: 6.6985 - val_loss: 507.0612 - val_mae: 507.7540\n",
      "Epoch 478/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 5.6598 - mae: 6.2055 - val_loss: 440.5447 - val_mae: 441.2372\n",
      "Epoch 479/1000\n",
      "1460/1460 [==============================] - 0s 214us/step - loss: 5.9320 - mae: 6.4871 - val_loss: 531.8535 - val_mae: 532.5453\n",
      "Epoch 480/1000\n",
      "1460/1460 [==============================] - 0s 216us/step - loss: 6.9057 - mae: 7.4557 - val_loss: 523.3564 - val_mae: 524.0466\n",
      "Epoch 481/1000\n",
      "1460/1460 [==============================] - 0s 212us/step - loss: 5.8257 - mae: 6.3625 - val_loss: 472.6646 - val_mae: 473.3572\n",
      "Epoch 482/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 5.6349 - mae: 6.1804 - val_loss: 476.8677 - val_mae: 477.5607\n",
      "Epoch 483/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 6.4151 - mae: 6.9624 - val_loss: 483.8374 - val_mae: 484.5277\n",
      "Epoch 484/1000\n",
      "1460/1460 [==============================] - 0s 233us/step - loss: 5.3897 - mae: 5.9385 - val_loss: 479.0511 - val_mae: 479.7440\n",
      "Epoch 485/1000\n",
      "1460/1460 [==============================] - 0s 226us/step - loss: 5.3486 - mae: 5.8959 - val_loss: 482.4530 - val_mae: 483.1460\n",
      "Epoch 486/1000\n",
      "1460/1460 [==============================] - 0s 215us/step - loss: 5.6710 - mae: 6.2161 - val_loss: 514.4606 - val_mae: 515.1531\n",
      "Epoch 487/1000\n",
      "1460/1460 [==============================] - 0s 212us/step - loss: 7.0913 - mae: 7.6359 - val_loss: 493.1898 - val_mae: 493.8818\n",
      "Epoch 488/1000\n",
      "1460/1460 [==============================] - 0s 224us/step - loss: 6.4702 - mae: 7.0161 - val_loss: 482.6330 - val_mae: 483.3259\n",
      "Epoch 489/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 6.2112 - mae: 6.7638 - val_loss: 456.1326 - val_mae: 456.8248\n",
      "Epoch 490/1000\n",
      "1460/1460 [==============================] - 0s 219us/step - loss: 6.3135 - mae: 6.8569 - val_loss: 517.2483 - val_mae: 517.9409\n",
      "Epoch 491/1000\n",
      "1460/1460 [==============================] - 0s 212us/step - loss: 5.5079 - mae: 6.0554 - val_loss: 460.8364 - val_mae: 461.5293\n",
      "Epoch 492/1000\n",
      "1460/1460 [==============================] - 0s 238us/step - loss: 5.6049 - mae: 6.1482 - val_loss: 495.7075 - val_mae: 496.4002\n",
      "Epoch 493/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 5.5779 - mae: 6.1150 - val_loss: 498.6653 - val_mae: 499.3579\n",
      "Epoch 494/1000\n",
      "1460/1460 [==============================] - 0s 216us/step - loss: 5.2365 - mae: 5.7668 - val_loss: 478.7708 - val_mae: 479.4604\n",
      "Epoch 495/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 6.1306 - mae: 6.6854 - val_loss: 500.8234 - val_mae: 501.5158\n",
      "Epoch 496/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 5.6281 - mae: 6.1686 - val_loss: 482.4086 - val_mae: 483.0997\n",
      "Epoch 497/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 5.2526 - mae: 5.7925 - val_loss: 466.5787 - val_mae: 467.2705\n",
      "Epoch 498/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 6.1341 - mae: 6.6731 - val_loss: 496.0627 - val_mae: 496.7552\n",
      "Epoch 499/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 6.4961 - mae: 7.0384 - val_loss: 508.8167 - val_mae: 509.5093\n",
      "Epoch 500/1000\n",
      "1460/1460 [==============================] - 0s 216us/step - loss: 6.3843 - mae: 6.9335 - val_loss: 444.4732 - val_mae: 445.1662\n",
      "Epoch 501/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 6.3221 - mae: 6.8783 - val_loss: 438.8673 - val_mae: 439.5578\n",
      "Epoch 502/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 5.6078 - mae: 6.1574 - val_loss: 512.4186 - val_mae: 513.1099\n",
      "Epoch 503/1000\n",
      "1460/1460 [==============================] - 0s 216us/step - loss: 6.5126 - mae: 7.0736 - val_loss: 500.1933 - val_mae: 500.8831\n",
      "Epoch 504/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 5.2102 - mae: 5.7569 - val_loss: 554.6372 - val_mae: 555.3295\n",
      "Epoch 505/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 5.9982 - mae: 6.5420 - val_loss: 483.5706 - val_mae: 484.2617\n",
      "Epoch 506/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 5.5099 - mae: 6.0522 - val_loss: 453.8309 - val_mae: 454.5233\n",
      "Epoch 507/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 6.4822 - mae: 7.0246 - val_loss: 483.8728 - val_mae: 484.5650\n",
      "Epoch 508/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 5.6861 - mae: 6.2281 - val_loss: 489.3931 - val_mae: 490.0859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 509/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 6.5238 - mae: 7.0713 - val_loss: 467.1250 - val_mae: 467.8181\n",
      "Epoch 510/1000\n",
      "1460/1460 [==============================] - 0s 206us/step - loss: 6.0616 - mae: 6.6112 - val_loss: 491.4178 - val_mae: 492.1106\n",
      "Epoch 511/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 5.5760 - mae: 6.1311 - val_loss: 474.4117 - val_mae: 475.1042\n",
      "Epoch 512/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 5.7765 - mae: 6.3135 - val_loss: 489.2035 - val_mae: 489.8965\n",
      "Epoch 513/1000\n",
      "1460/1460 [==============================] - 0s 212us/step - loss: 5.5924 - mae: 6.1336 - val_loss: 456.6579 - val_mae: 457.3495\n",
      "Epoch 514/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 6.5406 - mae: 7.0870 - val_loss: 482.5931 - val_mae: 483.2846\n",
      "Epoch 515/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 5.7646 - mae: 6.3035 - val_loss: 457.3773 - val_mae: 458.0695\n",
      "Epoch 516/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 6.2347 - mae: 6.7822 - val_loss: 457.6831 - val_mae: 458.3731\n",
      "Epoch 517/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 5.4371 - mae: 5.9808 - val_loss: 466.8473 - val_mae: 467.5394\n",
      "Epoch 518/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 5.9065 - mae: 6.4524 - val_loss: 469.4199 - val_mae: 470.1125\n",
      "Epoch 519/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 6.4817 - mae: 7.0271 - val_loss: 446.8375 - val_mae: 447.5297\n",
      "Epoch 520/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 7.0459 - mae: 7.6083 - val_loss: 505.2569 - val_mae: 505.9464\n",
      "Epoch 521/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 6.0300 - mae: 6.5740 - val_loss: 489.9729 - val_mae: 490.6650\n",
      "Epoch 522/1000\n",
      "1460/1460 [==============================] - 0s 206us/step - loss: 5.4645 - mae: 6.0021 - val_loss: 466.9242 - val_mae: 467.6171\n",
      "Epoch 523/1000\n",
      "1460/1460 [==============================] - 0s 220us/step - loss: 5.4968 - mae: 6.0420 - val_loss: 454.4043 - val_mae: 455.0974\n",
      "Epoch 524/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 5.1733 - mae: 5.7196 - val_loss: 455.0918 - val_mae: 455.7834\n",
      "Epoch 525/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 5.1555 - mae: 5.7069 - val_loss: 470.5315 - val_mae: 471.2244\n",
      "Epoch 526/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 5.3694 - mae: 5.9091 - val_loss: 484.4365 - val_mae: 485.1292\n",
      "Epoch 527/1000\n",
      "1460/1460 [==============================] - 0s 206us/step - loss: 6.2226 - mae: 6.7738 - val_loss: 410.1750 - val_mae: 410.8678\n",
      "Epoch 528/1000\n",
      "1460/1460 [==============================] - 0s 206us/step - loss: 5.2133 - mae: 5.7439 - val_loss: 492.8426 - val_mae: 493.5352\n",
      "Epoch 529/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 5.9226 - mae: 6.4502 - val_loss: 447.7836 - val_mae: 448.4766\n",
      "Epoch 530/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 5.8914 - mae: 6.4454 - val_loss: 426.1092 - val_mae: 426.8004\n",
      "Epoch 531/1000\n",
      "1460/1460 [==============================] - 0s 212us/step - loss: 5.8942 - mae: 6.4510 - val_loss: 448.0423 - val_mae: 448.7343\n",
      "Epoch 532/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 5.6289 - mae: 6.1660 - val_loss: 496.9751 - val_mae: 497.6682\n",
      "Epoch 533/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 5.7879 - mae: 6.3322 - val_loss: 416.4059 - val_mae: 417.0981\n",
      "Epoch 534/1000\n",
      "1460/1460 [==============================] - 0s 216us/step - loss: 6.2237 - mae: 6.7743 - val_loss: 486.2950 - val_mae: 486.9861\n",
      "Epoch 535/1000\n",
      "1460/1460 [==============================] - 0s 226us/step - loss: 5.2995 - mae: 5.8465 - val_loss: 480.0331 - val_mae: 480.7261\n",
      "Epoch 536/1000\n",
      "1460/1460 [==============================] - 0s 227us/step - loss: 6.0880 - mae: 6.6387 - val_loss: 429.1970 - val_mae: 429.8889\n",
      "Epoch 537/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 5.9019 - mae: 6.4438 - val_loss: 450.9781 - val_mae: 451.6703\n",
      "Epoch 538/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 5.1894 - mae: 5.7226 - val_loss: 508.3683 - val_mae: 509.0609\n",
      "Epoch 539/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 5.8612 - mae: 6.4021 - val_loss: 449.0496 - val_mae: 449.7412\n",
      "Epoch 540/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 6.9136 - mae: 7.4686 - val_loss: 434.0969 - val_mae: 434.7895\n",
      "Epoch 541/1000\n",
      "1460/1460 [==============================] - 0s 206us/step - loss: 6.5779 - mae: 7.1384 - val_loss: 446.3125 - val_mae: 447.0044\n",
      "Epoch 542/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 6.4526 - mae: 7.0105 - val_loss: 489.3677 - val_mae: 490.0597\n",
      "Epoch 543/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 5.7508 - mae: 6.2857 - val_loss: 424.7789 - val_mae: 425.4694\n",
      "Epoch 544/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 5.7130 - mae: 6.2479 - val_loss: 463.3014 - val_mae: 463.9924\n",
      "Epoch 545/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 5.0913 - mae: 5.6313 - val_loss: 471.2035 - val_mae: 471.8951\n",
      "Epoch 546/1000\n",
      "1460/1460 [==============================] - 0s 212us/step - loss: 6.0444 - mae: 6.5874 - val_loss: 471.4405 - val_mae: 472.1317\n",
      "Epoch 547/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 6.6298 - mae: 7.1769 - val_loss: 458.1360 - val_mae: 458.8279\n",
      "Epoch 548/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 6.0710 - mae: 6.6214 - val_loss: 437.3786 - val_mae: 438.0706\n",
      "Epoch 549/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 6.2274 - mae: 6.7711 - val_loss: 459.8483 - val_mae: 460.5400\n",
      "Epoch 550/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 5.5586 - mae: 6.0927 - val_loss: 464.3807 - val_mae: 465.0732\n",
      "Epoch 551/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 5.4252 - mae: 5.9641 - val_loss: 426.2818 - val_mae: 426.9738\n",
      "Epoch 552/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 5.0716 - mae: 5.6060 - val_loss: 440.6096 - val_mae: 441.3008\n",
      "Epoch 553/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 5.5484 - mae: 6.0888 - val_loss: 440.6020 - val_mae: 441.2923\n",
      "Epoch 554/1000\n",
      "1460/1460 [==============================] - 0s 204us/step - loss: 5.0308 - mae: 5.5617 - val_loss: 449.4217 - val_mae: 450.1147\n",
      "Epoch 555/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 5.3149 - mae: 5.8573 - val_loss: 487.4159 - val_mae: 488.1078\n",
      "Epoch 556/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 6.4868 - mae: 7.0294 - val_loss: 463.3788 - val_mae: 464.0719\n",
      "Epoch 557/1000\n",
      "1460/1460 [==============================] - 0s 212us/step - loss: 6.2336 - mae: 6.7751 - val_loss: 510.4096 - val_mae: 511.1011\n",
      "Epoch 558/1000\n",
      "1460/1460 [==============================] - 0s 216us/step - loss: 5.5703 - mae: 6.1187 - val_loss: 498.1949 - val_mae: 498.8876\n",
      "Epoch 559/1000\n",
      "1460/1460 [==============================] - 0s 220us/step - loss: 5.3348 - mae: 5.8708 - val_loss: 457.3559 - val_mae: 458.0484\n",
      "Epoch 560/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 5.3539 - mae: 5.8899 - val_loss: 476.3724 - val_mae: 477.0647\n",
      "Epoch 561/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 5.3536 - mae: 5.8940 - val_loss: 440.8481 - val_mae: 441.5399\n",
      "Epoch 562/1000\n",
      "1460/1460 [==============================] - 0s 215us/step - loss: 5.5899 - mae: 6.1264 - val_loss: 434.5199 - val_mae: 435.2122\n",
      "Epoch 563/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 6.1396 - mae: 6.6924 - val_loss: 457.5596 - val_mae: 458.2525\n",
      "Epoch 564/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 5.8151 - mae: 6.3567 - val_loss: 472.5345 - val_mae: 473.2269\n",
      "Epoch 565/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 5.8605 - mae: 6.4019 - val_loss: 438.2358 - val_mae: 438.9275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 566/1000\n",
      "1460/1460 [==============================] - 1s 565us/step - loss: 6.3882 - mae: 6.9303 - val_loss: 445.0708 - val_mae: 445.7635\n",
      "Epoch 567/1000\n",
      "1460/1460 [==============================] - 1s 551us/step - loss: 5.8145 - mae: 6.3591 - val_loss: 444.3721 - val_mae: 445.0646\n",
      "Epoch 568/1000\n",
      "1460/1460 [==============================] - 1s 591us/step - loss: 6.3015 - mae: 6.8478 - val_loss: 439.8025 - val_mae: 440.4946\n",
      "Epoch 569/1000\n",
      "1460/1460 [==============================] - 1s 527us/step - loss: 5.8012 - mae: 6.3537 - val_loss: 460.5838 - val_mae: 461.2763\n",
      "Epoch 570/1000\n",
      "1460/1460 [==============================] - 1s 586us/step - loss: 5.2697 - mae: 5.8117 - val_loss: 436.4060 - val_mae: 437.0989\n",
      "Epoch 571/1000\n",
      "1460/1460 [==============================] - 1s 541us/step - loss: 5.0991 - mae: 5.6304 - val_loss: 511.3584 - val_mae: 512.0502\n",
      "Epoch 572/1000\n",
      "1460/1460 [==============================] - 0s 285us/step - loss: 5.7675 - mae: 6.3143 - val_loss: 472.8119 - val_mae: 473.5031\n",
      "Epoch 573/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 5.9942 - mae: 6.5506 - val_loss: 487.8497 - val_mae: 488.5428\n",
      "Epoch 574/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 5.1651 - mae: 5.7014 - val_loss: 493.1845 - val_mae: 493.8766\n",
      "Epoch 575/1000\n",
      "1460/1460 [==============================] - 0s 220us/step - loss: 4.7674 - mae: 5.3085 - val_loss: 493.3946 - val_mae: 494.0861\n",
      "Epoch 576/1000\n",
      "1460/1460 [==============================] - 0s 206us/step - loss: 5.1759 - mae: 5.7145 - val_loss: 485.5547 - val_mae: 486.2477\n",
      "Epoch 577/1000\n",
      "1460/1460 [==============================] - 0s 236us/step - loss: 5.3095 - mae: 5.8366 - val_loss: 483.8493 - val_mae: 484.5406\n",
      "Epoch 578/1000\n",
      "1460/1460 [==============================] - 1s 414us/step - loss: 4.7594 - mae: 5.2783 - val_loss: 452.4401 - val_mae: 453.1332\n",
      "Epoch 579/1000\n",
      "1460/1460 [==============================] - 1s 489us/step - loss: 4.5425 - mae: 5.0652 - val_loss: 472.5923 - val_mae: 473.2829\n",
      "Epoch 580/1000\n",
      "1460/1460 [==============================] - 1s 503us/step - loss: 5.2920 - mae: 5.8176 - val_loss: 420.4562 - val_mae: 421.1492\n",
      "Epoch 581/1000\n",
      "1460/1460 [==============================] - 1s 546us/step - loss: 5.5171 - mae: 6.0460 - val_loss: 430.7399 - val_mae: 431.4314\n",
      "Epoch 582/1000\n",
      "1460/1460 [==============================] - 1s 519us/step - loss: 6.0608 - mae: 6.6047 - val_loss: 479.7243 - val_mae: 480.4163\n",
      "Epoch 583/1000\n",
      "1460/1460 [==============================] - 1s 538us/step - loss: 5.2866 - mae: 5.8277 - val_loss: 446.9321 - val_mae: 447.6243\n",
      "Epoch 584/1000\n",
      "1460/1460 [==============================] - 1s 579us/step - loss: 5.6700 - mae: 6.2174 - val_loss: 468.7178 - val_mae: 469.4098\n",
      "Epoch 585/1000\n",
      "1460/1460 [==============================] - 0s 262us/step - loss: 5.7833 - mae: 6.3266 - val_loss: 387.7371 - val_mae: 388.4286\n",
      "Epoch 586/1000\n",
      "1460/1460 [==============================] - 0s 225us/step - loss: 5.6154 - mae: 6.1573 - val_loss: 474.5690 - val_mae: 475.2614\n",
      "Epoch 587/1000\n",
      "1460/1460 [==============================] - 0s 241us/step - loss: 4.7445 - mae: 5.2621 - val_loss: 473.2760 - val_mae: 473.9678\n",
      "Epoch 588/1000\n",
      "1460/1460 [==============================] - 0s 227us/step - loss: 5.2473 - mae: 5.7783 - val_loss: 415.0113 - val_mae: 415.7037\n",
      "Epoch 589/1000\n",
      "1460/1460 [==============================] - 0s 241us/step - loss: 4.9401 - mae: 5.4706 - val_loss: 460.1094 - val_mae: 460.8004\n",
      "Epoch 590/1000\n",
      "1460/1460 [==============================] - 0s 233us/step - loss: 5.4695 - mae: 6.0046 - val_loss: 431.3643 - val_mae: 432.0574\n",
      "Epoch 591/1000\n",
      "1460/1460 [==============================] - 0s 246us/step - loss: 4.7035 - mae: 5.2246 - val_loss: 434.7737 - val_mae: 435.4651\n",
      "Epoch 592/1000\n",
      "1460/1460 [==============================] - 0s 218us/step - loss: 4.3341 - mae: 4.8473 - val_loss: 422.1663 - val_mae: 422.8588\n",
      "Epoch 593/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 5.0310 - mae: 5.5607 - val_loss: 457.0365 - val_mae: 457.7284\n",
      "Epoch 594/1000\n",
      "1460/1460 [==============================] - 0s 216us/step - loss: 5.3767 - mae: 5.9097 - val_loss: 392.1085 - val_mae: 392.7993\n",
      "Epoch 595/1000\n",
      "1460/1460 [==============================] - 0s 224us/step - loss: 6.3665 - mae: 6.9143 - val_loss: 424.0717 - val_mae: 424.7631\n",
      "Epoch 596/1000\n",
      "1460/1460 [==============================] - 0s 221us/step - loss: 4.9175 - mae: 5.4428 - val_loss: 432.5173 - val_mae: 433.2089\n",
      "Epoch 597/1000\n",
      "1460/1460 [==============================] - 0s 216us/step - loss: 4.8150 - mae: 5.3524 - val_loss: 419.4662 - val_mae: 420.1577\n",
      "Epoch 598/1000\n",
      "1460/1460 [==============================] - 0s 216us/step - loss: 4.7513 - mae: 5.2901 - val_loss: 444.6219 - val_mae: 445.3141\n",
      "Epoch 599/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 5.3372 - mae: 5.8863 - val_loss: 451.2086 - val_mae: 451.9014\n",
      "Epoch 600/1000\n",
      "1460/1460 [==============================] - 0s 217us/step - loss: 4.8963 - mae: 5.4419 - val_loss: 416.1482 - val_mae: 416.8410\n",
      "Epoch 601/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 4.8759 - mae: 5.4032 - val_loss: 417.4445 - val_mae: 418.1376\n",
      "Epoch 602/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 4.7310 - mae: 5.2560 - val_loss: 445.1361 - val_mae: 445.8285\n",
      "Epoch 603/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 4.6604 - mae: 5.1936 - val_loss: 409.0342 - val_mae: 409.7252\n",
      "Epoch 604/1000\n",
      "1460/1460 [==============================] - 0s 214us/step - loss: 6.7125 - mae: 7.2595 - val_loss: 403.7924 - val_mae: 404.4841\n",
      "Epoch 605/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 4.5718 - mae: 5.0971 - val_loss: 461.5504 - val_mae: 462.2432\n",
      "Epoch 606/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 5.3590 - mae: 5.8888 - val_loss: 437.4423 - val_mae: 438.1346\n",
      "Epoch 607/1000\n",
      "1460/1460 [==============================] - 0s 218us/step - loss: 5.7750 - mae: 6.3180 - val_loss: 410.0017 - val_mae: 410.6921\n",
      "Epoch 608/1000\n",
      "1460/1460 [==============================] - 0s 214us/step - loss: 5.4225 - mae: 5.9577 - val_loss: 397.7417 - val_mae: 398.4315\n",
      "Epoch 609/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 5.3908 - mae: 5.9248 - val_loss: 429.6133 - val_mae: 430.3058\n",
      "Epoch 610/1000\n",
      "1460/1460 [==============================] - 0s 216us/step - loss: 5.0732 - mae: 5.5966 - val_loss: 442.9322 - val_mae: 443.6242\n",
      "Epoch 611/1000\n",
      "1460/1460 [==============================] - 0s 218us/step - loss: 4.9261 - mae: 5.4463 - val_loss: 438.9792 - val_mae: 439.6718\n",
      "Epoch 612/1000\n",
      "1460/1460 [==============================] - 0s 217us/step - loss: 6.3875 - mae: 6.9454 - val_loss: 415.6056 - val_mae: 416.2976\n",
      "Epoch 613/1000\n",
      "1460/1460 [==============================] - 0s 220us/step - loss: 4.4427 - mae: 4.9684 - val_loss: 423.3419 - val_mae: 424.0341\n",
      "Epoch 614/1000\n",
      "1460/1460 [==============================] - 0s 230us/step - loss: 5.4672 - mae: 6.0117 - val_loss: 474.2441 - val_mae: 474.9368\n",
      "Epoch 615/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 5.3130 - mae: 5.8568 - val_loss: 418.2553 - val_mae: 418.9481\n",
      "Epoch 616/1000\n",
      "1460/1460 [==============================] - 0s 215us/step - loss: 5.6392 - mae: 6.1745 - val_loss: 454.6478 - val_mae: 455.3401\n",
      "Epoch 617/1000\n",
      "1460/1460 [==============================] - 0s 226us/step - loss: 5.4786 - mae: 6.0270 - val_loss: 460.3556 - val_mae: 461.0487\n",
      "Epoch 618/1000\n",
      "1460/1460 [==============================] - 0s 253us/step - loss: 4.4963 - mae: 5.0210 - val_loss: 440.2512 - val_mae: 440.9425\n",
      "Epoch 619/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 4.5391 - mae: 5.0763 - val_loss: 436.2414 - val_mae: 436.9323\n",
      "Epoch 620/1000\n",
      "1460/1460 [==============================] - 0s 212us/step - loss: 4.8482 - mae: 5.3771 - val_loss: 462.8095 - val_mae: 463.5012\n",
      "Epoch 621/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 5.2784 - mae: 5.8117 - val_loss: 428.6496 - val_mae: 429.3419\n",
      "Epoch 622/1000\n",
      "1460/1460 [==============================] - 0s 218us/step - loss: 5.0110 - mae: 5.5477 - val_loss: 421.3510 - val_mae: 422.0431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 623/1000\n",
      "1460/1460 [==============================] - 0s 250us/step - loss: 5.2132 - mae: 5.7510 - val_loss: 418.3378 - val_mae: 419.0289\n",
      "Epoch 624/1000\n",
      "1460/1460 [==============================] - 0s 220us/step - loss: 4.6849 - mae: 5.2227 - val_loss: 422.6637 - val_mae: 423.3549\n",
      "Epoch 625/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 4.7576 - mae: 5.2798 - val_loss: 381.0437 - val_mae: 381.7354\n",
      "Epoch 626/1000\n",
      "1460/1460 [==============================] - 0s 216us/step - loss: 5.1967 - mae: 5.7354 - val_loss: 416.7449 - val_mae: 417.4352\n",
      "Epoch 627/1000\n",
      "1460/1460 [==============================] - 0s 222us/step - loss: 5.8314 - mae: 6.3671 - val_loss: 454.9836 - val_mae: 455.6758\n",
      "Epoch 628/1000\n",
      "1460/1460 [==============================] - 0s 215us/step - loss: 5.3680 - mae: 5.8941 - val_loss: 438.8675 - val_mae: 439.5602\n",
      "Epoch 629/1000\n",
      "1460/1460 [==============================] - 0s 212us/step - loss: 5.9032 - mae: 6.4364 - val_loss: 443.4309 - val_mae: 444.1201\n",
      "Epoch 630/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 5.0157 - mae: 5.5517 - val_loss: 415.1380 - val_mae: 415.8310\n",
      "Epoch 631/1000\n",
      "1460/1460 [==============================] - 0s 256us/step - loss: 4.9260 - mae: 5.4651 - val_loss: 477.0033 - val_mae: 477.6963\n",
      "Epoch 632/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 4.6673 - mae: 5.1993 - val_loss: 439.2270 - val_mae: 439.9178\n",
      "Epoch 633/1000\n",
      "1460/1460 [==============================] - 0s 217us/step - loss: 4.4829 - mae: 5.0176 - val_loss: 414.4514 - val_mae: 415.1434\n",
      "Epoch 634/1000\n",
      "1460/1460 [==============================] - 0s 219us/step - loss: 5.0883 - mae: 5.6264 - val_loss: 440.5911 - val_mae: 441.2840\n",
      "Epoch 635/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 5.0875 - mae: 5.6190 - val_loss: 432.0951 - val_mae: 432.7852\n",
      "Epoch 636/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 4.9135 - mae: 5.4470 - val_loss: 429.4259 - val_mae: 430.1170\n",
      "Epoch 637/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 4.6756 - mae: 5.2004 - val_loss: 419.3628 - val_mae: 420.0553\n",
      "Epoch 638/1000\n",
      "1460/1460 [==============================] - 0s 215us/step - loss: 4.6314 - mae: 5.1697 - val_loss: 430.1700 - val_mae: 430.8621\n",
      "Epoch 639/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 4.8348 - mae: 5.3816 - val_loss: 413.2964 - val_mae: 413.9893\n",
      "Epoch 640/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 4.6907 - mae: 5.2096 - val_loss: 421.5456 - val_mae: 422.2387\n",
      "Epoch 641/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 4.1751 - mae: 4.6890 - val_loss: 440.6838 - val_mae: 441.3769\n",
      "Epoch 642/1000\n",
      "1460/1460 [==============================] - 0s 214us/step - loss: 4.6035 - mae: 5.1173 - val_loss: 443.4405 - val_mae: 444.1334\n",
      "Epoch 643/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 4.3848 - mae: 4.9011 - val_loss: 411.5400 - val_mae: 412.2316\n",
      "Epoch 644/1000\n",
      "1460/1460 [==============================] - 0s 214us/step - loss: 4.7055 - mae: 5.2380 - val_loss: 441.2492 - val_mae: 441.9409\n",
      "Epoch 645/1000\n",
      "1460/1460 [==============================] - 0s 215us/step - loss: 5.1512 - mae: 5.6820 - val_loss: 423.2308 - val_mae: 423.9235\n",
      "Epoch 646/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 4.2456 - mae: 4.7742 - val_loss: 441.9644 - val_mae: 442.6574\n",
      "Epoch 647/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 4.7966 - mae: 5.3244 - val_loss: 429.8677 - val_mae: 430.5593\n",
      "Epoch 648/1000\n",
      "1460/1460 [==============================] - 0s 212us/step - loss: 4.5600 - mae: 5.0854 - val_loss: 429.9503 - val_mae: 430.6424\n",
      "Epoch 649/1000\n",
      "1460/1460 [==============================] - 0s 206us/step - loss: 6.5453 - mae: 7.0911 - val_loss: 393.8846 - val_mae: 394.5767\n",
      "Epoch 650/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 5.4114 - mae: 5.9579 - val_loss: 429.8049 - val_mae: 430.4961\n",
      "Epoch 651/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 5.3129 - mae: 5.8435 - val_loss: 425.8473 - val_mae: 426.5376\n",
      "Epoch 652/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 5.7925 - mae: 6.3281 - val_loss: 476.9452 - val_mae: 477.6382\n",
      "Epoch 653/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 6.7387 - mae: 7.2763 - val_loss: 438.2900 - val_mae: 438.9824\n",
      "Epoch 654/1000\n",
      "1460/1460 [==============================] - 0s 218us/step - loss: 5.0191 - mae: 5.5552 - val_loss: 448.7304 - val_mae: 449.4218\n",
      "Epoch 655/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 4.7304 - mae: 5.2559 - val_loss: 421.5918 - val_mae: 422.2844\n",
      "Epoch 656/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 5.6349 - mae: 6.1909 - val_loss: 405.8898 - val_mae: 406.5827\n",
      "Epoch 657/1000\n",
      "1460/1460 [==============================] - 0s 212us/step - loss: 6.6109 - mae: 7.1794 - val_loss: 424.3262 - val_mae: 425.0173\n",
      "Epoch 658/1000\n",
      "1460/1460 [==============================] - 0s 216us/step - loss: 6.0229 - mae: 6.5835 - val_loss: 468.5478 - val_mae: 469.2391\n",
      "Epoch 659/1000\n",
      "1460/1460 [==============================] - 0s 225us/step - loss: 5.6518 - mae: 6.2029 - val_loss: 452.3600 - val_mae: 453.0529\n",
      "Epoch 660/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 4.8193 - mae: 5.3710 - val_loss: 431.4657 - val_mae: 432.1578\n",
      "Epoch 661/1000\n",
      "1460/1460 [==============================] - 0s 218us/step - loss: 5.1603 - mae: 5.6929 - val_loss: 438.7088 - val_mae: 439.4016\n",
      "Epoch 662/1000\n",
      "1460/1460 [==============================] - 0s 217us/step - loss: 4.5572 - mae: 5.0878 - val_loss: 442.4546 - val_mae: 443.1452\n",
      "Epoch 663/1000\n",
      "1460/1460 [==============================] - 0s 219us/step - loss: 4.7159 - mae: 5.2509 - val_loss: 436.9290 - val_mae: 437.6216\n",
      "Epoch 664/1000\n",
      "1460/1460 [==============================] - 1s 393us/step - loss: 5.1719 - mae: 5.7081 - val_loss: 402.6565 - val_mae: 403.3476\n",
      "Epoch 665/1000\n",
      "1460/1460 [==============================] - 0s 225us/step - loss: 4.8916 - mae: 5.4172 - val_loss: 400.1111 - val_mae: 400.8016\n",
      "Epoch 666/1000\n",
      "1460/1460 [==============================] - 0s 216us/step - loss: 4.9230 - mae: 5.4540 - val_loss: 390.4026 - val_mae: 391.0932\n",
      "Epoch 667/1000\n",
      "1460/1460 [==============================] - 0s 230us/step - loss: 4.6575 - mae: 5.1886 - val_loss: 439.2752 - val_mae: 439.9683\n",
      "Epoch 668/1000\n",
      "1460/1460 [==============================] - 0s 227us/step - loss: 5.2532 - mae: 5.7954 - val_loss: 421.1124 - val_mae: 421.8043\n",
      "Epoch 669/1000\n",
      "1460/1460 [==============================] - 0s 216us/step - loss: 4.9207 - mae: 5.4552 - val_loss: 390.0725 - val_mae: 390.7646\n",
      "Epoch 670/1000\n",
      "1460/1460 [==============================] - 0s 221us/step - loss: 4.5756 - mae: 5.1162 - val_loss: 433.0982 - val_mae: 433.7903\n",
      "Epoch 671/1000\n",
      "1460/1460 [==============================] - 0s 221us/step - loss: 4.4932 - mae: 5.0302 - val_loss: 423.6963 - val_mae: 424.3887\n",
      "Epoch 672/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 4.2402 - mae: 4.7619 - val_loss: 418.8276 - val_mae: 419.5206\n",
      "Epoch 673/1000\n",
      "1460/1460 [==============================] - 0s 219us/step - loss: 4.9465 - mae: 5.4804 - val_loss: 448.8509 - val_mae: 449.5437\n",
      "Epoch 674/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 4.7842 - mae: 5.3126 - val_loss: 395.9116 - val_mae: 396.6033\n",
      "Epoch 675/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 4.7661 - mae: 5.3024 - val_loss: 420.2600 - val_mae: 420.9527\n",
      "Epoch 676/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 4.2146 - mae: 4.7404 - val_loss: 398.9731 - val_mae: 399.6658\n",
      "Epoch 677/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 4.8753 - mae: 5.3975 - val_loss: 431.3674 - val_mae: 432.0600\n",
      "Epoch 678/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 4.6684 - mae: 5.1859 - val_loss: 383.4054 - val_mae: 384.0968\n",
      "Epoch 679/1000\n",
      "1460/1460 [==============================] - 0s 216us/step - loss: 5.6259 - mae: 6.1667 - val_loss: 377.4265 - val_mae: 378.1195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 680/1000\n",
      "1460/1460 [==============================] - 0s 206us/step - loss: 5.3817 - mae: 5.9257 - val_loss: 402.3680 - val_mae: 403.0580\n",
      "Epoch 681/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 4.6249 - mae: 5.1555 - val_loss: 428.0396 - val_mae: 428.7323\n",
      "Epoch 682/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 4.5476 - mae: 5.0722 - val_loss: 407.1291 - val_mae: 407.8221\n",
      "Epoch 683/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 4.1637 - mae: 4.6765 - val_loss: 422.3574 - val_mae: 423.0496\n",
      "Epoch 684/1000\n",
      "1460/1460 [==============================] - 0s 204us/step - loss: 4.8673 - mae: 5.3949 - val_loss: 408.4140 - val_mae: 409.1065\n",
      "Epoch 685/1000\n",
      "1460/1460 [==============================] - 0s 206us/step - loss: 4.3378 - mae: 4.8726 - val_loss: 404.5161 - val_mae: 405.2083\n",
      "Epoch 686/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 4.4404 - mae: 4.9595 - val_loss: 405.8624 - val_mae: 406.5542\n",
      "Epoch 687/1000\n",
      "1460/1460 [==============================] - 0s 214us/step - loss: 5.0666 - mae: 5.5890 - val_loss: 411.6974 - val_mae: 412.3904\n",
      "Epoch 688/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 5.4728 - mae: 6.0034 - val_loss: 424.4520 - val_mae: 425.1441\n",
      "Epoch 689/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 4.5448 - mae: 5.0663 - val_loss: 403.0483 - val_mae: 403.7409\n",
      "Epoch 690/1000\n",
      "1460/1460 [==============================] - 0s 206us/step - loss: 4.4876 - mae: 5.0071 - val_loss: 395.2123 - val_mae: 395.9023\n",
      "Epoch 691/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 4.6788 - mae: 5.1920 - val_loss: 427.8167 - val_mae: 428.5074\n",
      "Epoch 692/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 5.8046 - mae: 6.3563 - val_loss: 418.4803 - val_mae: 419.1722\n",
      "Epoch 693/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 5.3697 - mae: 5.9024 - val_loss: 411.8487 - val_mae: 412.5392\n",
      "Epoch 694/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 4.3007 - mae: 4.8225 - val_loss: 424.1416 - val_mae: 424.8332\n",
      "Epoch 695/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 4.9203 - mae: 5.4505 - val_loss: 406.4115 - val_mae: 407.1022\n",
      "Epoch 696/1000\n",
      "1460/1460 [==============================] - 0s 219us/step - loss: 4.4681 - mae: 4.9993 - val_loss: 405.3069 - val_mae: 405.9984\n",
      "Epoch 697/1000\n",
      "1460/1460 [==============================] - 0s 206us/step - loss: 5.6436 - mae: 6.1793 - val_loss: 426.0212 - val_mae: 426.7134\n",
      "Epoch 698/1000\n",
      "1460/1460 [==============================] - 0s 206us/step - loss: 4.3339 - mae: 4.8578 - val_loss: 436.1317 - val_mae: 436.8232\n",
      "Epoch 699/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 4.2215 - mae: 4.7412 - val_loss: 424.2411 - val_mae: 424.9336\n",
      "Epoch 700/1000\n",
      "1460/1460 [==============================] - 0s 216us/step - loss: 4.4689 - mae: 4.9913 - val_loss: 407.4296 - val_mae: 408.1208\n",
      "Epoch 701/1000\n",
      "1460/1460 [==============================] - 0s 214us/step - loss: 4.6825 - mae: 5.2168 - val_loss: 431.7728 - val_mae: 432.4656\n",
      "Epoch 702/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 4.5455 - mae: 5.0678 - val_loss: 450.4849 - val_mae: 451.1769\n",
      "Epoch 703/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 4.8368 - mae: 5.3609 - val_loss: 410.6959 - val_mae: 411.3876\n",
      "Epoch 704/1000\n",
      "1460/1460 [==============================] - 0s 206us/step - loss: 4.3134 - mae: 4.8432 - val_loss: 432.7826 - val_mae: 433.4734\n",
      "Epoch 705/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 5.1314 - mae: 5.6623 - val_loss: 415.4400 - val_mae: 416.1319\n",
      "Epoch 706/1000\n",
      "1460/1460 [==============================] - 0s 218us/step - loss: 5.2702 - mae: 5.8080 - val_loss: 395.8182 - val_mae: 396.5108\n",
      "Epoch 707/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 4.7343 - mae: 5.2488 - val_loss: 403.4379 - val_mae: 404.1300\n",
      "Epoch 708/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 4.5152 - mae: 5.0337 - val_loss: 425.7642 - val_mae: 426.4571\n",
      "Epoch 709/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 4.4712 - mae: 4.9947 - val_loss: 438.9295 - val_mae: 439.6212\n",
      "Epoch 710/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 4.3857 - mae: 4.9064 - val_loss: 431.4308 - val_mae: 432.1236\n",
      "Epoch 711/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 3.9892 - mae: 4.4935 - val_loss: 416.2209 - val_mae: 416.9130\n",
      "Epoch 712/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 4.3923 - mae: 4.9064 - val_loss: 432.1588 - val_mae: 432.8507\n",
      "Epoch 713/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 5.2363 - mae: 5.7675 - val_loss: 438.2432 - val_mae: 438.9344\n",
      "Epoch 714/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 4.8009 - mae: 5.3330 - val_loss: 412.6642 - val_mae: 413.3564\n",
      "Epoch 715/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 5.1297 - mae: 5.6647 - val_loss: 415.2126 - val_mae: 415.9050\n",
      "Epoch 716/1000\n",
      "1460/1460 [==============================] - 0s 215us/step - loss: 4.8477 - mae: 5.3833 - val_loss: 399.3087 - val_mae: 400.0018\n",
      "Epoch 717/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 5.0369 - mae: 5.5872 - val_loss: 433.6648 - val_mae: 434.3575\n",
      "Epoch 718/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 4.4776 - mae: 5.0079 - val_loss: 396.1237 - val_mae: 396.8153\n",
      "Epoch 719/1000\n",
      "1460/1460 [==============================] - 0s 233us/step - loss: 5.0357 - mae: 5.5587 - val_loss: 418.6682 - val_mae: 419.3608\n",
      "Epoch 720/1000\n",
      "1460/1460 [==============================] - 0s 226us/step - loss: 4.7084 - mae: 5.2334 - val_loss: 407.3680 - val_mae: 408.0583\n",
      "Epoch 721/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 4.3368 - mae: 4.8646 - val_loss: 397.3592 - val_mae: 398.0505\n",
      "Epoch 722/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 5.1575 - mae: 5.6858 - val_loss: 434.3327 - val_mae: 435.0254\n",
      "Epoch 723/1000\n",
      "1460/1460 [==============================] - 0s 212us/step - loss: 6.2351 - mae: 6.7738 - val_loss: 439.1346 - val_mae: 439.8272\n",
      "Epoch 724/1000\n",
      "1460/1460 [==============================] - 0s 212us/step - loss: 4.2044 - mae: 4.7104 - val_loss: 402.9174 - val_mae: 403.6080\n",
      "Epoch 725/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 4.2427 - mae: 4.7589 - val_loss: 429.3378 - val_mae: 430.0293\n",
      "Epoch 726/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 4.1560 - mae: 4.6697 - val_loss: 446.4354 - val_mae: 447.1285\n",
      "Epoch 727/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 4.4933 - mae: 5.0126 - val_loss: 408.0509 - val_mae: 408.7426\n",
      "Epoch 728/1000\n",
      "1460/1460 [==============================] - 0s 212us/step - loss: 4.2688 - mae: 4.7805 - val_loss: 433.2814 - val_mae: 433.9723\n",
      "Epoch 729/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 4.2900 - mae: 4.8036 - val_loss: 453.3752 - val_mae: 454.0650\n",
      "Epoch 730/1000\n",
      "1460/1460 [==============================] - 0s 212us/step - loss: 4.5638 - mae: 5.0806 - val_loss: 395.9542 - val_mae: 396.6448\n",
      "Epoch 731/1000\n",
      "1460/1460 [==============================] - 0s 219us/step - loss: 4.2632 - mae: 4.7778 - val_loss: 432.9312 - val_mae: 433.6229\n",
      "Epoch 732/1000\n",
      "1460/1460 [==============================] - 0s 225us/step - loss: 4.7121 - mae: 5.2300 - val_loss: 432.0632 - val_mae: 432.7552\n",
      "Epoch 733/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 3.9515 - mae: 4.4566 - val_loss: 430.6975 - val_mae: 431.3879\n",
      "Epoch 734/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 4.2125 - mae: 4.7378 - val_loss: 451.8247 - val_mae: 452.5169\n",
      "Epoch 735/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 4.5378 - mae: 5.0571 - val_loss: 434.7015 - val_mae: 435.3918\n",
      "Epoch 736/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 4.9198 - mae: 5.4442 - val_loss: 452.0668 - val_mae: 452.7589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 737/1000\n",
      "1460/1460 [==============================] - 0s 215us/step - loss: 4.4137 - mae: 4.9414 - val_loss: 423.3254 - val_mae: 424.0171\n",
      "Epoch 738/1000\n",
      "1460/1460 [==============================] - 0s 215us/step - loss: 4.4528 - mae: 4.9749 - val_loss: 404.3671 - val_mae: 405.0597\n",
      "Epoch 739/1000\n",
      "1460/1460 [==============================] - 0s 232us/step - loss: 4.0852 - mae: 4.6012 - val_loss: 407.2230 - val_mae: 407.9152\n",
      "Epoch 740/1000\n",
      "1460/1460 [==============================] - 0s 206us/step - loss: 4.2334 - mae: 4.7491 - val_loss: 405.3998 - val_mae: 406.0916\n",
      "Epoch 741/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 4.5360 - mae: 5.0591 - val_loss: 415.9209 - val_mae: 416.6108\n",
      "Epoch 742/1000\n",
      "1460/1460 [==============================] - 0s 204us/step - loss: 4.4301 - mae: 4.9514 - val_loss: 416.5191 - val_mae: 417.2095\n",
      "Epoch 743/1000\n",
      "1460/1460 [==============================] - 0s 215us/step - loss: 4.5299 - mae: 5.0547 - val_loss: 416.8220 - val_mae: 417.5141\n",
      "Epoch 744/1000\n",
      "1460/1460 [==============================] - 0s 205us/step - loss: 4.9428 - mae: 5.4628 - val_loss: 394.8526 - val_mae: 395.5443\n",
      "Epoch 745/1000\n",
      "1460/1460 [==============================] - 0s 203us/step - loss: 4.5405 - mae: 5.0757 - val_loss: 402.2782 - val_mae: 402.9702\n",
      "Epoch 746/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 4.1332 - mae: 4.6534 - val_loss: 399.5708 - val_mae: 400.2631\n",
      "Epoch 747/1000\n",
      "1460/1460 [==============================] - 0s 206us/step - loss: 4.4942 - mae: 5.0165 - val_loss: 421.6335 - val_mae: 422.3266\n",
      "Epoch 748/1000\n",
      "1460/1460 [==============================] - 0s 204us/step - loss: 4.1341 - mae: 4.6534 - val_loss: 403.7663 - val_mae: 404.4585\n",
      "Epoch 749/1000\n",
      "1460/1460 [==============================] - 0s 206us/step - loss: 3.9282 - mae: 4.4313 - val_loss: 408.1253 - val_mae: 408.8167\n",
      "Epoch 750/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 4.6053 - mae: 5.1368 - val_loss: 426.0962 - val_mae: 426.7880\n",
      "Epoch 751/1000\n",
      "1460/1460 [==============================] - 0s 212us/step - loss: 4.2641 - mae: 4.7730 - val_loss: 401.8180 - val_mae: 402.5094\n",
      "Epoch 752/1000\n",
      "1460/1460 [==============================] - 0s 204us/step - loss: 4.7123 - mae: 5.2226 - val_loss: 372.4767 - val_mae: 373.1683\n",
      "Epoch 753/1000\n",
      "1460/1460 [==============================] - 0s 205us/step - loss: 4.6894 - mae: 5.2012 - val_loss: 384.4502 - val_mae: 385.1415\n",
      "Epoch 754/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 5.7594 - mae: 6.2952 - val_loss: 462.8570 - val_mae: 463.5484\n",
      "Epoch 755/1000\n",
      "1460/1460 [==============================] - 0s 204us/step - loss: 5.1885 - mae: 5.7209 - val_loss: 427.5612 - val_mae: 428.2532\n",
      "Epoch 756/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 4.4609 - mae: 4.9887 - val_loss: 406.0713 - val_mae: 406.7628\n",
      "Epoch 757/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 4.4250 - mae: 4.9396 - val_loss: 393.5149 - val_mae: 394.2071\n",
      "Epoch 758/1000\n",
      "1460/1460 [==============================] - 0s 206us/step - loss: 4.1928 - mae: 4.6970 - val_loss: 396.0229 - val_mae: 396.7159\n",
      "Epoch 759/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 4.1919 - mae: 4.7089 - val_loss: 412.6079 - val_mae: 413.2993\n",
      "Epoch 760/1000\n",
      "1460/1460 [==============================] - 0s 206us/step - loss: 4.0135 - mae: 4.5413 - val_loss: 385.1131 - val_mae: 385.8055\n",
      "Epoch 761/1000\n",
      "1460/1460 [==============================] - 0s 214us/step - loss: 5.2180 - mae: 5.7450 - val_loss: 359.9380 - val_mae: 360.6312\n",
      "Epoch 762/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 5.1781 - mae: 5.7016 - val_loss: 412.0906 - val_mae: 412.7830\n",
      "Epoch 763/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 4.5230 - mae: 5.0385 - val_loss: 423.3916 - val_mae: 424.0845\n",
      "Epoch 764/1000\n",
      "1460/1460 [==============================] - 0s 232us/step - loss: 4.3445 - mae: 4.8582 - val_loss: 417.0865 - val_mae: 417.7794\n",
      "Epoch 765/1000\n",
      "1460/1460 [==============================] - 0s 214us/step - loss: 4.2586 - mae: 4.7920 - val_loss: 404.9982 - val_mae: 405.6895\n",
      "Epoch 766/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 4.3106 - mae: 4.8375 - val_loss: 395.5064 - val_mae: 396.1988\n",
      "Epoch 767/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 4.2663 - mae: 4.7836 - val_loss: 397.2418 - val_mae: 397.9340\n",
      "Epoch 768/1000\n",
      "1460/1460 [==============================] - 0s 206us/step - loss: 4.2835 - mae: 4.8012 - val_loss: 432.1779 - val_mae: 432.8687\n",
      "Epoch 769/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 4.7369 - mae: 5.2701 - val_loss: 384.0253 - val_mae: 384.7177\n",
      "Epoch 770/1000\n",
      "1460/1460 [==============================] - 0s 221us/step - loss: 4.5444 - mae: 5.0716 - val_loss: 395.8747 - val_mae: 396.5662\n",
      "Epoch 771/1000\n",
      "1460/1460 [==============================] - 0s 234us/step - loss: 4.4186 - mae: 4.9461 - val_loss: 415.5248 - val_mae: 416.2178\n",
      "Epoch 772/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 4.4100 - mae: 4.9271 - val_loss: 393.9428 - val_mae: 394.6344\n",
      "Epoch 773/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 4.5282 - mae: 5.0572 - val_loss: 410.1611 - val_mae: 410.8532\n",
      "Epoch 774/1000\n",
      "1460/1460 [==============================] - 0s 219us/step - loss: 4.5551 - mae: 5.0882 - val_loss: 391.5104 - val_mae: 392.2018\n",
      "Epoch 775/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 4.2736 - mae: 4.7940 - val_loss: 403.7739 - val_mae: 404.4663\n",
      "Epoch 776/1000\n",
      "1460/1460 [==============================] - 0s 214us/step - loss: 4.0082 - mae: 4.5220 - val_loss: 418.2917 - val_mae: 418.9825\n",
      "Epoch 777/1000\n",
      "1460/1460 [==============================] - 0s 216us/step - loss: 4.7203 - mae: 5.2403 - val_loss: 375.9551 - val_mae: 376.6459\n",
      "Epoch 778/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 3.8984 - mae: 4.4079 - val_loss: 406.2124 - val_mae: 406.9047\n",
      "Epoch 779/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 4.7263 - mae: 5.2432 - val_loss: 403.9666 - val_mae: 404.6597\n",
      "Epoch 780/1000\n",
      "1460/1460 [==============================] - 0s 212us/step - loss: 4.5160 - mae: 5.0300 - val_loss: 404.0296 - val_mae: 404.7221\n",
      "Epoch 781/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 4.5811 - mae: 5.1169 - val_loss: 394.7620 - val_mae: 395.4541\n",
      "Epoch 782/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 4.1494 - mae: 4.6660 - val_loss: 429.8161 - val_mae: 430.5090\n",
      "Epoch 783/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 4.3093 - mae: 4.8318 - val_loss: 399.7089 - val_mae: 400.4005\n",
      "Epoch 784/1000\n",
      "1460/1460 [==============================] - 0s 216us/step - loss: 4.1370 - mae: 4.6614 - val_loss: 391.1028 - val_mae: 391.7955\n",
      "Epoch 785/1000\n",
      "1460/1460 [==============================] - 0s 216us/step - loss: 4.0954 - mae: 4.6253 - val_loss: 389.5549 - val_mae: 390.2470\n",
      "Epoch 786/1000\n",
      "1460/1460 [==============================] - 0s 217us/step - loss: 4.3980 - mae: 4.9188 - val_loss: 375.5320 - val_mae: 376.2224\n",
      "Epoch 787/1000\n",
      "1460/1460 [==============================] - 0s 217us/step - loss: 3.9703 - mae: 4.4821 - val_loss: 432.2390 - val_mae: 432.9308\n",
      "Epoch 788/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 4.3952 - mae: 4.9111 - val_loss: 379.4141 - val_mae: 380.1064\n",
      "Epoch 789/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 4.1387 - mae: 4.6491 - val_loss: 385.8874 - val_mae: 386.5799\n",
      "Epoch 790/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 4.5502 - mae: 5.0626 - val_loss: 390.7358 - val_mae: 391.4287\n",
      "Epoch 791/1000\n",
      "1460/1460 [==============================] - 0s 214us/step - loss: 4.9116 - mae: 5.4455 - val_loss: 388.5271 - val_mae: 389.2184\n",
      "Epoch 792/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 5.3629 - mae: 5.8978 - val_loss: 384.5271 - val_mae: 385.2174\n",
      "Epoch 793/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 3.9889 - mae: 4.5218 - val_loss: 391.7238 - val_mae: 392.4165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 794/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 4.0345 - mae: 4.5507 - val_loss: 390.8601 - val_mae: 391.5522\n",
      "Epoch 795/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 4.6360 - mae: 5.1545 - val_loss: 406.5016 - val_mae: 407.1930\n",
      "Epoch 796/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 4.8614 - mae: 5.3813 - val_loss: 405.6815 - val_mae: 406.3734\n",
      "Epoch 797/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 3.6383 - mae: 4.1567 - val_loss: 426.3282 - val_mae: 427.0203\n",
      "Epoch 798/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 3.9308 - mae: 4.4488 - val_loss: 409.9159 - val_mae: 410.6061\n",
      "Epoch 799/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 4.1553 - mae: 4.6634 - val_loss: 402.0363 - val_mae: 402.7266\n",
      "Epoch 800/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 4.7513 - mae: 5.2767 - val_loss: 413.7966 - val_mae: 414.4879\n",
      "Epoch 801/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 4.6646 - mae: 5.1835 - val_loss: 425.5174 - val_mae: 426.2076\n",
      "Epoch 802/1000\n",
      "1460/1460 [==============================] - 0s 217us/step - loss: 4.9782 - mae: 5.4982 - val_loss: 415.0647 - val_mae: 415.7562\n",
      "Epoch 803/1000\n",
      "1460/1460 [==============================] - 0s 215us/step - loss: 4.3989 - mae: 4.9296 - val_loss: 398.3732 - val_mae: 399.0652\n",
      "Epoch 804/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 4.1704 - mae: 4.6884 - val_loss: 427.2649 - val_mae: 427.9579\n",
      "Epoch 805/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 4.4468 - mae: 4.9785 - val_loss: 402.2081 - val_mae: 402.8986\n",
      "Epoch 806/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 4.3921 - mae: 4.9058 - val_loss: 407.2509 - val_mae: 407.9428\n",
      "Epoch 807/1000\n",
      "1460/1460 [==============================] - 0s 206us/step - loss: 3.8053 - mae: 4.3133 - val_loss: 414.7768 - val_mae: 415.4688\n",
      "Epoch 808/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 4.8678 - mae: 5.3840 - val_loss: 410.1461 - val_mae: 410.8392\n",
      "Epoch 809/1000\n",
      "1460/1460 [==============================] - 0s 206us/step - loss: 4.5161 - mae: 5.0379 - val_loss: 413.7223 - val_mae: 414.4150\n",
      "Epoch 810/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 4.5112 - mae: 5.0222 - val_loss: 403.2116 - val_mae: 403.9021\n",
      "Epoch 811/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 4.3301 - mae: 4.8438 - val_loss: 411.5468 - val_mae: 412.2379\n",
      "Epoch 812/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 4.6178 - mae: 5.1514 - val_loss: 411.3361 - val_mae: 412.0278\n",
      "Epoch 813/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 5.4975 - mae: 6.0386 - val_loss: 416.8678 - val_mae: 417.5598\n",
      "Epoch 814/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 5.7766 - mae: 6.3214 - val_loss: 424.4789 - val_mae: 425.1720\n",
      "Epoch 815/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 4.4598 - mae: 4.9800 - val_loss: 400.2207 - val_mae: 400.9131\n",
      "Epoch 816/1000\n",
      "1460/1460 [==============================] - 0s 214us/step - loss: 4.1148 - mae: 4.6318 - val_loss: 417.7644 - val_mae: 418.4575\n",
      "Epoch 817/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 4.6635 - mae: 5.1827 - val_loss: 381.5037 - val_mae: 382.1947\n",
      "Epoch 818/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 4.2366 - mae: 4.7561 - val_loss: 408.5765 - val_mae: 409.2668\n",
      "Epoch 819/1000\n",
      "1460/1460 [==============================] - 0s 206us/step - loss: 4.3131 - mae: 4.8300 - val_loss: 414.0772 - val_mae: 414.7673\n",
      "Epoch 820/1000\n",
      "1460/1460 [==============================] - 0s 217us/step - loss: 4.2869 - mae: 4.8129 - val_loss: 416.9510 - val_mae: 417.6425\n",
      "Epoch 821/1000\n",
      "1460/1460 [==============================] - 0s 206us/step - loss: 4.6025 - mae: 5.1140 - val_loss: 402.6342 - val_mae: 403.3262\n",
      "Epoch 822/1000\n",
      "1460/1460 [==============================] - 0s 228us/step - loss: 4.2009 - mae: 4.7178 - val_loss: 383.9948 - val_mae: 384.6875\n",
      "Epoch 823/1000\n",
      "1460/1460 [==============================] - 0s 226us/step - loss: 4.1863 - mae: 4.6948 - val_loss: 411.4818 - val_mae: 412.1725\n",
      "Epoch 824/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 4.0550 - mae: 4.5494 - val_loss: 403.9399 - val_mae: 404.6317\n",
      "Epoch 825/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 5.0990 - mae: 5.6365 - val_loss: 425.3482 - val_mae: 426.0412\n",
      "Epoch 826/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 4.4719 - mae: 4.9896 - val_loss: 404.1545 - val_mae: 404.8451\n",
      "Epoch 827/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 4.7684 - mae: 5.2958 - val_loss: 428.4256 - val_mae: 429.1177\n",
      "Epoch 828/1000\n",
      "1460/1460 [==============================] - 0s 215us/step - loss: 4.3515 - mae: 4.8699 - val_loss: 415.2112 - val_mae: 415.9022\n",
      "Epoch 829/1000\n",
      "1460/1460 [==============================] - 0s 256us/step - loss: 3.9506 - mae: 4.4635 - val_loss: 415.7145 - val_mae: 416.4040\n",
      "Epoch 830/1000\n",
      "1460/1460 [==============================] - 0s 212us/step - loss: 4.4785 - mae: 4.9905 - val_loss: 416.5431 - val_mae: 417.2347\n",
      "Epoch 831/1000\n",
      "1460/1460 [==============================] - 0s 206us/step - loss: 3.9037 - mae: 4.4081 - val_loss: 396.5336 - val_mae: 397.2262\n",
      "Epoch 832/1000\n",
      "1460/1460 [==============================] - 0s 215us/step - loss: 4.4944 - mae: 5.0228 - val_loss: 385.4982 - val_mae: 386.1891\n",
      "Epoch 833/1000\n",
      "1460/1460 [==============================] - 0s 222us/step - loss: 4.0871 - mae: 4.5992 - val_loss: 394.7591 - val_mae: 395.4515\n",
      "Epoch 834/1000\n",
      "1460/1460 [==============================] - 0s 214us/step - loss: 4.5410 - mae: 5.0614 - val_loss: 387.4536 - val_mae: 388.1466\n",
      "Epoch 835/1000\n",
      "1460/1460 [==============================] - 0s 259us/step - loss: 4.5720 - mae: 5.0899 - val_loss: 405.7048 - val_mae: 406.3974\n",
      "Epoch 836/1000\n",
      "1460/1460 [==============================] - 0s 231us/step - loss: 4.2096 - mae: 4.7198 - val_loss: 399.5947 - val_mae: 400.2876\n",
      "Epoch 837/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 4.3033 - mae: 4.8232 - val_loss: 408.6035 - val_mae: 409.2965\n",
      "Epoch 838/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 4.0704 - mae: 4.5936 - val_loss: 372.9827 - val_mae: 373.6749\n",
      "Epoch 839/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 4.1384 - mae: 4.6626 - val_loss: 405.1330 - val_mae: 405.8260\n",
      "Epoch 840/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 3.9784 - mae: 4.4999 - val_loss: 408.9027 - val_mae: 409.5946\n",
      "Epoch 841/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 4.2139 - mae: 4.7224 - val_loss: 432.1313 - val_mae: 432.8229\n",
      "Epoch 842/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 3.7444 - mae: 4.2557 - val_loss: 423.0773 - val_mae: 423.7704\n",
      "Epoch 843/1000\n",
      "1460/1460 [==============================] - 0s 216us/step - loss: 5.0038 - mae: 5.5223 - val_loss: 399.5779 - val_mae: 400.2683\n",
      "Epoch 844/1000\n",
      "1460/1460 [==============================] - 0s 214us/step - loss: 4.4196 - mae: 4.9367 - val_loss: 417.7765 - val_mae: 418.4692\n",
      "Epoch 845/1000\n",
      "1460/1460 [==============================] - 0s 232us/step - loss: 4.4141 - mae: 4.9488 - val_loss: 400.8821 - val_mae: 401.5747\n",
      "Epoch 846/1000\n",
      "1460/1460 [==============================] - 0s 249us/step - loss: 3.7456 - mae: 4.2638 - val_loss: 403.5822 - val_mae: 404.2743\n",
      "Epoch 847/1000\n",
      "1460/1460 [==============================] - 0s 240us/step - loss: 4.2356 - mae: 4.7598 - val_loss: 392.1732 - val_mae: 392.8620\n",
      "Epoch 848/1000\n",
      "1460/1460 [==============================] - 0s 240us/step - loss: 4.2344 - mae: 4.7613 - val_loss: 415.2984 - val_mae: 415.9913\n",
      "Epoch 849/1000\n",
      "1460/1460 [==============================] - 0s 243us/step - loss: 4.3367 - mae: 4.8630 - val_loss: 419.1712 - val_mae: 419.8618\n",
      "Epoch 850/1000\n",
      "1460/1460 [==============================] - 0s 242us/step - loss: 4.5879 - mae: 5.1047 - val_loss: 402.0853 - val_mae: 402.7772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 851/1000\n",
      "1460/1460 [==============================] - 0s 248us/step - loss: 4.2945 - mae: 4.8241 - val_loss: 431.1164 - val_mae: 431.8080\n",
      "Epoch 852/1000\n",
      "1460/1460 [==============================] - 0s 223us/step - loss: 3.8146 - mae: 4.3275 - val_loss: 393.9899 - val_mae: 394.6826\n",
      "Epoch 853/1000\n",
      "1460/1460 [==============================] - 0s 229us/step - loss: 4.3424 - mae: 4.8606 - val_loss: 411.2043 - val_mae: 411.8960\n",
      "Epoch 854/1000\n",
      "1460/1460 [==============================] - 0s 230us/step - loss: 3.8943 - mae: 4.4001 - val_loss: 430.7631 - val_mae: 431.4544\n",
      "Epoch 855/1000\n",
      "1460/1460 [==============================] - 0s 224us/step - loss: 4.3797 - mae: 4.8959 - val_loss: 387.8272 - val_mae: 388.5190\n",
      "Epoch 856/1000\n",
      "1460/1460 [==============================] - 0s 228us/step - loss: 4.5729 - mae: 5.1066 - val_loss: 389.8672 - val_mae: 390.5583\n",
      "Epoch 857/1000\n",
      "1460/1460 [==============================] - 0s 235us/step - loss: 4.0734 - mae: 4.5977 - val_loss: 384.6874 - val_mae: 385.3800\n",
      "Epoch 858/1000\n",
      "1460/1460 [==============================] - 0s 238us/step - loss: 3.4802 - mae: 3.9832 - val_loss: 392.4626 - val_mae: 393.1544\n",
      "Epoch 859/1000\n",
      "1460/1460 [==============================] - 0s 223us/step - loss: 3.9862 - mae: 4.4955 - val_loss: 408.4932 - val_mae: 409.1859\n",
      "Epoch 860/1000\n",
      "1460/1460 [==============================] - 0s 226us/step - loss: 3.9773 - mae: 4.4857 - val_loss: 385.6087 - val_mae: 386.3003\n",
      "Epoch 861/1000\n",
      "1460/1460 [==============================] - 0s 228us/step - loss: 3.6712 - mae: 4.1930 - val_loss: 395.8141 - val_mae: 396.5045\n",
      "Epoch 862/1000\n",
      "1460/1460 [==============================] - 0s 237us/step - loss: 3.5438 - mae: 4.0553 - val_loss: 394.3725 - val_mae: 395.0641\n",
      "Epoch 863/1000\n",
      "1460/1460 [==============================] - 0s 240us/step - loss: 4.6061 - mae: 5.1257 - val_loss: 397.9261 - val_mae: 398.6186\n",
      "Epoch 864/1000\n",
      "1460/1460 [==============================] - 0s 234us/step - loss: 4.1986 - mae: 4.7135 - val_loss: 347.9225 - val_mae: 348.6147\n",
      "Epoch 865/1000\n",
      "1460/1460 [==============================] - 0s 229us/step - loss: 4.6204 - mae: 5.1448 - val_loss: 388.4098 - val_mae: 389.0984\n",
      "Epoch 866/1000\n",
      "1460/1460 [==============================] - 0s 231us/step - loss: 3.4770 - mae: 3.9803 - val_loss: 388.5660 - val_mae: 389.2588\n",
      "Epoch 867/1000\n",
      "1460/1460 [==============================] - 0s 244us/step - loss: 3.8487 - mae: 4.3623 - val_loss: 419.1443 - val_mae: 419.8356\n",
      "Epoch 868/1000\n",
      "1460/1460 [==============================] - 0s 229us/step - loss: 3.9448 - mae: 4.4541 - val_loss: 387.9197 - val_mae: 388.6113\n",
      "Epoch 869/1000\n",
      "1460/1460 [==============================] - 0s 223us/step - loss: 4.9904 - mae: 5.5131 - val_loss: 378.4995 - val_mae: 379.1924\n",
      "Epoch 870/1000\n",
      "1460/1460 [==============================] - 0s 236us/step - loss: 4.8970 - mae: 5.4286 - val_loss: 383.8750 - val_mae: 384.5672\n",
      "Epoch 871/1000\n",
      "1460/1460 [==============================] - 0s 249us/step - loss: 4.0775 - mae: 4.5891 - val_loss: 388.2593 - val_mae: 388.9504\n",
      "Epoch 872/1000\n",
      "1460/1460 [==============================] - 0s 221us/step - loss: 4.6477 - mae: 5.1715 - val_loss: 435.7156 - val_mae: 436.4080\n",
      "Epoch 873/1000\n",
      "1460/1460 [==============================] - 0s 221us/step - loss: 3.8186 - mae: 4.3277 - val_loss: 368.3972 - val_mae: 369.0844\n",
      "Epoch 874/1000\n",
      "1460/1460 [==============================] - 0s 226us/step - loss: 3.8709 - mae: 4.3842 - val_loss: 380.8934 - val_mae: 381.5850\n",
      "Epoch 875/1000\n",
      "1460/1460 [==============================] - 0s 223us/step - loss: 4.2025 - mae: 4.7142 - val_loss: 418.0556 - val_mae: 418.7484\n",
      "Epoch 876/1000\n",
      "1460/1460 [==============================] - 0s 222us/step - loss: 4.3492 - mae: 4.8594 - val_loss: 384.9432 - val_mae: 385.6355\n",
      "Epoch 877/1000\n",
      "1460/1460 [==============================] - 0s 221us/step - loss: 3.5186 - mae: 4.0162 - val_loss: 392.8237 - val_mae: 393.5168\n",
      "Epoch 878/1000\n",
      "1460/1460 [==============================] - 0s 215us/step - loss: 3.7702 - mae: 4.2751 - val_loss: 385.0855 - val_mae: 385.7781\n",
      "Epoch 879/1000\n",
      "1460/1460 [==============================] - 0s 229us/step - loss: 3.8865 - mae: 4.3832 - val_loss: 381.5675 - val_mae: 382.2591\n",
      "Epoch 880/1000\n",
      "1460/1460 [==============================] - 0s 225us/step - loss: 3.8951 - mae: 4.4078 - val_loss: 387.3560 - val_mae: 388.0484\n",
      "Epoch 881/1000\n",
      "1460/1460 [==============================] - 0s 214us/step - loss: 4.5480 - mae: 5.0731 - val_loss: 393.2836 - val_mae: 393.9759\n",
      "Epoch 882/1000\n",
      "1460/1460 [==============================] - 0s 212us/step - loss: 3.7584 - mae: 4.2667 - val_loss: 382.5773 - val_mae: 383.2686\n",
      "Epoch 883/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 3.5342 - mae: 4.0396 - val_loss: 377.7364 - val_mae: 378.4274\n",
      "Epoch 884/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 4.0419 - mae: 4.5378 - val_loss: 391.4405 - val_mae: 392.1317\n",
      "Epoch 885/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 4.6147 - mae: 5.1202 - val_loss: 390.9043 - val_mae: 391.5963\n",
      "Epoch 886/1000\n",
      "1460/1460 [==============================] - 0s 212us/step - loss: 3.8545 - mae: 4.3642 - val_loss: 394.2273 - val_mae: 394.9198\n",
      "Epoch 887/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 4.9063 - mae: 5.4182 - val_loss: 375.6802 - val_mae: 376.3720\n",
      "Epoch 888/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 4.0498 - mae: 4.5630 - val_loss: 371.8412 - val_mae: 372.5339\n",
      "Epoch 889/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 4.6453 - mae: 5.1678 - val_loss: 392.3930 - val_mae: 393.0858\n",
      "Epoch 890/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 3.6057 - mae: 4.1160 - val_loss: 387.0056 - val_mae: 387.6978\n",
      "Epoch 891/1000\n",
      "1460/1460 [==============================] - 0s 215us/step - loss: 4.5227 - mae: 5.0422 - val_loss: 370.8224 - val_mae: 371.5149\n",
      "Epoch 892/1000\n",
      "1460/1460 [==============================] - 0s 212us/step - loss: 3.4323 - mae: 3.9325 - val_loss: 360.7783 - val_mae: 361.4702\n",
      "Epoch 893/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 4.7299 - mae: 5.2559 - val_loss: 401.9481 - val_mae: 402.6405\n",
      "Epoch 894/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 3.9610 - mae: 4.4752 - val_loss: 367.9115 - val_mae: 368.6031\n",
      "Epoch 895/1000\n",
      "1460/1460 [==============================] - 0s 216us/step - loss: 4.1298 - mae: 4.6538 - val_loss: 384.0021 - val_mae: 384.6949\n",
      "Epoch 896/1000\n",
      "1460/1460 [==============================] - 0s 217us/step - loss: 4.0772 - mae: 4.5880 - val_loss: 407.8569 - val_mae: 408.5480\n",
      "Epoch 897/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 3.7958 - mae: 4.3085 - val_loss: 387.9851 - val_mae: 388.6763\n",
      "Epoch 898/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 4.0858 - mae: 4.5920 - val_loss: 389.9018 - val_mae: 390.5942\n",
      "Epoch 899/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 4.0763 - mae: 4.5846 - val_loss: 374.7304 - val_mae: 375.4211\n",
      "Epoch 900/1000\n",
      "1460/1460 [==============================] - 0s 216us/step - loss: 5.2341 - mae: 5.7633 - val_loss: 393.1791 - val_mae: 393.8711\n",
      "Epoch 901/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 4.0857 - mae: 4.5995 - val_loss: 388.5011 - val_mae: 389.1939\n",
      "Epoch 902/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 5.1016 - mae: 5.6184 - val_loss: 373.5142 - val_mae: 374.2068\n",
      "Epoch 903/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 3.7083 - mae: 4.2153 - val_loss: 384.1157 - val_mae: 384.8076\n",
      "Epoch 904/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 4.0907 - mae: 4.6005 - val_loss: 373.4914 - val_mae: 374.1843\n",
      "Epoch 905/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 3.7908 - mae: 4.3059 - val_loss: 384.9986 - val_mae: 385.6902\n",
      "Epoch 906/1000\n",
      "1460/1460 [==============================] - 0s 214us/step - loss: 3.8994 - mae: 4.4182 - val_loss: 386.0563 - val_mae: 386.7493\n",
      "Epoch 907/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 3.4326 - mae: 3.9337 - val_loss: 396.2703 - val_mae: 396.9633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 908/1000\n",
      "1460/1460 [==============================] - 0s 214us/step - loss: 4.3416 - mae: 4.8642 - val_loss: 385.1608 - val_mae: 385.8539\n",
      "Epoch 909/1000\n",
      "1460/1460 [==============================] - 0s 204us/step - loss: 3.9833 - mae: 4.4939 - val_loss: 393.9448 - val_mae: 394.6353\n",
      "Epoch 910/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 4.0563 - mae: 4.5598 - val_loss: 379.7605 - val_mae: 380.4502\n",
      "Epoch 911/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 4.4787 - mae: 4.9964 - val_loss: 392.8826 - val_mae: 393.5742\n",
      "Epoch 912/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 4.0698 - mae: 4.5950 - val_loss: 376.6240 - val_mae: 377.3145\n",
      "Epoch 913/1000\n",
      "1460/1460 [==============================] - 0s 214us/step - loss: 4.0285 - mae: 4.5349 - val_loss: 392.2861 - val_mae: 392.9790\n",
      "Epoch 914/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 4.3020 - mae: 4.8249 - val_loss: 379.4573 - val_mae: 380.1500\n",
      "Epoch 915/1000\n",
      "1460/1460 [==============================] - 0s 206us/step - loss: 4.4076 - mae: 4.9265 - val_loss: 408.0741 - val_mae: 408.7668\n",
      "Epoch 916/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 4.2241 - mae: 4.7361 - val_loss: 401.5153 - val_mae: 402.2084\n",
      "Epoch 917/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 3.8161 - mae: 4.3315 - val_loss: 385.5557 - val_mae: 386.2484\n",
      "Epoch 918/1000\n",
      "1460/1460 [==============================] - 0s 214us/step - loss: 3.3573 - mae: 3.8542 - val_loss: 423.5973 - val_mae: 424.2904\n",
      "Epoch 919/1000\n",
      "1460/1460 [==============================] - 0s 214us/step - loss: 4.0259 - mae: 4.5308 - val_loss: 402.0884 - val_mae: 402.7792\n",
      "Epoch 920/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 3.9278 - mae: 4.4308 - val_loss: 392.4420 - val_mae: 393.1321\n",
      "Epoch 921/1000\n",
      "1460/1460 [==============================] - 0s 224us/step - loss: 3.5651 - mae: 4.0708 - val_loss: 387.9054 - val_mae: 388.5977\n",
      "Epoch 922/1000\n",
      "1460/1460 [==============================] - 0s 226us/step - loss: 3.3408 - mae: 3.8325 - val_loss: 368.8322 - val_mae: 369.5231\n",
      "Epoch 923/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 3.8794 - mae: 4.3795 - val_loss: 399.6981 - val_mae: 400.3888\n",
      "Epoch 924/1000\n",
      "1460/1460 [==============================] - 0s 212us/step - loss: 3.8275 - mae: 4.3394 - val_loss: 400.5767 - val_mae: 401.2676\n",
      "Epoch 925/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 3.4639 - mae: 3.9640 - val_loss: 408.9664 - val_mae: 409.6590\n",
      "Epoch 926/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 4.2709 - mae: 4.7918 - val_loss: 377.8999 - val_mae: 378.5920\n",
      "Epoch 927/1000\n",
      "1460/1460 [==============================] - 0s 212us/step - loss: 3.8514 - mae: 4.3584 - val_loss: 411.3439 - val_mae: 412.0359\n",
      "Epoch 928/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 3.4343 - mae: 3.9212 - val_loss: 381.9070 - val_mae: 382.5976\n",
      "Epoch 929/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 4.5252 - mae: 5.0388 - val_loss: 356.9411 - val_mae: 357.6323\n",
      "Epoch 930/1000\n",
      "1460/1460 [==============================] - 0s 205us/step - loss: 4.3064 - mae: 4.8315 - val_loss: 408.8620 - val_mae: 409.5526\n",
      "Epoch 931/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 5.1986 - mae: 5.7249 - val_loss: 411.6201 - val_mae: 412.3128\n",
      "Epoch 932/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 3.8032 - mae: 4.3113 - val_loss: 391.0984 - val_mae: 391.7906\n",
      "Epoch 933/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 3.4366 - mae: 3.9311 - val_loss: 377.2200 - val_mae: 377.9107\n",
      "Epoch 934/1000\n",
      "1460/1460 [==============================] - 0s 217us/step - loss: 3.4337 - mae: 3.9332 - val_loss: 406.7245 - val_mae: 407.4167\n",
      "Epoch 935/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 3.4720 - mae: 3.9712 - val_loss: 384.2701 - val_mae: 384.9631\n",
      "Epoch 936/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 3.3544 - mae: 3.8450 - val_loss: 359.6961 - val_mae: 360.3875\n",
      "Epoch 937/1000\n",
      "1460/1460 [==============================] - 0s 225us/step - loss: 3.3811 - mae: 3.8814 - val_loss: 415.0324 - val_mae: 415.7245\n",
      "Epoch 938/1000\n",
      "1460/1460 [==============================] - 0s 204us/step - loss: 3.5407 - mae: 4.0492 - val_loss: 396.2503 - val_mae: 396.9414\n",
      "Epoch 939/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 3.4885 - mae: 4.0045 - val_loss: 403.0897 - val_mae: 403.7814\n",
      "Epoch 940/1000\n",
      "1460/1460 [==============================] - 0s 205us/step - loss: 3.1871 - mae: 3.6849 - val_loss: 383.2199 - val_mae: 383.9112\n",
      "Epoch 941/1000\n",
      "1460/1460 [==============================] - 0s 204us/step - loss: 3.4107 - mae: 3.9033 - val_loss: 405.1538 - val_mae: 405.8450\n",
      "Epoch 942/1000\n",
      "1460/1460 [==============================] - 0s 205us/step - loss: 3.8184 - mae: 4.3204 - val_loss: 367.1440 - val_mae: 367.8359\n",
      "Epoch 943/1000\n",
      "1460/1460 [==============================] - 0s 204us/step - loss: 3.9271 - mae: 4.4312 - val_loss: 381.5912 - val_mae: 382.2828\n",
      "Epoch 944/1000\n",
      "1460/1460 [==============================] - 0s 204us/step - loss: 4.3518 - mae: 4.8573 - val_loss: 373.9259 - val_mae: 374.6180\n",
      "Epoch 945/1000\n",
      "1460/1460 [==============================] - 0s 204us/step - loss: 3.7219 - mae: 4.2192 - val_loss: 391.6131 - val_mae: 392.3058\n",
      "Epoch 946/1000\n",
      "1460/1460 [==============================] - 0s 205us/step - loss: 4.5175 - mae: 5.0230 - val_loss: 385.7023 - val_mae: 386.3943\n",
      "Epoch 947/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 3.7768 - mae: 4.2777 - val_loss: 399.1809 - val_mae: 399.8732\n",
      "Epoch 948/1000\n",
      "1460/1460 [==============================] - 0s 203us/step - loss: 3.5412 - mae: 4.0400 - val_loss: 393.1847 - val_mae: 393.8770\n",
      "Epoch 949/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 4.3210 - mae: 4.8452 - val_loss: 383.4537 - val_mae: 384.1460\n",
      "Epoch 950/1000\n",
      "1460/1460 [==============================] - 0s 203us/step - loss: 4.0531 - mae: 4.5791 - val_loss: 387.1603 - val_mae: 387.8508\n",
      "Epoch 951/1000\n",
      "1460/1460 [==============================] - 0s 204us/step - loss: 3.4097 - mae: 3.9204 - val_loss: 426.8504 - val_mae: 427.5422\n",
      "Epoch 952/1000\n",
      "1460/1460 [==============================] - 0s 206us/step - loss: 5.2842 - mae: 5.8208 - val_loss: 390.2745 - val_mae: 390.9664\n",
      "Epoch 953/1000\n",
      "1460/1460 [==============================] - 0s 204us/step - loss: 4.5270 - mae: 5.0446 - val_loss: 392.6391 - val_mae: 393.3318\n",
      "Epoch 954/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 4.0907 - mae: 4.6104 - val_loss: 397.2702 - val_mae: 397.9632\n",
      "Epoch 955/1000\n",
      "1460/1460 [==============================] - 0s 247us/step - loss: 3.9955 - mae: 4.5095 - val_loss: 388.8221 - val_mae: 389.5151\n",
      "Epoch 956/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 3.4371 - mae: 3.9431 - val_loss: 399.1318 - val_mae: 399.8228\n",
      "Epoch 957/1000\n",
      "1460/1460 [==============================] - 0s 206us/step - loss: 3.5726 - mae: 4.0811 - val_loss: 392.8430 - val_mae: 393.5354\n",
      "Epoch 958/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 3.5251 - mae: 4.0231 - val_loss: 400.8384 - val_mae: 401.5305\n",
      "Epoch 959/1000\n",
      "1460/1460 [==============================] - 0s 220us/step - loss: 3.3450 - mae: 3.8419 - val_loss: 385.8882 - val_mae: 386.5809\n",
      "Epoch 960/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 3.5993 - mae: 4.0964 - val_loss: 380.2925 - val_mae: 380.9840\n",
      "Epoch 961/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 4.0027 - mae: 4.5130 - val_loss: 408.9651 - val_mae: 409.6567\n",
      "Epoch 962/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 5.1690 - mae: 5.7112 - val_loss: 379.2689 - val_mae: 379.9612\n",
      "Epoch 963/1000\n",
      "1460/1460 [==============================] - 0s 215us/step - loss: 4.6017 - mae: 5.1244 - val_loss: 391.1531 - val_mae: 391.8459\n",
      "Epoch 964/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 4.0035 - mae: 4.5142 - val_loss: 385.1006 - val_mae: 385.7938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 965/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 3.4232 - mae: 3.9323 - val_loss: 366.3711 - val_mae: 367.0625\n",
      "Epoch 966/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 4.0442 - mae: 4.5582 - val_loss: 393.2681 - val_mae: 393.9611\n",
      "Epoch 967/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 4.3994 - mae: 4.9074 - val_loss: 404.2962 - val_mae: 404.9877\n",
      "Epoch 968/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 4.6645 - mae: 5.1809 - val_loss: 380.5887 - val_mae: 381.2814\n",
      "Epoch 969/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 3.5405 - mae: 4.0421 - val_loss: 378.4286 - val_mae: 379.1193\n",
      "Epoch 970/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 3.1742 - mae: 3.6817 - val_loss: 441.4188 - val_mae: 442.1109\n",
      "Epoch 971/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 3.3078 - mae: 3.8010 - val_loss: 383.9672 - val_mae: 384.6595\n",
      "Epoch 972/1000\n",
      "1460/1460 [==============================] - 0s 212us/step - loss: 3.4154 - mae: 3.9155 - val_loss: 387.4603 - val_mae: 388.1520\n",
      "Epoch 973/1000\n",
      "1460/1460 [==============================] - 0s 220us/step - loss: 3.6424 - mae: 4.1460 - val_loss: 403.0695 - val_mae: 403.7609\n",
      "Epoch 974/1000\n",
      "1460/1460 [==============================] - 0s 230us/step - loss: 3.3981 - mae: 3.9005 - val_loss: 408.2430 - val_mae: 408.9347\n",
      "Epoch 975/1000\n",
      "1460/1460 [==============================] - 0s 207us/step - loss: 3.4616 - mae: 3.9570 - val_loss: 392.2388 - val_mae: 392.9314\n",
      "Epoch 976/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 3.3111 - mae: 3.8000 - val_loss: 404.7267 - val_mae: 405.4183\n",
      "Epoch 977/1000\n",
      "1460/1460 [==============================] - 0s 221us/step - loss: 3.6910 - mae: 4.1916 - val_loss: 392.6775 - val_mae: 393.3688\n",
      "Epoch 978/1000\n",
      "1460/1460 [==============================] - 0s 206us/step - loss: 3.2852 - mae: 3.7881 - val_loss: 383.9953 - val_mae: 384.6868\n",
      "Epoch 979/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 3.4440 - mae: 3.9394 - val_loss: 390.1923 - val_mae: 390.8836\n",
      "Epoch 980/1000\n",
      "1460/1460 [==============================] - 0s 215us/step - loss: 3.8436 - mae: 4.3417 - val_loss: 399.4482 - val_mae: 400.1380\n",
      "Epoch 981/1000\n",
      "1460/1460 [==============================] - 0s 215us/step - loss: 3.2543 - mae: 3.7426 - val_loss: 394.6399 - val_mae: 395.3305\n",
      "Epoch 982/1000\n",
      "1460/1460 [==============================] - 0s 206us/step - loss: 3.2684 - mae: 3.7814 - val_loss: 356.1170 - val_mae: 356.8074\n",
      "Epoch 983/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 4.1478 - mae: 4.6674 - val_loss: 371.5388 - val_mae: 372.2313\n",
      "Epoch 984/1000\n",
      "1460/1460 [==============================] - 0s 222us/step - loss: 4.0009 - mae: 4.5160 - val_loss: 388.4322 - val_mae: 389.1240\n",
      "Epoch 985/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 4.0818 - mae: 4.5912 - val_loss: 393.7698 - val_mae: 394.4620\n",
      "Epoch 986/1000\n",
      "1460/1460 [==============================] - 0s 216us/step - loss: 4.1098 - mae: 4.6227 - val_loss: 375.1294 - val_mae: 375.8211\n",
      "Epoch 987/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 3.7983 - mae: 4.3082 - val_loss: 398.2581 - val_mae: 398.9499\n",
      "Epoch 988/1000\n",
      "1460/1460 [==============================] - 0s 214us/step - loss: 4.1981 - mae: 4.7038 - val_loss: 388.3854 - val_mae: 389.0781\n",
      "Epoch 989/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 3.6574 - mae: 4.1608 - val_loss: 410.8121 - val_mae: 411.5040\n",
      "Epoch 990/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 3.5399 - mae: 4.0403 - val_loss: 377.4275 - val_mae: 378.1183\n",
      "Epoch 991/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 3.6956 - mae: 4.1953 - val_loss: 380.8497 - val_mae: 381.5426\n",
      "Epoch 992/1000\n",
      "1460/1460 [==============================] - 0s 214us/step - loss: 3.8461 - mae: 4.3681 - val_loss: 395.0919 - val_mae: 395.7841\n",
      "Epoch 993/1000\n",
      "1460/1460 [==============================] - 0s 209us/step - loss: 3.9433 - mae: 4.4593 - val_loss: 394.7161 - val_mae: 395.4092\n",
      "Epoch 994/1000\n",
      "1460/1460 [==============================] - 0s 208us/step - loss: 3.7811 - mae: 4.2878 - val_loss: 383.3305 - val_mae: 384.0222\n",
      "Epoch 995/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 3.9820 - mae: 4.4873 - val_loss: 375.4963 - val_mae: 376.1890\n",
      "Epoch 996/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 3.6687 - mae: 4.1752 - val_loss: 378.1699 - val_mae: 378.8625\n",
      "Epoch 997/1000\n",
      "1460/1460 [==============================] - 0s 213us/step - loss: 3.9266 - mae: 4.4296 - val_loss: 399.9306 - val_mae: 400.6225\n",
      "Epoch 998/1000\n",
      "1460/1460 [==============================] - 0s 215us/step - loss: 4.4881 - mae: 5.0052 - val_loss: 391.2608 - val_mae: 391.9528\n",
      "Epoch 999/1000\n",
      "1460/1460 [==============================] - 0s 211us/step - loss: 4.4897 - mae: 5.0039 - val_loss: 386.6993 - val_mae: 387.3915\n",
      "Epoch 1000/1000\n",
      "1460/1460 [==============================] - 0s 210us/step - loss: 3.5641 - mae: 4.0648 - val_loss: 359.1233 - val_mae: 359.8156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f92fb0e9ed0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(X_train,Y_train,validation_data=(X_val, Y_val))\n",
    "#Audio(sound_file,autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "872acaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460/1460 [==============================] - 0s 271us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.998254724193597"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for check\n",
    "y_train_pred=regressor.predict(X_train)\n",
    "r2_score(Y_train, y_train_pred) #training score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c039db73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "547/547 [==============================] - 0s 154us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-06-01</th>\n",
       "      <td>2149.0</td>\n",
       "      <td>955.913330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-02</th>\n",
       "      <td>2080.0</td>\n",
       "      <td>906.411194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-03</th>\n",
       "      <td>2192.0</td>\n",
       "      <td>951.555054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-04</th>\n",
       "      <td>2305.0</td>\n",
       "      <td>899.773438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-05</th>\n",
       "      <td>2250.0</td>\n",
       "      <td>768.609131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>472.052612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>473.447205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>500.780914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>572.192566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>522.899109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>547 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_test      y_pred\n",
       "Date                          \n",
       "2021-06-01  2149.0  955.913330\n",
       "2021-06-02  2080.0  906.411194\n",
       "2021-06-03  2192.0  951.555054\n",
       "2021-06-04  2305.0  899.773438\n",
       "2021-06-05  2250.0  768.609131\n",
       "...            ...         ...\n",
       "2022-11-25     NaN  472.052612\n",
       "2022-11-26     NaN  473.447205\n",
       "2022-11-27     NaN  500.780914\n",
       "2022-11-28     NaN  572.192566\n",
       "2022-11-29     NaN  522.899109\n",
       "\n",
       "[547 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=regressor.predict(X_test)\n",
    "pre_df = pd.DataFrame(zip(Y_test,y_pred),columns=['y_test','y_pred'])\n",
    "pre_df.index = Y_test.index\n",
    "pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89e59fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6918716816481871"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2=r2_score(Y_test[0:-30],y_pred[0:-30]) #testing score/ r^2\n",
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdece09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1339.3482118564361"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse=np.sqrt(mean_squared_error(Y_test[0:-30],y_pred[0:-30])) #rmse\n",
    "rmse#太特么大了，感觉数据集划分有问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "178cd814",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df['pred_returns'] = pre_df['y_pred'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a84e1ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>pred_returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-06-01</th>\n",
       "      <td>2149.0</td>\n",
       "      <td>955.913330</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-02</th>\n",
       "      <td>2080.0</td>\n",
       "      <td>906.411194</td>\n",
       "      <td>-0.051785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-03</th>\n",
       "      <td>2192.0</td>\n",
       "      <td>951.555054</td>\n",
       "      <td>0.049805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-04</th>\n",
       "      <td>2305.0</td>\n",
       "      <td>899.773438</td>\n",
       "      <td>-0.054418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-05</th>\n",
       "      <td>2250.0</td>\n",
       "      <td>768.609131</td>\n",
       "      <td>-0.145775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>472.052612</td>\n",
       "      <td>-0.152696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>473.447205</td>\n",
       "      <td>0.002954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>500.780914</td>\n",
       "      <td>0.057733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>572.192566</td>\n",
       "      <td>0.142601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>522.899109</td>\n",
       "      <td>-0.086148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>547 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_test      y_pred  pred_returns\n",
       "Date                                        \n",
       "2021-06-01  2149.0  955.913330           NaN\n",
       "2021-06-02  2080.0  906.411194     -0.051785\n",
       "2021-06-03  2192.0  951.555054      0.049805\n",
       "2021-06-04  2305.0  899.773438     -0.054418\n",
       "2021-06-05  2250.0  768.609131     -0.145775\n",
       "...            ...         ...           ...\n",
       "2022-11-25     NaN  472.052612     -0.152696\n",
       "2022-11-26     NaN  473.447205      0.002954\n",
       "2022-11-27     NaN  500.780914      0.057733\n",
       "2022-11-28     NaN  572.192566      0.142601\n",
       "2022-11-29     NaN  522.899109     -0.086148\n",
       "\n",
       "[547 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08d9891c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(zip(Y_test,y_pred),columns=['y_test','y_pred']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b32ea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df.to_csv(\"eth_NN.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c9d5c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
