{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5b918ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/liuxuyang/opt/anaconda3/envs/mytensorflow/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from datetime import datetime\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n",
    "from keras import losses\n",
    "from keras.constraints import maxnorm\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_validate\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from collections import Counter\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import randint\n",
    "import shap\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, accuracy_score, f1_score, classification_report\n",
    "import pickle\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from commons import mean_absolute_percentage_error\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, roc_auc_score\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import Audio\n",
    "#sound_file = 'beep.wav'\n",
    "from keras.callbacks import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535a3fac",
   "metadata": {},
   "source": [
    "### Bitcoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1467391e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "regs = pd.read_csv(\"btc_reg.csv\")\n",
    "btc = pd.read_csv(\"btc_Data.csv\")\n",
    "for i in range(len(btc['Date'])):\n",
    "    btc['Date'][i]  =  datetime.strptime(btc['Date'][i], '%Y/%m/%d')\n",
    "\n",
    "btc = btc.set_index(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d665a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "btcData = btc[regs.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8227b01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "btcData['returns'] = btcData['priceUSD'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0204bfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = btcData.drop(columns=['priceUSD'])\n",
    "Data = Data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a926d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>difficulty14mom</th>\n",
       "      <th>difficulty30mom</th>\n",
       "      <th>difficulty3rsi</th>\n",
       "      <th>difficulty90mom</th>\n",
       "      <th>difficulty90sma</th>\n",
       "      <th>hashrate90std</th>\n",
       "      <th>hashrate90var</th>\n",
       "      <th>median_transaction_fee90rocUSD</th>\n",
       "      <th>median_transaction_feeUSD</th>\n",
       "      <th>mining_profitability</th>\n",
       "      <th>...</th>\n",
       "      <th>price90wmaUSD</th>\n",
       "      <th>sentinusd90emaUSD</th>\n",
       "      <th>sentinusdUSD</th>\n",
       "      <th>top100cap</th>\n",
       "      <th>top100cap14trx</th>\n",
       "      <th>top100cap30rsi</th>\n",
       "      <th>top100cap30trx</th>\n",
       "      <th>top100cap90rsi</th>\n",
       "      <th>top100cap90trx</th>\n",
       "      <th>returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-07-18</th>\n",
       "      <td>1.621380e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.737230e+02</td>\n",
       "      <td>2.055500e+01</td>\n",
       "      <td>8.384389e+08</td>\n",
       "      <td>1.757449e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>401834.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.612000e+03</td>\n",
       "      <td>25.782</td>\n",
       "      <td>0.139</td>\n",
       "      <td>71.191</td>\n",
       "      <td>0.308</td>\n",
       "      <td>79.756</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-19</th>\n",
       "      <td>1.621380e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.737230e+02</td>\n",
       "      <td>2.248600e+01</td>\n",
       "      <td>8.819952e+08</td>\n",
       "      <td>1.944789e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>481473.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.047000e+03</td>\n",
       "      <td>25.685</td>\n",
       "      <td>0.123</td>\n",
       "      <td>68.863</td>\n",
       "      <td>0.300</td>\n",
       "      <td>78.999</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.183196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-20</th>\n",
       "      <td>1.584060e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.735570e+02</td>\n",
       "      <td>2.441400e+01</td>\n",
       "      <td>9.280542e+08</td>\n",
       "      <td>2.153212e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>431831.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.341000e+03</td>\n",
       "      <td>25.602</td>\n",
       "      <td>0.107</td>\n",
       "      <td>66.923</td>\n",
       "      <td>0.292</td>\n",
       "      <td>78.355</td>\n",
       "      <td>0.544</td>\n",
       "      <td>-0.088475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-21</th>\n",
       "      <td>1.580420e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.700790e+02</td>\n",
       "      <td>2.630400e+01</td>\n",
       "      <td>9.761949e+08</td>\n",
       "      <td>2.382391e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>460783.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.122000e+03</td>\n",
       "      <td>25.638</td>\n",
       "      <td>0.092</td>\n",
       "      <td>67.334</td>\n",
       "      <td>0.284</td>\n",
       "      <td>78.431</td>\n",
       "      <td>0.541</td>\n",
       "      <td>-0.020434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-22</th>\n",
       "      <td>1.580420e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.700790e+02</td>\n",
       "      <td>2.819400e+01</td>\n",
       "      <td>1.019474e+09</td>\n",
       "      <td>2.598318e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>334641.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.570000e+03</td>\n",
       "      <td>25.604</td>\n",
       "      <td>0.078</td>\n",
       "      <td>66.526</td>\n",
       "      <td>0.275</td>\n",
       "      <td>78.166</td>\n",
       "      <td>0.537</td>\n",
       "      <td>-0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-24</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.384949e+13</td>\n",
       "      <td>4.612156e+19</td>\n",
       "      <td>5.317997e+38</td>\n",
       "      <td>165.380</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.0577</td>\n",
       "      <td>...</td>\n",
       "      <td>18820.0</td>\n",
       "      <td>28544682257</td>\n",
       "      <td>6.114577e+09</td>\n",
       "      <td>15.236</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>40.118</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>45.140</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.008075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-25</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.394503e+13</td>\n",
       "      <td>4.542600e+19</td>\n",
       "      <td>5.158804e+38</td>\n",
       "      <td>214.464</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.0541</td>\n",
       "      <td>...</td>\n",
       "      <td>18759.0</td>\n",
       "      <td>28076113671</td>\n",
       "      <td>7.224812e+09</td>\n",
       "      <td>15.231</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>39.880</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>45.047</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.006565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-26</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.404058e+13</td>\n",
       "      <td>4.624135e+19</td>\n",
       "      <td>5.345655e+38</td>\n",
       "      <td>255.004</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.0571</td>\n",
       "      <td>...</td>\n",
       "      <td>18701.0</td>\n",
       "      <td>27566714445</td>\n",
       "      <td>4.898449e+09</td>\n",
       "      <td>15.233</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>40.069</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>45.106</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.004001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-27</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.413612e+13</td>\n",
       "      <td>4.576227e+19</td>\n",
       "      <td>5.235464e+38</td>\n",
       "      <td>113.430</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.0618</td>\n",
       "      <td>...</td>\n",
       "      <td>18642.0</td>\n",
       "      <td>27053515674</td>\n",
       "      <td>4.216170e+09</td>\n",
       "      <td>15.259</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>41.833</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>45.656</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.423166e+13</td>\n",
       "      <td>4.552126e+19</td>\n",
       "      <td>5.180463e+38</td>\n",
       "      <td>104.294</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.0592</td>\n",
       "      <td>...</td>\n",
       "      <td>18578.0</td>\n",
       "      <td>26631320968</td>\n",
       "      <td>7.843657e+09</td>\n",
       "      <td>15.219</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>39.965</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>44.957</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.019828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4517 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            difficulty14mom  difficulty30mom  difficulty3rsi  difficulty90mom  \\\n",
       "Date                                                                            \n",
       "2010-07-18     1.621380e+02     1.641620e+02          100.00     1.737230e+02   \n",
       "2010-07-19     1.621380e+02     1.641620e+02          100.00     1.737230e+02   \n",
       "2010-07-20     1.584060e+02     1.641620e+02          100.00     1.735570e+02   \n",
       "2010-07-21     1.580420e+02     1.641620e+02          100.00     1.700790e+02   \n",
       "2010-07-22     1.580420e+02     1.641620e+02          100.00     1.700790e+02   \n",
       "...                     ...              ...             ...              ...   \n",
       "2022-11-24     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-25     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-26     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-27     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-28     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "\n",
       "            difficulty90sma  hashrate90std  hashrate90var  \\\n",
       "Date                                                        \n",
       "2010-07-18     2.055500e+01   8.384389e+08   1.757449e+17   \n",
       "2010-07-19     2.248600e+01   8.819952e+08   1.944789e+17   \n",
       "2010-07-20     2.441400e+01   9.280542e+08   2.153212e+17   \n",
       "2010-07-21     2.630400e+01   9.761949e+08   2.382391e+17   \n",
       "2010-07-22     2.819400e+01   1.019474e+09   2.598318e+17   \n",
       "...                     ...            ...            ...   \n",
       "2022-11-24     3.384949e+13   4.612156e+19   5.317997e+38   \n",
       "2022-11-25     3.394503e+13   4.542600e+19   5.158804e+38   \n",
       "2022-11-26     3.404058e+13   4.624135e+19   5.345655e+38   \n",
       "2022-11-27     3.413612e+13   4.576227e+19   5.235464e+38   \n",
       "2022-11-28     3.423166e+13   4.552126e+19   5.180463e+38   \n",
       "\n",
       "            median_transaction_fee90rocUSD  median_transaction_feeUSD  \\\n",
       "Date                                                                    \n",
       "2010-07-18                           0.000                      0.000   \n",
       "2010-07-19                           0.000                      0.000   \n",
       "2010-07-20                           0.000                      0.000   \n",
       "2010-07-21                           0.000                      0.000   \n",
       "2010-07-22                           0.000                      0.000   \n",
       "...                                    ...                        ...   \n",
       "2022-11-24                         165.380                      0.818   \n",
       "2022-11-25                         214.464                      0.812   \n",
       "2022-11-26                         255.004                      0.809   \n",
       "2022-11-27                         113.430                      0.619   \n",
       "2022-11-28                         104.294                      0.622   \n",
       "\n",
       "            mining_profitability  ...  price90wmaUSD  sentinusd90emaUSD  \\\n",
       "Date                              ...                                     \n",
       "2010-07-18           401834.0000  ...            0.0                  0   \n",
       "2010-07-19           481473.0000  ...            0.0                  0   \n",
       "2010-07-20           431831.0000  ...            0.0                  0   \n",
       "2010-07-21           460783.0000  ...            0.0                  0   \n",
       "2010-07-22           334641.0000  ...            0.0                  0   \n",
       "...                          ...  ...            ...                ...   \n",
       "2022-11-24                0.0577  ...        18820.0        28544682257   \n",
       "2022-11-25                0.0541  ...        18759.0        28076113671   \n",
       "2022-11-26                0.0571  ...        18701.0        27566714445   \n",
       "2022-11-27                0.0618  ...        18642.0        27053515674   \n",
       "2022-11-28                0.0592  ...        18578.0        26631320968   \n",
       "\n",
       "            sentinusdUSD  top100cap  top100cap14trx  top100cap30rsi  \\\n",
       "Date                                                                  \n",
       "2010-07-18  2.612000e+03     25.782           0.139          71.191   \n",
       "2010-07-19  4.047000e+03     25.685           0.123          68.863   \n",
       "2010-07-20  2.341000e+03     25.602           0.107          66.923   \n",
       "2010-07-21  2.122000e+03     25.638           0.092          67.334   \n",
       "2010-07-22  2.570000e+03     25.604           0.078          66.526   \n",
       "...                  ...        ...             ...             ...   \n",
       "2022-11-24  6.114577e+09     15.236          -0.080          40.118   \n",
       "2022-11-25  7.224812e+09     15.231          -0.075          39.880   \n",
       "2022-11-26  4.898449e+09     15.233          -0.070          40.069   \n",
       "2022-11-27  4.216170e+09     15.259          -0.065          41.833   \n",
       "2022-11-28  7.843657e+09     15.219          -0.061          39.965   \n",
       "\n",
       "            top100cap30trx  top100cap90rsi  top100cap90trx   returns  \n",
       "Date                                                                  \n",
       "2010-07-18           0.308          79.756           0.550  0.466667  \n",
       "2010-07-19           0.300          78.999           0.547  0.183196  \n",
       "2010-07-20           0.292          78.355           0.544 -0.088475  \n",
       "2010-07-21           0.284          78.431           0.541 -0.020434  \n",
       "2010-07-22           0.275          78.166           0.537 -0.153846  \n",
       "...                    ...             ...             ...       ...  \n",
       "2022-11-24          -0.042          45.140           0.001  0.008075  \n",
       "2022-11-25          -0.043          45.047           0.001 -0.006565  \n",
       "2022-11-26          -0.043          45.106           0.001  0.004001  \n",
       "2022-11-27          -0.044          45.656           0.000 -0.001147  \n",
       "2022-11-28          -0.044          44.957           0.000 -0.019828  \n",
       "\n",
       "[4517 rows x 32 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5d4f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide X and Y\n",
    "X = Data.iloc[:,0:]\n",
    "#Y = Data['returns']   # 用returns的话就用这一行，然后把下一行comment掉\n",
    "Y = btcData['priceUSD'].shift(-30)[1:] # 反之亦然"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb3275b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>difficulty14mom</th>\n",
       "      <th>difficulty30mom</th>\n",
       "      <th>difficulty3rsi</th>\n",
       "      <th>difficulty90mom</th>\n",
       "      <th>difficulty90sma</th>\n",
       "      <th>hashrate90std</th>\n",
       "      <th>hashrate90var</th>\n",
       "      <th>median_transaction_fee90rocUSD</th>\n",
       "      <th>median_transaction_feeUSD</th>\n",
       "      <th>mining_profitability</th>\n",
       "      <th>...</th>\n",
       "      <th>price90wmaUSD</th>\n",
       "      <th>sentinusd90emaUSD</th>\n",
       "      <th>sentinusdUSD</th>\n",
       "      <th>top100cap</th>\n",
       "      <th>top100cap14trx</th>\n",
       "      <th>top100cap30rsi</th>\n",
       "      <th>top100cap30trx</th>\n",
       "      <th>top100cap90rsi</th>\n",
       "      <th>top100cap90trx</th>\n",
       "      <th>returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-07-18</th>\n",
       "      <td>162.138</td>\n",
       "      <td>164.162</td>\n",
       "      <td>100.0</td>\n",
       "      <td>173.723</td>\n",
       "      <td>20.555</td>\n",
       "      <td>838438881.0</td>\n",
       "      <td>1.757449e+17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>401834.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2612.0</td>\n",
       "      <td>25.782</td>\n",
       "      <td>0.139</td>\n",
       "      <td>71.191</td>\n",
       "      <td>0.308</td>\n",
       "      <td>79.756</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-19</th>\n",
       "      <td>162.138</td>\n",
       "      <td>164.162</td>\n",
       "      <td>100.0</td>\n",
       "      <td>173.723</td>\n",
       "      <td>22.486</td>\n",
       "      <td>881995244.0</td>\n",
       "      <td>1.944789e+17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>481473.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4047.0</td>\n",
       "      <td>25.685</td>\n",
       "      <td>0.123</td>\n",
       "      <td>68.863</td>\n",
       "      <td>0.300</td>\n",
       "      <td>78.999</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.183196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-20</th>\n",
       "      <td>158.406</td>\n",
       "      <td>164.162</td>\n",
       "      <td>100.0</td>\n",
       "      <td>173.557</td>\n",
       "      <td>24.414</td>\n",
       "      <td>928054231.0</td>\n",
       "      <td>2.153212e+17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>431831.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2341.0</td>\n",
       "      <td>25.602</td>\n",
       "      <td>0.107</td>\n",
       "      <td>66.923</td>\n",
       "      <td>0.292</td>\n",
       "      <td>78.355</td>\n",
       "      <td>0.544</td>\n",
       "      <td>-0.088475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            difficulty14mom  difficulty30mom  difficulty3rsi  difficulty90mom  \\\n",
       "Date                                                                            \n",
       "2010-07-18          162.138          164.162           100.0          173.723   \n",
       "2010-07-19          162.138          164.162           100.0          173.723   \n",
       "2010-07-20          158.406          164.162           100.0          173.557   \n",
       "\n",
       "            difficulty90sma  hashrate90std  hashrate90var  \\\n",
       "Date                                                        \n",
       "2010-07-18           20.555    838438881.0   1.757449e+17   \n",
       "2010-07-19           22.486    881995244.0   1.944789e+17   \n",
       "2010-07-20           24.414    928054231.0   2.153212e+17   \n",
       "\n",
       "            median_transaction_fee90rocUSD  median_transaction_feeUSD  \\\n",
       "Date                                                                    \n",
       "2010-07-18                             0.0                        0.0   \n",
       "2010-07-19                             0.0                        0.0   \n",
       "2010-07-20                             0.0                        0.0   \n",
       "\n",
       "            mining_profitability  ...  price90wmaUSD  sentinusd90emaUSD  \\\n",
       "Date                              ...                                     \n",
       "2010-07-18              401834.0  ...            0.0                  0   \n",
       "2010-07-19              481473.0  ...            0.0                  0   \n",
       "2010-07-20              431831.0  ...            0.0                  0   \n",
       "\n",
       "            sentinusdUSD  top100cap  top100cap14trx  top100cap30rsi  \\\n",
       "Date                                                                  \n",
       "2010-07-18        2612.0     25.782           0.139          71.191   \n",
       "2010-07-19        4047.0     25.685           0.123          68.863   \n",
       "2010-07-20        2341.0     25.602           0.107          66.923   \n",
       "\n",
       "            top100cap30trx  top100cap90rsi  top100cap90trx   returns  \n",
       "Date                                                                  \n",
       "2010-07-18           0.308          79.756           0.550  0.466667  \n",
       "2010-07-19           0.300          78.999           0.547  0.183196  \n",
       "2010-07-20           0.292          78.355           0.544 -0.088475  \n",
       "\n",
       "[3 rows x 32 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd589ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into three data sets\n",
    "X_train = X['2016-01-01':'2019-12-31']\n",
    "X_val = X['2020-01-01':'2021-05-31']\n",
    "X_test = X['2021-06-01':'2023-01-01']\n",
    "\n",
    "Y_train = Y['2016-01-01':'2019-12-31']\n",
    "Y_val = Y['2020-01-01':'2021-05-31']\n",
    "Y_test = Y['2021-06-01':'2023-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e5e9fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............ (step 1 of 2) Processing mixmax, total=   0.0s\n",
      "[Pipeline] ............ (step 2 of 2) Processing robust, total=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('mixmax', MinMaxScaler()), ['robust', RobustScaler()]],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators=[]\n",
    "estimators.append(['mixmax',MinMaxScaler()])\n",
    "estimators.append(['robust',RobustScaler()])\n",
    "scale=Pipeline(estimators,verbose=True)\n",
    "scale.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6fc0859",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=scale.transform(X_train)\n",
    "X_test=scale.transform(X_test)\n",
    "X_val = scale.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8a84d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f6916ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "# define neural network model\n",
    "shape=X.shape[1]\n",
    "def sequential_model(initializer='normal', activation='relu', neurons=300, NUM_FEATURES=shape):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_shape=(NUM_FEATURES,), kernel_initializer=initializer, activation=activation))\n",
    "    model.add(Dense(neurons, activation=activation))\n",
    "    model.add(Dense(neurons, activation=activation))\n",
    "    model.add(Dense(1, activation=activation))\n",
    "    # Compile model\n",
    "    adam=keras.optimizers.adam(lr=lr_schedule(0), amsgrad=True)\n",
    "    #sgd=keras.optimizers.SGD(learning_rate=0.08, momentum=0.9, nesterov=False)\n",
    "    model.compile(loss='logcosh', optimizer=adam, metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b48b24b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mcp_save = ModelCheckpoint('trained_models/ANN_reg_seven_new.hdf5', save_best_only=True, monitor='val_loss', mode='auto')\n",
    "#earlyStopping = EarlyStopping(monitor='val_loss', patience=100,verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85174a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor=KerasRegressor(build_fn=sequential_model,epochs=1000,verbose=1, shuffle=True,use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a17646de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 19:28:13.515580: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-30 19:28:13.517608: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Train on 1461 samples, validate on 517 samples\n",
      "Epoch 1/1000\n",
      "1461/1461 [==============================] - 1s 786us/step - loss: 4636.4784 - mae: 4637.1724 - val_loss: 18833.6614 - val_mae: 18834.3535\n",
      "Epoch 2/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 2743.2584 - mae: 2743.9512 - val_loss: 6128.2585 - val_mae: 6128.9517\n",
      "Epoch 3/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 1545.8519 - mae: 1546.5447 - val_loss: 6430.2637 - val_mae: 6430.9575\n",
      "Epoch 4/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 1296.0449 - mae: 1296.7380 - val_loss: 6555.6965 - val_mae: 6556.3896\n",
      "Epoch 5/1000\n",
      "1461/1461 [==============================] - 0s 283us/step - loss: 1137.8176 - mae: 1138.5103 - val_loss: 6858.2478 - val_mae: 6858.9409\n",
      "Epoch 6/1000\n",
      "1461/1461 [==============================] - 1s 351us/step - loss: 1049.0567 - mae: 1049.7493 - val_loss: 7548.5681 - val_mae: 7549.2617\n",
      "Epoch 7/1000\n",
      "1461/1461 [==============================] - 0s 263us/step - loss: 975.6088 - mae: 976.3013 - val_loss: 7473.2753 - val_mae: 7473.9688\n",
      "Epoch 8/1000\n",
      "1461/1461 [==============================] - 0s 261us/step - loss: 927.1854 - mae: 927.8777 - val_loss: 6782.4438 - val_mae: 6783.1372\n",
      "Epoch 9/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 866.6625 - mae: 867.3550 - val_loss: 7233.6710 - val_mae: 7234.3628\n",
      "Epoch 10/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 819.6092 - mae: 820.3007 - val_loss: 7268.4896 - val_mae: 7269.1826\n",
      "Epoch 11/1000\n",
      "1461/1461 [==============================] - 0s 258us/step - loss: 765.5517 - mae: 766.2443 - val_loss: 7346.4208 - val_mae: 7347.1143\n",
      "Epoch 12/1000\n",
      "1461/1461 [==============================] - 0s 257us/step - loss: 737.8139 - mae: 738.5062 - val_loss: 7512.0349 - val_mae: 7512.7280\n",
      "Epoch 13/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 721.0961 - mae: 721.7888 - val_loss: 7658.9941 - val_mae: 7659.6875\n",
      "Epoch 14/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 692.5363 - mae: 693.2278 - val_loss: 7574.6910 - val_mae: 7575.3843\n",
      "Epoch 15/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 674.9129 - mae: 675.6050 - val_loss: 7744.4907 - val_mae: 7745.1836\n",
      "Epoch 16/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 644.2551 - mae: 644.9465 - val_loss: 7981.7372 - val_mae: 7982.4302\n",
      "Epoch 17/1000\n",
      "1461/1461 [==============================] - 0s 261us/step - loss: 622.8355 - mae: 623.5258 - val_loss: 7612.1284 - val_mae: 7612.8223\n",
      "Epoch 18/1000\n",
      "1461/1461 [==============================] - 0s 259us/step - loss: 612.6106 - mae: 613.3032 - val_loss: 7894.3199 - val_mae: 7895.0132\n",
      "Epoch 19/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 625.2251 - mae: 625.9173 - val_loss: 6877.2737 - val_mae: 6877.9673\n",
      "Epoch 20/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 614.2926 - mae: 614.9844 - val_loss: 7334.7236 - val_mae: 7335.4155\n",
      "Epoch 21/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 620.9318 - mae: 621.6239 - val_loss: 7228.5557 - val_mae: 7229.2490\n",
      "Epoch 22/1000\n",
      "1461/1461 [==============================] - 0s 262us/step - loss: 580.1475 - mae: 580.8395 - val_loss: 7537.6397 - val_mae: 7538.3330\n",
      "Epoch 23/1000\n",
      "1461/1461 [==============================] - 0s 257us/step - loss: 595.3509 - mae: 596.0427 - val_loss: 7482.2882 - val_mae: 7482.9810\n",
      "Epoch 24/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 569.8892 - mae: 570.5814 - val_loss: 7670.4396 - val_mae: 7671.1323\n",
      "Epoch 25/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 550.8133 - mae: 551.5057 - val_loss: 7296.7405 - val_mae: 7297.4331\n",
      "Epoch 26/1000\n",
      "1461/1461 [==============================] - 0s 293us/step - loss: 525.6328 - mae: 526.3234 - val_loss: 7476.5649 - val_mae: 7477.2583\n",
      "Epoch 27/1000\n",
      "1461/1461 [==============================] - 0s 262us/step - loss: 522.7504 - mae: 523.4412 - val_loss: 7272.2080 - val_mae: 7272.9009\n",
      "Epoch 28/1000\n",
      "1461/1461 [==============================] - 0s 296us/step - loss: 558.5833 - mae: 559.2751 - val_loss: 7773.5903 - val_mae: 7774.2837\n",
      "Epoch 29/1000\n",
      "1461/1461 [==============================] - 0s 288us/step - loss: 517.5049 - mae: 518.1976 - val_loss: 7703.5774 - val_mae: 7704.2700\n",
      "Epoch 30/1000\n",
      "1461/1461 [==============================] - 0s 283us/step - loss: 490.4010 - mae: 491.0918 - val_loss: 7882.5134 - val_mae: 7883.2065\n",
      "Epoch 31/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 483.1457 - mae: 483.8365 - val_loss: 7910.9863 - val_mae: 7911.6782\n",
      "Epoch 32/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 486.3034 - mae: 486.9948 - val_loss: 7361.5244 - val_mae: 7362.2178\n",
      "Epoch 33/1000\n",
      "1461/1461 [==============================] - 0s 263us/step - loss: 465.5694 - mae: 466.2618 - val_loss: 7324.3753 - val_mae: 7325.0684\n",
      "Epoch 34/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 455.2132 - mae: 455.9033 - val_loss: 7645.2927 - val_mae: 7645.9858\n",
      "Epoch 35/1000\n",
      "1461/1461 [==============================] - 0s 308us/step - loss: 454.4454 - mae: 455.1354 - val_loss: 7737.9414 - val_mae: 7738.6343\n",
      "Epoch 36/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 449.8750 - mae: 450.5666 - val_loss: 7792.7960 - val_mae: 7793.4897\n",
      "Epoch 37/1000\n",
      "1461/1461 [==============================] - 0s 287us/step - loss: 445.2623 - mae: 445.9542 - val_loss: 7860.3420 - val_mae: 7861.0352\n",
      "Epoch 38/1000\n",
      "1461/1461 [==============================] - 0s 323us/step - loss: 441.0665 - mae: 441.7564 - val_loss: 8298.9940 - val_mae: 8299.6875\n",
      "Epoch 39/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 447.4692 - mae: 448.1614 - val_loss: 7558.0004 - val_mae: 7558.6938\n",
      "Epoch 40/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 423.1007 - mae: 423.7910 - val_loss: 7890.2266 - val_mae: 7890.9204\n",
      "Epoch 41/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 406.3313 - mae: 407.0220 - val_loss: 8406.8346 - val_mae: 8407.5283\n",
      "Epoch 42/1000\n",
      "1461/1461 [==============================] - 0s 302us/step - loss: 457.2486 - mae: 457.9376 - val_loss: 7527.1445 - val_mae: 7527.8374\n",
      "Epoch 43/1000\n",
      "1461/1461 [==============================] - 0s 288us/step - loss: 437.4136 - mae: 438.1044 - val_loss: 8204.5867 - val_mae: 8205.2803\n",
      "Epoch 44/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 409.5218 - mae: 410.2124 - val_loss: 8038.4123 - val_mae: 8039.1055\n",
      "Epoch 45/1000\n",
      "1461/1461 [==============================] - 0s 299us/step - loss: 440.1076 - mae: 440.7985 - val_loss: 8031.7313 - val_mae: 8032.4248\n",
      "Epoch 46/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 436.8295 - mae: 437.5207 - val_loss: 7871.0954 - val_mae: 7871.7881\n",
      "Epoch 47/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 391.7171 - mae: 392.4081 - val_loss: 7840.3008 - val_mae: 7840.9937\n",
      "Epoch 48/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 396.4165 - mae: 397.1057 - val_loss: 7668.0127 - val_mae: 7668.7056\n",
      "Epoch 49/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 400.9454 - mae: 401.6365 - val_loss: 8464.1708 - val_mae: 8464.8633\n",
      "Epoch 50/1000\n",
      "1461/1461 [==============================] - 0s 296us/step - loss: 421.2082 - mae: 421.8993 - val_loss: 8680.6290 - val_mae: 8681.3223\n",
      "Epoch 51/1000\n",
      "1461/1461 [==============================] - 0s 290us/step - loss: 416.0889 - mae: 416.7799 - val_loss: 8085.7090 - val_mae: 8086.4014\n",
      "Epoch 52/1000\n",
      "1461/1461 [==============================] - 0s 263us/step - loss: 382.3258 - mae: 383.0164 - val_loss: 8412.6181 - val_mae: 8413.3115\n",
      "Epoch 53/1000\n",
      "1461/1461 [==============================] - 0s 287us/step - loss: 389.6924 - mae: 390.3834 - val_loss: 8439.5462 - val_mae: 8440.2402\n",
      "Epoch 54/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 354.5380 - mae: 355.2292 - val_loss: 8065.1979 - val_mae: 8065.8906\n",
      "Epoch 55/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 291us/step - loss: 365.1279 - mae: 365.8185 - val_loss: 8181.2210 - val_mae: 8181.9150\n",
      "Epoch 56/1000\n",
      "1461/1461 [==============================] - 0s 283us/step - loss: 364.2990 - mae: 364.9890 - val_loss: 8598.1099 - val_mae: 8598.8027\n",
      "Epoch 57/1000\n",
      "1461/1461 [==============================] - 1s 428us/step - loss: 364.8214 - mae: 365.5129 - val_loss: 8319.3006 - val_mae: 8319.9932\n",
      "Epoch 58/1000\n",
      "1461/1461 [==============================] - 1s 350us/step - loss: 384.7969 - mae: 385.4875 - val_loss: 8129.6881 - val_mae: 8130.3809\n",
      "Epoch 59/1000\n",
      "1461/1461 [==============================] - 0s 312us/step - loss: 354.0414 - mae: 354.7322 - val_loss: 8325.4941 - val_mae: 8326.1875\n",
      "Epoch 60/1000\n",
      "1461/1461 [==============================] - 0s 319us/step - loss: 342.9448 - mae: 343.6352 - val_loss: 7951.3542 - val_mae: 7952.0474\n",
      "Epoch 61/1000\n",
      "1461/1461 [==============================] - 0s 322us/step - loss: 369.3251 - mae: 370.0153 - val_loss: 8040.6173 - val_mae: 8041.3110\n",
      "Epoch 62/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 356.3991 - mae: 357.0902 - val_loss: 7895.0457 - val_mae: 7895.7388\n",
      "Epoch 63/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 346.4231 - mae: 347.1133 - val_loss: 7971.5417 - val_mae: 7972.2349\n",
      "Epoch 64/1000\n",
      "1461/1461 [==============================] - 0s 305us/step - loss: 351.7706 - mae: 352.4590 - val_loss: 8702.1617 - val_mae: 8702.8545\n",
      "Epoch 65/1000\n",
      "1461/1461 [==============================] - 0s 287us/step - loss: 328.3540 - mae: 329.0445 - val_loss: 8327.7064 - val_mae: 8328.3994\n",
      "Epoch 66/1000\n",
      "1461/1461 [==============================] - 0s 316us/step - loss: 360.6830 - mae: 361.3740 - val_loss: 8388.0906 - val_mae: 8388.7842\n",
      "Epoch 67/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 336.5123 - mae: 337.2026 - val_loss: 8782.5122 - val_mae: 8783.2051\n",
      "Epoch 68/1000\n",
      "1461/1461 [==============================] - 0s 313us/step - loss: 332.6536 - mae: 333.3444 - val_loss: 8546.2526 - val_mae: 8546.9463\n",
      "Epoch 69/1000\n",
      "1461/1461 [==============================] - 0s 310us/step - loss: 327.0653 - mae: 327.7551 - val_loss: 8443.7050 - val_mae: 8444.3984\n",
      "Epoch 70/1000\n",
      "1461/1461 [==============================] - 0s 294us/step - loss: 325.0980 - mae: 325.7874 - val_loss: 8241.4108 - val_mae: 8242.1035\n",
      "Epoch 71/1000\n",
      "1461/1461 [==============================] - 0s 290us/step - loss: 324.8060 - mae: 325.4960 - val_loss: 8828.3270 - val_mae: 8829.0205\n",
      "Epoch 72/1000\n",
      "1461/1461 [==============================] - 0s 287us/step - loss: 331.2036 - mae: 331.8950 - val_loss: 8637.7320 - val_mae: 8638.4258\n",
      "Epoch 73/1000\n",
      "1461/1461 [==============================] - 0s 298us/step - loss: 321.2551 - mae: 321.9449 - val_loss: 8427.5221 - val_mae: 8428.2148\n",
      "Epoch 74/1000\n",
      "1461/1461 [==============================] - 0s 290us/step - loss: 325.6859 - mae: 326.3760 - val_loss: 8698.8959 - val_mae: 8699.5898\n",
      "Epoch 75/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 334.0488 - mae: 334.7382 - val_loss: 8819.5789 - val_mae: 8820.2715\n",
      "Epoch 76/1000\n",
      "1461/1461 [==============================] - 0s 290us/step - loss: 304.6850 - mae: 305.3748 - val_loss: 8698.6062 - val_mae: 8699.2988\n",
      "Epoch 77/1000\n",
      "1461/1461 [==============================] - 0s 302us/step - loss: 312.6371 - mae: 313.3262 - val_loss: 8606.5013 - val_mae: 8607.1943\n",
      "Epoch 78/1000\n",
      "1461/1461 [==============================] - 0s 289us/step - loss: 306.6029 - mae: 307.2944 - val_loss: 8666.1251 - val_mae: 8666.8184\n",
      "Epoch 79/1000\n",
      "1461/1461 [==============================] - 0s 289us/step - loss: 337.2056 - mae: 337.8957 - val_loss: 8730.1901 - val_mae: 8730.8828\n",
      "Epoch 80/1000\n",
      "1461/1461 [==============================] - 0s 303us/step - loss: 337.4054 - mae: 338.0954 - val_loss: 8813.1325 - val_mae: 8813.8262\n",
      "Epoch 81/1000\n",
      "1461/1461 [==============================] - 0s 291us/step - loss: 311.1791 - mae: 311.8684 - val_loss: 8439.7639 - val_mae: 8440.4570\n",
      "Epoch 82/1000\n",
      "1461/1461 [==============================] - 0s 288us/step - loss: 310.6802 - mae: 311.3714 - val_loss: 8528.0169 - val_mae: 8528.7100\n",
      "Epoch 83/1000\n",
      "1461/1461 [==============================] - 0s 283us/step - loss: 283.2818 - mae: 283.9714 - val_loss: 8823.6152 - val_mae: 8824.3086\n",
      "Epoch 84/1000\n",
      "1461/1461 [==============================] - 0s 302us/step - loss: 330.9415 - mae: 331.6326 - val_loss: 8248.0403 - val_mae: 8248.7344\n",
      "Epoch 85/1000\n",
      "1461/1461 [==============================] - 0s 290us/step - loss: 303.1822 - mae: 303.8717 - val_loss: 8684.8559 - val_mae: 8685.5498\n",
      "Epoch 86/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 320.2485 - mae: 320.9381 - val_loss: 8442.4654 - val_mae: 8443.1582\n",
      "Epoch 87/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 307.7805 - mae: 308.4713 - val_loss: 9100.3305 - val_mae: 9101.0215\n",
      "Epoch 88/1000\n",
      "1461/1461 [==============================] - 0s 263us/step - loss: 297.6136 - mae: 298.3030 - val_loss: 8556.2944 - val_mae: 8556.9883\n",
      "Epoch 89/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 297.1219 - mae: 297.8121 - val_loss: 8085.8555 - val_mae: 8086.5493\n",
      "Epoch 90/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 298.4390 - mae: 299.1278 - val_loss: 8108.8620 - val_mae: 8109.5552\n",
      "Epoch 91/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 357.3370 - mae: 358.0269 - val_loss: 8139.3101 - val_mae: 8140.0039\n",
      "Epoch 92/1000\n",
      "1461/1461 [==============================] - 0s 261us/step - loss: 292.8031 - mae: 293.4928 - val_loss: 8444.0775 - val_mae: 8444.7715\n",
      "Epoch 93/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 315.8474 - mae: 316.5362 - val_loss: 8704.3911 - val_mae: 8705.0850\n",
      "Epoch 94/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 299.7883 - mae: 300.4785 - val_loss: 8353.1642 - val_mae: 8353.8574\n",
      "Epoch 95/1000\n",
      "1461/1461 [==============================] - 0s 262us/step - loss: 293.4495 - mae: 294.1394 - val_loss: 8419.7241 - val_mae: 8420.4170\n",
      "Epoch 96/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 291.9864 - mae: 292.6749 - val_loss: 8500.9045 - val_mae: 8501.5977\n",
      "Epoch 97/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 293.2311 - mae: 293.9209 - val_loss: 8629.3965 - val_mae: 8630.0898\n",
      "Epoch 98/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 316.3397 - mae: 317.0297 - val_loss: 9200.7134 - val_mae: 9201.4062\n",
      "Epoch 99/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 326.6994 - mae: 327.3913 - val_loss: 8340.6743 - val_mae: 8341.3672\n",
      "Epoch 100/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 276.8627 - mae: 277.5528 - val_loss: 8422.6630 - val_mae: 8423.3564\n",
      "Epoch 101/1000\n",
      "1461/1461 [==============================] - 0s 283us/step - loss: 277.4203 - mae: 278.1120 - val_loss: 8624.4684 - val_mae: 8625.1602\n",
      "Epoch 102/1000\n",
      "1461/1461 [==============================] - 0s 259us/step - loss: 270.5459 - mae: 271.2360 - val_loss: 8581.9534 - val_mae: 8582.6465\n",
      "Epoch 103/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 278.4750 - mae: 279.1639 - val_loss: 8011.9236 - val_mae: 8012.6172\n",
      "Epoch 104/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 277.0096 - mae: 277.6988 - val_loss: 8455.5808 - val_mae: 8456.2734\n",
      "Epoch 105/1000\n",
      "1461/1461 [==============================] - 0s 257us/step - loss: 275.0914 - mae: 275.7812 - val_loss: 8756.0384 - val_mae: 8756.7314\n",
      "Epoch 106/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 283.0466 - mae: 283.7375 - val_loss: 8763.0950 - val_mae: 8763.7881\n",
      "Epoch 107/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 290.2958 - mae: 290.9846 - val_loss: 8553.2773 - val_mae: 8553.9697\n",
      "Epoch 108/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 284.8719 - mae: 285.5620 - val_loss: 8017.3706 - val_mae: 8018.0640\n",
      "Epoch 109/1000\n",
      "1461/1461 [==============================] - 0s 256us/step - loss: 274.3326 - mae: 275.0230 - val_loss: 8355.5363 - val_mae: 8356.2305\n",
      "Epoch 110/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 252us/step - loss: 278.9065 - mae: 279.5964 - val_loss: 8317.9356 - val_mae: 8318.6299\n",
      "Epoch 111/1000\n",
      "1461/1461 [==============================] - 0s 253us/step - loss: 281.4308 - mae: 282.1210 - val_loss: 7850.4060 - val_mae: 7851.0991\n",
      "Epoch 112/1000\n",
      "1461/1461 [==============================] - 0s 254us/step - loss: 270.3037 - mae: 270.9939 - val_loss: 8850.7579 - val_mae: 8851.4512\n",
      "Epoch 113/1000\n",
      "1461/1461 [==============================] - 0s 253us/step - loss: 275.0052 - mae: 275.6960 - val_loss: 8417.5966 - val_mae: 8418.2900\n",
      "Epoch 114/1000\n",
      "1461/1461 [==============================] - 0s 247us/step - loss: 259.0244 - mae: 259.7135 - val_loss: 8543.8063 - val_mae: 8544.5000\n",
      "Epoch 115/1000\n",
      "1461/1461 [==============================] - 0s 256us/step - loss: 274.4622 - mae: 275.1496 - val_loss: 8975.7475 - val_mae: 8976.4414\n",
      "Epoch 116/1000\n",
      "1461/1461 [==============================] - 0s 259us/step - loss: 283.5217 - mae: 284.2134 - val_loss: 8317.2785 - val_mae: 8317.9707\n",
      "Epoch 117/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 285.2505 - mae: 285.9412 - val_loss: 8083.2617 - val_mae: 8083.9551\n",
      "Epoch 118/1000\n",
      "1461/1461 [==============================] - 0s 253us/step - loss: 299.4834 - mae: 300.1707 - val_loss: 8243.7386 - val_mae: 8244.4326\n",
      "Epoch 119/1000\n",
      "1461/1461 [==============================] - 0s 256us/step - loss: 275.1765 - mae: 275.8663 - val_loss: 7972.0498 - val_mae: 7972.7432\n",
      "Epoch 120/1000\n",
      "1461/1461 [==============================] - 0s 256us/step - loss: 277.8983 - mae: 278.5886 - val_loss: 8614.3752 - val_mae: 8615.0684\n",
      "Epoch 121/1000\n",
      "1461/1461 [==============================] - 0s 254us/step - loss: 263.7793 - mae: 264.4686 - val_loss: 8494.4683 - val_mae: 8495.1611\n",
      "Epoch 122/1000\n",
      "1461/1461 [==============================] - 0s 261us/step - loss: 273.6247 - mae: 274.3152 - val_loss: 8158.7883 - val_mae: 8159.4824\n",
      "Epoch 123/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 257.1395 - mae: 257.8297 - val_loss: 8535.5804 - val_mae: 8536.2734\n",
      "Epoch 124/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 264.0568 - mae: 264.7471 - val_loss: 8274.6007 - val_mae: 8275.2930\n",
      "Epoch 125/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 272.4272 - mae: 273.1159 - val_loss: 8973.1157 - val_mae: 8973.8096\n",
      "Epoch 126/1000\n",
      "1461/1461 [==============================] - 0s 263us/step - loss: 287.8815 - mae: 288.5717 - val_loss: 8790.6399 - val_mae: 8791.3330\n",
      "Epoch 127/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 308.0131 - mae: 308.7031 - val_loss: 8576.1685 - val_mae: 8576.8623\n",
      "Epoch 128/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 258.7554 - mae: 259.4452 - val_loss: 8392.8136 - val_mae: 8393.5059\n",
      "Epoch 129/1000\n",
      "1461/1461 [==============================] - 0s 259us/step - loss: 273.0095 - mae: 273.6994 - val_loss: 8158.0333 - val_mae: 8158.7261\n",
      "Epoch 130/1000\n",
      "1461/1461 [==============================] - 0s 258us/step - loss: 271.3149 - mae: 272.0021 - val_loss: 8259.8693 - val_mae: 8260.5615\n",
      "Epoch 131/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 277.3116 - mae: 278.0010 - val_loss: 8257.1758 - val_mae: 8257.8682\n",
      "Epoch 132/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 255.5306 - mae: 256.2199 - val_loss: 8261.6681 - val_mae: 8262.3604\n",
      "Epoch 133/1000\n",
      "1461/1461 [==============================] - 0s 256us/step - loss: 261.1439 - mae: 261.8337 - val_loss: 8403.2699 - val_mae: 8403.9629\n",
      "Epoch 134/1000\n",
      "1461/1461 [==============================] - 0s 251us/step - loss: 276.5942 - mae: 277.2829 - val_loss: 8075.5129 - val_mae: 8076.2056\n",
      "Epoch 135/1000\n",
      "1461/1461 [==============================] - 0s 256us/step - loss: 241.2182 - mae: 241.9076 - val_loss: 8264.3002 - val_mae: 8264.9932\n",
      "Epoch 136/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 248.2967 - mae: 248.9891 - val_loss: 8301.0495 - val_mae: 8301.7432\n",
      "Epoch 137/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 263.0072 - mae: 263.6975 - val_loss: 8304.0196 - val_mae: 8304.7119\n",
      "Epoch 138/1000\n",
      "1461/1461 [==============================] - 0s 253us/step - loss: 255.9869 - mae: 256.6773 - val_loss: 8022.3127 - val_mae: 8023.0059\n",
      "Epoch 139/1000\n",
      "1461/1461 [==============================] - 0s 254us/step - loss: 247.3803 - mae: 248.0687 - val_loss: 8039.8863 - val_mae: 8040.5796\n",
      "Epoch 140/1000\n",
      "1461/1461 [==============================] - 0s 254us/step - loss: 263.1362 - mae: 263.8265 - val_loss: 8597.1749 - val_mae: 8597.8682\n",
      "Epoch 141/1000\n",
      "1461/1461 [==============================] - 0s 262us/step - loss: 271.0839 - mae: 271.7723 - val_loss: 8476.1653 - val_mae: 8476.8584\n",
      "Epoch 142/1000\n",
      "1461/1461 [==============================] - 0s 261us/step - loss: 340.9969 - mae: 341.6860 - val_loss: 7902.9997 - val_mae: 7903.6934\n",
      "Epoch 143/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 258.3961 - mae: 259.0868 - val_loss: 8414.0366 - val_mae: 8414.7305\n",
      "Epoch 144/1000\n",
      "1461/1461 [==============================] - 0s 257us/step - loss: 269.5661 - mae: 270.2565 - val_loss: 7972.8279 - val_mae: 7973.5215\n",
      "Epoch 145/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 266.0679 - mae: 266.7552 - val_loss: 8414.9889 - val_mae: 8415.6816\n",
      "Epoch 146/1000\n",
      "1461/1461 [==============================] - 0s 258us/step - loss: 247.9140 - mae: 248.6028 - val_loss: 8373.1844 - val_mae: 8373.8770\n",
      "Epoch 147/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 265.6500 - mae: 266.3404 - val_loss: 8174.8840 - val_mae: 8175.5771\n",
      "Epoch 148/1000\n",
      "1461/1461 [==============================] - 0s 253us/step - loss: 236.4777 - mae: 237.1680 - val_loss: 8119.3902 - val_mae: 8120.0840\n",
      "Epoch 149/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 265.3777 - mae: 266.0677 - val_loss: 8233.9996 - val_mae: 8234.6924\n",
      "Epoch 150/1000\n",
      "1461/1461 [==============================] - 0s 254us/step - loss: 257.2569 - mae: 257.9456 - val_loss: 7904.8941 - val_mae: 7905.5869\n",
      "Epoch 151/1000\n",
      "1461/1461 [==============================] - 0s 259us/step - loss: 252.5475 - mae: 253.2381 - val_loss: 7858.4777 - val_mae: 7859.1714\n",
      "Epoch 152/1000\n",
      "1461/1461 [==============================] - 0s 253us/step - loss: 258.0771 - mae: 258.7668 - val_loss: 8334.3965 - val_mae: 8335.0889\n",
      "Epoch 153/1000\n",
      "1461/1461 [==============================] - 0s 251us/step - loss: 250.2452 - mae: 250.9349 - val_loss: 8448.1653 - val_mae: 8448.8574\n",
      "Epoch 154/1000\n",
      "1461/1461 [==============================] - 0s 263us/step - loss: 267.7061 - mae: 268.3955 - val_loss: 7984.4898 - val_mae: 7985.1826\n",
      "Epoch 155/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 255.6691 - mae: 256.3586 - val_loss: 7958.1142 - val_mae: 7958.8076\n",
      "Epoch 156/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 302.2163 - mae: 302.9073 - val_loss: 7802.4035 - val_mae: 7803.0962\n",
      "Epoch 157/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 256.1889 - mae: 256.8782 - val_loss: 7970.0189 - val_mae: 7970.7119\n",
      "Epoch 158/1000\n",
      "1461/1461 [==============================] - 0s 283us/step - loss: 247.6257 - mae: 248.3149 - val_loss: 7841.9593 - val_mae: 7842.6514\n",
      "Epoch 159/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 254.6974 - mae: 255.3878 - val_loss: 8232.0403 - val_mae: 8232.7334\n",
      "Epoch 160/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 242.8710 - mae: 243.5605 - val_loss: 8258.7998 - val_mae: 8259.4922\n",
      "Epoch 161/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 241.1912 - mae: 241.8768 - val_loss: 8167.0705 - val_mae: 8167.7632\n",
      "Epoch 162/1000\n",
      "1461/1461 [==============================] - 0s 289us/step - loss: 239.8136 - mae: 240.5012 - val_loss: 8027.7482 - val_mae: 8028.4409\n",
      "Epoch 163/1000\n",
      "1461/1461 [==============================] - 0s 263us/step - loss: 235.9168 - mae: 236.6049 - val_loss: 8140.4153 - val_mae: 8141.1084\n",
      "Epoch 164/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 230.9550 - mae: 231.6447 - val_loss: 8372.5831 - val_mae: 8373.2764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165/1000\n",
      "1461/1461 [==============================] - 0s 287us/step - loss: 234.9040 - mae: 235.5931 - val_loss: 8656.6268 - val_mae: 8657.3203\n",
      "Epoch 166/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 247.9033 - mae: 248.5940 - val_loss: 7711.1931 - val_mae: 7711.8867\n",
      "Epoch 167/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 248.8437 - mae: 249.5335 - val_loss: 8204.5707 - val_mae: 8205.2627\n",
      "Epoch 168/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 238.5077 - mae: 239.1952 - val_loss: 8067.8182 - val_mae: 8068.5112\n",
      "Epoch 169/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 226.9380 - mae: 227.6263 - val_loss: 8472.7195 - val_mae: 8473.4121\n",
      "Epoch 170/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 236.9844 - mae: 237.6750 - val_loss: 8445.6539 - val_mae: 8446.3477\n",
      "Epoch 171/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 248.8876 - mae: 249.5777 - val_loss: 8174.6468 - val_mae: 8175.3394\n",
      "Epoch 172/1000\n",
      "1461/1461 [==============================] - 0s 287us/step - loss: 235.6461 - mae: 236.3367 - val_loss: 8243.7939 - val_mae: 8244.4873\n",
      "Epoch 173/1000\n",
      "1461/1461 [==============================] - 0s 295us/step - loss: 232.1134 - mae: 232.8022 - val_loss: 8147.1389 - val_mae: 8147.8325\n",
      "Epoch 174/1000\n",
      "1461/1461 [==============================] - 0s 287us/step - loss: 235.5884 - mae: 236.2760 - val_loss: 8308.2450 - val_mae: 8308.9385\n",
      "Epoch 175/1000\n",
      "1461/1461 [==============================] - 0s 300us/step - loss: 254.2713 - mae: 254.9608 - val_loss: 8398.4403 - val_mae: 8399.1338\n",
      "Epoch 176/1000\n",
      "1461/1461 [==============================] - 1s 370us/step - loss: 224.4046 - mae: 225.0927 - val_loss: 8169.5467 - val_mae: 8170.2397\n",
      "Epoch 177/1000\n",
      "1461/1461 [==============================] - 1s 363us/step - loss: 228.6394 - mae: 229.3278 - val_loss: 7716.0348 - val_mae: 7716.7271\n",
      "Epoch 178/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 232.2182 - mae: 232.9070 - val_loss: 7831.8075 - val_mae: 7832.5005\n",
      "Epoch 179/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 253.7450 - mae: 254.4350 - val_loss: 8211.5553 - val_mae: 8212.2490\n",
      "Epoch 180/1000\n",
      "1461/1461 [==============================] - 0s 257us/step - loss: 218.4079 - mae: 219.0953 - val_loss: 8056.3382 - val_mae: 8057.0312\n",
      "Epoch 181/1000\n",
      "1461/1461 [==============================] - 0s 248us/step - loss: 247.4013 - mae: 248.0874 - val_loss: 7851.4865 - val_mae: 7852.1797\n",
      "Epoch 182/1000\n",
      "1461/1461 [==============================] - 0s 341us/step - loss: 237.9542 - mae: 238.6447 - val_loss: 7795.9086 - val_mae: 7796.6025\n",
      "Epoch 183/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 236.1676 - mae: 236.8550 - val_loss: 8182.5113 - val_mae: 8183.2041\n",
      "Epoch 184/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 240.1637 - mae: 240.8516 - val_loss: 8217.4488 - val_mae: 8218.1426\n",
      "Epoch 185/1000\n",
      "1461/1461 [==============================] - 3s 2ms/step - loss: 219.4566 - mae: 220.1479 - val_loss: 8282.6574 - val_mae: 8283.3506\n",
      "Epoch 186/1000\n",
      "1461/1461 [==============================] - 0s 297us/step - loss: 220.5953 - mae: 221.2854 - val_loss: 7643.4511 - val_mae: 7644.1440\n",
      "Epoch 187/1000\n",
      "1461/1461 [==============================] - 1s 395us/step - loss: 247.8995 - mae: 248.5895 - val_loss: 8085.4110 - val_mae: 8086.1040\n",
      "Epoch 188/1000\n",
      "1461/1461 [==============================] - 0s 316us/step - loss: 247.1209 - mae: 247.8110 - val_loss: 8255.9419 - val_mae: 8256.6348\n",
      "Epoch 189/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 232.3943 - mae: 233.0835 - val_loss: 7822.1383 - val_mae: 7822.8306\n",
      "Epoch 190/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 224.6132 - mae: 225.3020 - val_loss: 8371.5018 - val_mae: 8372.1963\n",
      "Epoch 191/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 228.8001 - mae: 229.4877 - val_loss: 8478.6935 - val_mae: 8479.3867\n",
      "Epoch 192/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 256.6270 - mae: 257.3161 - val_loss: 8359.0480 - val_mae: 8359.7422\n",
      "Epoch 193/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 225.4590 - mae: 226.1495 - val_loss: 8276.4719 - val_mae: 8277.1660\n",
      "Epoch 194/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 234.8688 - mae: 235.5576 - val_loss: 8074.0755 - val_mae: 8074.7690\n",
      "Epoch 195/1000\n",
      "1461/1461 [==============================] - 0s 263us/step - loss: 237.3221 - mae: 238.0113 - val_loss: 8141.6609 - val_mae: 8142.3540\n",
      "Epoch 196/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 233.0813 - mae: 233.7702 - val_loss: 8072.3661 - val_mae: 8073.0591\n",
      "Epoch 197/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 205.8899 - mae: 206.5776 - val_loss: 8096.1845 - val_mae: 8096.8779\n",
      "Epoch 198/1000\n",
      "1461/1461 [==============================] - 0s 262us/step - loss: 225.8163 - mae: 226.5056 - val_loss: 7998.2518 - val_mae: 7998.9438\n",
      "Epoch 199/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 210.6175 - mae: 211.3048 - val_loss: 8189.1062 - val_mae: 8189.7998\n",
      "Epoch 200/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 230.0660 - mae: 230.7551 - val_loss: 7917.3586 - val_mae: 7918.0518\n",
      "Epoch 201/1000\n",
      "1461/1461 [==============================] - 0s 262us/step - loss: 237.2498 - mae: 237.9389 - val_loss: 7913.3665 - val_mae: 7914.0591\n",
      "Epoch 202/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 228.3397 - mae: 229.0296 - val_loss: 8233.2230 - val_mae: 8233.9160\n",
      "Epoch 203/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 215.1852 - mae: 215.8728 - val_loss: 7835.9204 - val_mae: 7836.6138\n",
      "Epoch 204/1000\n",
      "1461/1461 [==============================] - 0s 290us/step - loss: 218.7957 - mae: 219.4846 - val_loss: 8343.8041 - val_mae: 8344.4971\n",
      "Epoch 205/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 211.8476 - mae: 212.5360 - val_loss: 8033.7232 - val_mae: 8034.4160\n",
      "Epoch 206/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 208.5434 - mae: 209.2320 - val_loss: 8280.4747 - val_mae: 8281.1680\n",
      "Epoch 207/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 231.2041 - mae: 231.8949 - val_loss: 7740.4739 - val_mae: 7741.1675\n",
      "Epoch 208/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 220.9659 - mae: 221.6548 - val_loss: 7875.3942 - val_mae: 7876.0869\n",
      "Epoch 209/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 238.5862 - mae: 239.2763 - val_loss: 8143.0944 - val_mae: 8143.7871\n",
      "Epoch 210/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 213.8797 - mae: 214.5695 - val_loss: 7676.9620 - val_mae: 7677.6558\n",
      "Epoch 211/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 259.7003 - mae: 260.3909 - val_loss: 7845.2716 - val_mae: 7845.9648\n",
      "Epoch 212/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 226.7443 - mae: 227.4350 - val_loss: 7982.6616 - val_mae: 7983.3550\n",
      "Epoch 213/1000\n",
      "1461/1461 [==============================] - 0s 283us/step - loss: 228.9969 - mae: 229.6884 - val_loss: 7616.4056 - val_mae: 7617.0996\n",
      "Epoch 214/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 227.1798 - mae: 227.8681 - val_loss: 8184.2114 - val_mae: 8184.9053\n",
      "Epoch 215/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 220.1558 - mae: 220.8460 - val_loss: 7566.8342 - val_mae: 7567.5269\n",
      "Epoch 216/1000\n",
      "1461/1461 [==============================] - 0s 263us/step - loss: 210.0690 - mae: 210.7583 - val_loss: 7868.1774 - val_mae: 7868.8701\n",
      "Epoch 217/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 207.4895 - mae: 208.1781 - val_loss: 7380.9671 - val_mae: 7381.6606\n",
      "Epoch 218/1000\n",
      "1461/1461 [==============================] - 0s 261us/step - loss: 211.8800 - mae: 212.5680 - val_loss: 7956.3657 - val_mae: 7957.0591\n",
      "Epoch 219/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 265us/step - loss: 218.3432 - mae: 219.0311 - val_loss: 8592.6005 - val_mae: 8593.2939\n",
      "Epoch 220/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 225.4770 - mae: 226.1667 - val_loss: 7960.0334 - val_mae: 7960.7261\n",
      "Epoch 221/1000\n",
      "1461/1461 [==============================] - 0s 301us/step - loss: 229.0850 - mae: 229.7749 - val_loss: 8004.6870 - val_mae: 8005.3804\n",
      "Epoch 222/1000\n",
      "1461/1461 [==============================] - 0s 256us/step - loss: 223.4431 - mae: 224.1324 - val_loss: 8072.0137 - val_mae: 8072.7070\n",
      "Epoch 223/1000\n",
      "1461/1461 [==============================] - 0s 257us/step - loss: 217.9363 - mae: 218.6254 - val_loss: 7953.0833 - val_mae: 7953.7759\n",
      "Epoch 224/1000\n",
      "1461/1461 [==============================] - 0s 250us/step - loss: 228.9251 - mae: 229.6141 - val_loss: 8172.8108 - val_mae: 8173.5039\n",
      "Epoch 225/1000\n",
      "1461/1461 [==============================] - 0s 261us/step - loss: 205.8754 - mae: 206.5639 - val_loss: 8292.2099 - val_mae: 8292.9023\n",
      "Epoch 226/1000\n",
      "1461/1461 [==============================] - 0s 244us/step - loss: 221.4031 - mae: 222.0913 - val_loss: 7722.4997 - val_mae: 7723.1919\n",
      "Epoch 227/1000\n",
      "1461/1461 [==============================] - 0s 248us/step - loss: 203.9781 - mae: 204.6672 - val_loss: 8178.1778 - val_mae: 8178.8716\n",
      "Epoch 228/1000\n",
      "1461/1461 [==============================] - 0s 251us/step - loss: 202.9989 - mae: 203.6885 - val_loss: 8196.8411 - val_mae: 8197.5352\n",
      "Epoch 229/1000\n",
      "1461/1461 [==============================] - 0s 249us/step - loss: 223.7301 - mae: 224.4207 - val_loss: 7994.2320 - val_mae: 7994.9253\n",
      "Epoch 230/1000\n",
      "1461/1461 [==============================] - 0s 248us/step - loss: 203.6551 - mae: 204.3463 - val_loss: 7877.5615 - val_mae: 7878.2544\n",
      "Epoch 231/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 226.4129 - mae: 227.1028 - val_loss: 7842.3137 - val_mae: 7843.0068\n",
      "Epoch 232/1000\n",
      "1461/1461 [==============================] - 0s 261us/step - loss: 206.0738 - mae: 206.7628 - val_loss: 8072.5496 - val_mae: 8073.2432\n",
      "Epoch 233/1000\n",
      "1461/1461 [==============================] - 0s 250us/step - loss: 201.5211 - mae: 202.2090 - val_loss: 8061.5708 - val_mae: 8062.2642\n",
      "Epoch 234/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 207.4958 - mae: 208.1869 - val_loss: 7998.6419 - val_mae: 7999.3335\n",
      "Epoch 235/1000\n",
      "1461/1461 [==============================] - 0s 253us/step - loss: 213.9970 - mae: 214.6865 - val_loss: 8591.9895 - val_mae: 8592.6836\n",
      "Epoch 236/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 244.9716 - mae: 245.6610 - val_loss: 7896.0890 - val_mae: 7896.7817\n",
      "Epoch 237/1000\n",
      "1461/1461 [==============================] - 0s 253us/step - loss: 219.6567 - mae: 220.3457 - val_loss: 8693.8026 - val_mae: 8694.4961\n",
      "Epoch 238/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 214.2978 - mae: 214.9858 - val_loss: 7945.3225 - val_mae: 7946.0161\n",
      "Epoch 239/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 212.6165 - mae: 213.3065 - val_loss: 7817.5742 - val_mae: 7818.2666\n",
      "Epoch 240/1000\n",
      "1461/1461 [==============================] - 0s 250us/step - loss: 192.4762 - mae: 193.1628 - val_loss: 7726.5862 - val_mae: 7727.2793\n",
      "Epoch 241/1000\n",
      "1461/1461 [==============================] - 0s 249us/step - loss: 201.9412 - mae: 202.6313 - val_loss: 7597.9892 - val_mae: 7598.6821\n",
      "Epoch 242/1000\n",
      "1461/1461 [==============================] - 0s 258us/step - loss: 254.7496 - mae: 255.4396 - val_loss: 8278.0089 - val_mae: 8278.7021\n",
      "Epoch 243/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 212.2715 - mae: 212.9609 - val_loss: 7869.4336 - val_mae: 7870.1270\n",
      "Epoch 244/1000\n",
      "1461/1461 [==============================] - 0s 253us/step - loss: 206.7105 - mae: 207.3982 - val_loss: 8104.7914 - val_mae: 8105.4839\n",
      "Epoch 245/1000\n",
      "1461/1461 [==============================] - 0s 261us/step - loss: 206.6647 - mae: 207.3526 - val_loss: 8150.0621 - val_mae: 8150.7554\n",
      "Epoch 246/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 227.3256 - mae: 228.0138 - val_loss: 7757.8041 - val_mae: 7758.4980\n",
      "Epoch 247/1000\n",
      "1461/1461 [==============================] - 0s 254us/step - loss: 202.4052 - mae: 203.0956 - val_loss: 8469.6609 - val_mae: 8470.3535\n",
      "Epoch 248/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 206.4521 - mae: 207.1382 - val_loss: 8307.7099 - val_mae: 8308.4033\n",
      "Epoch 249/1000\n",
      "1461/1461 [==============================] - 0s 253us/step - loss: 204.4896 - mae: 205.1768 - val_loss: 7746.8157 - val_mae: 7747.5088\n",
      "Epoch 250/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 225.6460 - mae: 226.3342 - val_loss: 8329.1591 - val_mae: 8329.8525\n",
      "Epoch 251/1000\n",
      "1461/1461 [==============================] - 0s 254us/step - loss: 198.6009 - mae: 199.2908 - val_loss: 8178.5343 - val_mae: 8179.2271\n",
      "Epoch 252/1000\n",
      "1461/1461 [==============================] - 0s 256us/step - loss: 217.0624 - mae: 217.7498 - val_loss: 7713.3544 - val_mae: 7714.0483\n",
      "Epoch 253/1000\n",
      "1461/1461 [==============================] - 0s 257us/step - loss: 202.5676 - mae: 203.2549 - val_loss: 8078.0483 - val_mae: 8078.7417\n",
      "Epoch 254/1000\n",
      "1461/1461 [==============================] - 0s 254us/step - loss: 196.3887 - mae: 197.0776 - val_loss: 8483.4595 - val_mae: 8484.1523\n",
      "Epoch 255/1000\n",
      "1461/1461 [==============================] - 0s 259us/step - loss: 215.6461 - mae: 216.3361 - val_loss: 7823.2712 - val_mae: 7823.9644\n",
      "Epoch 256/1000\n",
      "1461/1461 [==============================] - 0s 253us/step - loss: 199.3193 - mae: 200.0092 - val_loss: 8085.7037 - val_mae: 8086.3970\n",
      "Epoch 257/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 194.9331 - mae: 195.6204 - val_loss: 8186.9501 - val_mae: 8187.6431\n",
      "Epoch 258/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 211.3143 - mae: 212.0027 - val_loss: 8316.3257 - val_mae: 8317.0195\n",
      "Epoch 259/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 206.5725 - mae: 207.2615 - val_loss: 8049.5136 - val_mae: 8050.2070\n",
      "Epoch 260/1000\n",
      "1461/1461 [==============================] - 0s 251us/step - loss: 199.7399 - mae: 200.4301 - val_loss: 7911.9656 - val_mae: 7912.6592\n",
      "Epoch 261/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 207.0902 - mae: 207.7793 - val_loss: 8379.8621 - val_mae: 8380.5547\n",
      "Epoch 262/1000\n",
      "1461/1461 [==============================] - 0s 254us/step - loss: 189.1153 - mae: 189.8032 - val_loss: 8043.1839 - val_mae: 8043.8760\n",
      "Epoch 263/1000\n",
      "1461/1461 [==============================] - 0s 257us/step - loss: 188.5874 - mae: 189.2758 - val_loss: 7834.3932 - val_mae: 7835.0859\n",
      "Epoch 264/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 209.0086 - mae: 209.6971 - val_loss: 7799.8241 - val_mae: 7800.5151\n",
      "Epoch 265/1000\n",
      "1461/1461 [==============================] - 0s 256us/step - loss: 192.7871 - mae: 193.4767 - val_loss: 8236.1476 - val_mae: 8236.8418\n",
      "Epoch 266/1000\n",
      "1461/1461 [==============================] - 0s 256us/step - loss: 196.1144 - mae: 196.8035 - val_loss: 7862.3711 - val_mae: 7863.0640\n",
      "Epoch 267/1000\n",
      "1461/1461 [==============================] - 0s 256us/step - loss: 195.6582 - mae: 196.3484 - val_loss: 8236.9712 - val_mae: 8237.6641\n",
      "Epoch 268/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 203.6230 - mae: 204.3103 - val_loss: 8081.2844 - val_mae: 8081.9775\n",
      "Epoch 269/1000\n",
      "1461/1461 [==============================] - 0s 257us/step - loss: 202.5310 - mae: 203.2185 - val_loss: 8224.2018 - val_mae: 8224.8945\n",
      "Epoch 270/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 187.0591 - mae: 187.7481 - val_loss: 7745.8194 - val_mae: 7746.5127\n",
      "Epoch 271/1000\n",
      "1461/1461 [==============================] - 0s 254us/step - loss: 200.3374 - mae: 201.0249 - val_loss: 8000.3986 - val_mae: 8001.0913\n",
      "Epoch 272/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 215.9940 - mae: 216.6837 - val_loss: 8209.1441 - val_mae: 8209.8379\n",
      "Epoch 273/1000\n",
      "1461/1461 [==============================] - 0s 254us/step - loss: 203.9306 - mae: 204.6195 - val_loss: 8131.2449 - val_mae: 8131.9380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 274/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 223.1484 - mae: 223.8364 - val_loss: 8031.0208 - val_mae: 8031.7139\n",
      "Epoch 275/1000\n",
      "1461/1461 [==============================] - 0s 251us/step - loss: 190.4372 - mae: 191.1280 - val_loss: 7676.6274 - val_mae: 7677.3208\n",
      "Epoch 276/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 194.2061 - mae: 194.8954 - val_loss: 8214.4758 - val_mae: 8215.1689\n",
      "Epoch 277/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 210.9564 - mae: 211.6449 - val_loss: 7955.4773 - val_mae: 7956.1714\n",
      "Epoch 278/1000\n",
      "1461/1461 [==============================] - 0s 250us/step - loss: 183.5784 - mae: 184.2661 - val_loss: 7870.3141 - val_mae: 7871.0078\n",
      "Epoch 279/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 188.6658 - mae: 189.3537 - val_loss: 8039.3029 - val_mae: 8039.9956\n",
      "Epoch 280/1000\n",
      "1461/1461 [==============================] - 0s 251us/step - loss: 186.0599 - mae: 186.7477 - val_loss: 8457.2574 - val_mae: 8457.9502\n",
      "Epoch 281/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 201.9393 - mae: 202.6290 - val_loss: 8490.1901 - val_mae: 8490.8838\n",
      "Epoch 282/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 192.2319 - mae: 192.9194 - val_loss: 7953.5948 - val_mae: 7954.2876\n",
      "Epoch 283/1000\n",
      "1461/1461 [==============================] - 0s 249us/step - loss: 180.1690 - mae: 180.8587 - val_loss: 7843.5263 - val_mae: 7844.2197\n",
      "Epoch 284/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 190.8121 - mae: 191.5000 - val_loss: 8166.4850 - val_mae: 8167.1777\n",
      "Epoch 285/1000\n",
      "1461/1461 [==============================] - 0s 259us/step - loss: 177.9516 - mae: 178.6376 - val_loss: 7990.5938 - val_mae: 7991.2876\n",
      "Epoch 286/1000\n",
      "1461/1461 [==============================] - 0s 257us/step - loss: 198.0821 - mae: 198.7725 - val_loss: 8260.0945 - val_mae: 8260.7881\n",
      "Epoch 287/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 194.1478 - mae: 194.8339 - val_loss: 8317.4845 - val_mae: 8318.1768\n",
      "Epoch 288/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 193.3952 - mae: 194.0852 - val_loss: 8527.8065 - val_mae: 8528.5000\n",
      "Epoch 289/1000\n",
      "1461/1461 [==============================] - 0s 262us/step - loss: 190.6377 - mae: 191.3246 - val_loss: 8267.8738 - val_mae: 8268.5664\n",
      "Epoch 290/1000\n",
      "1461/1461 [==============================] - 0s 248us/step - loss: 198.1771 - mae: 198.8638 - val_loss: 8560.5888 - val_mae: 8561.2812\n",
      "Epoch 291/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 198.9578 - mae: 199.6481 - val_loss: 7951.6277 - val_mae: 7952.3218\n",
      "Epoch 292/1000\n",
      "1461/1461 [==============================] - 0s 256us/step - loss: 185.8943 - mae: 186.5828 - val_loss: 8191.3338 - val_mae: 8192.0273\n",
      "Epoch 293/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 198.3831 - mae: 199.0712 - val_loss: 8292.6970 - val_mae: 8293.3906\n",
      "Epoch 294/1000\n",
      "1461/1461 [==============================] - 0s 246us/step - loss: 193.4559 - mae: 194.1431 - val_loss: 8274.6691 - val_mae: 8275.3613\n",
      "Epoch 295/1000\n",
      "1461/1461 [==============================] - 0s 251us/step - loss: 198.6404 - mae: 199.3310 - val_loss: 7780.7373 - val_mae: 7781.4312\n",
      "Epoch 296/1000\n",
      "1461/1461 [==============================] - 0s 249us/step - loss: 189.6684 - mae: 190.3571 - val_loss: 8219.3880 - val_mae: 8220.0811\n",
      "Epoch 297/1000\n",
      "1461/1461 [==============================] - 0s 251us/step - loss: 186.6699 - mae: 187.3571 - val_loss: 8281.6308 - val_mae: 8282.3242\n",
      "Epoch 298/1000\n",
      "1461/1461 [==============================] - 0s 248us/step - loss: 187.7891 - mae: 188.4788 - val_loss: 8132.4426 - val_mae: 8133.1353\n",
      "Epoch 299/1000\n",
      "1461/1461 [==============================] - 0s 253us/step - loss: 203.9965 - mae: 204.6845 - val_loss: 8273.2463 - val_mae: 8273.9395\n",
      "Epoch 300/1000\n",
      "1461/1461 [==============================] - 0s 251us/step - loss: 193.6351 - mae: 194.3229 - val_loss: 8212.3164 - val_mae: 8213.0088\n",
      "Epoch 301/1000\n",
      "1461/1461 [==============================] - 0s 258us/step - loss: 212.8336 - mae: 213.5239 - val_loss: 8093.8192 - val_mae: 8094.5127\n",
      "Epoch 302/1000\n",
      "1461/1461 [==============================] - 0s 253us/step - loss: 184.0198 - mae: 184.7080 - val_loss: 7983.9162 - val_mae: 7984.6084\n",
      "Epoch 303/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 190.8776 - mae: 191.5669 - val_loss: 8600.7424 - val_mae: 8601.4365\n",
      "Epoch 304/1000\n",
      "1461/1461 [==============================] - 0s 283us/step - loss: 200.1548 - mae: 200.8451 - val_loss: 8119.2047 - val_mae: 8119.8975\n",
      "Epoch 305/1000\n",
      "1461/1461 [==============================] - 0s 262us/step - loss: 191.6476 - mae: 192.3352 - val_loss: 7672.1372 - val_mae: 7672.8306\n",
      "Epoch 306/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 193.8390 - mae: 194.5287 - val_loss: 7920.8348 - val_mae: 7921.5269\n",
      "Epoch 307/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 190.4230 - mae: 191.1121 - val_loss: 8103.3447 - val_mae: 8104.0371\n",
      "Epoch 308/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 177.7391 - mae: 178.4278 - val_loss: 8346.9410 - val_mae: 8347.6328\n",
      "Epoch 309/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 204.0059 - mae: 204.6927 - val_loss: 8377.4957 - val_mae: 8378.1885\n",
      "Epoch 310/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 183.8589 - mae: 184.5473 - val_loss: 7758.9709 - val_mae: 7759.6641\n",
      "Epoch 311/1000\n",
      "1461/1461 [==============================] - 0s 257us/step - loss: 186.7215 - mae: 187.4086 - val_loss: 8281.0540 - val_mae: 8281.7480\n",
      "Epoch 312/1000\n",
      "1461/1461 [==============================] - 0s 261us/step - loss: 179.9569 - mae: 180.6466 - val_loss: 8034.5291 - val_mae: 8035.2227\n",
      "Epoch 313/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 181.3716 - mae: 182.0605 - val_loss: 7933.2339 - val_mae: 7933.9272\n",
      "Epoch 314/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 187.4363 - mae: 188.1245 - val_loss: 8092.1921 - val_mae: 8092.8853\n",
      "Epoch 315/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 201.4026 - mae: 202.0909 - val_loss: 8109.1469 - val_mae: 8109.8398\n",
      "Epoch 316/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 179.2162 - mae: 179.9040 - val_loss: 7937.0420 - val_mae: 7937.7354\n",
      "Epoch 317/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 166.4671 - mae: 167.1557 - val_loss: 7981.9723 - val_mae: 7982.6655\n",
      "Epoch 318/1000\n",
      "1461/1461 [==============================] - 0s 281us/step - loss: 196.0474 - mae: 196.7380 - val_loss: 8623.3733 - val_mae: 8624.0674\n",
      "Epoch 319/1000\n",
      "1461/1461 [==============================] - 0s 281us/step - loss: 190.5921 - mae: 191.2799 - val_loss: 8249.4037 - val_mae: 8250.0967\n",
      "Epoch 320/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 184.6836 - mae: 185.3731 - val_loss: 8398.7054 - val_mae: 8399.3994\n",
      "Epoch 321/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 168.7006 - mae: 169.3883 - val_loss: 8036.8131 - val_mae: 8037.5063\n",
      "Epoch 322/1000\n",
      "1461/1461 [==============================] - 0s 263us/step - loss: 179.7327 - mae: 180.4200 - val_loss: 8017.2773 - val_mae: 8017.9712\n",
      "Epoch 323/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 183.1161 - mae: 183.8046 - val_loss: 8143.9113 - val_mae: 8144.6035\n",
      "Epoch 324/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 173.3342 - mae: 174.0228 - val_loss: 7990.9164 - val_mae: 7991.6089\n",
      "Epoch 325/1000\n",
      "1461/1461 [==============================] - 0s 259us/step - loss: 178.3034 - mae: 178.9903 - val_loss: 8120.7572 - val_mae: 8121.4507\n",
      "Epoch 326/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 182.6738 - mae: 183.3605 - val_loss: 7732.1580 - val_mae: 7732.8501\n",
      "Epoch 327/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 179.9951 - mae: 180.6832 - val_loss: 8113.5150 - val_mae: 8114.2080\n",
      "Epoch 328/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 268us/step - loss: 176.3279 - mae: 177.0182 - val_loss: 8094.4059 - val_mae: 8095.0991\n",
      "Epoch 329/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 181.5197 - mae: 182.2103 - val_loss: 8018.0352 - val_mae: 8018.7290\n",
      "Epoch 330/1000\n",
      "1461/1461 [==============================] - 0s 289us/step - loss: 176.3007 - mae: 176.9890 - val_loss: 8094.7636 - val_mae: 8095.4570\n",
      "Epoch 331/1000\n",
      "1461/1461 [==============================] - 0s 261us/step - loss: 187.2730 - mae: 187.9599 - val_loss: 8071.9427 - val_mae: 8072.6357\n",
      "Epoch 332/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 169.2580 - mae: 169.9476 - val_loss: 8190.1958 - val_mae: 8190.8896\n",
      "Epoch 333/1000\n",
      "1461/1461 [==============================] - 0s 263us/step - loss: 176.6611 - mae: 177.3499 - val_loss: 7710.5019 - val_mae: 7711.1953\n",
      "Epoch 334/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 177.9824 - mae: 178.6684 - val_loss: 8204.4322 - val_mae: 8205.1250\n",
      "Epoch 335/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 179.1544 - mae: 179.8424 - val_loss: 8060.8232 - val_mae: 8061.5166\n",
      "Epoch 336/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 177.8689 - mae: 178.5583 - val_loss: 8151.7217 - val_mae: 8152.4150\n",
      "Epoch 337/1000\n",
      "1461/1461 [==============================] - 0s 256us/step - loss: 179.4465 - mae: 180.1340 - val_loss: 8282.8424 - val_mae: 8283.5361\n",
      "Epoch 338/1000\n",
      "1461/1461 [==============================] - 0s 251us/step - loss: 195.2324 - mae: 195.9214 - val_loss: 8029.5247 - val_mae: 8030.2178\n",
      "Epoch 339/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 183.7344 - mae: 184.4232 - val_loss: 8134.6118 - val_mae: 8135.3047\n",
      "Epoch 340/1000\n",
      "1461/1461 [==============================] - 0s 253us/step - loss: 191.3530 - mae: 192.0395 - val_loss: 7835.7130 - val_mae: 7836.4062\n",
      "Epoch 341/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 200.4000 - mae: 201.0884 - val_loss: 8255.8153 - val_mae: 8256.5088\n",
      "Epoch 342/1000\n",
      "1461/1461 [==============================] - 0s 251us/step - loss: 175.5106 - mae: 176.1994 - val_loss: 8114.6949 - val_mae: 8115.3877\n",
      "Epoch 343/1000\n",
      "1461/1461 [==============================] - 0s 247us/step - loss: 179.9162 - mae: 180.6045 - val_loss: 7937.0150 - val_mae: 7937.7075\n",
      "Epoch 344/1000\n",
      "1461/1461 [==============================] - 0s 253us/step - loss: 156.7665 - mae: 157.4539 - val_loss: 8141.4335 - val_mae: 8142.1265\n",
      "Epoch 345/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 171.7855 - mae: 172.4750 - val_loss: 8106.8596 - val_mae: 8107.5532\n",
      "Epoch 346/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 175.9861 - mae: 176.6749 - val_loss: 8241.2787 - val_mae: 8241.9717\n",
      "Epoch 347/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 179.9034 - mae: 180.5918 - val_loss: 8279.0933 - val_mae: 8279.7842\n",
      "Epoch 348/1000\n",
      "1461/1461 [==============================] - 0s 258us/step - loss: 189.4200 - mae: 190.1083 - val_loss: 8095.9870 - val_mae: 8096.6792\n",
      "Epoch 349/1000\n",
      "1461/1461 [==============================] - 0s 259us/step - loss: 176.5365 - mae: 177.2244 - val_loss: 8084.9331 - val_mae: 8085.6265\n",
      "Epoch 350/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 182.2602 - mae: 182.9498 - val_loss: 8059.0345 - val_mae: 8059.7275\n",
      "Epoch 351/1000\n",
      "1461/1461 [==============================] - 0s 247us/step - loss: 191.9246 - mae: 192.6140 - val_loss: 8061.2991 - val_mae: 8061.9922\n",
      "Epoch 352/1000\n",
      "1461/1461 [==============================] - 0s 250us/step - loss: 173.4843 - mae: 174.1716 - val_loss: 7985.3813 - val_mae: 7986.0752\n",
      "Epoch 353/1000\n",
      "1461/1461 [==============================] - 0s 254us/step - loss: 167.7724 - mae: 168.4627 - val_loss: 8034.6210 - val_mae: 8035.3140\n",
      "Epoch 354/1000\n",
      "1461/1461 [==============================] - 0s 253us/step - loss: 170.5034 - mae: 171.1926 - val_loss: 8062.5153 - val_mae: 8063.2090\n",
      "Epoch 355/1000\n",
      "1461/1461 [==============================] - 0s 254us/step - loss: 187.7109 - mae: 188.3992 - val_loss: 8089.1877 - val_mae: 8089.8809\n",
      "Epoch 356/1000\n",
      "1461/1461 [==============================] - 0s 256us/step - loss: 172.2411 - mae: 172.9290 - val_loss: 8159.7923 - val_mae: 8160.4854\n",
      "Epoch 357/1000\n",
      "1461/1461 [==============================] - 0s 256us/step - loss: 184.0102 - mae: 184.6982 - val_loss: 8277.8252 - val_mae: 8278.5186\n",
      "Epoch 358/1000\n",
      "1461/1461 [==============================] - 0s 259us/step - loss: 175.5505 - mae: 176.2382 - val_loss: 8029.3729 - val_mae: 8030.0664\n",
      "Epoch 359/1000\n",
      "1461/1461 [==============================] - 0s 249us/step - loss: 171.6323 - mae: 172.3186 - val_loss: 8143.4798 - val_mae: 8144.1733\n",
      "Epoch 360/1000\n",
      "1461/1461 [==============================] - 0s 253us/step - loss: 166.2130 - mae: 166.9011 - val_loss: 8297.0186 - val_mae: 8297.7129\n",
      "Epoch 361/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 194.6012 - mae: 195.2905 - val_loss: 7887.8346 - val_mae: 7888.5273\n",
      "Epoch 362/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 199.6028 - mae: 200.2916 - val_loss: 8189.4205 - val_mae: 8190.1123\n",
      "Epoch 363/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 186.6587 - mae: 187.3476 - val_loss: 7972.5691 - val_mae: 7973.2622\n",
      "Epoch 364/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 180.8968 - mae: 181.5856 - val_loss: 8145.9850 - val_mae: 8146.6777\n",
      "Epoch 365/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 174.6967 - mae: 175.3860 - val_loss: 7838.8881 - val_mae: 7839.5801\n",
      "Epoch 366/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 186.9095 - mae: 187.5965 - val_loss: 8043.8106 - val_mae: 8044.5044\n",
      "Epoch 367/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 162.6575 - mae: 163.3435 - val_loss: 8136.7843 - val_mae: 8137.4766\n",
      "Epoch 368/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 174.9230 - mae: 175.6103 - val_loss: 8056.5146 - val_mae: 8057.2080\n",
      "Epoch 369/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 161.8169 - mae: 162.5049 - val_loss: 8211.9007 - val_mae: 8212.5938\n",
      "Epoch 370/1000\n",
      "1461/1461 [==============================] - 0s 262us/step - loss: 173.3091 - mae: 173.9981 - val_loss: 8135.8193 - val_mae: 8136.5127\n",
      "Epoch 371/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 173.0887 - mae: 173.7766 - val_loss: 8008.6611 - val_mae: 8009.3540\n",
      "Epoch 372/1000\n",
      "1461/1461 [==============================] - 0s 288us/step - loss: 191.3899 - mae: 192.0760 - val_loss: 8290.4658 - val_mae: 8291.1582\n",
      "Epoch 373/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 196.8861 - mae: 197.5750 - val_loss: 7996.1308 - val_mae: 7996.8247\n",
      "Epoch 374/1000\n",
      "1461/1461 [==============================] - 0s 258us/step - loss: 161.8098 - mae: 162.4966 - val_loss: 7953.5913 - val_mae: 7954.2847\n",
      "Epoch 375/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 156.7341 - mae: 157.4212 - val_loss: 8010.5696 - val_mae: 8011.2637\n",
      "Epoch 376/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 166.1962 - mae: 166.8844 - val_loss: 7931.0842 - val_mae: 7931.7769\n",
      "Epoch 377/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 155.6016 - mae: 156.2888 - val_loss: 7983.8968 - val_mae: 7984.5898\n",
      "Epoch 378/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 182.7921 - mae: 183.4795 - val_loss: 8270.3127 - val_mae: 8271.0068\n",
      "Epoch 379/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 159.3112 - mae: 159.9974 - val_loss: 8239.7220 - val_mae: 8240.4160\n",
      "Epoch 380/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 166.7123 - mae: 167.4001 - val_loss: 8106.1898 - val_mae: 8106.8828\n",
      "Epoch 381/1000\n",
      "1461/1461 [==============================] - 1s 355us/step - loss: 157.9193 - mae: 158.6044 - val_loss: 8202.9656 - val_mae: 8203.6582\n",
      "Epoch 382/1000\n",
      "1461/1461 [==============================] - 0s 259us/step - loss: 171.7857 - mae: 172.4717 - val_loss: 8245.6038 - val_mae: 8246.2969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 383/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 170.0885 - mae: 170.7762 - val_loss: 8315.3497 - val_mae: 8316.0439\n",
      "Epoch 384/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 165.8065 - mae: 166.4964 - val_loss: 8309.0054 - val_mae: 8309.6982\n",
      "Epoch 385/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 164.9764 - mae: 165.6632 - val_loss: 8133.4213 - val_mae: 8134.1143\n",
      "Epoch 386/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 174.3392 - mae: 175.0263 - val_loss: 8123.6694 - val_mae: 8124.3628\n",
      "Epoch 387/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 182.2098 - mae: 182.8950 - val_loss: 8302.3637 - val_mae: 8303.0566\n",
      "Epoch 388/1000\n",
      "1461/1461 [==============================] - 0s 305us/step - loss: 162.5675 - mae: 163.2563 - val_loss: 8090.4001 - val_mae: 8091.0938\n",
      "Epoch 389/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 187.4613 - mae: 188.1500 - val_loss: 8422.4407 - val_mae: 8423.1348\n",
      "Epoch 390/1000\n",
      "1461/1461 [==============================] - 0s 262us/step - loss: 182.1249 - mae: 182.8143 - val_loss: 8298.9176 - val_mae: 8299.6113\n",
      "Epoch 391/1000\n",
      "1461/1461 [==============================] - 0s 259us/step - loss: 169.9636 - mae: 170.6516 - val_loss: 7912.0655 - val_mae: 7912.7588\n",
      "Epoch 392/1000\n",
      "1461/1461 [==============================] - 0s 291us/step - loss: 167.5157 - mae: 168.2037 - val_loss: 8173.3865 - val_mae: 8174.0791\n",
      "Epoch 393/1000\n",
      "1461/1461 [==============================] - 0s 307us/step - loss: 203.2578 - mae: 203.9431 - val_loss: 8173.3255 - val_mae: 8174.0176\n",
      "Epoch 394/1000\n",
      "1461/1461 [==============================] - 0s 300us/step - loss: 167.2469 - mae: 167.9371 - val_loss: 8254.0673 - val_mae: 8254.7588\n",
      "Epoch 395/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 151.0304 - mae: 151.7166 - val_loss: 7948.4838 - val_mae: 7949.1748\n",
      "Epoch 396/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 177.3897 - mae: 178.0781 - val_loss: 8245.5034 - val_mae: 8246.1973\n",
      "Epoch 397/1000\n",
      "1461/1461 [==============================] - 0s 262us/step - loss: 155.9738 - mae: 156.6616 - val_loss: 8241.9508 - val_mae: 8242.6445\n",
      "Epoch 398/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 173.5306 - mae: 174.2183 - val_loss: 8410.1903 - val_mae: 8410.8828\n",
      "Epoch 399/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 171.3421 - mae: 172.0306 - val_loss: 8489.3393 - val_mae: 8490.0322\n",
      "Epoch 400/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 176.0190 - mae: 176.7066 - val_loss: 8006.8750 - val_mae: 8007.5679\n",
      "Epoch 401/1000\n",
      "1461/1461 [==============================] - 0s 243us/step - loss: 163.7477 - mae: 164.4363 - val_loss: 8354.0514 - val_mae: 8354.7441\n",
      "Epoch 402/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 172.3372 - mae: 173.0271 - val_loss: 8061.5697 - val_mae: 8062.2632\n",
      "Epoch 403/1000\n",
      "1461/1461 [==============================] - 0s 247us/step - loss: 179.7581 - mae: 180.4450 - val_loss: 8184.5025 - val_mae: 8185.1953\n",
      "Epoch 404/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 159.0593 - mae: 159.7472 - val_loss: 8450.6341 - val_mae: 8451.3271\n",
      "Epoch 405/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 166.1490 - mae: 166.8355 - val_loss: 8044.3232 - val_mae: 8045.0166\n",
      "Epoch 406/1000\n",
      "1461/1461 [==============================] - 0s 247us/step - loss: 152.1371 - mae: 152.8262 - val_loss: 8096.4110 - val_mae: 8097.1040\n",
      "Epoch 407/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 157.1083 - mae: 157.7972 - val_loss: 8095.9685 - val_mae: 8096.6621\n",
      "Epoch 408/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 155.5928 - mae: 156.2817 - val_loss: 7943.6040 - val_mae: 7944.2974\n",
      "Epoch 409/1000\n",
      "1461/1461 [==============================] - 0s 243us/step - loss: 160.8850 - mae: 161.5738 - val_loss: 8318.1384 - val_mae: 8318.8320\n",
      "Epoch 410/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 171.5431 - mae: 172.2299 - val_loss: 8482.6152 - val_mae: 8483.3086\n",
      "Epoch 411/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 191.7849 - mae: 192.4750 - val_loss: 8664.1302 - val_mae: 8664.8223\n",
      "Epoch 412/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 182.3939 - mae: 183.0820 - val_loss: 8057.0375 - val_mae: 8057.7300\n",
      "Epoch 413/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 166.3508 - mae: 167.0393 - val_loss: 7882.5469 - val_mae: 7883.2407\n",
      "Epoch 414/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 164.7376 - mae: 165.4241 - val_loss: 8092.1141 - val_mae: 8092.8071\n",
      "Epoch 415/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 165.3741 - mae: 166.0613 - val_loss: 8171.4943 - val_mae: 8172.1875\n",
      "Epoch 416/1000\n",
      "1461/1461 [==============================] - 0s 257us/step - loss: 164.7297 - mae: 165.4153 - val_loss: 8248.6502 - val_mae: 8249.3428\n",
      "Epoch 417/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 161.7095 - mae: 162.3977 - val_loss: 8146.7769 - val_mae: 8147.4702\n",
      "Epoch 418/1000\n",
      "1461/1461 [==============================] - 0s 261us/step - loss: 164.8247 - mae: 165.5110 - val_loss: 8120.7940 - val_mae: 8121.4873\n",
      "Epoch 419/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 159.7303 - mae: 160.4169 - val_loss: 8016.3976 - val_mae: 8017.0903\n",
      "Epoch 420/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 174.3072 - mae: 174.9947 - val_loss: 7681.6263 - val_mae: 7682.3198\n",
      "Epoch 421/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 162.5391 - mae: 163.2272 - val_loss: 8323.5802 - val_mae: 8324.2734\n",
      "Epoch 422/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 167.4687 - mae: 168.1579 - val_loss: 8143.5948 - val_mae: 8144.2871\n",
      "Epoch 423/1000\n",
      "1461/1461 [==============================] - 0s 248us/step - loss: 170.8131 - mae: 171.5005 - val_loss: 8145.1871 - val_mae: 8145.8809\n",
      "Epoch 424/1000\n",
      "1461/1461 [==============================] - 0s 256us/step - loss: 159.6280 - mae: 160.3161 - val_loss: 8056.5416 - val_mae: 8057.2339\n",
      "Epoch 425/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 167.8669 - mae: 168.5562 - val_loss: 8456.3569 - val_mae: 8457.0498\n",
      "Epoch 426/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 159.7674 - mae: 160.4549 - val_loss: 7913.6988 - val_mae: 7914.3911\n",
      "Epoch 427/1000\n",
      "1461/1461 [==============================] - 0s 299us/step - loss: 157.1115 - mae: 157.7986 - val_loss: 8301.8240 - val_mae: 8302.5176\n",
      "Epoch 428/1000\n",
      "1461/1461 [==============================] - 0s 281us/step - loss: 164.3257 - mae: 165.0141 - val_loss: 8025.9560 - val_mae: 8026.6489\n",
      "Epoch 429/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 164.9890 - mae: 165.6766 - val_loss: 8042.4969 - val_mae: 8043.1895\n",
      "Epoch 430/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 172.4778 - mae: 173.1655 - val_loss: 8397.3474 - val_mae: 8398.0420\n",
      "Epoch 431/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 159.9280 - mae: 160.6142 - val_loss: 8214.3010 - val_mae: 8214.9941\n",
      "Epoch 432/1000\n",
      "1461/1461 [==============================] - 1s 358us/step - loss: 151.3440 - mae: 152.0313 - val_loss: 8112.1286 - val_mae: 8112.8223\n",
      "Epoch 433/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 172.6167 - mae: 173.3062 - val_loss: 8123.4174 - val_mae: 8124.1104\n",
      "Epoch 434/1000\n",
      "1461/1461 [==============================] - 0s 245us/step - loss: 160.2713 - mae: 160.9575 - val_loss: 8110.3908 - val_mae: 8111.0840\n",
      "Epoch 435/1000\n",
      "1461/1461 [==============================] - 0s 246us/step - loss: 163.2285 - mae: 163.9184 - val_loss: 7878.0904 - val_mae: 7878.7842\n",
      "Epoch 436/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 171.0561 - mae: 171.7427 - val_loss: 7986.6315 - val_mae: 7987.3247\n",
      "Epoch 437/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 266us/step - loss: 155.8830 - mae: 156.5726 - val_loss: 7907.0919 - val_mae: 7907.7837\n",
      "Epoch 438/1000\n",
      "1461/1461 [==============================] - 0s 247us/step - loss: 151.7088 - mae: 152.3971 - val_loss: 8124.4193 - val_mae: 8125.1133\n",
      "Epoch 439/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 170.7872 - mae: 171.4741 - val_loss: 7783.1457 - val_mae: 7783.8384\n",
      "Epoch 440/1000\n",
      "1461/1461 [==============================] - 0s 257us/step - loss: 152.9263 - mae: 153.6126 - val_loss: 8101.9181 - val_mae: 8102.6118\n",
      "Epoch 441/1000\n",
      "1461/1461 [==============================] - 0s 263us/step - loss: 156.6589 - mae: 157.3482 - val_loss: 8162.8147 - val_mae: 8163.5078\n",
      "Epoch 442/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 168.2304 - mae: 168.9202 - val_loss: 7946.8388 - val_mae: 7947.5317\n",
      "Epoch 443/1000\n",
      "1461/1461 [==============================] - 0s 244us/step - loss: 151.3306 - mae: 152.0165 - val_loss: 8126.9156 - val_mae: 8127.6064\n",
      "Epoch 444/1000\n",
      "1461/1461 [==============================] - 0s 290us/step - loss: 158.5215 - mae: 159.2105 - val_loss: 8029.6242 - val_mae: 8030.3174\n",
      "Epoch 445/1000\n",
      "1461/1461 [==============================] - 0s 257us/step - loss: 147.6001 - mae: 148.2875 - val_loss: 8040.2364 - val_mae: 8040.9292\n",
      "Epoch 446/1000\n",
      "1461/1461 [==============================] - 0s 257us/step - loss: 158.4674 - mae: 159.1571 - val_loss: 8009.6612 - val_mae: 8010.3540\n",
      "Epoch 447/1000\n",
      "1461/1461 [==============================] - 0s 254us/step - loss: 176.4511 - mae: 177.1378 - val_loss: 8092.7744 - val_mae: 8093.4673\n",
      "Epoch 448/1000\n",
      "1461/1461 [==============================] - 0s 253us/step - loss: 149.1555 - mae: 149.8409 - val_loss: 8038.4017 - val_mae: 8039.0947\n",
      "Epoch 449/1000\n",
      "1461/1461 [==============================] - 0s 248us/step - loss: 155.7903 - mae: 156.4777 - val_loss: 7949.2334 - val_mae: 7949.9268\n",
      "Epoch 450/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 153.5291 - mae: 154.2158 - val_loss: 8169.0901 - val_mae: 8169.7832\n",
      "Epoch 451/1000\n",
      "1461/1461 [==============================] - 0s 250us/step - loss: 153.9287 - mae: 154.6160 - val_loss: 8019.5092 - val_mae: 8020.2021\n",
      "Epoch 452/1000\n",
      "1461/1461 [==============================] - 0s 263us/step - loss: 162.2918 - mae: 162.9773 - val_loss: 8106.7713 - val_mae: 8107.4653\n",
      "Epoch 453/1000\n",
      "1461/1461 [==============================] - 0s 311us/step - loss: 154.3828 - mae: 155.0673 - val_loss: 8132.0490 - val_mae: 8132.7417\n",
      "Epoch 454/1000\n",
      "1461/1461 [==============================] - 1s 363us/step - loss: 170.2945 - mae: 170.9818 - val_loss: 7769.0601 - val_mae: 7769.7534\n",
      "Epoch 455/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 150.8154 - mae: 151.5033 - val_loss: 7969.1926 - val_mae: 7969.8862\n",
      "Epoch 456/1000\n",
      "1461/1461 [==============================] - 0s 321us/step - loss: 155.2051 - mae: 155.8924 - val_loss: 8143.0185 - val_mae: 8143.7119\n",
      "Epoch 457/1000\n",
      "1461/1461 [==============================] - 0s 290us/step - loss: 163.9220 - mae: 164.6083 - val_loss: 7859.3677 - val_mae: 7860.0610\n",
      "Epoch 458/1000\n",
      "1461/1461 [==============================] - 0s 292us/step - loss: 187.1584 - mae: 187.8462 - val_loss: 8383.2928 - val_mae: 8383.9854\n",
      "Epoch 459/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 153.3360 - mae: 154.0257 - val_loss: 7837.6310 - val_mae: 7838.3247\n",
      "Epoch 460/1000\n",
      "1461/1461 [==============================] - 0s 261us/step - loss: 163.3743 - mae: 164.0618 - val_loss: 8458.8731 - val_mae: 8459.5654\n",
      "Epoch 461/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 161.2758 - mae: 161.9644 - val_loss: 7717.2921 - val_mae: 7717.9849\n",
      "Epoch 462/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 165.0342 - mae: 165.7222 - val_loss: 8170.3424 - val_mae: 8171.0356\n",
      "Epoch 463/1000\n",
      "1461/1461 [==============================] - 0s 262us/step - loss: 154.5993 - mae: 155.2863 - val_loss: 8068.8348 - val_mae: 8069.5273\n",
      "Epoch 464/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 160.0953 - mae: 160.7803 - val_loss: 8294.3892 - val_mae: 8295.0830\n",
      "Epoch 465/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 153.1491 - mae: 153.8346 - val_loss: 8063.8153 - val_mae: 8064.5088\n",
      "Epoch 466/1000\n",
      "1461/1461 [==============================] - 0s 263us/step - loss: 134.0073 - mae: 134.6946 - val_loss: 8048.9899 - val_mae: 8049.6826\n",
      "Epoch 467/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 154.1482 - mae: 154.8349 - val_loss: 8147.9682 - val_mae: 8148.6616\n",
      "Epoch 468/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 174.0320 - mae: 174.7204 - val_loss: 8110.6608 - val_mae: 8111.3540\n",
      "Epoch 469/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 155.6905 - mae: 156.3779 - val_loss: 8072.0691 - val_mae: 8072.7622\n",
      "Epoch 470/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 154.7791 - mae: 155.4664 - val_loss: 8327.9207 - val_mae: 8328.6133\n",
      "Epoch 471/1000\n",
      "1461/1461 [==============================] - 0s 263us/step - loss: 152.0502 - mae: 152.7370 - val_loss: 8455.0192 - val_mae: 8455.7129\n",
      "Epoch 472/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 184.0374 - mae: 184.7255 - val_loss: 8468.7177 - val_mae: 8469.4111\n",
      "Epoch 473/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 174.4108 - mae: 175.0991 - val_loss: 8310.5503 - val_mae: 8311.2441\n",
      "Epoch 474/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 186.4597 - mae: 187.1480 - val_loss: 7992.8346 - val_mae: 7993.5269\n",
      "Epoch 475/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 166.6784 - mae: 167.3647 - val_loss: 8292.8439 - val_mae: 8293.5381\n",
      "Epoch 476/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 147.8088 - mae: 148.4945 - val_loss: 8133.6684 - val_mae: 8134.3618\n",
      "Epoch 477/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 156.4792 - mae: 157.1692 - val_loss: 7814.1783 - val_mae: 7814.8716\n",
      "Epoch 478/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 159.7719 - mae: 160.4592 - val_loss: 7884.6511 - val_mae: 7885.3442\n",
      "Epoch 479/1000\n",
      "1461/1461 [==============================] - 1s 361us/step - loss: 165.2876 - mae: 165.9756 - val_loss: 8466.0311 - val_mae: 8466.7256\n",
      "Epoch 480/1000\n",
      "1461/1461 [==============================] - 0s 326us/step - loss: 149.7440 - mae: 150.4314 - val_loss: 8110.0669 - val_mae: 8110.7593\n",
      "Epoch 481/1000\n",
      "1461/1461 [==============================] - 0s 258us/step - loss: 169.8636 - mae: 170.5499 - val_loss: 8206.3594 - val_mae: 8207.0518\n",
      "Epoch 482/1000\n",
      "1461/1461 [==============================] - 0s 249us/step - loss: 155.7343 - mae: 156.4200 - val_loss: 8076.8029 - val_mae: 8077.4961\n",
      "Epoch 483/1000\n",
      "1461/1461 [==============================] - 0s 317us/step - loss: 153.5752 - mae: 154.2634 - val_loss: 8313.8633 - val_mae: 8314.5566\n",
      "Epoch 484/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 183.1110 - mae: 183.8007 - val_loss: 8089.7384 - val_mae: 8090.4312\n",
      "Epoch 485/1000\n",
      "1461/1461 [==============================] - 0s 256us/step - loss: 152.1643 - mae: 152.8537 - val_loss: 7918.5964 - val_mae: 7919.2891\n",
      "Epoch 486/1000\n",
      "1461/1461 [==============================] - 0s 247us/step - loss: 158.6206 - mae: 159.3071 - val_loss: 8096.0772 - val_mae: 8096.7715\n",
      "Epoch 487/1000\n",
      "1461/1461 [==============================] - 0s 263us/step - loss: 165.5992 - mae: 166.2872 - val_loss: 8301.6398 - val_mae: 8302.3330\n",
      "Epoch 488/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 155.0488 - mae: 155.7360 - val_loss: 8261.2530 - val_mae: 8261.9463\n",
      "Epoch 489/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 169.9366 - mae: 170.6247 - val_loss: 8364.9529 - val_mae: 8365.6465\n",
      "Epoch 490/1000\n",
      "1461/1461 [==============================] - 0s 251us/step - loss: 159.6182 - mae: 160.3055 - val_loss: 7958.5647 - val_mae: 7959.2578\n",
      "Epoch 491/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 147.5105 - mae: 148.1959 - val_loss: 7887.3491 - val_mae: 7888.0420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 492/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 162.0039 - mae: 162.6930 - val_loss: 8282.5652 - val_mae: 8283.2588\n",
      "Epoch 493/1000\n",
      "1461/1461 [==============================] - 0s 263us/step - loss: 153.5873 - mae: 154.2753 - val_loss: 7982.3222 - val_mae: 7983.0156\n",
      "Epoch 494/1000\n",
      "1461/1461 [==============================] - 0s 256us/step - loss: 148.6084 - mae: 149.2973 - val_loss: 8127.1699 - val_mae: 8127.8638\n",
      "Epoch 495/1000\n",
      "1461/1461 [==============================] - 0s 262us/step - loss: 146.3834 - mae: 147.0709 - val_loss: 8107.2094 - val_mae: 8107.9023\n",
      "Epoch 496/1000\n",
      "1461/1461 [==============================] - 0s 244us/step - loss: 137.7872 - mae: 138.4756 - val_loss: 8173.1230 - val_mae: 8173.8154\n",
      "Epoch 497/1000\n",
      "1461/1461 [==============================] - 0s 246us/step - loss: 149.8014 - mae: 150.4863 - val_loss: 8395.6365 - val_mae: 8396.3301\n",
      "Epoch 498/1000\n",
      "1461/1461 [==============================] - 0s 251us/step - loss: 157.3805 - mae: 158.0674 - val_loss: 7920.7175 - val_mae: 7921.4106\n",
      "Epoch 499/1000\n",
      "1461/1461 [==============================] - 0s 245us/step - loss: 151.1385 - mae: 151.8243 - val_loss: 7979.2595 - val_mae: 7979.9531\n",
      "Epoch 500/1000\n",
      "1461/1461 [==============================] - 0s 244us/step - loss: 144.6111 - mae: 145.2992 - val_loss: 8053.9110 - val_mae: 8054.6055\n",
      "Epoch 501/1000\n",
      "1461/1461 [==============================] - 0s 241us/step - loss: 143.8707 - mae: 144.5568 - val_loss: 8268.5538 - val_mae: 8269.2471\n",
      "Epoch 502/1000\n",
      "1461/1461 [==============================] - 0s 244us/step - loss: 144.3061 - mae: 144.9921 - val_loss: 8269.9878 - val_mae: 8270.6816\n",
      "Epoch 503/1000\n",
      "1461/1461 [==============================] - 0s 243us/step - loss: 164.4514 - mae: 165.1360 - val_loss: 8177.0722 - val_mae: 8177.7651\n",
      "Epoch 504/1000\n",
      "1461/1461 [==============================] - 0s 251us/step - loss: 160.8487 - mae: 161.5359 - val_loss: 7982.4880 - val_mae: 7983.1812\n",
      "Epoch 505/1000\n",
      "1461/1461 [==============================] - 0s 249us/step - loss: 139.4978 - mae: 140.1840 - val_loss: 7756.6512 - val_mae: 7757.3428\n",
      "Epoch 506/1000\n",
      "1461/1461 [==============================] - 0s 245us/step - loss: 143.2378 - mae: 143.9231 - val_loss: 8167.7392 - val_mae: 8168.4321\n",
      "Epoch 507/1000\n",
      "1461/1461 [==============================] - 0s 242us/step - loss: 150.6419 - mae: 151.3311 - val_loss: 8411.9629 - val_mae: 8412.6562\n",
      "Epoch 508/1000\n",
      "1461/1461 [==============================] - 0s 249us/step - loss: 144.3270 - mae: 145.0142 - val_loss: 8083.5092 - val_mae: 8084.2026\n",
      "Epoch 509/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 142.8334 - mae: 143.5197 - val_loss: 8040.2407 - val_mae: 8040.9316\n",
      "Epoch 510/1000\n",
      "1461/1461 [==============================] - 0s 291us/step - loss: 158.2192 - mae: 158.9055 - val_loss: 8300.7412 - val_mae: 8301.4355\n",
      "Epoch 511/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 156.1332 - mae: 156.8214 - val_loss: 8242.1528 - val_mae: 8242.8457\n",
      "Epoch 512/1000\n",
      "1461/1461 [==============================] - 0s 246us/step - loss: 151.6071 - mae: 152.2935 - val_loss: 8273.6909 - val_mae: 8274.3838\n",
      "Epoch 513/1000\n",
      "1461/1461 [==============================] - 0s 249us/step - loss: 145.1552 - mae: 145.8416 - val_loss: 8122.3084 - val_mae: 8123.0020\n",
      "Epoch 514/1000\n",
      "1461/1461 [==============================] - 0s 254us/step - loss: 159.0674 - mae: 159.7544 - val_loss: 8644.9814 - val_mae: 8645.6748\n",
      "Epoch 515/1000\n",
      "1461/1461 [==============================] - 0s 254us/step - loss: 160.2037 - mae: 160.8900 - val_loss: 8274.2051 - val_mae: 8274.8984\n",
      "Epoch 516/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 156.5195 - mae: 157.2056 - val_loss: 8096.4158 - val_mae: 8097.1084\n",
      "Epoch 517/1000\n",
      "1461/1461 [==============================] - 0s 244us/step - loss: 147.9922 - mae: 148.6811 - val_loss: 8088.0763 - val_mae: 8088.7700\n",
      "Epoch 518/1000\n",
      "1461/1461 [==============================] - 0s 246us/step - loss: 144.0833 - mae: 144.7695 - val_loss: 7754.9609 - val_mae: 7755.6533\n",
      "Epoch 519/1000\n",
      "1461/1461 [==============================] - 0s 257us/step - loss: 136.1758 - mae: 136.8625 - val_loss: 8164.0671 - val_mae: 8164.7593\n",
      "Epoch 520/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 142.3581 - mae: 143.0433 - val_loss: 8283.6660 - val_mae: 8284.3584\n",
      "Epoch 521/1000\n",
      "1461/1461 [==============================] - 0s 250us/step - loss: 146.1471 - mae: 146.8343 - val_loss: 7935.9794 - val_mae: 7936.6729\n",
      "Epoch 522/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 177.3464 - mae: 178.0342 - val_loss: 8103.3657 - val_mae: 8104.0591\n",
      "Epoch 523/1000\n",
      "1461/1461 [==============================] - 0s 242us/step - loss: 139.3150 - mae: 140.0004 - val_loss: 8097.0979 - val_mae: 8097.7905\n",
      "Epoch 524/1000\n",
      "1461/1461 [==============================] - 0s 243us/step - loss: 141.2602 - mae: 141.9489 - val_loss: 8279.3547 - val_mae: 8280.0488\n",
      "Epoch 525/1000\n",
      "1461/1461 [==============================] - 0s 248us/step - loss: 137.7714 - mae: 138.4545 - val_loss: 8114.8646 - val_mae: 8115.5581\n",
      "Epoch 526/1000\n",
      "1461/1461 [==============================] - 0s 240us/step - loss: 143.1510 - mae: 143.8371 - val_loss: 7894.4525 - val_mae: 7895.1450\n",
      "Epoch 527/1000\n",
      "1461/1461 [==============================] - 0s 245us/step - loss: 146.8171 - mae: 147.5052 - val_loss: 8056.4836 - val_mae: 8057.1768\n",
      "Epoch 528/1000\n",
      "1461/1461 [==============================] - 0s 241us/step - loss: 149.9033 - mae: 150.5888 - val_loss: 8078.5576 - val_mae: 8079.2510\n",
      "Epoch 529/1000\n",
      "1461/1461 [==============================] - 0s 245us/step - loss: 149.6329 - mae: 150.3197 - val_loss: 8308.5775 - val_mae: 8309.2686\n",
      "Epoch 530/1000\n",
      "1461/1461 [==============================] - 0s 244us/step - loss: 188.4022 - mae: 189.0898 - val_loss: 8337.3979 - val_mae: 8338.0918\n",
      "Epoch 531/1000\n",
      "1461/1461 [==============================] - 0s 242us/step - loss: 195.4814 - mae: 196.1706 - val_loss: 7757.2656 - val_mae: 7757.9585\n",
      "Epoch 532/1000\n",
      "1461/1461 [==============================] - 0s 241us/step - loss: 152.0803 - mae: 152.7673 - val_loss: 8034.3482 - val_mae: 8035.0415\n",
      "Epoch 533/1000\n",
      "1461/1461 [==============================] - 0s 243us/step - loss: 194.1146 - mae: 194.8004 - val_loss: 7605.6152 - val_mae: 7606.3086\n",
      "Epoch 534/1000\n",
      "1461/1461 [==============================] - 0s 241us/step - loss: 150.4651 - mae: 151.1496 - val_loss: 7827.8968 - val_mae: 7828.5894\n",
      "Epoch 535/1000\n",
      "1461/1461 [==============================] - 0s 243us/step - loss: 143.4053 - mae: 144.0906 - val_loss: 7770.0428 - val_mae: 7770.7358\n",
      "Epoch 536/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 148.2232 - mae: 148.9102 - val_loss: 8096.1308 - val_mae: 8096.8237\n",
      "Epoch 537/1000\n",
      "1461/1461 [==============================] - 0s 247us/step - loss: 151.0138 - mae: 151.7022 - val_loss: 8183.8691 - val_mae: 8184.5620\n",
      "Epoch 538/1000\n",
      "1461/1461 [==============================] - 0s 245us/step - loss: 143.8856 - mae: 144.5714 - val_loss: 8050.4892 - val_mae: 8051.1826\n",
      "Epoch 539/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 150.6035 - mae: 151.2921 - val_loss: 8074.4862 - val_mae: 8075.1792\n",
      "Epoch 540/1000\n",
      "1461/1461 [==============================] - 0s 248us/step - loss: 143.5152 - mae: 144.2033 - val_loss: 8100.4787 - val_mae: 8101.1719\n",
      "Epoch 541/1000\n",
      "1461/1461 [==============================] - 0s 248us/step - loss: 141.1747 - mae: 141.8618 - val_loss: 8012.7809 - val_mae: 8013.4727\n",
      "Epoch 542/1000\n",
      "1461/1461 [==============================] - 0s 245us/step - loss: 152.8125 - mae: 153.5006 - val_loss: 8284.7756 - val_mae: 8285.4697\n",
      "Epoch 543/1000\n",
      "1461/1461 [==============================] - 0s 246us/step - loss: 145.6819 - mae: 146.3680 - val_loss: 8192.5649 - val_mae: 8193.2578\n",
      "Epoch 544/1000\n",
      "1461/1461 [==============================] - 0s 242us/step - loss: 142.2477 - mae: 142.9347 - val_loss: 7990.6020 - val_mae: 7991.2959\n",
      "Epoch 545/1000\n",
      "1461/1461 [==============================] - 0s 244us/step - loss: 163.2885 - mae: 163.9751 - val_loss: 8181.7271 - val_mae: 8182.4209\n",
      "Epoch 546/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 246us/step - loss: 169.6976 - mae: 170.3846 - val_loss: 8298.6319 - val_mae: 8299.3252\n",
      "Epoch 547/1000\n",
      "1461/1461 [==============================] - 0s 241us/step - loss: 152.1664 - mae: 152.8536 - val_loss: 7859.7274 - val_mae: 7860.4214\n",
      "Epoch 548/1000\n",
      "1461/1461 [==============================] - 0s 244us/step - loss: 143.0471 - mae: 143.7337 - val_loss: 8203.5196 - val_mae: 8204.2129\n",
      "Epoch 549/1000\n",
      "1461/1461 [==============================] - 0s 244us/step - loss: 133.9345 - mae: 134.6200 - val_loss: 8246.1162 - val_mae: 8246.8096\n",
      "Epoch 550/1000\n",
      "1461/1461 [==============================] - 0s 241us/step - loss: 143.6161 - mae: 144.2996 - val_loss: 8011.4409 - val_mae: 8012.1338\n",
      "Epoch 551/1000\n",
      "1461/1461 [==============================] - 0s 244us/step - loss: 152.3790 - mae: 153.0655 - val_loss: 8042.4219 - val_mae: 8043.1152\n",
      "Epoch 552/1000\n",
      "1461/1461 [==============================] - 0s 241us/step - loss: 155.0117 - mae: 155.6979 - val_loss: 8116.2948 - val_mae: 8116.9883\n",
      "Epoch 553/1000\n",
      "1461/1461 [==============================] - 0s 243us/step - loss: 144.3928 - mae: 145.0801 - val_loss: 8121.2303 - val_mae: 8121.9238\n",
      "Epoch 554/1000\n",
      "1461/1461 [==============================] - 0s 244us/step - loss: 149.1058 - mae: 149.7917 - val_loss: 8252.8276 - val_mae: 8253.5215\n",
      "Epoch 555/1000\n",
      "1461/1461 [==============================] - 0s 246us/step - loss: 136.3950 - mae: 137.0817 - val_loss: 8068.8345 - val_mae: 8069.5273\n",
      "Epoch 556/1000\n",
      "1461/1461 [==============================] - 0s 242us/step - loss: 145.5515 - mae: 146.2370 - val_loss: 8810.4063 - val_mae: 8811.1006\n",
      "Epoch 557/1000\n",
      "1461/1461 [==============================] - 0s 246us/step - loss: 161.7785 - mae: 162.4659 - val_loss: 8158.8199 - val_mae: 8159.5127\n",
      "Epoch 558/1000\n",
      "1461/1461 [==============================] - 0s 246us/step - loss: 138.2155 - mae: 138.9019 - val_loss: 8576.2476 - val_mae: 8576.9424\n",
      "Epoch 559/1000\n",
      "1461/1461 [==============================] - 0s 256us/step - loss: 139.2616 - mae: 139.9458 - val_loss: 8066.4404 - val_mae: 8067.1333\n",
      "Epoch 560/1000\n",
      "1461/1461 [==============================] - 0s 249us/step - loss: 133.9702 - mae: 134.6551 - val_loss: 8168.0848 - val_mae: 8168.7783\n",
      "Epoch 561/1000\n",
      "1461/1461 [==============================] - 0s 243us/step - loss: 146.8198 - mae: 147.5053 - val_loss: 8307.5815 - val_mae: 8308.2744\n",
      "Epoch 562/1000\n",
      "1461/1461 [==============================] - 0s 247us/step - loss: 138.3735 - mae: 139.0623 - val_loss: 7952.7542 - val_mae: 7953.4463\n",
      "Epoch 563/1000\n",
      "1461/1461 [==============================] - 0s 250us/step - loss: 141.4353 - mae: 142.1198 - val_loss: 8135.9944 - val_mae: 8136.6875\n",
      "Epoch 564/1000\n",
      "1461/1461 [==============================] - 0s 240us/step - loss: 140.5594 - mae: 141.2489 - val_loss: 8158.0559 - val_mae: 8158.7485\n",
      "Epoch 565/1000\n",
      "1461/1461 [==============================] - 0s 242us/step - loss: 147.5171 - mae: 148.2039 - val_loss: 7981.8318 - val_mae: 7982.5249\n",
      "Epoch 566/1000\n",
      "1461/1461 [==============================] - 0s 244us/step - loss: 163.1700 - mae: 163.8559 - val_loss: 7871.3437 - val_mae: 7872.0366\n",
      "Epoch 567/1000\n",
      "1461/1461 [==============================] - 0s 250us/step - loss: 139.7609 - mae: 140.4482 - val_loss: 7895.0365 - val_mae: 7895.7300\n",
      "Epoch 568/1000\n",
      "1461/1461 [==============================] - 0s 246us/step - loss: 131.1917 - mae: 131.8757 - val_loss: 8190.7709 - val_mae: 8191.4653\n",
      "Epoch 569/1000\n",
      "1461/1461 [==============================] - 0s 246us/step - loss: 128.8635 - mae: 129.5501 - val_loss: 8114.3184 - val_mae: 8115.0117\n",
      "Epoch 570/1000\n",
      "1461/1461 [==============================] - 0s 245us/step - loss: 133.8752 - mae: 134.5583 - val_loss: 8145.4520 - val_mae: 8146.1450\n",
      "Epoch 571/1000\n",
      "1461/1461 [==============================] - 0s 248us/step - loss: 134.6708 - mae: 135.3546 - val_loss: 7975.0074 - val_mae: 7975.7012\n",
      "Epoch 572/1000\n",
      "1461/1461 [==============================] - 0s 246us/step - loss: 140.9053 - mae: 141.5896 - val_loss: 8353.9355 - val_mae: 8354.6289\n",
      "Epoch 573/1000\n",
      "1461/1461 [==============================] - 0s 245us/step - loss: 137.9258 - mae: 138.6101 - val_loss: 8335.7826 - val_mae: 8336.4766\n",
      "Epoch 574/1000\n",
      "1461/1461 [==============================] - 0s 246us/step - loss: 155.3208 - mae: 156.0080 - val_loss: 7856.7314 - val_mae: 7857.4238\n",
      "Epoch 575/1000\n",
      "1461/1461 [==============================] - 0s 244us/step - loss: 150.5785 - mae: 151.2675 - val_loss: 8291.0531 - val_mae: 8291.7461\n",
      "Epoch 576/1000\n",
      "1461/1461 [==============================] - 0s 250us/step - loss: 145.1218 - mae: 145.8073 - val_loss: 8225.6931 - val_mae: 8226.3867\n",
      "Epoch 577/1000\n",
      "1461/1461 [==============================] - 0s 245us/step - loss: 136.1010 - mae: 136.7859 - val_loss: 8124.0372 - val_mae: 8124.7310\n",
      "Epoch 578/1000\n",
      "1461/1461 [==============================] - 0s 247us/step - loss: 142.3418 - mae: 143.0266 - val_loss: 7984.1779 - val_mae: 7984.8711\n",
      "Epoch 579/1000\n",
      "1461/1461 [==============================] - 0s 241us/step - loss: 140.8354 - mae: 141.5242 - val_loss: 8250.3768 - val_mae: 8251.0703\n",
      "Epoch 580/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 141.8626 - mae: 142.5489 - val_loss: 8232.6547 - val_mae: 8233.3486\n",
      "Epoch 581/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 134.7424 - mae: 135.4299 - val_loss: 8122.3939 - val_mae: 8123.0869\n",
      "Epoch 582/1000\n",
      "1461/1461 [==============================] - 0s 251us/step - loss: 139.2168 - mae: 139.9048 - val_loss: 8174.4695 - val_mae: 8175.1626\n",
      "Epoch 583/1000\n",
      "1461/1461 [==============================] - 0s 244us/step - loss: 145.5764 - mae: 146.2614 - val_loss: 8221.7578 - val_mae: 8222.4502\n",
      "Epoch 584/1000\n",
      "1461/1461 [==============================] - 0s 246us/step - loss: 134.0623 - mae: 134.7494 - val_loss: 8104.2254 - val_mae: 8104.9189\n",
      "Epoch 585/1000\n",
      "1461/1461 [==============================] - 0s 246us/step - loss: 133.6892 - mae: 134.3726 - val_loss: 8207.3554 - val_mae: 8208.0488\n",
      "Epoch 586/1000\n",
      "1461/1461 [==============================] - 0s 248us/step - loss: 131.2192 - mae: 131.9033 - val_loss: 8123.2425 - val_mae: 8123.9341\n",
      "Epoch 587/1000\n",
      "1461/1461 [==============================] - 0s 244us/step - loss: 142.3763 - mae: 143.0623 - val_loss: 8205.3453 - val_mae: 8206.0381\n",
      "Epoch 588/1000\n",
      "1461/1461 [==============================] - 0s 246us/step - loss: 150.3865 - mae: 151.0684 - val_loss: 8239.8399 - val_mae: 8240.5312\n",
      "Epoch 589/1000\n",
      "1461/1461 [==============================] - 0s 244us/step - loss: 151.1204 - mae: 151.8079 - val_loss: 8748.0859 - val_mae: 8748.7793\n",
      "Epoch 590/1000\n",
      "1461/1461 [==============================] - 0s 245us/step - loss: 166.4777 - mae: 167.1641 - val_loss: 8383.2801 - val_mae: 8383.9736\n",
      "Epoch 591/1000\n",
      "1461/1461 [==============================] - 0s 247us/step - loss: 143.9524 - mae: 144.6377 - val_loss: 7792.4409 - val_mae: 7793.1343\n",
      "Epoch 592/1000\n",
      "1461/1461 [==============================] - 0s 247us/step - loss: 162.3885 - mae: 163.0755 - val_loss: 8268.5974 - val_mae: 8269.2900\n",
      "Epoch 593/1000\n",
      "1461/1461 [==============================] - 0s 244us/step - loss: 140.7376 - mae: 141.4229 - val_loss: 8067.6831 - val_mae: 8068.3755\n",
      "Epoch 594/1000\n",
      "1461/1461 [==============================] - 0s 245us/step - loss: 135.1548 - mae: 135.8437 - val_loss: 8185.5287 - val_mae: 8186.2227\n",
      "Epoch 595/1000\n",
      "1461/1461 [==============================] - 0s 246us/step - loss: 155.2967 - mae: 155.9829 - val_loss: 8226.7014 - val_mae: 8227.3945\n",
      "Epoch 596/1000\n",
      "1461/1461 [==============================] - 0s 250us/step - loss: 149.7771 - mae: 150.4595 - val_loss: 8072.4563 - val_mae: 8073.1489\n",
      "Epoch 597/1000\n",
      "1461/1461 [==============================] - 0s 243us/step - loss: 134.9853 - mae: 135.6727 - val_loss: 7930.3661 - val_mae: 7931.0586\n",
      "Epoch 598/1000\n",
      "1461/1461 [==============================] - 0s 242us/step - loss: 147.1417 - mae: 147.8266 - val_loss: 8326.5174 - val_mae: 8327.2109\n",
      "Epoch 599/1000\n",
      "1461/1461 [==============================] - 0s 243us/step - loss: 140.1913 - mae: 140.8745 - val_loss: 8099.0973 - val_mae: 8099.7900\n",
      "Epoch 600/1000\n",
      "1461/1461 [==============================] - 0s 243us/step - loss: 146.9523 - mae: 147.6411 - val_loss: 8130.8996 - val_mae: 8131.5928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 601/1000\n",
      "1461/1461 [==============================] - 0s 244us/step - loss: 151.9283 - mae: 152.6150 - val_loss: 7970.8897 - val_mae: 7971.5825\n",
      "Epoch 602/1000\n",
      "1461/1461 [==============================] - 0s 246us/step - loss: 142.4955 - mae: 143.1789 - val_loss: 8099.4406 - val_mae: 8100.1338\n",
      "Epoch 603/1000\n",
      "1461/1461 [==============================] - 0s 245us/step - loss: 131.0167 - mae: 131.7049 - val_loss: 8173.1519 - val_mae: 8173.8462\n",
      "Epoch 604/1000\n",
      "1461/1461 [==============================] - 0s 246us/step - loss: 132.1958 - mae: 132.8820 - val_loss: 8226.4794 - val_mae: 8227.1729\n",
      "Epoch 605/1000\n",
      "1461/1461 [==============================] - 0s 247us/step - loss: 136.5374 - mae: 137.2239 - val_loss: 8200.6646 - val_mae: 8201.3574\n",
      "Epoch 606/1000\n",
      "1461/1461 [==============================] - 0s 243us/step - loss: 142.7676 - mae: 143.4546 - val_loss: 7642.5558 - val_mae: 7643.2490\n",
      "Epoch 607/1000\n",
      "1461/1461 [==============================] - 0s 249us/step - loss: 161.2174 - mae: 161.9032 - val_loss: 8528.9363 - val_mae: 8529.6299\n",
      "Epoch 608/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 144.8154 - mae: 145.5028 - val_loss: 8016.3840 - val_mae: 8017.0771\n",
      "Epoch 609/1000\n",
      "1461/1461 [==============================] - 0s 245us/step - loss: 130.0249 - mae: 130.7100 - val_loss: 8132.0552 - val_mae: 8132.7476\n",
      "Epoch 610/1000\n",
      "1461/1461 [==============================] - 0s 245us/step - loss: 137.7592 - mae: 138.4451 - val_loss: 7938.2551 - val_mae: 7938.9492\n",
      "Epoch 611/1000\n",
      "1461/1461 [==============================] - 0s 245us/step - loss: 164.2957 - mae: 164.9790 - val_loss: 8085.2753 - val_mae: 8085.9692\n",
      "Epoch 612/1000\n",
      "1461/1461 [==============================] - 0s 246us/step - loss: 128.7886 - mae: 129.4727 - val_loss: 8172.2874 - val_mae: 8172.9805\n",
      "Epoch 613/1000\n",
      "1461/1461 [==============================] - 0s 242us/step - loss: 148.7440 - mae: 149.4268 - val_loss: 8305.1605 - val_mae: 8305.8525\n",
      "Epoch 614/1000\n",
      "1461/1461 [==============================] - 0s 246us/step - loss: 145.1773 - mae: 145.8644 - val_loss: 8131.9886 - val_mae: 8132.6816\n",
      "Epoch 615/1000\n",
      "1461/1461 [==============================] - 0s 244us/step - loss: 138.3851 - mae: 139.0705 - val_loss: 8016.4066 - val_mae: 8017.0991\n",
      "Epoch 616/1000\n",
      "1461/1461 [==============================] - 0s 240us/step - loss: 137.0442 - mae: 137.7299 - val_loss: 8151.7312 - val_mae: 8152.4238\n",
      "Epoch 617/1000\n",
      "1461/1461 [==============================] - 0s 241us/step - loss: 147.7500 - mae: 148.4378 - val_loss: 8202.5819 - val_mae: 8203.2744\n",
      "Epoch 618/1000\n",
      "1461/1461 [==============================] - 0s 243us/step - loss: 151.7385 - mae: 152.4241 - val_loss: 8069.2386 - val_mae: 8069.9302\n",
      "Epoch 619/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 139.9526 - mae: 140.6414 - val_loss: 8056.6005 - val_mae: 8057.2935\n",
      "Epoch 620/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 152.3884 - mae: 153.0737 - val_loss: 7953.9578 - val_mae: 7954.6509\n",
      "Epoch 621/1000\n",
      "1461/1461 [==============================] - 1s 401us/step - loss: 138.4358 - mae: 139.1229 - val_loss: 8348.3516 - val_mae: 8349.0459\n",
      "Epoch 622/1000\n",
      "1461/1461 [==============================] - 0s 295us/step - loss: 125.2967 - mae: 125.9812 - val_loss: 7952.2000 - val_mae: 7952.8931\n",
      "Epoch 623/1000\n",
      "1461/1461 [==============================] - 0s 338us/step - loss: 149.1568 - mae: 149.8442 - val_loss: 8492.2261 - val_mae: 8492.9189\n",
      "Epoch 624/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 144.3913 - mae: 145.0775 - val_loss: 8372.6446 - val_mae: 8373.3389\n",
      "Epoch 625/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 137.5888 - mae: 138.2732 - val_loss: 8094.3863 - val_mae: 8095.0786\n",
      "Epoch 626/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 134.9717 - mae: 135.6566 - val_loss: 8133.1523 - val_mae: 8133.8442\n",
      "Epoch 627/1000\n",
      "1461/1461 [==============================] - 0s 259us/step - loss: 138.5888 - mae: 139.2750 - val_loss: 8378.5677 - val_mae: 8379.2607\n",
      "Epoch 628/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 139.9795 - mae: 140.6654 - val_loss: 8166.8566 - val_mae: 8167.5493\n",
      "Epoch 629/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 133.9767 - mae: 134.6593 - val_loss: 8217.0842 - val_mae: 8217.7773\n",
      "Epoch 630/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 142.9001 - mae: 143.5882 - val_loss: 8247.0014 - val_mae: 8247.6943\n",
      "Epoch 631/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 136.9490 - mae: 137.6356 - val_loss: 8138.1401 - val_mae: 8138.8335\n",
      "Epoch 632/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 131.8621 - mae: 132.5470 - val_loss: 8567.7888 - val_mae: 8568.4814\n",
      "Epoch 633/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 134.6311 - mae: 135.3188 - val_loss: 8177.5431 - val_mae: 8178.2368\n",
      "Epoch 634/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 132.3898 - mae: 133.0768 - val_loss: 8147.5255 - val_mae: 8148.2178\n",
      "Epoch 635/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 127.1322 - mae: 127.8182 - val_loss: 8192.1758 - val_mae: 8192.8691\n",
      "Epoch 636/1000\n",
      "1461/1461 [==============================] - 0s 262us/step - loss: 125.6828 - mae: 126.3684 - val_loss: 8049.6282 - val_mae: 8050.3223\n",
      "Epoch 637/1000\n",
      "1461/1461 [==============================] - 0s 259us/step - loss: 135.0130 - mae: 135.6996 - val_loss: 8357.3025 - val_mae: 8357.9961\n",
      "Epoch 638/1000\n",
      "1461/1461 [==============================] - 0s 259us/step - loss: 144.0343 - mae: 144.7193 - val_loss: 8434.7180 - val_mae: 8435.4111\n",
      "Epoch 639/1000\n",
      "1461/1461 [==============================] - 0s 258us/step - loss: 160.3130 - mae: 160.9994 - val_loss: 8190.1691 - val_mae: 8190.8628\n",
      "Epoch 640/1000\n",
      "1461/1461 [==============================] - 0s 258us/step - loss: 130.7672 - mae: 131.4511 - val_loss: 8140.3526 - val_mae: 8141.0464\n",
      "Epoch 641/1000\n",
      "1461/1461 [==============================] - 0s 259us/step - loss: 151.7853 - mae: 152.4724 - val_loss: 8135.4005 - val_mae: 8136.0938\n",
      "Epoch 642/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 137.1914 - mae: 137.8775 - val_loss: 7945.8443 - val_mae: 7946.5376\n",
      "Epoch 643/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 141.2607 - mae: 141.9492 - val_loss: 7877.3711 - val_mae: 7878.0645\n",
      "Epoch 644/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 122.1195 - mae: 122.8015 - val_loss: 8015.4504 - val_mae: 8016.1436\n",
      "Epoch 645/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 121.7498 - mae: 122.4339 - val_loss: 8200.0166 - val_mae: 8200.7100\n",
      "Epoch 646/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 134.3673 - mae: 135.0523 - val_loss: 8184.0351 - val_mae: 8184.7280\n",
      "Epoch 647/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 146.2027 - mae: 146.8884 - val_loss: 8146.7676 - val_mae: 8147.4614\n",
      "Epoch 648/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 137.1723 - mae: 137.8552 - val_loss: 8235.6492 - val_mae: 8236.3428\n",
      "Epoch 649/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 154.8826 - mae: 155.5695 - val_loss: 8170.8414 - val_mae: 8171.5347\n",
      "Epoch 650/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 137.1798 - mae: 137.8646 - val_loss: 8341.2777 - val_mae: 8341.9697\n",
      "Epoch 651/1000\n",
      "1461/1461 [==============================] - 0s 262us/step - loss: 141.3879 - mae: 142.0733 - val_loss: 8021.2679 - val_mae: 8021.9609\n",
      "Epoch 652/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 138.2779 - mae: 138.9630 - val_loss: 8255.1266 - val_mae: 8255.8213\n",
      "Epoch 653/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 124.7715 - mae: 125.4542 - val_loss: 8040.2518 - val_mae: 8040.9448\n",
      "Epoch 654/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 143.3694 - mae: 144.0571 - val_loss: 8117.5394 - val_mae: 8118.2329\n",
      "Epoch 655/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 266us/step - loss: 131.4761 - mae: 132.1619 - val_loss: 8180.3378 - val_mae: 8181.0317\n",
      "Epoch 656/1000\n",
      "1461/1461 [==============================] - 0s 289us/step - loss: 133.0101 - mae: 133.6934 - val_loss: 8243.2539 - val_mae: 8243.9473\n",
      "Epoch 657/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 133.2761 - mae: 133.9611 - val_loss: 8217.1942 - val_mae: 8217.8877\n",
      "Epoch 658/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 131.1154 - mae: 131.8000 - val_loss: 8146.8287 - val_mae: 8147.5215\n",
      "Epoch 659/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 130.0757 - mae: 130.7556 - val_loss: 8253.4149 - val_mae: 8254.1084\n",
      "Epoch 660/1000\n",
      "1461/1461 [==============================] - 0s 261us/step - loss: 150.5253 - mae: 151.2104 - val_loss: 8025.5661 - val_mae: 8026.2588\n",
      "Epoch 661/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 130.5956 - mae: 131.2800 - val_loss: 8283.2398 - val_mae: 8283.9336\n",
      "Epoch 662/1000\n",
      "1461/1461 [==============================] - 0s 283us/step - loss: 137.1944 - mae: 137.8810 - val_loss: 8195.6948 - val_mae: 8196.3877\n",
      "Epoch 663/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 130.5992 - mae: 131.2852 - val_loss: 8025.1599 - val_mae: 8025.8521\n",
      "Epoch 664/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 146.3163 - mae: 147.0027 - val_loss: 8091.3291 - val_mae: 8092.0220\n",
      "Epoch 665/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 138.8095 - mae: 139.4943 - val_loss: 8241.2171 - val_mae: 8241.9102\n",
      "Epoch 666/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 131.1597 - mae: 131.8475 - val_loss: 8535.9028 - val_mae: 8536.5947\n",
      "Epoch 667/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 144.1692 - mae: 144.8565 - val_loss: 7956.1234 - val_mae: 7956.8169\n",
      "Epoch 668/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 131.6939 - mae: 132.3795 - val_loss: 8004.5753 - val_mae: 8005.2686\n",
      "Epoch 669/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 127.7031 - mae: 128.3874 - val_loss: 8267.3791 - val_mae: 8268.0723\n",
      "Epoch 670/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 130.1826 - mae: 130.8672 - val_loss: 8183.0039 - val_mae: 8183.6973\n",
      "Epoch 671/1000\n",
      "1461/1461 [==============================] - 0s 254us/step - loss: 121.6513 - mae: 122.3377 - val_loss: 8256.9545 - val_mae: 8257.6475\n",
      "Epoch 672/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 129.5399 - mae: 130.2272 - val_loss: 8387.6000 - val_mae: 8388.2930\n",
      "Epoch 673/1000\n",
      "1461/1461 [==============================] - 0s 263us/step - loss: 125.8065 - mae: 126.4919 - val_loss: 8339.8248 - val_mae: 8340.5186\n",
      "Epoch 674/1000\n",
      "1461/1461 [==============================] - 0s 261us/step - loss: 126.7707 - mae: 127.4571 - val_loss: 8125.4456 - val_mae: 8126.1382\n",
      "Epoch 675/1000\n",
      "1461/1461 [==============================] - 0s 253us/step - loss: 122.4581 - mae: 123.1443 - val_loss: 8194.1008 - val_mae: 8194.7939\n",
      "Epoch 676/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 137.1230 - mae: 137.8085 - val_loss: 8258.1137 - val_mae: 8258.8066\n",
      "Epoch 677/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 123.1143 - mae: 123.7978 - val_loss: 8247.2901 - val_mae: 8247.9834\n",
      "Epoch 678/1000\n",
      "1461/1461 [==============================] - 0s 263us/step - loss: 141.8803 - mae: 142.5677 - val_loss: 8553.9762 - val_mae: 8554.6689\n",
      "Epoch 679/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 152.6031 - mae: 153.2873 - val_loss: 8577.1194 - val_mae: 8577.8125\n",
      "Epoch 680/1000\n",
      "1461/1461 [==============================] - 0s 262us/step - loss: 133.5802 - mae: 134.2648 - val_loss: 8294.8065 - val_mae: 8295.5000\n",
      "Epoch 681/1000\n",
      "1461/1461 [==============================] - 0s 263us/step - loss: 140.8417 - mae: 141.5291 - val_loss: 8317.5545 - val_mae: 8318.2480\n",
      "Epoch 682/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 135.9526 - mae: 136.6382 - val_loss: 8565.7527 - val_mae: 8566.4463\n",
      "Epoch 683/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 123.2055 - mae: 123.8910 - val_loss: 8448.4156 - val_mae: 8449.1094\n",
      "Epoch 684/1000\n",
      "1461/1461 [==============================] - 0s 263us/step - loss: 132.6793 - mae: 133.3652 - val_loss: 8394.5811 - val_mae: 8395.2744\n",
      "Epoch 685/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 132.6354 - mae: 133.3196 - val_loss: 8359.6825 - val_mae: 8360.3760\n",
      "Epoch 686/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 122.3875 - mae: 123.0718 - val_loss: 8056.3767 - val_mae: 8057.0698\n",
      "Epoch 687/1000\n",
      "1461/1461 [==============================] - 0s 258us/step - loss: 124.7884 - mae: 125.4743 - val_loss: 8170.4926 - val_mae: 8171.1846\n",
      "Epoch 688/1000\n",
      "1461/1461 [==============================] - 0s 258us/step - loss: 141.1210 - mae: 141.8071 - val_loss: 8086.0872 - val_mae: 8086.7808\n",
      "Epoch 689/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 127.2744 - mae: 127.9589 - val_loss: 8305.1136 - val_mae: 8305.8076\n",
      "Epoch 690/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 133.5393 - mae: 134.2234 - val_loss: 8416.0363 - val_mae: 8416.7305\n",
      "Epoch 691/1000\n",
      "1461/1461 [==============================] - 0s 250us/step - loss: 120.9696 - mae: 121.6553 - val_loss: 8385.0936 - val_mae: 8385.7861\n",
      "Epoch 692/1000\n",
      "1461/1461 [==============================] - 0s 250us/step - loss: 133.7406 - mae: 134.4253 - val_loss: 8175.6277 - val_mae: 8176.3213\n",
      "Epoch 693/1000\n",
      "1461/1461 [==============================] - 0s 256us/step - loss: 129.6638 - mae: 130.3491 - val_loss: 8101.7688 - val_mae: 8102.4624\n",
      "Epoch 694/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 129.1303 - mae: 129.8165 - val_loss: 8384.3316 - val_mae: 8385.0254\n",
      "Epoch 695/1000\n",
      "1461/1461 [==============================] - 0s 259us/step - loss: 140.8234 - mae: 141.5103 - val_loss: 8611.1288 - val_mae: 8611.8223\n",
      "Epoch 696/1000\n",
      "1461/1461 [==============================] - 0s 259us/step - loss: 136.9087 - mae: 137.5941 - val_loss: 8045.6191 - val_mae: 8046.3120\n",
      "Epoch 697/1000\n",
      "1461/1461 [==============================] - 0s 257us/step - loss: 141.2855 - mae: 141.9728 - val_loss: 8092.0180 - val_mae: 8092.7119\n",
      "Epoch 698/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 123.2164 - mae: 123.9023 - val_loss: 8337.5441 - val_mae: 8338.2383\n",
      "Epoch 699/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 151.8602 - mae: 152.5477 - val_loss: 8206.3408 - val_mae: 8207.0342\n",
      "Epoch 700/1000\n",
      "1461/1461 [==============================] - 0s 310us/step - loss: 133.1102 - mae: 133.7974 - val_loss: 8167.3971 - val_mae: 8168.0898\n",
      "Epoch 701/1000\n",
      "1461/1461 [==============================] - 0s 283us/step - loss: 130.0447 - mae: 130.7300 - val_loss: 8354.3980 - val_mae: 8355.0918\n",
      "Epoch 702/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 134.0296 - mae: 134.7180 - val_loss: 8318.2670 - val_mae: 8318.9609\n",
      "Epoch 703/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 146.0637 - mae: 146.7471 - val_loss: 8143.9797 - val_mae: 8144.6733\n",
      "Epoch 704/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 131.6735 - mae: 132.3572 - val_loss: 8168.3789 - val_mae: 8169.0708\n",
      "Epoch 705/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 133.8124 - mae: 134.4968 - val_loss: 8316.6423 - val_mae: 8317.3359\n",
      "Epoch 706/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 138.5659 - mae: 139.2505 - val_loss: 8124.9742 - val_mae: 8125.6675\n",
      "Epoch 707/1000\n",
      "1461/1461 [==============================] - 0s 261us/step - loss: 128.9661 - mae: 129.6495 - val_loss: 8378.8925 - val_mae: 8379.5859\n",
      "Epoch 708/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 143.5063 - mae: 144.1933 - val_loss: 8093.4629 - val_mae: 8094.1553\n",
      "Epoch 709/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 135.2919 - mae: 135.9794 - val_loss: 8100.5021 - val_mae: 8101.1948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 710/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 132.8234 - mae: 133.5094 - val_loss: 8564.0713 - val_mae: 8564.7646\n",
      "Epoch 711/1000\n",
      "1461/1461 [==============================] - 0s 261us/step - loss: 142.6758 - mae: 143.3601 - val_loss: 8215.6763 - val_mae: 8216.3672\n",
      "Epoch 712/1000\n",
      "1461/1461 [==============================] - 0s 262us/step - loss: 131.5641 - mae: 132.2508 - val_loss: 8502.0790 - val_mae: 8502.7725\n",
      "Epoch 713/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 130.4821 - mae: 131.1681 - val_loss: 8391.4923 - val_mae: 8392.1855\n",
      "Epoch 714/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 121.5618 - mae: 122.2462 - val_loss: 8333.3045 - val_mae: 8333.9971\n",
      "Epoch 715/1000\n",
      "1461/1461 [==============================] - 0s 254us/step - loss: 124.5952 - mae: 125.2791 - val_loss: 8168.5920 - val_mae: 8169.2852\n",
      "Epoch 716/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 117.5929 - mae: 118.2773 - val_loss: 8275.4782 - val_mae: 8276.1699\n",
      "Epoch 717/1000\n",
      "1461/1461 [==============================] - 0s 251us/step - loss: 140.9351 - mae: 141.6178 - val_loss: 8524.7616 - val_mae: 8525.4541\n",
      "Epoch 718/1000\n",
      "1461/1461 [==============================] - 0s 262us/step - loss: 129.5580 - mae: 130.2423 - val_loss: 8054.4770 - val_mae: 8055.1714\n",
      "Epoch 719/1000\n",
      "1461/1461 [==============================] - 0s 259us/step - loss: 121.3597 - mae: 122.0431 - val_loss: 8088.4365 - val_mae: 8089.1294\n",
      "Epoch 720/1000\n",
      "1461/1461 [==============================] - 0s 262us/step - loss: 115.5300 - mae: 116.2146 - val_loss: 8047.3527 - val_mae: 8048.0459\n",
      "Epoch 721/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 132.0506 - mae: 132.7372 - val_loss: 8412.4936 - val_mae: 8413.1865\n",
      "Epoch 722/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 134.3244 - mae: 135.0096 - val_loss: 8253.6507 - val_mae: 8254.3447\n",
      "Epoch 723/1000\n",
      "1461/1461 [==============================] - 0s 258us/step - loss: 126.8246 - mae: 127.5111 - val_loss: 8208.9495 - val_mae: 8209.6436\n",
      "Epoch 724/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 144.3905 - mae: 145.0775 - val_loss: 8146.7752 - val_mae: 8147.4683\n",
      "Epoch 725/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 130.2323 - mae: 130.9161 - val_loss: 8343.3450 - val_mae: 8344.0391\n",
      "Epoch 726/1000\n",
      "1461/1461 [==============================] - 0s 257us/step - loss: 120.8539 - mae: 121.5384 - val_loss: 8224.1107 - val_mae: 8224.8047\n",
      "Epoch 727/1000\n",
      "1461/1461 [==============================] - 0s 256us/step - loss: 136.6833 - mae: 137.3694 - val_loss: 8354.4584 - val_mae: 8355.1514\n",
      "Epoch 728/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 131.3814 - mae: 132.0638 - val_loss: 8139.4660 - val_mae: 8140.1587\n",
      "Epoch 729/1000\n",
      "1461/1461 [==============================] - 0s 253us/step - loss: 130.4689 - mae: 131.1542 - val_loss: 8438.4151 - val_mae: 8439.1074\n",
      "Epoch 730/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 124.4859 - mae: 125.1711 - val_loss: 8421.2639 - val_mae: 8421.9561\n",
      "Epoch 731/1000\n",
      "1461/1461 [==============================] - 0s 256us/step - loss: 137.4439 - mae: 138.1286 - val_loss: 8535.6188 - val_mae: 8536.3115\n",
      "Epoch 732/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 159.2257 - mae: 159.9115 - val_loss: 8284.6356 - val_mae: 8285.3291\n",
      "Epoch 733/1000\n",
      "1461/1461 [==============================] - 0s 256us/step - loss: 130.0184 - mae: 130.7066 - val_loss: 8552.3780 - val_mae: 8553.0713\n",
      "Epoch 734/1000\n",
      "1461/1461 [==============================] - 0s 254us/step - loss: 131.9462 - mae: 132.6303 - val_loss: 8408.3013 - val_mae: 8408.9941\n",
      "Epoch 735/1000\n",
      "1461/1461 [==============================] - 0s 254us/step - loss: 134.8085 - mae: 135.4923 - val_loss: 8374.3358 - val_mae: 8375.0283\n",
      "Epoch 736/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 138.1372 - mae: 138.8231 - val_loss: 8230.2459 - val_mae: 8230.9385\n",
      "Epoch 737/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 119.2432 - mae: 119.9289 - val_loss: 8443.7269 - val_mae: 8444.4199\n",
      "Epoch 738/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 135.9452 - mae: 136.6322 - val_loss: 8522.4852 - val_mae: 8523.1777\n",
      "Epoch 739/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 128.0089 - mae: 128.6925 - val_loss: 8194.9383 - val_mae: 8195.6318\n",
      "Epoch 740/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 126.4656 - mae: 127.1506 - val_loss: 8040.3902 - val_mae: 8041.0835\n",
      "Epoch 741/1000\n",
      "1461/1461 [==============================] - 0s 259us/step - loss: 139.0457 - mae: 139.7332 - val_loss: 8352.7437 - val_mae: 8353.4385\n",
      "Epoch 742/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 119.8588 - mae: 120.5424 - val_loss: 8307.8437 - val_mae: 8308.5361\n",
      "Epoch 743/1000\n",
      "1461/1461 [==============================] - 0s 254us/step - loss: 126.2890 - mae: 126.9770 - val_loss: 8371.3709 - val_mae: 8372.0625\n",
      "Epoch 744/1000\n",
      "1461/1461 [==============================] - 0s 263us/step - loss: 126.2755 - mae: 126.9614 - val_loss: 8254.2677 - val_mae: 8254.9600\n",
      "Epoch 745/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 117.5119 - mae: 118.1989 - val_loss: 8102.0633 - val_mae: 8102.7563\n",
      "Epoch 746/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 154.7359 - mae: 155.4221 - val_loss: 9178.8852 - val_mae: 9179.5791\n",
      "Epoch 747/1000\n",
      "1461/1461 [==============================] - 0s 301us/step - loss: 161.7758 - mae: 162.4596 - val_loss: 8458.9145 - val_mae: 8459.6064\n",
      "Epoch 748/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 120.2200 - mae: 120.9045 - val_loss: 8203.3579 - val_mae: 8204.0498\n",
      "Epoch 749/1000\n",
      "1461/1461 [==============================] - 0s 263us/step - loss: 126.0352 - mae: 126.7213 - val_loss: 8220.5964 - val_mae: 8221.2891\n",
      "Epoch 750/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 129.1329 - mae: 129.8166 - val_loss: 8215.5689 - val_mae: 8216.2607\n",
      "Epoch 751/1000\n",
      "1461/1461 [==============================] - 0s 259us/step - loss: 140.8081 - mae: 141.4949 - val_loss: 8037.5933 - val_mae: 8038.2852\n",
      "Epoch 752/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 146.0122 - mae: 146.6977 - val_loss: 8287.8067 - val_mae: 8288.5000\n",
      "Epoch 753/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 121.9911 - mae: 122.6777 - val_loss: 8343.1709 - val_mae: 8343.8633\n",
      "Epoch 754/1000\n",
      "1461/1461 [==============================] - 0s 263us/step - loss: 136.7134 - mae: 137.3967 - val_loss: 8470.3686 - val_mae: 8471.0615\n",
      "Epoch 755/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 118.1065 - mae: 118.7913 - val_loss: 8119.4019 - val_mae: 8120.0957\n",
      "Epoch 756/1000\n",
      "1461/1461 [==============================] - 0s 290us/step - loss: 140.7113 - mae: 141.3980 - val_loss: 8408.6964 - val_mae: 8409.3896\n",
      "Epoch 757/1000\n",
      "1461/1461 [==============================] - 0s 307us/step - loss: 137.4977 - mae: 138.1841 - val_loss: 8180.9043 - val_mae: 8181.5977\n",
      "Epoch 758/1000\n",
      "1461/1461 [==============================] - 0s 283us/step - loss: 117.9423 - mae: 118.6268 - val_loss: 8430.2935 - val_mae: 8430.9873\n",
      "Epoch 759/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 128.4370 - mae: 129.1215 - val_loss: 8252.0754 - val_mae: 8252.7686\n",
      "Epoch 760/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 128.1065 - mae: 128.7916 - val_loss: 8306.3516 - val_mae: 8307.0449\n",
      "Epoch 761/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 119.3575 - mae: 120.0414 - val_loss: 8420.2999 - val_mae: 8420.9922\n",
      "Epoch 762/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 122.4511 - mae: 123.1356 - val_loss: 8177.2381 - val_mae: 8177.9321\n",
      "Epoch 763/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 120.7872 - mae: 121.4709 - val_loss: 8240.8068 - val_mae: 8241.5000\n",
      "Epoch 764/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 266us/step - loss: 122.9379 - mae: 123.6241 - val_loss: 8349.7320 - val_mae: 8350.4238\n",
      "Epoch 765/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 143.7488 - mae: 144.4347 - val_loss: 7957.4366 - val_mae: 7958.1294\n",
      "Epoch 766/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 156.9257 - mae: 157.6113 - val_loss: 8656.6683 - val_mae: 8657.3604\n",
      "Epoch 767/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 169.2264 - mae: 169.9120 - val_loss: 8662.2120 - val_mae: 8662.9053\n",
      "Epoch 768/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 132.0498 - mae: 132.7354 - val_loss: 8189.6920 - val_mae: 8190.3857\n",
      "Epoch 769/1000\n",
      "1461/1461 [==============================] - 0s 262us/step - loss: 133.9159 - mae: 134.6013 - val_loss: 8180.8257 - val_mae: 8181.5195\n",
      "Epoch 770/1000\n",
      "1461/1461 [==============================] - 0s 251us/step - loss: 130.7452 - mae: 131.4334 - val_loss: 8311.7446 - val_mae: 8312.4385\n",
      "Epoch 771/1000\n",
      "1461/1461 [==============================] - 0s 256us/step - loss: 110.6219 - mae: 111.3081 - val_loss: 8272.9014 - val_mae: 8273.5938\n",
      "Epoch 772/1000\n",
      "1461/1461 [==============================] - 0s 257us/step - loss: 117.6408 - mae: 118.3243 - val_loss: 8172.6322 - val_mae: 8173.3257\n",
      "Epoch 773/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 119.5947 - mae: 120.2801 - val_loss: 8585.7797 - val_mae: 8586.4727\n",
      "Epoch 774/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 124.1597 - mae: 124.8436 - val_loss: 8183.4409 - val_mae: 8184.1333\n",
      "Epoch 775/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 123.3806 - mae: 124.0638 - val_loss: 8383.0614 - val_mae: 8383.7539\n",
      "Epoch 776/1000\n",
      "1461/1461 [==============================] - 0s 256us/step - loss: 130.5835 - mae: 131.2665 - val_loss: 8433.9236 - val_mae: 8434.6172\n",
      "Epoch 777/1000\n",
      "1461/1461 [==============================] - 0s 253us/step - loss: 116.1360 - mae: 116.8200 - val_loss: 8238.8679 - val_mae: 8239.5615\n",
      "Epoch 778/1000\n",
      "1461/1461 [==============================] - 0s 253us/step - loss: 124.9457 - mae: 125.6340 - val_loss: 8202.7763 - val_mae: 8203.4697\n",
      "Epoch 779/1000\n",
      "1461/1461 [==============================] - 0s 253us/step - loss: 125.3201 - mae: 126.0042 - val_loss: 8126.2428 - val_mae: 8126.9360\n",
      "Epoch 780/1000\n",
      "1461/1461 [==============================] - 0s 256us/step - loss: 111.7452 - mae: 112.4274 - val_loss: 8128.5764 - val_mae: 8129.2700\n",
      "Epoch 781/1000\n",
      "1461/1461 [==============================] - 0s 250us/step - loss: 122.7626 - mae: 123.4428 - val_loss: 8267.2181 - val_mae: 8267.9111\n",
      "Epoch 782/1000\n",
      "1461/1461 [==============================] - 0s 247us/step - loss: 118.1507 - mae: 118.8338 - val_loss: 8148.0031 - val_mae: 8148.6953\n",
      "Epoch 783/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 119.3097 - mae: 119.9941 - val_loss: 8348.8904 - val_mae: 8349.5830\n",
      "Epoch 784/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 122.0731 - mae: 122.7593 - val_loss: 8409.3768 - val_mae: 8410.0693\n",
      "Epoch 785/1000\n",
      "1461/1461 [==============================] - 0s 248us/step - loss: 136.1160 - mae: 136.8014 - val_loss: 8447.7612 - val_mae: 8448.4531\n",
      "Epoch 786/1000\n",
      "1461/1461 [==============================] - 0s 253us/step - loss: 124.4060 - mae: 125.0916 - val_loss: 8490.1091 - val_mae: 8490.8018\n",
      "Epoch 787/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 131.4170 - mae: 132.1034 - val_loss: 8017.8169 - val_mae: 8018.5112\n",
      "Epoch 788/1000\n",
      "1461/1461 [==============================] - 0s 253us/step - loss: 131.8202 - mae: 132.5043 - val_loss: 8687.3885 - val_mae: 8688.0811\n",
      "Epoch 789/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 156.0247 - mae: 156.7123 - val_loss: 8391.2231 - val_mae: 8391.9170\n",
      "Epoch 790/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 133.7435 - mae: 134.4290 - val_loss: 8289.6936 - val_mae: 8290.3867\n",
      "Epoch 791/1000\n",
      "1461/1461 [==============================] - 0s 259us/step - loss: 123.3395 - mae: 124.0237 - val_loss: 8353.4640 - val_mae: 8354.1562\n",
      "Epoch 792/1000\n",
      "1461/1461 [==============================] - 0s 254us/step - loss: 122.5403 - mae: 123.2285 - val_loss: 7959.5375 - val_mae: 7960.2300\n",
      "Epoch 793/1000\n",
      "1461/1461 [==============================] - 0s 250us/step - loss: 122.2622 - mae: 122.9445 - val_loss: 8384.7132 - val_mae: 8385.4062\n",
      "Epoch 794/1000\n",
      "1461/1461 [==============================] - 0s 254us/step - loss: 133.1072 - mae: 133.7905 - val_loss: 8422.1626 - val_mae: 8422.8555\n",
      "Epoch 795/1000\n",
      "1461/1461 [==============================] - 0s 251us/step - loss: 169.8029 - mae: 170.4878 - val_loss: 8578.7742 - val_mae: 8579.4678\n",
      "Epoch 796/1000\n",
      "1461/1461 [==============================] - 0s 253us/step - loss: 147.6923 - mae: 148.3801 - val_loss: 8200.6106 - val_mae: 8201.3027\n",
      "Epoch 797/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 128.5563 - mae: 129.2409 - val_loss: 8256.8319 - val_mae: 8257.5254\n",
      "Epoch 798/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 112.4250 - mae: 113.1114 - val_loss: 8213.1108 - val_mae: 8213.8037\n",
      "Epoch 799/1000\n",
      "1461/1461 [==============================] - ETA: 0s - loss: 119.4962 - mae: 120.181 - 0s 252us/step - loss: 119.3215 - mae: 120.0062 - val_loss: 8082.0538 - val_mae: 8082.7466\n",
      "Epoch 800/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 128.5505 - mae: 129.2375 - val_loss: 8294.1235 - val_mae: 8294.8154\n",
      "Epoch 801/1000\n",
      "1461/1461 [==============================] - 0s 253us/step - loss: 118.0683 - mae: 118.7542 - val_loss: 8089.7886 - val_mae: 8090.4824\n",
      "Epoch 802/1000\n",
      "1461/1461 [==============================] - 0s 250us/step - loss: 124.8974 - mae: 125.5852 - val_loss: 8464.6762 - val_mae: 8465.3682\n",
      "Epoch 803/1000\n",
      "1461/1461 [==============================] - 0s 254us/step - loss: 151.4968 - mae: 152.1846 - val_loss: 8325.2180 - val_mae: 8325.9111\n",
      "Epoch 804/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 134.7576 - mae: 135.4439 - val_loss: 8288.2847 - val_mae: 8288.9775\n",
      "Epoch 805/1000\n",
      "1461/1461 [==============================] - 0s 253us/step - loss: 132.5514 - mae: 133.2357 - val_loss: 8238.2951 - val_mae: 8238.9873\n",
      "Epoch 806/1000\n",
      "1461/1461 [==============================] - 0s 257us/step - loss: 125.6443 - mae: 126.3247 - val_loss: 8387.5582 - val_mae: 8388.2510\n",
      "Epoch 807/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 122.1524 - mae: 122.8388 - val_loss: 8574.3557 - val_mae: 8575.0498\n",
      "Epoch 808/1000\n",
      "1461/1461 [==============================] - 0s 254us/step - loss: 124.7284 - mae: 125.4136 - val_loss: 8548.0432 - val_mae: 8548.7363\n",
      "Epoch 809/1000\n",
      "1461/1461 [==============================] - 0s 253us/step - loss: 108.3010 - mae: 108.9823 - val_loss: 8435.6578 - val_mae: 8436.3506\n",
      "Epoch 810/1000\n",
      "1461/1461 [==============================] - 0s 258us/step - loss: 121.5450 - mae: 122.2287 - val_loss: 8450.1157 - val_mae: 8450.8086\n",
      "Epoch 811/1000\n",
      "1461/1461 [==============================] - 0s 257us/step - loss: 124.5749 - mae: 125.2602 - val_loss: 8232.7136 - val_mae: 8233.4072\n",
      "Epoch 812/1000\n",
      "1461/1461 [==============================] - 0s 253us/step - loss: 115.4929 - mae: 116.1775 - val_loss: 8246.2606 - val_mae: 8246.9541\n",
      "Epoch 813/1000\n",
      "1461/1461 [==============================] - 0s 259us/step - loss: 118.9081 - mae: 119.5933 - val_loss: 8578.5205 - val_mae: 8579.2139\n",
      "Epoch 814/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 140.0792 - mae: 140.7654 - val_loss: 8360.1994 - val_mae: 8360.8926\n",
      "Epoch 815/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 134.9019 - mae: 135.5869 - val_loss: 8119.1820 - val_mae: 8119.8750\n",
      "Epoch 816/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 117.9553 - mae: 118.6395 - val_loss: 8265.6803 - val_mae: 8266.3730\n",
      "Epoch 817/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 128.8611 - mae: 129.5458 - val_loss: 8333.9353 - val_mae: 8334.6289\n",
      "Epoch 818/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 249us/step - loss: 115.2241 - mae: 115.9090 - val_loss: 8244.6281 - val_mae: 8245.3213\n",
      "Epoch 819/1000\n",
      "1461/1461 [==============================] - 0s 248us/step - loss: 114.4082 - mae: 115.0970 - val_loss: 8359.5233 - val_mae: 8360.2178\n",
      "Epoch 820/1000\n",
      "1461/1461 [==============================] - 0s 248us/step - loss: 120.7408 - mae: 121.4229 - val_loss: 8367.6674 - val_mae: 8368.3594\n",
      "Epoch 821/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 122.5300 - mae: 123.2139 - val_loss: 8249.1790 - val_mae: 8249.8730\n",
      "Epoch 822/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 129.3622 - mae: 130.0486 - val_loss: 8509.6416 - val_mae: 8510.3350\n",
      "Epoch 823/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 135.8864 - mae: 136.5730 - val_loss: 8293.9506 - val_mae: 8294.6445\n",
      "Epoch 824/1000\n",
      "1461/1461 [==============================] - 0s 248us/step - loss: 118.8847 - mae: 119.5688 - val_loss: 8137.6664 - val_mae: 8138.3589\n",
      "Epoch 825/1000\n",
      "1461/1461 [==============================] - 0s 251us/step - loss: 127.1407 - mae: 127.8266 - val_loss: 8237.4204 - val_mae: 8238.1143\n",
      "Epoch 826/1000\n",
      "1461/1461 [==============================] - 0s 257us/step - loss: 114.9677 - mae: 115.6510 - val_loss: 8314.3186 - val_mae: 8315.0127\n",
      "Epoch 827/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 122.7542 - mae: 123.4386 - val_loss: 8256.1343 - val_mae: 8256.8281\n",
      "Epoch 828/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 112.9265 - mae: 113.6096 - val_loss: 8382.7496 - val_mae: 8383.4434\n",
      "Epoch 829/1000\n",
      "1461/1461 [==============================] - 0s 248us/step - loss: 123.3485 - mae: 124.0329 - val_loss: 8446.7595 - val_mae: 8447.4521\n",
      "Epoch 830/1000\n",
      "1461/1461 [==============================] - 0s 250us/step - loss: 116.1316 - mae: 116.8145 - val_loss: 8568.0706 - val_mae: 8568.7627\n",
      "Epoch 831/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 110.9160 - mae: 111.5979 - val_loss: 8299.4635 - val_mae: 8300.1562\n",
      "Epoch 832/1000\n",
      "1461/1461 [==============================] - 0s 295us/step - loss: 118.8243 - mae: 119.5088 - val_loss: 8466.8834 - val_mae: 8467.5762\n",
      "Epoch 833/1000\n",
      "1461/1461 [==============================] - 0s 254us/step - loss: 112.3335 - mae: 113.0169 - val_loss: 8366.8357 - val_mae: 8367.5293\n",
      "Epoch 834/1000\n",
      "1461/1461 [==============================] - 0s 258us/step - loss: 108.1485 - mae: 108.8348 - val_loss: 8151.0007 - val_mae: 8151.6943\n",
      "Epoch 835/1000\n",
      "1461/1461 [==============================] - 0s 257us/step - loss: 130.6292 - mae: 131.3142 - val_loss: 8424.3321 - val_mae: 8425.0264\n",
      "Epoch 836/1000\n",
      "1461/1461 [==============================] - 0s 257us/step - loss: 125.3477 - mae: 126.0357 - val_loss: 8537.1452 - val_mae: 8537.8389\n",
      "Epoch 837/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 115.9467 - mae: 116.6318 - val_loss: 8303.2273 - val_mae: 8303.9209\n",
      "Epoch 838/1000\n",
      "1461/1461 [==============================] - 0s 245us/step - loss: 117.3082 - mae: 117.9901 - val_loss: 8430.2409 - val_mae: 8430.9346\n",
      "Epoch 839/1000\n",
      "1461/1461 [==============================] - 0s 249us/step - loss: 135.2445 - mae: 135.9302 - val_loss: 8284.8979 - val_mae: 8285.5908\n",
      "Epoch 840/1000\n",
      "1461/1461 [==============================] - 0s 245us/step - loss: 117.7065 - mae: 118.3946 - val_loss: 8585.8897 - val_mae: 8586.5830\n",
      "Epoch 841/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 115.4224 - mae: 116.1061 - val_loss: 8377.8483 - val_mae: 8378.5420\n",
      "Epoch 842/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 112.5358 - mae: 113.2208 - val_loss: 8530.4436 - val_mae: 8531.1377\n",
      "Epoch 843/1000\n",
      "1461/1461 [==============================] - 0s 262us/step - loss: 120.6462 - mae: 121.3315 - val_loss: 8458.4137 - val_mae: 8459.1074\n",
      "Epoch 844/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 114.3284 - mae: 115.0083 - val_loss: 8402.6716 - val_mae: 8403.3643\n",
      "Epoch 845/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 114.8387 - mae: 115.5242 - val_loss: 8074.7424 - val_mae: 8075.4351\n",
      "Epoch 846/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 110.7212 - mae: 111.4053 - val_loss: 8247.9877 - val_mae: 8248.6797\n",
      "Epoch 847/1000\n",
      "1461/1461 [==============================] - 0s 287us/step - loss: 128.4090 - mae: 129.0942 - val_loss: 8463.1114 - val_mae: 8463.8037\n",
      "Epoch 848/1000\n",
      "1461/1461 [==============================] - 0s 261us/step - loss: 121.5051 - mae: 122.1906 - val_loss: 8360.0098 - val_mae: 8360.7021\n",
      "Epoch 849/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 112.3310 - mae: 113.0134 - val_loss: 8308.9864 - val_mae: 8309.6787\n",
      "Epoch 850/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 125.0781 - mae: 125.7622 - val_loss: 8212.8376 - val_mae: 8213.5312\n",
      "Epoch 851/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 114.2747 - mae: 114.9610 - val_loss: 8496.6090 - val_mae: 8497.3027\n",
      "Epoch 852/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 143.0070 - mae: 143.6898 - val_loss: 8364.6622 - val_mae: 8365.3545\n",
      "Epoch 853/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 117.0866 - mae: 117.7683 - val_loss: 8443.4770 - val_mae: 8444.1699\n",
      "Epoch 854/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 107.5590 - mae: 108.2413 - val_loss: 8518.7146 - val_mae: 8519.4082\n",
      "Epoch 855/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 121.5344 - mae: 122.2181 - val_loss: 8368.7788 - val_mae: 8369.4717\n",
      "Epoch 856/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 121.6041 - mae: 122.2904 - val_loss: 8298.6072 - val_mae: 8299.3008\n",
      "Epoch 857/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 117.0323 - mae: 117.7169 - val_loss: 8364.7207 - val_mae: 8365.4141\n",
      "Epoch 858/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 120.3597 - mae: 121.0438 - val_loss: 8508.3942 - val_mae: 8509.0869\n",
      "Epoch 859/1000\n",
      "1461/1461 [==============================] - 0s 263us/step - loss: 125.4074 - mae: 126.0934 - val_loss: 8165.3984 - val_mae: 8166.0918\n",
      "Epoch 860/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 116.4328 - mae: 117.1177 - val_loss: 8483.0838 - val_mae: 8483.7773\n",
      "Epoch 861/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 107.1483 - mae: 107.8316 - val_loss: 8533.9324 - val_mae: 8534.6260\n",
      "Epoch 862/1000\n",
      "1461/1461 [==============================] - 0s 262us/step - loss: 121.7911 - mae: 122.4749 - val_loss: 8214.9890 - val_mae: 8215.6816\n",
      "Epoch 863/1000\n",
      "1461/1461 [==============================] - 0s 262us/step - loss: 115.3644 - mae: 116.0502 - val_loss: 8261.3914 - val_mae: 8262.0840\n",
      "Epoch 864/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 125.2386 - mae: 125.9254 - val_loss: 8277.0171 - val_mae: 8277.7109\n",
      "Epoch 865/1000\n",
      "1461/1461 [==============================] - 0s 261us/step - loss: 127.9678 - mae: 128.6530 - val_loss: 8219.9230 - val_mae: 8220.6162\n",
      "Epoch 866/1000\n",
      "1461/1461 [==============================] - 0s 263us/step - loss: 104.0366 - mae: 104.7193 - val_loss: 8621.6607 - val_mae: 8622.3535\n",
      "Epoch 867/1000\n",
      "1461/1461 [==============================] - 0s 261us/step - loss: 133.7755 - mae: 134.4590 - val_loss: 8290.8798 - val_mae: 8291.5723\n",
      "Epoch 868/1000\n",
      "1461/1461 [==============================] - 0s 263us/step - loss: 116.4752 - mae: 117.1611 - val_loss: 8522.6018 - val_mae: 8523.2949\n",
      "Epoch 869/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 113.5794 - mae: 114.2609 - val_loss: 8290.5788 - val_mae: 8291.2705\n",
      "Epoch 870/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 111.7157 - mae: 112.4001 - val_loss: 8423.7219 - val_mae: 8424.4150\n",
      "Epoch 871/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 106.8152 - mae: 107.4992 - val_loss: 8191.9004 - val_mae: 8192.5938\n",
      "Epoch 872/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 123.0767 - mae: 123.7624 - val_loss: 8299.2606 - val_mae: 8299.9531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 873/1000\n",
      "1461/1461 [==============================] - 0s 292us/step - loss: 120.0991 - mae: 120.7857 - val_loss: 8401.6973 - val_mae: 8402.3906\n",
      "Epoch 874/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 136.0487 - mae: 136.7352 - val_loss: 8450.7378 - val_mae: 8451.4316\n",
      "Epoch 875/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 134.1739 - mae: 134.8573 - val_loss: 8561.6878 - val_mae: 8562.3818\n",
      "Epoch 876/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 114.4109 - mae: 115.0934 - val_loss: 8356.4183 - val_mae: 8357.1113\n",
      "Epoch 877/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 109.9671 - mae: 110.6499 - val_loss: 8188.6804 - val_mae: 8189.3735\n",
      "Epoch 878/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 110.4373 - mae: 111.1216 - val_loss: 8400.8857 - val_mae: 8401.5791\n",
      "Epoch 879/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 115.7937 - mae: 116.4809 - val_loss: 8448.9394 - val_mae: 8449.6318\n",
      "Epoch 880/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 110.7286 - mae: 111.4111 - val_loss: 8427.1025 - val_mae: 8427.7949\n",
      "Epoch 881/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 118.5334 - mae: 119.2162 - val_loss: 8318.9157 - val_mae: 8319.6094\n",
      "Epoch 882/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 137.6233 - mae: 138.3079 - val_loss: 8363.3782 - val_mae: 8364.0703\n",
      "Epoch 883/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 113.9223 - mae: 114.6092 - val_loss: 8388.7721 - val_mae: 8389.4648\n",
      "Epoch 884/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 115.1269 - mae: 115.8085 - val_loss: 8513.5513 - val_mae: 8514.2451\n",
      "Epoch 885/1000\n",
      "1461/1461 [==============================] - 0s 262us/step - loss: 126.3942 - mae: 127.0805 - val_loss: 8275.5130 - val_mae: 8276.2061\n",
      "Epoch 886/1000\n",
      "1461/1461 [==============================] - 0s 259us/step - loss: 118.3686 - mae: 119.0537 - val_loss: 8225.1643 - val_mae: 8225.8564\n",
      "Epoch 887/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 128.8639 - mae: 129.5505 - val_loss: 8527.1564 - val_mae: 8527.8496\n",
      "Epoch 888/1000\n",
      "1461/1461 [==============================] - 0s 261us/step - loss: 137.5161 - mae: 138.2011 - val_loss: 8345.4394 - val_mae: 8346.1309\n",
      "Epoch 889/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 165.7438 - mae: 166.4293 - val_loss: 8526.3413 - val_mae: 8527.0352\n",
      "Epoch 890/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 125.2610 - mae: 125.9475 - val_loss: 8670.5662 - val_mae: 8671.2588\n",
      "Epoch 891/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 110.7418 - mae: 111.4233 - val_loss: 8379.8306 - val_mae: 8380.5234\n",
      "Epoch 892/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 117.9229 - mae: 118.6067 - val_loss: 8320.3160 - val_mae: 8321.0098\n",
      "Epoch 893/1000\n",
      "1461/1461 [==============================] - 0s 263us/step - loss: 129.7908 - mae: 130.4774 - val_loss: 8494.2980 - val_mae: 8494.9902\n",
      "Epoch 894/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 131.3124 - mae: 131.9969 - val_loss: 8153.8667 - val_mae: 8154.5601\n",
      "Epoch 895/1000\n",
      "1461/1461 [==============================] - 0s 258us/step - loss: 125.6907 - mae: 126.3755 - val_loss: 8296.2438 - val_mae: 8296.9385\n",
      "Epoch 896/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 124.7297 - mae: 125.4135 - val_loss: 8374.4995 - val_mae: 8375.1934\n",
      "Epoch 897/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 129.7074 - mae: 130.3922 - val_loss: 8310.9560 - val_mae: 8311.6494\n",
      "Epoch 898/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 116.6710 - mae: 117.3546 - val_loss: 8460.3771 - val_mae: 8461.0703\n",
      "Epoch 899/1000\n",
      "1461/1461 [==============================] - 0s 263us/step - loss: 106.1819 - mae: 106.8671 - val_loss: 8420.5393 - val_mae: 8421.2324\n",
      "Epoch 900/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 114.7992 - mae: 115.4826 - val_loss: 8473.9076 - val_mae: 8474.6006\n",
      "Epoch 901/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 109.8697 - mae: 110.5525 - val_loss: 8228.4502 - val_mae: 8229.1436\n",
      "Epoch 902/1000\n",
      "1461/1461 [==============================] - 0s 257us/step - loss: 110.2738 - mae: 110.9575 - val_loss: 8580.4051 - val_mae: 8581.0977\n",
      "Epoch 903/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 111.0374 - mae: 111.7243 - val_loss: 8411.3835 - val_mae: 8412.0762\n",
      "Epoch 904/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 102.6263 - mae: 103.3082 - val_loss: 8427.5284 - val_mae: 8428.2227\n",
      "Epoch 905/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 114.4100 - mae: 115.0954 - val_loss: 8571.1455 - val_mae: 8571.8398\n",
      "Epoch 906/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 109.7375 - mae: 110.4183 - val_loss: 8241.1104 - val_mae: 8241.8037\n",
      "Epoch 907/1000\n",
      "1461/1461 [==============================] - 0s 261us/step - loss: 128.5257 - mae: 129.2138 - val_loss: 8457.6142 - val_mae: 8458.3076\n",
      "Epoch 908/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 118.3119 - mae: 118.9978 - val_loss: 8489.4021 - val_mae: 8490.0957\n",
      "Epoch 909/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 109.5474 - mae: 110.2312 - val_loss: 8240.9648 - val_mae: 8241.6572\n",
      "Epoch 910/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 125.1950 - mae: 125.8815 - val_loss: 8356.4514 - val_mae: 8357.1455\n",
      "Epoch 911/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 125.4057 - mae: 126.0896 - val_loss: 8622.9354 - val_mae: 8623.6289\n",
      "Epoch 912/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 113.9254 - mae: 114.6089 - val_loss: 8540.7749 - val_mae: 8541.4668\n",
      "Epoch 913/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 105.0329 - mae: 105.7142 - val_loss: 8549.1701 - val_mae: 8549.8623\n",
      "Epoch 914/1000\n",
      "1461/1461 [==============================] - 0s 300us/step - loss: 119.5277 - mae: 120.2096 - val_loss: 8533.6005 - val_mae: 8534.2939\n",
      "Epoch 915/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 113.5306 - mae: 114.2146 - val_loss: 8512.7918 - val_mae: 8513.4844\n",
      "Epoch 916/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 107.7736 - mae: 108.4568 - val_loss: 8458.5994 - val_mae: 8459.2930\n",
      "Epoch 917/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 129.9840 - mae: 130.6707 - val_loss: 8577.7399 - val_mae: 8578.4336\n",
      "Epoch 918/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 135.8765 - mae: 136.5598 - val_loss: 8402.7926 - val_mae: 8403.4863\n",
      "Epoch 919/1000\n",
      "1461/1461 [==============================] - 0s 262us/step - loss: 108.3041 - mae: 108.9865 - val_loss: 8538.5184 - val_mae: 8539.2109\n",
      "Epoch 920/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 120.1741 - mae: 120.8577 - val_loss: 8429.5355 - val_mae: 8430.2285\n",
      "Epoch 921/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 123.4573 - mae: 124.1384 - val_loss: 8346.9521 - val_mae: 8347.6465\n",
      "Epoch 922/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 117.4980 - mae: 118.1819 - val_loss: 8377.8025 - val_mae: 8378.4961\n",
      "Epoch 923/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 121.7142 - mae: 122.4011 - val_loss: 8169.3116 - val_mae: 8170.0039\n",
      "Epoch 924/1000\n",
      "1461/1461 [==============================] - 0s 263us/step - loss: 113.8103 - mae: 114.4955 - val_loss: 8308.0522 - val_mae: 8308.7461\n",
      "Epoch 925/1000\n",
      "1461/1461 [==============================] - 0s 259us/step - loss: 111.8386 - mae: 112.5237 - val_loss: 8411.0075 - val_mae: 8411.7012\n",
      "Epoch 926/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 113.7615 - mae: 114.4471 - val_loss: 8410.4105 - val_mae: 8411.1035\n",
      "Epoch 927/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 258us/step - loss: 115.4398 - mae: 116.1254 - val_loss: 8188.6099 - val_mae: 8189.3037\n",
      "Epoch 928/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 124.0877 - mae: 124.7734 - val_loss: 8499.9806 - val_mae: 8500.6748\n",
      "Epoch 929/1000\n",
      "1461/1461 [==============================] - 0s 261us/step - loss: 106.1354 - mae: 106.8215 - val_loss: 8377.6659 - val_mae: 8378.3584\n",
      "Epoch 930/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 124.2397 - mae: 124.9262 - val_loss: 8307.1086 - val_mae: 8307.8027\n",
      "Epoch 931/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 106.4754 - mae: 107.1573 - val_loss: 8368.7608 - val_mae: 8369.4541\n",
      "Epoch 932/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 106.3215 - mae: 107.0076 - val_loss: 8375.8836 - val_mae: 8376.5762\n",
      "Epoch 933/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 107.6066 - mae: 108.2907 - val_loss: 8442.6401 - val_mae: 8443.3350\n",
      "Epoch 934/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 118.3166 - mae: 118.9999 - val_loss: 8508.1181 - val_mae: 8508.8115\n",
      "Epoch 935/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 122.9522 - mae: 123.6348 - val_loss: 8379.3447 - val_mae: 8380.0381\n",
      "Epoch 936/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 120.0029 - mae: 120.6910 - val_loss: 8532.6415 - val_mae: 8533.3340\n",
      "Epoch 937/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 108.8524 - mae: 109.5347 - val_loss: 8405.4871 - val_mae: 8406.1816\n",
      "Epoch 938/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 111.9530 - mae: 112.6392 - val_loss: 8470.2645 - val_mae: 8470.9570\n",
      "Epoch 939/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 111.5849 - mae: 112.2650 - val_loss: 8166.4413 - val_mae: 8167.1343\n",
      "Epoch 940/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 114.0851 - mae: 114.7709 - val_loss: 8451.0615 - val_mae: 8451.7529\n",
      "Epoch 941/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 108.5612 - mae: 109.2460 - val_loss: 8387.3264 - val_mae: 8388.0195\n",
      "Epoch 942/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 119.2513 - mae: 119.9346 - val_loss: 8338.9415 - val_mae: 8339.6348\n",
      "Epoch 943/1000\n",
      "1461/1461 [==============================] - 0s 256us/step - loss: 105.6559 - mae: 106.3401 - val_loss: 8459.1272 - val_mae: 8459.8193\n",
      "Epoch 944/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 122.5091 - mae: 123.1948 - val_loss: 8467.1121 - val_mae: 8467.8047\n",
      "Epoch 945/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 121.3381 - mae: 122.0221 - val_loss: 8495.6689 - val_mae: 8496.3623\n",
      "Epoch 946/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 108.6577 - mae: 109.3385 - val_loss: 8508.4222 - val_mae: 8509.1152\n",
      "Epoch 947/1000\n",
      "1461/1461 [==============================] - 0s 263us/step - loss: 109.8103 - mae: 110.4959 - val_loss: 8483.0189 - val_mae: 8483.7119\n",
      "Epoch 948/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 108.7851 - mae: 109.4692 - val_loss: 8401.3379 - val_mae: 8402.0322\n",
      "Epoch 949/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 129.6729 - mae: 130.3574 - val_loss: 8444.2817 - val_mae: 8444.9746\n",
      "Epoch 950/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 119.9423 - mae: 120.6264 - val_loss: 8592.2223 - val_mae: 8592.9150\n",
      "Epoch 951/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 125.6913 - mae: 126.3778 - val_loss: 8295.5873 - val_mae: 8296.2793\n",
      "Epoch 952/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 111.9434 - mae: 112.6254 - val_loss: 8573.0072 - val_mae: 8573.7012\n",
      "Epoch 953/1000\n",
      "1461/1461 [==============================] - 0s 253us/step - loss: 123.8554 - mae: 124.5410 - val_loss: 8392.8891 - val_mae: 8393.5820\n",
      "Epoch 954/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 120.8606 - mae: 121.5457 - val_loss: 8164.0406 - val_mae: 8164.7339\n",
      "Epoch 955/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 121.5378 - mae: 122.2205 - val_loss: 8049.0311 - val_mae: 8049.7246\n",
      "Epoch 956/1000\n",
      "1461/1461 [==============================] - 0s 254us/step - loss: 134.0410 - mae: 134.7248 - val_loss: 8540.6098 - val_mae: 8541.3037\n",
      "Epoch 957/1000\n",
      "1461/1461 [==============================] - 0s 256us/step - loss: 124.4586 - mae: 125.1417 - val_loss: 8042.9287 - val_mae: 8043.6221\n",
      "Epoch 958/1000\n",
      "1461/1461 [==============================] - 0s 251us/step - loss: 119.3194 - mae: 120.0018 - val_loss: 8373.6414 - val_mae: 8374.3340\n",
      "Epoch 959/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 116.9666 - mae: 117.6496 - val_loss: 8491.8483 - val_mae: 8492.5410\n",
      "Epoch 960/1000\n",
      "1461/1461 [==============================] - 0s 253us/step - loss: 114.0495 - mae: 114.7338 - val_loss: 8274.8848 - val_mae: 8275.5781\n",
      "Epoch 961/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 124.0922 - mae: 124.7746 - val_loss: 8682.7041 - val_mae: 8683.3984\n",
      "Epoch 962/1000\n",
      "1461/1461 [==============================] - 0s 251us/step - loss: 120.2839 - mae: 120.9679 - val_loss: 8304.2390 - val_mae: 8304.9336\n",
      "Epoch 963/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 107.4063 - mae: 108.0898 - val_loss: 8320.0275 - val_mae: 8320.7217\n",
      "Epoch 964/1000\n",
      "1461/1461 [==============================] - 0s 257us/step - loss: 114.9332 - mae: 115.6183 - val_loss: 8267.4317 - val_mae: 8268.1250\n",
      "Epoch 965/1000\n",
      "1461/1461 [==============================] - 0s 259us/step - loss: 121.2776 - mae: 121.9599 - val_loss: 8522.4522 - val_mae: 8523.1455\n",
      "Epoch 966/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 115.4046 - mae: 116.0898 - val_loss: 8298.8208 - val_mae: 8299.5137\n",
      "Epoch 967/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 109.6251 - mae: 110.3087 - val_loss: 8473.5675 - val_mae: 8474.2607\n",
      "Epoch 968/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 103.6675 - mae: 104.3491 - val_loss: 8499.7809 - val_mae: 8500.4736\n",
      "Epoch 969/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 113.3142 - mae: 113.9975 - val_loss: 8645.4439 - val_mae: 8646.1367\n",
      "Epoch 970/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 113.5759 - mae: 114.2592 - val_loss: 8429.1815 - val_mae: 8429.8740\n",
      "Epoch 971/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 114.4214 - mae: 115.1073 - val_loss: 8460.1093 - val_mae: 8460.8027\n",
      "Epoch 972/1000\n",
      "1461/1461 [==============================] - 0s 252us/step - loss: 119.7144 - mae: 120.3975 - val_loss: 8365.7116 - val_mae: 8366.4043\n",
      "Epoch 973/1000\n",
      "1461/1461 [==============================] - 0s 257us/step - loss: 114.8910 - mae: 115.5769 - val_loss: 8342.3103 - val_mae: 8343.0029\n",
      "Epoch 974/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 119.8088 - mae: 120.4946 - val_loss: 8319.1053 - val_mae: 8319.7979\n",
      "Epoch 975/1000\n",
      "1461/1461 [==============================] - 0s 261us/step - loss: 118.6182 - mae: 119.3036 - val_loss: 8524.8109 - val_mae: 8525.5029\n",
      "Epoch 976/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 105.2065 - mae: 105.8916 - val_loss: 8335.5270 - val_mae: 8336.2207\n",
      "Epoch 977/1000\n",
      "1461/1461 [==============================] - 0s 259us/step - loss: 108.6797 - mae: 109.3654 - val_loss: 8402.3770 - val_mae: 8403.0703\n",
      "Epoch 978/1000\n",
      "1461/1461 [==============================] - 0s 262us/step - loss: 107.4176 - mae: 108.1017 - val_loss: 8463.1896 - val_mae: 8463.8838\n",
      "Epoch 979/1000\n",
      "1461/1461 [==============================] - 0s 263us/step - loss: 119.8975 - mae: 120.5831 - val_loss: 8597.5270 - val_mae: 8598.2207\n",
      "Epoch 980/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 111.1447 - mae: 111.8310 - val_loss: 8336.9508 - val_mae: 8337.6445\n",
      "Epoch 981/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 106.1788 - mae: 106.8608 - val_loss: 8425.0175 - val_mae: 8425.7109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 982/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 116.8883 - mae: 117.5704 - val_loss: 8524.2416 - val_mae: 8524.9355\n",
      "Epoch 983/1000\n",
      "1461/1461 [==============================] - 0s 257us/step - loss: 137.0277 - mae: 137.7111 - val_loss: 8519.5200 - val_mae: 8520.2129\n",
      "Epoch 984/1000\n",
      "1461/1461 [==============================] - 0s 262us/step - loss: 108.3072 - mae: 108.9915 - val_loss: 8457.2195 - val_mae: 8457.9121\n",
      "Epoch 985/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 106.8702 - mae: 107.5542 - val_loss: 8599.0988 - val_mae: 8599.7920\n",
      "Epoch 986/1000\n",
      "1461/1461 [==============================] - 0s 262us/step - loss: 125.5394 - mae: 126.2252 - val_loss: 8538.1386 - val_mae: 8538.8291\n",
      "Epoch 987/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 109.9260 - mae: 110.6113 - val_loss: 8626.8533 - val_mae: 8627.5469\n",
      "Epoch 988/1000\n",
      "1461/1461 [==============================] - 0s 261us/step - loss: 115.6181 - mae: 116.3031 - val_loss: 8398.7094 - val_mae: 8399.4014\n",
      "Epoch 989/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 100.3841 - mae: 101.0636 - val_loss: 8434.0139 - val_mae: 8434.7070\n",
      "Epoch 990/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 107.9390 - mae: 108.6212 - val_loss: 8339.1433 - val_mae: 8339.8359\n",
      "Epoch 991/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 142.4438 - mae: 143.1297 - val_loss: 8627.8268 - val_mae: 8628.5205\n",
      "Epoch 992/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 117.8798 - mae: 118.5633 - val_loss: 8542.0091 - val_mae: 8542.7021\n",
      "Epoch 993/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 111.0523 - mae: 111.7372 - val_loss: 8479.2735 - val_mae: 8479.9658\n",
      "Epoch 994/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 113.0138 - mae: 113.6996 - val_loss: 8194.9329 - val_mae: 8195.6260\n",
      "Epoch 995/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 109.4046 - mae: 110.0895 - val_loss: 8554.2777 - val_mae: 8554.9697\n",
      "Epoch 996/1000\n",
      "1461/1461 [==============================] - 0s 309us/step - loss: 109.9299 - mae: 110.6139 - val_loss: 8454.4416 - val_mae: 8455.1348\n",
      "Epoch 997/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 106.0928 - mae: 106.7781 - val_loss: 8258.8006 - val_mae: 8259.4932\n",
      "Epoch 998/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 106.1405 - mae: 106.8241 - val_loss: 8469.3757 - val_mae: 8470.0684\n",
      "Epoch 999/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 140.8136 - mae: 141.5008 - val_loss: 8201.4706 - val_mae: 8202.1631\n",
      "Epoch 1000/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 107.7748 - mae: 108.4604 - val_loss: 8159.8763 - val_mae: 8160.5698\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fa838fc2cd0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(X_train,Y_train,validation_data=(X_val, Y_val))\n",
    "#Audio(sound_file,autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b0f090b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 127us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9966428561796283"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for check\n",
    "y_train_pred=regressor.predict(X_train)\n",
    "r2_score(Y_train, y_train_pred) #training score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5641115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546/546 [==============================] - 0s 83us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-06-01</th>\n",
       "      <td>33731.0</td>\n",
       "      <td>34394.320312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-02</th>\n",
       "      <td>33285.0</td>\n",
       "      <td>34062.855469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-03</th>\n",
       "      <td>34298.0</td>\n",
       "      <td>33482.195312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-04</th>\n",
       "      <td>35271.0</td>\n",
       "      <td>34105.074219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-05</th>\n",
       "      <td>34100.0</td>\n",
       "      <td>33992.710938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>24159.792969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>23844.636719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>23678.773438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>23188.109375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>22679.246094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>546 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             y_test        y_pred\n",
       "Date                             \n",
       "2021-06-01  33731.0  34394.320312\n",
       "2021-06-02  33285.0  34062.855469\n",
       "2021-06-03  34298.0  33482.195312\n",
       "2021-06-04  35271.0  34105.074219\n",
       "2021-06-05  34100.0  33992.710938\n",
       "...             ...           ...\n",
       "2022-11-24      NaN  24159.792969\n",
       "2022-11-25      NaN  23844.636719\n",
       "2022-11-26      NaN  23678.773438\n",
       "2022-11-27      NaN  23188.109375\n",
       "2022-11-28      NaN  22679.246094\n",
       "\n",
       "[546 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=regressor.predict(X_test)\n",
    "pre_df = pd.DataFrame(zip(Y_test,y_pred),columns=['y_test','y_pred'])\n",
    "pre_df.index = Y_test.index\n",
    "pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1453bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.528306537077153"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2=r2_score(Y_test[0:-30],y_pred[0:-30]) #testing score/ r^2\n",
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70b87143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9293.30533975775"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse=np.sqrt(mean_squared_error(Y_test[0:-30],y_pred[0:-30])) #rmse\n",
    "rmse#太特么大了，感觉数据集划分有问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f4bd2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df['pred_returns'] = pre_df['y_pred'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b94aa95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>pred_returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-06-01</th>\n",
       "      <td>33731.0</td>\n",
       "      <td>34394.320312</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-02</th>\n",
       "      <td>33285.0</td>\n",
       "      <td>34062.855469</td>\n",
       "      <td>-0.009637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-03</th>\n",
       "      <td>34298.0</td>\n",
       "      <td>33482.195312</td>\n",
       "      <td>-0.017047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-04</th>\n",
       "      <td>35271.0</td>\n",
       "      <td>34105.074219</td>\n",
       "      <td>0.018603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-05</th>\n",
       "      <td>34100.0</td>\n",
       "      <td>33992.710938</td>\n",
       "      <td>-0.003295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>24159.792969</td>\n",
       "      <td>-0.012123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>23844.636719</td>\n",
       "      <td>-0.013045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>23678.773438</td>\n",
       "      <td>-0.006956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>23188.109375</td>\n",
       "      <td>-0.020722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>22679.246094</td>\n",
       "      <td>-0.021945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>546 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             y_test        y_pred  pred_returns\n",
       "Date                                           \n",
       "2021-06-01  33731.0  34394.320312           NaN\n",
       "2021-06-02  33285.0  34062.855469     -0.009637\n",
       "2021-06-03  34298.0  33482.195312     -0.017047\n",
       "2021-06-04  35271.0  34105.074219      0.018603\n",
       "2021-06-05  34100.0  33992.710938     -0.003295\n",
       "...             ...           ...           ...\n",
       "2022-11-24      NaN  24159.792969     -0.012123\n",
       "2022-11-25      NaN  23844.636719     -0.013045\n",
       "2022-11-26      NaN  23678.773438     -0.006956\n",
       "2022-11-27      NaN  23188.109375     -0.020722\n",
       "2022-11-28      NaN  22679.246094     -0.021945\n",
       "\n",
       "[546 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d9674c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(zip(Y_test,y_pred),columns=['y_test','y_pred']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "732393ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df.to_csv(\"btc_NN.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12ff04f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
