{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5b918ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/liuxuyang/opt/anaconda3/envs/mytensorflow/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from datetime import datetime\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n",
    "from keras import losses\n",
    "from keras.constraints import maxnorm\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_validate\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from collections import Counter\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import randint\n",
    "import shap\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, accuracy_score, f1_score, classification_report\n",
    "import pickle\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from commons import mean_absolute_percentage_error\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, roc_auc_score\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import Audio\n",
    "#sound_file = 'beep.wav'\n",
    "from keras.callbacks import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ef882a",
   "metadata": {},
   "source": [
    "### Litecoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "605e6303",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "regs = pd.read_csv(\"ltc_reg.csv\")\n",
    "btc = pd.read_csv(\"litecoin_Data.csv\")\n",
    "for i in range(len(btc['Date'])):\n",
    "    btc['Date'][i]  =  datetime.strptime(btc['Date'][i], '%Y/%m/%d')\n",
    "\n",
    "btc = btc.set_index(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0bc752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "btcData = btc[regs.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d4afe6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "btcData['returns'] = btcData['priceUSD'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9320c632",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = btcData.drop(columns=['priceUSD'])\n",
    "Data = Data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4e2c565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activeaddresses30std</th>\n",
       "      <th>activeaddresses30var</th>\n",
       "      <th>activeaddresses7trx</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>difficulty30mom</th>\n",
       "      <th>difficulty30trx</th>\n",
       "      <th>difficulty90mom</th>\n",
       "      <th>difficulty90roc</th>\n",
       "      <th>difficulty90rsi</th>\n",
       "      <th>difficulty90trx</th>\n",
       "      <th>...</th>\n",
       "      <th>sentinusd30emaUSD</th>\n",
       "      <th>sentinusd30smaUSD</th>\n",
       "      <th>sentinusd30wmaUSD</th>\n",
       "      <th>sentinusd90emaUSD</th>\n",
       "      <th>sentinusd90varUSD</th>\n",
       "      <th>sentinusd90wmaUSD</th>\n",
       "      <th>sentinusdUSD</th>\n",
       "      <th>top100cap</th>\n",
       "      <th>transactionvalueUSD</th>\n",
       "      <th>returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-02</th>\n",
       "      <td>7666</td>\n",
       "      <td>14691594</td>\n",
       "      <td>0.296</td>\n",
       "      <td>51718</td>\n",
       "      <td>2875.000</td>\n",
       "      <td>0.146</td>\n",
       "      <td>3694.0</td>\n",
       "      <td>7.693</td>\n",
       "      <td>55.143</td>\n",
       "      <td>0.073</td>\n",
       "      <td>...</td>\n",
       "      <td>13119990</td>\n",
       "      <td>14447549</td>\n",
       "      <td>13084361</td>\n",
       "      <td>15062343</td>\n",
       "      <td>9.207781e+13</td>\n",
       "      <td>15267519</td>\n",
       "      <td>6145265</td>\n",
       "      <td>49.907</td>\n",
       "      <td>1299.0</td>\n",
       "      <td>-0.004836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-03</th>\n",
       "      <td>7553</td>\n",
       "      <td>14263642</td>\n",
       "      <td>0.531</td>\n",
       "      <td>50653</td>\n",
       "      <td>1810.000</td>\n",
       "      <td>0.152</td>\n",
       "      <td>2764.0</td>\n",
       "      <td>5.771</td>\n",
       "      <td>53.920</td>\n",
       "      <td>0.074</td>\n",
       "      <td>...</td>\n",
       "      <td>12661408</td>\n",
       "      <td>14247155</td>\n",
       "      <td>12540130</td>\n",
       "      <td>14863434</td>\n",
       "      <td>9.237091e+13</td>\n",
       "      <td>15058214</td>\n",
       "      <td>6011974</td>\n",
       "      <td>49.914</td>\n",
       "      <td>1650.0</td>\n",
       "      <td>-0.008005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-04</th>\n",
       "      <td>6990</td>\n",
       "      <td>12215979</td>\n",
       "      <td>0.492</td>\n",
       "      <td>50158</td>\n",
       "      <td>1315.000</td>\n",
       "      <td>0.156</td>\n",
       "      <td>2268.0</td>\n",
       "      <td>4.737</td>\n",
       "      <td>53.364</td>\n",
       "      <td>0.074</td>\n",
       "      <td>...</td>\n",
       "      <td>12160118</td>\n",
       "      <td>13934412</td>\n",
       "      <td>11936534</td>\n",
       "      <td>14644269</td>\n",
       "      <td>9.336173e+13</td>\n",
       "      <td>14824650</td>\n",
       "      <td>4891418</td>\n",
       "      <td>49.963</td>\n",
       "      <td>1112.0</td>\n",
       "      <td>0.004323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-05</th>\n",
       "      <td>7207</td>\n",
       "      <td>12983796</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>50158</td>\n",
       "      <td>436.688</td>\n",
       "      <td>0.159</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>3.574</td>\n",
       "      <td>53.364</td>\n",
       "      <td>0.075</td>\n",
       "      <td>...</td>\n",
       "      <td>11733962</td>\n",
       "      <td>13588124</td>\n",
       "      <td>11395907</td>\n",
       "      <td>14444498</td>\n",
       "      <td>9.368327e+13</td>\n",
       "      <td>14607077</td>\n",
       "      <td>5554693</td>\n",
       "      <td>49.976</td>\n",
       "      <td>1505.0</td>\n",
       "      <td>-0.004878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-06</th>\n",
       "      <td>7368</td>\n",
       "      <td>13572232</td>\n",
       "      <td>-0.950</td>\n",
       "      <td>51299</td>\n",
       "      <td>1184.000</td>\n",
       "      <td>0.161</td>\n",
       "      <td>1473.0</td>\n",
       "      <td>2.956</td>\n",
       "      <td>54.471</td>\n",
       "      <td>0.075</td>\n",
       "      <td>...</td>\n",
       "      <td>11652530</td>\n",
       "      <td>13295257</td>\n",
       "      <td>11194853</td>\n",
       "      <td>14357186</td>\n",
       "      <td>9.312956e+13</td>\n",
       "      <td>14497960</td>\n",
       "      <td>10471777</td>\n",
       "      <td>49.916</td>\n",
       "      <td>3469.0</td>\n",
       "      <td>-0.004902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-24</th>\n",
       "      <td>55298</td>\n",
       "      <td>764459612</td>\n",
       "      <td>-0.329</td>\n",
       "      <td>19352505</td>\n",
       "      <td>2570167.000</td>\n",
       "      <td>0.274</td>\n",
       "      <td>3935133.0</td>\n",
       "      <td>25.524</td>\n",
       "      <td>61.809</td>\n",
       "      <td>0.095</td>\n",
       "      <td>...</td>\n",
       "      <td>1038289555</td>\n",
       "      <td>1236990379</td>\n",
       "      <td>1125469305</td>\n",
       "      <td>906646434</td>\n",
       "      <td>7.982758e+17</td>\n",
       "      <td>1088633253</td>\n",
       "      <td>361588723</td>\n",
       "      <td>44.670</td>\n",
       "      <td>30828.0</td>\n",
       "      <td>-0.001286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-25</th>\n",
       "      <td>55854</td>\n",
       "      <td>779921846</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>19996996</td>\n",
       "      <td>3214659.000</td>\n",
       "      <td>0.279</td>\n",
       "      <td>4316584.0</td>\n",
       "      <td>27.529</td>\n",
       "      <td>63.623</td>\n",
       "      <td>0.097</td>\n",
       "      <td>...</td>\n",
       "      <td>990339490</td>\n",
       "      <td>1228280282</td>\n",
       "      <td>1064699832</td>\n",
       "      <td>893205052</td>\n",
       "      <td>7.936632e+17</td>\n",
       "      <td>1076450910</td>\n",
       "      <td>295063546</td>\n",
       "      <td>44.667</td>\n",
       "      <td>37548.0</td>\n",
       "      <td>-0.023929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-26</th>\n",
       "      <td>55376</td>\n",
       "      <td>766634614</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>19996996</td>\n",
       "      <td>3214659.000</td>\n",
       "      <td>0.284</td>\n",
       "      <td>4316584.0</td>\n",
       "      <td>27.529</td>\n",
       "      <td>63.623</td>\n",
       "      <td>0.099</td>\n",
       "      <td>...</td>\n",
       "      <td>939695376</td>\n",
       "      <td>1214390963</td>\n",
       "      <td>998704699</td>\n",
       "      <td>878087484</td>\n",
       "      <td>7.902356e+17</td>\n",
       "      <td>1062224912</td>\n",
       "      <td>205355717</td>\n",
       "      <td>44.746</td>\n",
       "      <td>35847.0</td>\n",
       "      <td>0.012488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-27</th>\n",
       "      <td>55508</td>\n",
       "      <td>770273876</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>19996996</td>\n",
       "      <td>2274388.000</td>\n",
       "      <td>0.289</td>\n",
       "      <td>4316584.0</td>\n",
       "      <td>27.529</td>\n",
       "      <td>63.623</td>\n",
       "      <td>0.101</td>\n",
       "      <td>...</td>\n",
       "      <td>890366945</td>\n",
       "      <td>1190592377</td>\n",
       "      <td>931653972</td>\n",
       "      <td>862637313</td>\n",
       "      <td>7.872458e+17</td>\n",
       "      <td>1047283908</td>\n",
       "      <td>175104696</td>\n",
       "      <td>44.739</td>\n",
       "      <td>23536.0</td>\n",
       "      <td>0.001146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28</th>\n",
       "      <td>54692</td>\n",
       "      <td>747797946</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>19429695</td>\n",
       "      <td>1678156.000</td>\n",
       "      <td>0.294</td>\n",
       "      <td>4598305.0</td>\n",
       "      <td>31.004</td>\n",
       "      <td>60.986</td>\n",
       "      <td>0.103</td>\n",
       "      <td>...</td>\n",
       "      <td>845359022</td>\n",
       "      <td>1183566854</td>\n",
       "      <td>867276666</td>\n",
       "      <td>847914386</td>\n",
       "      <td>7.839838e+17</td>\n",
       "      <td>1032687824</td>\n",
       "      <td>192744138</td>\n",
       "      <td>44.742</td>\n",
       "      <td>24643.0</td>\n",
       "      <td>-0.054550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2523 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            activeaddresses30std  activeaddresses30var  activeaddresses7trx  \\\n",
       "Date                                                                          \n",
       "2016-01-02                  7666              14691594                0.296   \n",
       "2016-01-03                  7553              14263642                0.531   \n",
       "2016-01-04                  6990              12215979                0.492   \n",
       "2016-01-05                  7207              12983796               -0.176   \n",
       "2016-01-06                  7368              13572232               -0.950   \n",
       "...                          ...                   ...                  ...   \n",
       "2022-11-24                 55298             764459612               -0.329   \n",
       "2022-11-25                 55854             779921846               -0.097   \n",
       "2022-11-26                 55376             766634614               -0.101   \n",
       "2022-11-27                 55508             770273876               -0.228   \n",
       "2022-11-28                 54692             747797946               -0.210   \n",
       "\n",
       "            difficulty  difficulty30mom  difficulty30trx  difficulty90mom  \\\n",
       "Date                                                                        \n",
       "2016-01-02       51718         2875.000            0.146           3694.0   \n",
       "2016-01-03       50653         1810.000            0.152           2764.0   \n",
       "2016-01-04       50158         1315.000            0.156           2268.0   \n",
       "2016-01-05       50158          436.688            0.159           1731.0   \n",
       "2016-01-06       51299         1184.000            0.161           1473.0   \n",
       "...                ...              ...              ...              ...   \n",
       "2022-11-24    19352505      2570167.000            0.274        3935133.0   \n",
       "2022-11-25    19996996      3214659.000            0.279        4316584.0   \n",
       "2022-11-26    19996996      3214659.000            0.284        4316584.0   \n",
       "2022-11-27    19996996      2274388.000            0.289        4316584.0   \n",
       "2022-11-28    19429695      1678156.000            0.294        4598305.0   \n",
       "\n",
       "            difficulty90roc  difficulty90rsi  difficulty90trx  ...  \\\n",
       "Date                                                           ...   \n",
       "2016-01-02            7.693           55.143            0.073  ...   \n",
       "2016-01-03            5.771           53.920            0.074  ...   \n",
       "2016-01-04            4.737           53.364            0.074  ...   \n",
       "2016-01-05            3.574           53.364            0.075  ...   \n",
       "2016-01-06            2.956           54.471            0.075  ...   \n",
       "...                     ...              ...              ...  ...   \n",
       "2022-11-24           25.524           61.809            0.095  ...   \n",
       "2022-11-25           27.529           63.623            0.097  ...   \n",
       "2022-11-26           27.529           63.623            0.099  ...   \n",
       "2022-11-27           27.529           63.623            0.101  ...   \n",
       "2022-11-28           31.004           60.986            0.103  ...   \n",
       "\n",
       "            sentinusd30emaUSD  sentinusd30smaUSD  sentinusd30wmaUSD  \\\n",
       "Date                                                                  \n",
       "2016-01-02           13119990           14447549           13084361   \n",
       "2016-01-03           12661408           14247155           12540130   \n",
       "2016-01-04           12160118           13934412           11936534   \n",
       "2016-01-05           11733962           13588124           11395907   \n",
       "2016-01-06           11652530           13295257           11194853   \n",
       "...                       ...                ...                ...   \n",
       "2022-11-24         1038289555         1236990379         1125469305   \n",
       "2022-11-25          990339490         1228280282         1064699832   \n",
       "2022-11-26          939695376         1214390963          998704699   \n",
       "2022-11-27          890366945         1190592377          931653972   \n",
       "2022-11-28          845359022         1183566854          867276666   \n",
       "\n",
       "            sentinusd90emaUSD  sentinusd90varUSD  sentinusd90wmaUSD  \\\n",
       "Date                                                                  \n",
       "2016-01-02           15062343       9.207781e+13           15267519   \n",
       "2016-01-03           14863434       9.237091e+13           15058214   \n",
       "2016-01-04           14644269       9.336173e+13           14824650   \n",
       "2016-01-05           14444498       9.368327e+13           14607077   \n",
       "2016-01-06           14357186       9.312956e+13           14497960   \n",
       "...                       ...                ...                ...   \n",
       "2022-11-24          906646434       7.982758e+17         1088633253   \n",
       "2022-11-25          893205052       7.936632e+17         1076450910   \n",
       "2022-11-26          878087484       7.902356e+17         1062224912   \n",
       "2022-11-27          862637313       7.872458e+17         1047283908   \n",
       "2022-11-28          847914386       7.839838e+17         1032687824   \n",
       "\n",
       "            sentinusdUSD  top100cap  transactionvalueUSD   returns  \n",
       "Date                                                                \n",
       "2016-01-02       6145265     49.907               1299.0 -0.004836  \n",
       "2016-01-03       6011974     49.914               1650.0 -0.008005  \n",
       "2016-01-04       4891418     49.963               1112.0  0.004323  \n",
       "2016-01-05       5554693     49.976               1505.0 -0.004878  \n",
       "2016-01-06      10471777     49.916               3469.0 -0.004902  \n",
       "...                  ...        ...                  ...       ...  \n",
       "2022-11-24     361588723     44.670              30828.0 -0.001286  \n",
       "2022-11-25     295063546     44.667              37548.0 -0.023929  \n",
       "2022-11-26     205355717     44.746              35847.0  0.012488  \n",
       "2022-11-27     175104696     44.739              23536.0  0.001146  \n",
       "2022-11-28     192744138     44.742              24643.0 -0.054550  \n",
       "\n",
       "[2523 rows x 31 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c865f7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide X and Y\n",
    "X = Data.iloc[:,0:]\n",
    "#Y = Data['returns']   # 用returns的话就用这一行，然后把下一行comment掉\n",
    "Y = btcData['priceUSD'].shift(-30)[1:] # 反之亦然"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "003b426b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activeaddresses30std</th>\n",
       "      <th>activeaddresses30var</th>\n",
       "      <th>activeaddresses7trx</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>difficulty30mom</th>\n",
       "      <th>difficulty30trx</th>\n",
       "      <th>difficulty90mom</th>\n",
       "      <th>difficulty90roc</th>\n",
       "      <th>difficulty90rsi</th>\n",
       "      <th>difficulty90trx</th>\n",
       "      <th>...</th>\n",
       "      <th>sentinusd30emaUSD</th>\n",
       "      <th>sentinusd30smaUSD</th>\n",
       "      <th>sentinusd30wmaUSD</th>\n",
       "      <th>sentinusd90emaUSD</th>\n",
       "      <th>sentinusd90varUSD</th>\n",
       "      <th>sentinusd90wmaUSD</th>\n",
       "      <th>sentinusdUSD</th>\n",
       "      <th>top100cap</th>\n",
       "      <th>transactionvalueUSD</th>\n",
       "      <th>returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-02</th>\n",
       "      <td>7666</td>\n",
       "      <td>14691594</td>\n",
       "      <td>0.296</td>\n",
       "      <td>51718</td>\n",
       "      <td>2875.0</td>\n",
       "      <td>0.146</td>\n",
       "      <td>3694.0</td>\n",
       "      <td>7.693</td>\n",
       "      <td>55.143</td>\n",
       "      <td>0.073</td>\n",
       "      <td>...</td>\n",
       "      <td>13119990</td>\n",
       "      <td>14447549</td>\n",
       "      <td>13084361</td>\n",
       "      <td>15062343</td>\n",
       "      <td>9.207781e+13</td>\n",
       "      <td>15267519</td>\n",
       "      <td>6145265</td>\n",
       "      <td>49.907</td>\n",
       "      <td>1299.0</td>\n",
       "      <td>-0.004836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-03</th>\n",
       "      <td>7553</td>\n",
       "      <td>14263642</td>\n",
       "      <td>0.531</td>\n",
       "      <td>50653</td>\n",
       "      <td>1810.0</td>\n",
       "      <td>0.152</td>\n",
       "      <td>2764.0</td>\n",
       "      <td>5.771</td>\n",
       "      <td>53.920</td>\n",
       "      <td>0.074</td>\n",
       "      <td>...</td>\n",
       "      <td>12661408</td>\n",
       "      <td>14247155</td>\n",
       "      <td>12540130</td>\n",
       "      <td>14863434</td>\n",
       "      <td>9.237091e+13</td>\n",
       "      <td>15058214</td>\n",
       "      <td>6011974</td>\n",
       "      <td>49.914</td>\n",
       "      <td>1650.0</td>\n",
       "      <td>-0.008005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-04</th>\n",
       "      <td>6990</td>\n",
       "      <td>12215979</td>\n",
       "      <td>0.492</td>\n",
       "      <td>50158</td>\n",
       "      <td>1315.0</td>\n",
       "      <td>0.156</td>\n",
       "      <td>2268.0</td>\n",
       "      <td>4.737</td>\n",
       "      <td>53.364</td>\n",
       "      <td>0.074</td>\n",
       "      <td>...</td>\n",
       "      <td>12160118</td>\n",
       "      <td>13934412</td>\n",
       "      <td>11936534</td>\n",
       "      <td>14644269</td>\n",
       "      <td>9.336173e+13</td>\n",
       "      <td>14824650</td>\n",
       "      <td>4891418</td>\n",
       "      <td>49.963</td>\n",
       "      <td>1112.0</td>\n",
       "      <td>0.004323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            activeaddresses30std  activeaddresses30var  activeaddresses7trx  \\\n",
       "Date                                                                          \n",
       "2016-01-02                  7666              14691594                0.296   \n",
       "2016-01-03                  7553              14263642                0.531   \n",
       "2016-01-04                  6990              12215979                0.492   \n",
       "\n",
       "            difficulty  difficulty30mom  difficulty30trx  difficulty90mom  \\\n",
       "Date                                                                        \n",
       "2016-01-02       51718           2875.0            0.146           3694.0   \n",
       "2016-01-03       50653           1810.0            0.152           2764.0   \n",
       "2016-01-04       50158           1315.0            0.156           2268.0   \n",
       "\n",
       "            difficulty90roc  difficulty90rsi  difficulty90trx  ...  \\\n",
       "Date                                                           ...   \n",
       "2016-01-02            7.693           55.143            0.073  ...   \n",
       "2016-01-03            5.771           53.920            0.074  ...   \n",
       "2016-01-04            4.737           53.364            0.074  ...   \n",
       "\n",
       "            sentinusd30emaUSD  sentinusd30smaUSD  sentinusd30wmaUSD  \\\n",
       "Date                                                                  \n",
       "2016-01-02           13119990           14447549           13084361   \n",
       "2016-01-03           12661408           14247155           12540130   \n",
       "2016-01-04           12160118           13934412           11936534   \n",
       "\n",
       "            sentinusd90emaUSD  sentinusd90varUSD  sentinusd90wmaUSD  \\\n",
       "Date                                                                  \n",
       "2016-01-02           15062343       9.207781e+13           15267519   \n",
       "2016-01-03           14863434       9.237091e+13           15058214   \n",
       "2016-01-04           14644269       9.336173e+13           14824650   \n",
       "\n",
       "            sentinusdUSD  top100cap  transactionvalueUSD   returns  \n",
       "Date                                                                \n",
       "2016-01-02       6145265     49.907               1299.0 -0.004836  \n",
       "2016-01-03       6011974     49.914               1650.0 -0.008005  \n",
       "2016-01-04       4891418     49.963               1112.0  0.004323  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11bf954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into three data sets\n",
    "X_train = X['2016-01-01':'2019-12-31']\n",
    "X_val = X['2020-01-01':'2021-05-31']\n",
    "X_test = X['2021-06-01':'2023-01-01']\n",
    "\n",
    "Y_train = Y['2016-01-01':'2019-12-31']\n",
    "Y_val = Y['2020-01-01':'2021-05-31']\n",
    "Y_test = Y['2021-06-01':'2023-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "719c7f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............ (step 1 of 2) Processing mixmax, total=   0.0s\n",
      "[Pipeline] ............ (step 2 of 2) Processing robust, total=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('mixmax', MinMaxScaler()), ['robust', RobustScaler()]],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators=[]\n",
    "estimators.append(['mixmax',MinMaxScaler()])\n",
    "estimators.append(['robust',RobustScaler()])\n",
    "scale=Pipeline(estimators,verbose=True)\n",
    "scale.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74f45b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=scale.transform(X_train)\n",
    "X_test=scale.transform(X_test)\n",
    "X_val = scale.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef064ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9f910c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "# define neural network model\n",
    "shape=X.shape[1]\n",
    "def sequential_model(initializer='normal', activation='relu', neurons=300, NUM_FEATURES=shape):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_shape=(NUM_FEATURES,), kernel_initializer=initializer, activation=activation))\n",
    "    model.add(Dense(neurons, activation=activation))\n",
    "    model.add(Dense(neurons, activation=activation))\n",
    "    model.add(Dense(1, activation=activation))\n",
    "    # Compile model\n",
    "    adam=keras.optimizers.adam(lr=lr_schedule(0), amsgrad=True)\n",
    "    #sgd=keras.optimizers.SGD(learning_rate=0.08, momentum=0.9, nesterov=False)\n",
    "    model.compile(loss='logcosh', optimizer=adam, metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d282ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mcp_save = ModelCheckpoint('trained_models/ANN_reg_seven_new.hdf5', save_best_only=True, monitor='val_loss', mode='auto')\n",
    "#earlyStopping = EarlyStopping(monitor='val_loss', patience=100,verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd62b225",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor=KerasRegressor(build_fn=sequential_model,epochs=1000,verbose=1, shuffle=True,use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b59af925",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 19:16:54.410464: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-30 19:16:54.413014: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Train on 1460 samples, validate on 517 samples\n",
      "Epoch 1/1000\n",
      "1460/1460 [==============================] - 1s 855us/step - loss: 33.7555 - mae: 34.4062 - val_loss: 51.7082 - val_mae: 52.3987\n",
      "Epoch 2/1000\n",
      "1460/1460 [==============================] - 0s 287us/step - loss: 16.5764 - mae: 17.1856 - val_loss: 37.0793 - val_mae: 37.7594\n",
      "Epoch 3/1000\n",
      "1460/1460 [==============================] - 0s 283us/step - loss: 12.7259 - mae: 13.3295 - val_loss: 39.1565 - val_mae: 39.8393\n",
      "Epoch 4/1000\n",
      "1460/1460 [==============================] - 0s 276us/step - loss: 11.3543 - mae: 11.9527 - val_loss: 39.3681 - val_mae: 40.0523\n",
      "Epoch 5/1000\n",
      "1460/1460 [==============================] - 0s 273us/step - loss: 10.5871 - mae: 11.1776 - val_loss: 42.6258 - val_mae: 43.3083\n",
      "Epoch 6/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 9.2377 - mae: 9.8268 - val_loss: 41.7024 - val_mae: 42.3844\n",
      "Epoch 7/1000\n",
      "1460/1460 [==============================] - 0s 275us/step - loss: 9.3950 - mae: 9.9803 - val_loss: 44.7852 - val_mae: 45.4691\n",
      "Epoch 8/1000\n",
      "1460/1460 [==============================] - 0s 276us/step - loss: 7.9620 - mae: 8.5308 - val_loss: 42.5114 - val_mae: 43.1964\n",
      "Epoch 9/1000\n",
      "1460/1460 [==============================] - 0s 277us/step - loss: 7.3930 - mae: 7.9713 - val_loss: 47.9147 - val_mae: 48.5970\n",
      "Epoch 10/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 7.0145 - mae: 7.5960 - val_loss: 42.4310 - val_mae: 43.1159\n",
      "Epoch 11/1000\n",
      "1460/1460 [==============================] - 0s 270us/step - loss: 7.0256 - mae: 7.6038 - val_loss: 44.5940 - val_mae: 45.2807\n",
      "Epoch 12/1000\n",
      "1460/1460 [==============================] - 0s 290us/step - loss: 5.9730 - mae: 6.5473 - val_loss: 43.9678 - val_mae: 44.6543\n",
      "Epoch 13/1000\n",
      "1460/1460 [==============================] - 0s 282us/step - loss: 5.4943 - mae: 6.0728 - val_loss: 46.9629 - val_mae: 47.6465\n",
      "Epoch 14/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 5.1564 - mae: 5.7174 - val_loss: 46.7836 - val_mae: 47.4672\n",
      "Epoch 15/1000\n",
      "1460/1460 [==============================] - 0s 275us/step - loss: 5.1887 - mae: 5.7395 - val_loss: 49.7016 - val_mae: 50.3841\n",
      "Epoch 16/1000\n",
      "1460/1460 [==============================] - 0s 278us/step - loss: 5.0572 - mae: 5.6136 - val_loss: 46.7361 - val_mae: 47.4239\n",
      "Epoch 17/1000\n",
      "1460/1460 [==============================] - 0s 289us/step - loss: 4.9081 - mae: 5.4575 - val_loss: 44.1292 - val_mae: 44.8114\n",
      "Epoch 18/1000\n",
      "1460/1460 [==============================] - 0s 279us/step - loss: 5.0325 - mae: 5.5960 - val_loss: 46.4356 - val_mae: 47.1194\n",
      "Epoch 19/1000\n",
      "1460/1460 [==============================] - 0s 278us/step - loss: 4.3805 - mae: 4.9419 - val_loss: 46.4152 - val_mae: 47.1036\n",
      "Epoch 20/1000\n",
      "1460/1460 [==============================] - 0s 279us/step - loss: 4.8109 - mae: 5.3720 - val_loss: 47.5332 - val_mae: 48.2158\n",
      "Epoch 21/1000\n",
      "1460/1460 [==============================] - 0s 285us/step - loss: 4.3126 - mae: 4.8713 - val_loss: 47.6920 - val_mae: 48.3800\n",
      "Epoch 22/1000\n",
      "1460/1460 [==============================] - 0s 280us/step - loss: 4.0995 - mae: 4.6431 - val_loss: 46.4716 - val_mae: 47.1549\n",
      "Epoch 23/1000\n",
      "1460/1460 [==============================] - 0s 276us/step - loss: 3.8412 - mae: 4.3844 - val_loss: 46.0314 - val_mae: 46.7084\n",
      "Epoch 24/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 3.9644 - mae: 4.4977 - val_loss: 47.9728 - val_mae: 48.6548\n",
      "Epoch 25/1000\n",
      "1460/1460 [==============================] - 0s 292us/step - loss: 4.5179 - mae: 5.0606 - val_loss: 45.5648 - val_mae: 46.2428\n",
      "Epoch 26/1000\n",
      "1460/1460 [==============================] - 0s 276us/step - loss: 3.7066 - mae: 4.2557 - val_loss: 47.8657 - val_mae: 48.5468\n",
      "Epoch 27/1000\n",
      "1460/1460 [==============================] - 0s 322us/step - loss: 3.9154 - mae: 4.4436 - val_loss: 47.4297 - val_mae: 48.1103\n",
      "Epoch 28/1000\n",
      "1460/1460 [==============================] - 0s 275us/step - loss: 4.0961 - mae: 4.6327 - val_loss: 45.6026 - val_mae: 46.2862\n",
      "Epoch 29/1000\n",
      "1460/1460 [==============================] - 0s 294us/step - loss: 3.7164 - mae: 4.2340 - val_loss: 46.2850 - val_mae: 46.9704\n",
      "Epoch 30/1000\n",
      "1460/1460 [==============================] - 0s 294us/step - loss: 3.8611 - mae: 4.3888 - val_loss: 48.5865 - val_mae: 49.2632\n",
      "Epoch 31/1000\n",
      "1460/1460 [==============================] - 0s 303us/step - loss: 3.4768 - mae: 3.9973 - val_loss: 48.7920 - val_mae: 49.4749\n",
      "Epoch 32/1000\n",
      "1460/1460 [==============================] - 0s 279us/step - loss: 3.2877 - mae: 3.7988 - val_loss: 47.7357 - val_mae: 48.4202\n",
      "Epoch 33/1000\n",
      "1460/1460 [==============================] - 0s 279us/step - loss: 3.2975 - mae: 3.8360 - val_loss: 46.8148 - val_mae: 47.4905\n",
      "Epoch 34/1000\n",
      "1460/1460 [==============================] - 0s 273us/step - loss: 3.1933 - mae: 3.7068 - val_loss: 49.0233 - val_mae: 49.7033\n",
      "Epoch 35/1000\n",
      "1460/1460 [==============================] - 0s 292us/step - loss: 2.9273 - mae: 3.4387 - val_loss: 46.8455 - val_mae: 47.5216\n",
      "Epoch 36/1000\n",
      "1460/1460 [==============================] - 0s 280us/step - loss: 3.4945 - mae: 4.0141 - val_loss: 48.8221 - val_mae: 49.5032\n",
      "Epoch 37/1000\n",
      "1460/1460 [==============================] - 0s 292us/step - loss: 3.3258 - mae: 3.8470 - val_loss: 46.8811 - val_mae: 47.5630\n",
      "Epoch 38/1000\n",
      "1460/1460 [==============================] - 0s 285us/step - loss: 4.0543 - mae: 4.5707 - val_loss: 48.3883 - val_mae: 49.0624\n",
      "Epoch 39/1000\n",
      "1460/1460 [==============================] - 0s 306us/step - loss: 3.1483 - mae: 3.6500 - val_loss: 47.6637 - val_mae: 48.3420\n",
      "Epoch 40/1000\n",
      "1460/1460 [==============================] - 0s 339us/step - loss: 2.8563 - mae: 3.3687 - val_loss: 47.6567 - val_mae: 48.3379\n",
      "Epoch 41/1000\n",
      "1460/1460 [==============================] - 0s 296us/step - loss: 2.8848 - mae: 3.3885 - val_loss: 47.9834 - val_mae: 48.6603\n",
      "Epoch 42/1000\n",
      "1460/1460 [==============================] - 0s 301us/step - loss: 3.5282 - mae: 4.0604 - val_loss: 50.2352 - val_mae: 50.9139\n",
      "Epoch 43/1000\n",
      "1460/1460 [==============================] - 0s 294us/step - loss: 3.0687 - mae: 3.5704 - val_loss: 49.8199 - val_mae: 50.4923\n",
      "Epoch 44/1000\n",
      "1460/1460 [==============================] - 0s 286us/step - loss: 2.7823 - mae: 3.2883 - val_loss: 48.1153 - val_mae: 48.7912\n",
      "Epoch 45/1000\n",
      "1460/1460 [==============================] - 0s 324us/step - loss: 2.9437 - mae: 3.4628 - val_loss: 48.7795 - val_mae: 49.4634\n",
      "Epoch 46/1000\n",
      "1460/1460 [==============================] - 0s 301us/step - loss: 3.3103 - mae: 3.8277 - val_loss: 50.1856 - val_mae: 50.8635\n",
      "Epoch 47/1000\n",
      "1460/1460 [==============================] - 1s 364us/step - loss: 3.0519 - mae: 3.5482 - val_loss: 46.7056 - val_mae: 47.3893\n",
      "Epoch 48/1000\n",
      "1460/1460 [==============================] - 1s 379us/step - loss: 2.6401 - mae: 3.1378 - val_loss: 46.8169 - val_mae: 47.4892\n",
      "Epoch 49/1000\n",
      "1460/1460 [==============================] - 1s 396us/step - loss: 2.8726 - mae: 3.3691 - val_loss: 45.9312 - val_mae: 46.6134\n",
      "Epoch 50/1000\n",
      "1460/1460 [==============================] - 0s 342us/step - loss: 2.8963 - mae: 3.3906 - val_loss: 48.3067 - val_mae: 48.9874\n",
      "Epoch 51/1000\n",
      "1460/1460 [==============================] - 1s 366us/step - loss: 2.8798 - mae: 3.3705 - val_loss: 48.4511 - val_mae: 49.1352\n",
      "Epoch 52/1000\n",
      "1460/1460 [==============================] - 1s 373us/step - loss: 2.9089 - mae: 3.4200 - val_loss: 46.2188 - val_mae: 46.9029\n",
      "Epoch 53/1000\n",
      "1460/1460 [==============================] - 1s 352us/step - loss: 3.4730 - mae: 3.9856 - val_loss: 48.0630 - val_mae: 48.7396\n",
      "Epoch 54/1000\n",
      "1460/1460 [==============================] - 1s 345us/step - loss: 3.2094 - mae: 3.7074 - val_loss: 47.7749 - val_mae: 48.4531\n",
      "Epoch 55/1000\n",
      "1460/1460 [==============================] - 0s 329us/step - loss: 3.2697 - mae: 3.7743 - val_loss: 48.1016 - val_mae: 48.7783\n",
      "Epoch 56/1000\n",
      "1460/1460 [==============================] - 0s 318us/step - loss: 3.2158 - mae: 3.7176 - val_loss: 48.1291 - val_mae: 48.8076\n",
      "Epoch 57/1000\n",
      "1460/1460 [==============================] - 0s 316us/step - loss: 2.8153 - mae: 3.3180 - val_loss: 47.5510 - val_mae: 48.2287\n",
      "Epoch 58/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460/1460 [==============================] - 0s 332us/step - loss: 2.5391 - mae: 3.0166 - val_loss: 47.4454 - val_mae: 48.1206\n",
      "Epoch 59/1000\n",
      "1460/1460 [==============================] - 0s 332us/step - loss: 2.7200 - mae: 3.2016 - val_loss: 48.5609 - val_mae: 49.2396\n",
      "Epoch 60/1000\n",
      "1460/1460 [==============================] - 0s 304us/step - loss: 2.7893 - mae: 3.2813 - val_loss: 46.9856 - val_mae: 47.6620\n",
      "Epoch 61/1000\n",
      "1460/1460 [==============================] - 0s 336us/step - loss: 2.5601 - mae: 3.0532 - val_loss: 45.7705 - val_mae: 46.4466\n",
      "Epoch 62/1000\n",
      "1460/1460 [==============================] - 1s 343us/step - loss: 2.9417 - mae: 3.4448 - val_loss: 47.4528 - val_mae: 48.1280\n",
      "Epoch 63/1000\n",
      "1460/1460 [==============================] - 0s 317us/step - loss: 2.6530 - mae: 3.1433 - val_loss: 45.2880 - val_mae: 45.9669\n",
      "Epoch 64/1000\n",
      "1460/1460 [==============================] - 0s 299us/step - loss: 2.9355 - mae: 3.4257 - val_loss: 45.0836 - val_mae: 45.7627\n",
      "Epoch 65/1000\n",
      "1460/1460 [==============================] - 1s 356us/step - loss: 2.8338 - mae: 3.3283 - val_loss: 48.4307 - val_mae: 49.1064\n",
      "Epoch 66/1000\n",
      "1460/1460 [==============================] - 0s 302us/step - loss: 2.6575 - mae: 3.1494 - val_loss: 47.1169 - val_mae: 47.7934\n",
      "Epoch 67/1000\n",
      "1460/1460 [==============================] - 0s 307us/step - loss: 2.5483 - mae: 3.0424 - val_loss: 45.9524 - val_mae: 46.6320\n",
      "Epoch 68/1000\n",
      "1460/1460 [==============================] - 0s 298us/step - loss: 2.5207 - mae: 3.0000 - val_loss: 46.5068 - val_mae: 47.1812\n",
      "Epoch 69/1000\n",
      "1460/1460 [==============================] - 0s 305us/step - loss: 2.4613 - mae: 2.9521 - val_loss: 46.7615 - val_mae: 47.4352\n",
      "Epoch 70/1000\n",
      "1460/1460 [==============================] - 0s 307us/step - loss: 2.3269 - mae: 2.8110 - val_loss: 46.6317 - val_mae: 47.3073\n",
      "Epoch 71/1000\n",
      "1460/1460 [==============================] - 0s 297us/step - loss: 2.6068 - mae: 3.0889 - val_loss: 46.9090 - val_mae: 47.5886\n",
      "Epoch 72/1000\n",
      "1460/1460 [==============================] - 0s 316us/step - loss: 2.3718 - mae: 2.8399 - val_loss: 45.4175 - val_mae: 46.0974\n",
      "Epoch 73/1000\n",
      "1460/1460 [==============================] - 0s 306us/step - loss: 2.5357 - mae: 3.0273 - val_loss: 46.9761 - val_mae: 47.6460\n",
      "Epoch 74/1000\n",
      "1460/1460 [==============================] - 0s 278us/step - loss: 2.6043 - mae: 3.0932 - val_loss: 46.7600 - val_mae: 47.4352\n",
      "Epoch 75/1000\n",
      "1460/1460 [==============================] - 0s 269us/step - loss: 3.0902 - mae: 3.5931 - val_loss: 46.0913 - val_mae: 46.7621\n",
      "Epoch 76/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 2.3441 - mae: 2.8137 - val_loss: 45.2455 - val_mae: 45.9302\n",
      "Epoch 77/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 2.5240 - mae: 3.0072 - val_loss: 46.3072 - val_mae: 46.9796\n",
      "Epoch 78/1000\n",
      "1460/1460 [==============================] - 0s 270us/step - loss: 2.8342 - mae: 3.3259 - val_loss: 46.7532 - val_mae: 47.4295\n",
      "Epoch 79/1000\n",
      "1460/1460 [==============================] - 0s 273us/step - loss: 2.2563 - mae: 2.7258 - val_loss: 46.7077 - val_mae: 47.3843\n",
      "Epoch 80/1000\n",
      "1460/1460 [==============================] - 0s 291us/step - loss: 2.1933 - mae: 2.6644 - val_loss: 46.1288 - val_mae: 46.8028\n",
      "Epoch 81/1000\n",
      "1460/1460 [==============================] - 0s 281us/step - loss: 2.2192 - mae: 2.6847 - val_loss: 47.3998 - val_mae: 48.0738\n",
      "Epoch 82/1000\n",
      "1460/1460 [==============================] - 0s 288us/step - loss: 2.2065 - mae: 2.6805 - val_loss: 46.9379 - val_mae: 47.6129\n",
      "Epoch 83/1000\n",
      "1460/1460 [==============================] - 0s 288us/step - loss: 2.2329 - mae: 2.7049 - val_loss: 45.0689 - val_mae: 45.7496\n",
      "Epoch 84/1000\n",
      "1460/1460 [==============================] - 0s 285us/step - loss: 2.2295 - mae: 2.6918 - val_loss: 47.3409 - val_mae: 48.0120\n",
      "Epoch 85/1000\n",
      "1460/1460 [==============================] - 0s 288us/step - loss: 2.2804 - mae: 2.7466 - val_loss: 46.7007 - val_mae: 47.3794\n",
      "Epoch 86/1000\n",
      "1460/1460 [==============================] - 0s 313us/step - loss: 2.3439 - mae: 2.8263 - val_loss: 46.3578 - val_mae: 47.0330\n",
      "Epoch 87/1000\n",
      "1460/1460 [==============================] - 0s 279us/step - loss: 2.3401 - mae: 2.8187 - val_loss: 45.6245 - val_mae: 46.3019\n",
      "Epoch 88/1000\n",
      "1460/1460 [==============================] - 0s 285us/step - loss: 2.5033 - mae: 2.9759 - val_loss: 48.4728 - val_mae: 49.1486\n",
      "Epoch 89/1000\n",
      "1460/1460 [==============================] - 0s 281us/step - loss: 2.2536 - mae: 2.7382 - val_loss: 46.6402 - val_mae: 47.3142\n",
      "Epoch 90/1000\n",
      "1460/1460 [==============================] - 0s 283us/step - loss: 2.3007 - mae: 2.7827 - val_loss: 47.1836 - val_mae: 47.8550\n",
      "Epoch 91/1000\n",
      "1460/1460 [==============================] - 0s 284us/step - loss: 2.4054 - mae: 2.8848 - val_loss: 48.8487 - val_mae: 49.5301\n",
      "Epoch 92/1000\n",
      "1460/1460 [==============================] - 0s 279us/step - loss: 2.3537 - mae: 2.8261 - val_loss: 45.9813 - val_mae: 46.6588\n",
      "Epoch 93/1000\n",
      "1460/1460 [==============================] - 0s 287us/step - loss: 2.3106 - mae: 2.7717 - val_loss: 46.2497 - val_mae: 46.9252\n",
      "Epoch 94/1000\n",
      "1460/1460 [==============================] - 0s 285us/step - loss: 2.3585 - mae: 2.8263 - val_loss: 45.5895 - val_mae: 46.2658\n",
      "Epoch 95/1000\n",
      "1460/1460 [==============================] - 0s 318us/step - loss: 2.3067 - mae: 2.8050 - val_loss: 46.3619 - val_mae: 47.0373\n",
      "Epoch 96/1000\n",
      "1460/1460 [==============================] - 0s 281us/step - loss: 2.3170 - mae: 2.7857 - val_loss: 46.0911 - val_mae: 46.7706\n",
      "Epoch 97/1000\n",
      "1460/1460 [==============================] - 0s 285us/step - loss: 2.5200 - mae: 3.0097 - val_loss: 46.6649 - val_mae: 47.3394\n",
      "Epoch 98/1000\n",
      "1460/1460 [==============================] - 0s 289us/step - loss: 2.1650 - mae: 2.6382 - val_loss: 47.4475 - val_mae: 48.1222\n",
      "Epoch 99/1000\n",
      "1460/1460 [==============================] - 0s 280us/step - loss: 2.9632 - mae: 3.4613 - val_loss: 46.8441 - val_mae: 47.5232\n",
      "Epoch 100/1000\n",
      "1460/1460 [==============================] - 0s 283us/step - loss: 2.5371 - mae: 3.0157 - val_loss: 46.2923 - val_mae: 46.9675\n",
      "Epoch 101/1000\n",
      "1460/1460 [==============================] - 0s 281us/step - loss: 2.0212 - mae: 2.4809 - val_loss: 46.7161 - val_mae: 47.3956\n",
      "Epoch 102/1000\n",
      "1460/1460 [==============================] - 0s 283us/step - loss: 2.1044 - mae: 2.5629 - val_loss: 46.2965 - val_mae: 46.9757\n",
      "Epoch 103/1000\n",
      "1460/1460 [==============================] - 0s 305us/step - loss: 2.2734 - mae: 2.7465 - val_loss: 48.3727 - val_mae: 49.0552\n",
      "Epoch 104/1000\n",
      "1460/1460 [==============================] - 0s 284us/step - loss: 2.0648 - mae: 2.5217 - val_loss: 48.4776 - val_mae: 49.1595\n",
      "Epoch 105/1000\n",
      "1460/1460 [==============================] - 0s 283us/step - loss: 2.6353 - mae: 3.1165 - val_loss: 46.7058 - val_mae: 47.3858\n",
      "Epoch 106/1000\n",
      "1460/1460 [==============================] - 0s 284us/step - loss: 2.4075 - mae: 2.8784 - val_loss: 48.3754 - val_mae: 49.0530\n",
      "Epoch 107/1000\n",
      "1460/1460 [==============================] - 0s 281us/step - loss: 2.0453 - mae: 2.4995 - val_loss: 47.0654 - val_mae: 47.7407\n",
      "Epoch 108/1000\n",
      "1460/1460 [==============================] - 0s 282us/step - loss: 2.2642 - mae: 2.7322 - val_loss: 45.6655 - val_mae: 46.3456\n",
      "Epoch 109/1000\n",
      "1460/1460 [==============================] - 0s 277us/step - loss: 2.1760 - mae: 2.6344 - val_loss: 45.9589 - val_mae: 46.6342\n",
      "Epoch 110/1000\n",
      "1460/1460 [==============================] - 0s 273us/step - loss: 2.3962 - mae: 2.8658 - val_loss: 47.2493 - val_mae: 47.9258\n",
      "Epoch 111/1000\n",
      "1460/1460 [==============================] - 0s 280us/step - loss: 2.2224 - mae: 2.7009 - val_loss: 46.4983 - val_mae: 47.1765\n",
      "Epoch 112/1000\n",
      "1460/1460 [==============================] - 0s 287us/step - loss: 1.8942 - mae: 2.3527 - val_loss: 47.1376 - val_mae: 47.8159\n",
      "Epoch 113/1000\n",
      "1460/1460 [==============================] - 0s 314us/step - loss: 2.1198 - mae: 2.5775 - val_loss: 46.6045 - val_mae: 47.2793\n",
      "Epoch 114/1000\n",
      "1460/1460 [==============================] - 0s 331us/step - loss: 2.3274 - mae: 2.8027 - val_loss: 45.8246 - val_mae: 46.5062\n",
      "Epoch 115/1000\n",
      "1460/1460 [==============================] - 0s 334us/step - loss: 2.2021 - mae: 2.6641 - val_loss: 47.0811 - val_mae: 47.7599\n",
      "Epoch 116/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460/1460 [==============================] - 0s 290us/step - loss: 1.8842 - mae: 2.3366 - val_loss: 46.7307 - val_mae: 47.4104\n",
      "Epoch 117/1000\n",
      "1460/1460 [==============================] - 0s 258us/step - loss: 1.8616 - mae: 2.3116 - val_loss: 46.8611 - val_mae: 47.5388\n",
      "Epoch 118/1000\n",
      "1460/1460 [==============================] - 0s 309us/step - loss: 2.2623 - mae: 2.7382 - val_loss: 48.2820 - val_mae: 48.9581\n",
      "Epoch 119/1000\n",
      "1460/1460 [==============================] - 0s 293us/step - loss: 2.2146 - mae: 2.6758 - val_loss: 45.9570 - val_mae: 46.6357\n",
      "Epoch 120/1000\n",
      "1460/1460 [==============================] - 0s 276us/step - loss: 2.1015 - mae: 2.5685 - val_loss: 47.0101 - val_mae: 47.6856\n",
      "Epoch 121/1000\n",
      "1460/1460 [==============================] - 0s 291us/step - loss: 1.7685 - mae: 2.2178 - val_loss: 48.6779 - val_mae: 49.3554\n",
      "Epoch 122/1000\n",
      "1460/1460 [==============================] - 0s 275us/step - loss: 2.0090 - mae: 2.4654 - val_loss: 47.2760 - val_mae: 47.9504\n",
      "Epoch 123/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 2.1521 - mae: 2.6124 - val_loss: 46.9340 - val_mae: 47.6120\n",
      "Epoch 124/1000\n",
      "1460/1460 [==============================] - 0s 279us/step - loss: 2.1173 - mae: 2.5811 - val_loss: 46.2411 - val_mae: 46.9240\n",
      "Epoch 125/1000\n",
      "1460/1460 [==============================] - 0s 305us/step - loss: 2.3292 - mae: 2.7965 - val_loss: 47.2533 - val_mae: 47.9266\n",
      "Epoch 126/1000\n",
      "1460/1460 [==============================] - 0s 312us/step - loss: 1.7657 - mae: 2.1999 - val_loss: 46.7317 - val_mae: 47.4104\n",
      "Epoch 127/1000\n",
      "1460/1460 [==============================] - 0s 338us/step - loss: 1.8804 - mae: 2.3362 - val_loss: 47.1907 - val_mae: 47.8632\n",
      "Epoch 128/1000\n",
      "1460/1460 [==============================] - 0s 281us/step - loss: 2.1217 - mae: 2.5800 - val_loss: 47.8411 - val_mae: 48.5203\n",
      "Epoch 129/1000\n",
      "1460/1460 [==============================] - 0s 316us/step - loss: 1.8357 - mae: 2.2930 - val_loss: 47.0685 - val_mae: 47.7480\n",
      "Epoch 130/1000\n",
      "1460/1460 [==============================] - 0s 284us/step - loss: 1.9399 - mae: 2.3929 - val_loss: 46.3166 - val_mae: 46.9982\n",
      "Epoch 131/1000\n",
      "1460/1460 [==============================] - 0s 318us/step - loss: 1.8539 - mae: 2.2964 - val_loss: 47.0057 - val_mae: 47.6822\n",
      "Epoch 132/1000\n",
      "1460/1460 [==============================] - 0s 309us/step - loss: 1.6855 - mae: 2.1212 - val_loss: 48.3943 - val_mae: 49.0758\n",
      "Epoch 133/1000\n",
      "1460/1460 [==============================] - 0s 308us/step - loss: 1.8614 - mae: 2.2988 - val_loss: 48.8258 - val_mae: 49.5045\n",
      "Epoch 134/1000\n",
      "1460/1460 [==============================] - 0s 291us/step - loss: 1.8993 - mae: 2.3400 - val_loss: 48.8608 - val_mae: 49.5431\n",
      "Epoch 135/1000\n",
      "1460/1460 [==============================] - 0s 331us/step - loss: 1.8565 - mae: 2.3087 - val_loss: 46.9158 - val_mae: 47.5939\n",
      "Epoch 136/1000\n",
      "1460/1460 [==============================] - 0s 314us/step - loss: 1.7135 - mae: 2.1543 - val_loss: 48.1377 - val_mae: 48.8123\n",
      "Epoch 137/1000\n",
      "1460/1460 [==============================] - 0s 304us/step - loss: 1.8788 - mae: 2.3197 - val_loss: 47.1442 - val_mae: 47.8284\n",
      "Epoch 138/1000\n",
      "1460/1460 [==============================] - 0s 329us/step - loss: 1.8898 - mae: 2.3478 - val_loss: 46.6672 - val_mae: 47.3486\n",
      "Epoch 139/1000\n",
      "1460/1460 [==============================] - 0s 330us/step - loss: 1.8067 - mae: 2.2504 - val_loss: 48.8296 - val_mae: 49.5095\n",
      "Epoch 140/1000\n",
      "1460/1460 [==============================] - 0s 323us/step - loss: 1.8188 - mae: 2.2556 - val_loss: 48.1780 - val_mae: 48.8590\n",
      "Epoch 141/1000\n",
      "1460/1460 [==============================] - 0s 275us/step - loss: 1.7689 - mae: 2.2051 - val_loss: 48.0508 - val_mae: 48.7294\n",
      "Epoch 142/1000\n",
      "1460/1460 [==============================] - 0s 313us/step - loss: 1.8068 - mae: 2.2484 - val_loss: 46.6158 - val_mae: 47.2914\n",
      "Epoch 143/1000\n",
      "1460/1460 [==============================] - 0s 300us/step - loss: 1.7097 - mae: 2.1455 - val_loss: 47.4231 - val_mae: 48.1022\n",
      "Epoch 144/1000\n",
      "1460/1460 [==============================] - 0s 295us/step - loss: 1.6135 - mae: 2.0564 - val_loss: 46.8812 - val_mae: 47.5613\n",
      "Epoch 145/1000\n",
      "1460/1460 [==============================] - 0s 328us/step - loss: 2.0818 - mae: 2.5497 - val_loss: 47.9060 - val_mae: 48.5820\n",
      "Epoch 146/1000\n",
      "1460/1460 [==============================] - 0s 291us/step - loss: 1.8068 - mae: 2.2605 - val_loss: 48.6958 - val_mae: 49.3743\n",
      "Epoch 147/1000\n",
      "1460/1460 [==============================] - 0s 303us/step - loss: 1.7308 - mae: 2.1686 - val_loss: 47.3359 - val_mae: 48.0127\n",
      "Epoch 148/1000\n",
      "1460/1460 [==============================] - 0s 302us/step - loss: 1.7690 - mae: 2.2179 - val_loss: 46.7739 - val_mae: 47.4500\n",
      "Epoch 149/1000\n",
      "1460/1460 [==============================] - 0s 305us/step - loss: 1.8732 - mae: 2.3171 - val_loss: 48.6875 - val_mae: 49.3677\n",
      "Epoch 150/1000\n",
      "1460/1460 [==============================] - 0s 284us/step - loss: 1.8234 - mae: 2.2731 - val_loss: 47.9417 - val_mae: 48.6188\n",
      "Epoch 151/1000\n",
      "1460/1460 [==============================] - 0s 308us/step - loss: 1.9640 - mae: 2.4348 - val_loss: 48.4285 - val_mae: 49.1014\n",
      "Epoch 152/1000\n",
      "1460/1460 [==============================] - 0s 314us/step - loss: 1.7506 - mae: 2.1940 - val_loss: 45.6414 - val_mae: 46.3197\n",
      "Epoch 153/1000\n",
      "1460/1460 [==============================] - 0s 291us/step - loss: 1.7912 - mae: 2.2320 - val_loss: 47.6691 - val_mae: 48.3396\n",
      "Epoch 154/1000\n",
      "1460/1460 [==============================] - 0s 321us/step - loss: 1.9586 - mae: 2.4193 - val_loss: 47.4109 - val_mae: 48.0878\n",
      "Epoch 155/1000\n",
      "1460/1460 [==============================] - 0s 320us/step - loss: 1.6516 - mae: 2.0862 - val_loss: 48.2945 - val_mae: 48.9734\n",
      "Epoch 156/1000\n",
      "1460/1460 [==============================] - 0s 308us/step - loss: 1.6997 - mae: 2.1356 - val_loss: 47.9516 - val_mae: 48.6326\n",
      "Epoch 157/1000\n",
      "1460/1460 [==============================] - 0s 302us/step - loss: 1.6679 - mae: 2.1071 - val_loss: 48.6029 - val_mae: 49.2821\n",
      "Epoch 158/1000\n",
      "1460/1460 [==============================] - 0s 288us/step - loss: 2.4155 - mae: 2.8836 - val_loss: 45.2930 - val_mae: 45.9753\n",
      "Epoch 159/1000\n",
      "1460/1460 [==============================] - 0s 296us/step - loss: 1.8815 - mae: 2.3172 - val_loss: 47.5061 - val_mae: 48.1850\n",
      "Epoch 160/1000\n",
      "1460/1460 [==============================] - 0s 292us/step - loss: 1.7354 - mae: 2.1858 - val_loss: 47.0393 - val_mae: 47.7190\n",
      "Epoch 161/1000\n",
      "1460/1460 [==============================] - 0s 286us/step - loss: 1.9135 - mae: 2.3561 - val_loss: 47.7554 - val_mae: 48.4358\n",
      "Epoch 162/1000\n",
      "1460/1460 [==============================] - 0s 281us/step - loss: 1.6890 - mae: 2.1278 - val_loss: 47.6400 - val_mae: 48.3189\n",
      "Epoch 163/1000\n",
      "1460/1460 [==============================] - 0s 282us/step - loss: 1.6780 - mae: 2.1154 - val_loss: 47.0147 - val_mae: 47.6962\n",
      "Epoch 164/1000\n",
      "1460/1460 [==============================] - 0s 296us/step - loss: 1.9474 - mae: 2.4037 - val_loss: 48.9403 - val_mae: 49.6154\n",
      "Epoch 165/1000\n",
      "1460/1460 [==============================] - 0s 288us/step - loss: 1.8649 - mae: 2.3071 - val_loss: 48.6908 - val_mae: 49.3653\n",
      "Epoch 166/1000\n",
      "1460/1460 [==============================] - 0s 283us/step - loss: 1.7889 - mae: 2.2362 - val_loss: 48.3099 - val_mae: 48.9813\n",
      "Epoch 167/1000\n",
      "1460/1460 [==============================] - 0s 303us/step - loss: 1.7082 - mae: 2.1385 - val_loss: 48.3175 - val_mae: 48.9932\n",
      "Epoch 168/1000\n",
      "1460/1460 [==============================] - 0s 293us/step - loss: 1.7342 - mae: 2.1756 - val_loss: 47.3543 - val_mae: 48.0329\n",
      "Epoch 169/1000\n",
      "1460/1460 [==============================] - 0s 295us/step - loss: 1.7441 - mae: 2.1823 - val_loss: 48.2716 - val_mae: 48.9477\n",
      "Epoch 170/1000\n",
      "1460/1460 [==============================] - 0s 290us/step - loss: 1.6299 - mae: 2.0565 - val_loss: 46.7700 - val_mae: 47.4490\n",
      "Epoch 171/1000\n",
      "1460/1460 [==============================] - 0s 288us/step - loss: 1.5634 - mae: 2.0035 - val_loss: 48.5110 - val_mae: 49.1877\n",
      "Epoch 172/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 1.5552 - mae: 1.9845 - val_loss: 49.0690 - val_mae: 49.7469\n",
      "Epoch 173/1000\n",
      "1460/1460 [==============================] - 0s 281us/step - loss: 1.8085 - mae: 2.2502 - val_loss: 48.4323 - val_mae: 49.1113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 174/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 1.6944 - mae: 2.1229 - val_loss: 47.8673 - val_mae: 48.5528\n",
      "Epoch 175/1000\n",
      "1460/1460 [==============================] - 0s 282us/step - loss: 1.8195 - mae: 2.2614 - val_loss: 48.2319 - val_mae: 48.9110\n",
      "Epoch 176/1000\n",
      "1460/1460 [==============================] - 0s 286us/step - loss: 2.1786 - mae: 2.6546 - val_loss: 48.3446 - val_mae: 49.0182\n",
      "Epoch 177/1000\n",
      "1460/1460 [==============================] - 0s 288us/step - loss: 1.7793 - mae: 2.2282 - val_loss: 47.7623 - val_mae: 48.4405\n",
      "Epoch 178/1000\n",
      "1460/1460 [==============================] - 0s 286us/step - loss: 1.7034 - mae: 2.1419 - val_loss: 46.8933 - val_mae: 47.5691\n",
      "Epoch 179/1000\n",
      "1460/1460 [==============================] - 0s 287us/step - loss: 1.8815 - mae: 2.3315 - val_loss: 48.1683 - val_mae: 48.8457\n",
      "Epoch 180/1000\n",
      "1460/1460 [==============================] - 0s 273us/step - loss: 1.9096 - mae: 2.3522 - val_loss: 49.5302 - val_mae: 50.2046\n",
      "Epoch 181/1000\n",
      "1460/1460 [==============================] - 0s 279us/step - loss: 1.6928 - mae: 2.1343 - val_loss: 48.0445 - val_mae: 48.7226\n",
      "Epoch 182/1000\n",
      "1460/1460 [==============================] - 0s 304us/step - loss: 1.5550 - mae: 1.9926 - val_loss: 49.0235 - val_mae: 49.7021\n",
      "Epoch 183/1000\n",
      "1460/1460 [==============================] - 0s 319us/step - loss: 1.8897 - mae: 2.3370 - val_loss: 48.0743 - val_mae: 48.7480\n",
      "Epoch 184/1000\n",
      "1460/1460 [==============================] - 3s 2ms/step - loss: 1.6543 - mae: 2.0868 - val_loss: 48.3956 - val_mae: 49.0700\n",
      "Epoch 185/1000\n",
      "1460/1460 [==============================] - 0s 321us/step - loss: 1.6945 - mae: 2.1256 - val_loss: 47.2471 - val_mae: 47.9282\n",
      "Epoch 186/1000\n",
      "1460/1460 [==============================] - 0s 281us/step - loss: 1.5821 - mae: 2.0212 - val_loss: 49.2357 - val_mae: 49.9102\n",
      "Epoch 187/1000\n",
      "1460/1460 [==============================] - 0s 298us/step - loss: 1.7431 - mae: 2.1926 - val_loss: 47.4646 - val_mae: 48.1363\n",
      "Epoch 188/1000\n",
      "1460/1460 [==============================] - 0s 286us/step - loss: 1.5687 - mae: 1.9978 - val_loss: 47.8797 - val_mae: 48.5561\n",
      "Epoch 189/1000\n",
      "1460/1460 [==============================] - 0s 304us/step - loss: 1.3907 - mae: 1.8141 - val_loss: 48.2794 - val_mae: 48.9593\n",
      "Epoch 190/1000\n",
      "1460/1460 [==============================] - 0s 284us/step - loss: 1.4369 - mae: 1.8535 - val_loss: 47.8105 - val_mae: 48.4864\n",
      "Epoch 191/1000\n",
      "1460/1460 [==============================] - 0s 290us/step - loss: 1.7619 - mae: 2.2010 - val_loss: 46.7768 - val_mae: 47.4570\n",
      "Epoch 192/1000\n",
      "1460/1460 [==============================] - 0s 291us/step - loss: 1.5808 - mae: 2.0161 - val_loss: 48.7660 - val_mae: 49.4419\n",
      "Epoch 193/1000\n",
      "1460/1460 [==============================] - 0s 289us/step - loss: 1.2758 - mae: 1.6945 - val_loss: 48.5921 - val_mae: 49.2641\n",
      "Epoch 194/1000\n",
      "1460/1460 [==============================] - 0s 293us/step - loss: 1.3600 - mae: 1.7754 - val_loss: 47.8868 - val_mae: 48.5590\n",
      "Epoch 195/1000\n",
      "1460/1460 [==============================] - 0s 291us/step - loss: 1.6461 - mae: 2.0694 - val_loss: 48.2720 - val_mae: 48.9456\n",
      "Epoch 196/1000\n",
      "1460/1460 [==============================] - 0s 283us/step - loss: 1.7854 - mae: 2.2154 - val_loss: 46.8942 - val_mae: 47.5731\n",
      "Epoch 197/1000\n",
      "1460/1460 [==============================] - 0s 313us/step - loss: 1.5653 - mae: 2.0049 - val_loss: 47.9700 - val_mae: 48.6448\n",
      "Epoch 198/1000\n",
      "1460/1460 [==============================] - 0s 285us/step - loss: 1.5141 - mae: 1.9331 - val_loss: 49.4556 - val_mae: 50.1285\n",
      "Epoch 199/1000\n",
      "1460/1460 [==============================] - 0s 302us/step - loss: 1.3784 - mae: 1.7939 - val_loss: 48.4937 - val_mae: 49.1637\n",
      "Epoch 200/1000\n",
      "1460/1460 [==============================] - 0s 279us/step - loss: 1.4763 - mae: 1.8952 - val_loss: 48.6950 - val_mae: 49.3705\n",
      "Epoch 201/1000\n",
      "1460/1460 [==============================] - 0s 285us/step - loss: 1.4894 - mae: 1.9126 - val_loss: 48.0631 - val_mae: 48.7424\n",
      "Epoch 202/1000\n",
      "1460/1460 [==============================] - 0s 299us/step - loss: 1.5466 - mae: 1.9732 - val_loss: 48.6281 - val_mae: 49.3017\n",
      "Epoch 203/1000\n",
      "1460/1460 [==============================] - 0s 289us/step - loss: 1.4227 - mae: 1.8439 - val_loss: 48.4397 - val_mae: 49.1090\n",
      "Epoch 204/1000\n",
      "1460/1460 [==============================] - 0s 288us/step - loss: 1.4596 - mae: 1.8745 - val_loss: 48.3266 - val_mae: 49.0020\n",
      "Epoch 205/1000\n",
      "1460/1460 [==============================] - 0s 286us/step - loss: 1.2345 - mae: 1.6522 - val_loss: 48.2319 - val_mae: 48.9086\n",
      "Epoch 206/1000\n",
      "1460/1460 [==============================] - 0s 296us/step - loss: 1.8610 - mae: 2.3197 - val_loss: 48.8890 - val_mae: 49.5654\n",
      "Epoch 207/1000\n",
      "1460/1460 [==============================] - 0s 307us/step - loss: 1.5155 - mae: 1.9499 - val_loss: 50.3037 - val_mae: 50.9776\n",
      "Epoch 208/1000\n",
      "1460/1460 [==============================] - 0s 293us/step - loss: 1.7854 - mae: 2.2203 - val_loss: 48.4620 - val_mae: 49.1415\n",
      "Epoch 209/1000\n",
      "1460/1460 [==============================] - 0s 292us/step - loss: 1.5910 - mae: 2.0266 - val_loss: 48.0803 - val_mae: 48.7605\n",
      "Epoch 210/1000\n",
      "1460/1460 [==============================] - 0s 284us/step - loss: 1.4834 - mae: 1.9138 - val_loss: 48.2264 - val_mae: 48.9056\n",
      "Epoch 211/1000\n",
      "1460/1460 [==============================] - 0s 288us/step - loss: 1.4488 - mae: 1.8834 - val_loss: 48.5593 - val_mae: 49.2371\n",
      "Epoch 212/1000\n",
      "1460/1460 [==============================] - 0s 285us/step - loss: 1.5425 - mae: 1.9663 - val_loss: 48.5382 - val_mae: 49.2143\n",
      "Epoch 213/1000\n",
      "1460/1460 [==============================] - 0s 282us/step - loss: 1.6381 - mae: 2.0575 - val_loss: 49.0388 - val_mae: 49.7116\n",
      "Epoch 214/1000\n",
      "1460/1460 [==============================] - 0s 278us/step - loss: 1.3456 - mae: 1.7570 - val_loss: 49.0896 - val_mae: 49.7671\n",
      "Epoch 215/1000\n",
      "1460/1460 [==============================] - 0s 285us/step - loss: 1.5092 - mae: 1.9356 - val_loss: 48.6189 - val_mae: 49.2967\n",
      "Epoch 216/1000\n",
      "1460/1460 [==============================] - 0s 285us/step - loss: 1.3677 - mae: 1.7851 - val_loss: 49.0883 - val_mae: 49.7640\n",
      "Epoch 217/1000\n",
      "1460/1460 [==============================] - 0s 288us/step - loss: 1.2663 - mae: 1.6751 - val_loss: 48.2387 - val_mae: 48.9194\n",
      "Epoch 218/1000\n",
      "1460/1460 [==============================] - 0s 297us/step - loss: 1.4869 - mae: 1.9109 - val_loss: 49.1187 - val_mae: 49.7940\n",
      "Epoch 219/1000\n",
      "1460/1460 [==============================] - 0s 285us/step - loss: 1.2435 - mae: 1.6566 - val_loss: 48.8682 - val_mae: 49.5454\n",
      "Epoch 220/1000\n",
      "1460/1460 [==============================] - 0s 291us/step - loss: 1.4782 - mae: 1.8883 - val_loss: 48.5494 - val_mae: 49.2276\n",
      "Epoch 221/1000\n",
      "1460/1460 [==============================] - 0s 283us/step - loss: 1.7424 - mae: 2.1800 - val_loss: 47.8014 - val_mae: 48.4833\n",
      "Epoch 222/1000\n",
      "1460/1460 [==============================] - 0s 286us/step - loss: 1.3440 - mae: 1.7550 - val_loss: 48.0424 - val_mae: 48.7188\n",
      "Epoch 223/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 1.5000 - mae: 1.9269 - val_loss: 48.9699 - val_mae: 49.6439\n",
      "Epoch 224/1000\n",
      "1460/1460 [==============================] - 0s 287us/step - loss: 1.5403 - mae: 1.9663 - val_loss: 49.3098 - val_mae: 49.9855\n",
      "Epoch 225/1000\n",
      "1460/1460 [==============================] - 0s 287us/step - loss: 1.5863 - mae: 2.0093 - val_loss: 49.4649 - val_mae: 50.1327\n",
      "Epoch 226/1000\n",
      "1460/1460 [==============================] - 0s 281us/step - loss: 1.4191 - mae: 1.8396 - val_loss: 48.8021 - val_mae: 49.4815\n",
      "Epoch 227/1000\n",
      "1460/1460 [==============================] - 0s 290us/step - loss: 1.5679 - mae: 1.9953 - val_loss: 47.8432 - val_mae: 48.5230\n",
      "Epoch 228/1000\n",
      "1460/1460 [==============================] - 0s 283us/step - loss: 1.4265 - mae: 1.8486 - val_loss: 48.1166 - val_mae: 48.7935\n",
      "Epoch 229/1000\n",
      "1460/1460 [==============================] - 0s 284us/step - loss: 1.3139 - mae: 1.7128 - val_loss: 49.5810 - val_mae: 50.2615\n",
      "Epoch 230/1000\n",
      "1460/1460 [==============================] - 0s 287us/step - loss: 1.4220 - mae: 1.8465 - val_loss: 48.4667 - val_mae: 49.1425\n",
      "Epoch 231/1000\n",
      "1460/1460 [==============================] - 0s 285us/step - loss: 1.4145 - mae: 1.8285 - val_loss: 49.0596 - val_mae: 49.7338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 232/1000\n",
      "1460/1460 [==============================] - 0s 283us/step - loss: 1.4223 - mae: 1.8456 - val_loss: 49.3022 - val_mae: 49.9782\n",
      "Epoch 233/1000\n",
      "1460/1460 [==============================] - 0s 283us/step - loss: 1.3193 - mae: 1.7285 - val_loss: 48.3395 - val_mae: 49.0183\n",
      "Epoch 234/1000\n",
      "1460/1460 [==============================] - 0s 283us/step - loss: 1.5529 - mae: 1.9743 - val_loss: 48.2874 - val_mae: 48.9708\n",
      "Epoch 235/1000\n",
      "1460/1460 [==============================] - 0s 291us/step - loss: 1.3304 - mae: 1.7400 - val_loss: 47.2532 - val_mae: 47.9320\n",
      "Epoch 236/1000\n",
      "1460/1460 [==============================] - 0s 280us/step - loss: 1.5423 - mae: 1.9722 - val_loss: 48.9583 - val_mae: 49.6316\n",
      "Epoch 237/1000\n",
      "1460/1460 [==============================] - 0s 281us/step - loss: 1.3418 - mae: 1.7518 - val_loss: 48.4850 - val_mae: 49.1657\n",
      "Epoch 238/1000\n",
      "1460/1460 [==============================] - 0s 284us/step - loss: 1.2455 - mae: 1.6413 - val_loss: 49.8367 - val_mae: 50.5050\n",
      "Epoch 239/1000\n",
      "1460/1460 [==============================] - 0s 279us/step - loss: 1.4610 - mae: 1.8845 - val_loss: 47.7160 - val_mae: 48.3946\n",
      "Epoch 240/1000\n",
      "1460/1460 [==============================] - 0s 291us/step - loss: 1.4056 - mae: 1.8152 - val_loss: 48.5907 - val_mae: 49.2646\n",
      "Epoch 241/1000\n",
      "1460/1460 [==============================] - 0s 288us/step - loss: 1.4000 - mae: 1.8183 - val_loss: 49.1528 - val_mae: 49.8310\n",
      "Epoch 242/1000\n",
      "1460/1460 [==============================] - 0s 293us/step - loss: 1.2758 - mae: 1.6786 - val_loss: 49.6323 - val_mae: 50.3096\n",
      "Epoch 243/1000\n",
      "1460/1460 [==============================] - 0s 280us/step - loss: 1.4918 - mae: 1.9122 - val_loss: 49.2410 - val_mae: 49.9140\n",
      "Epoch 244/1000\n",
      "1460/1460 [==============================] - 0s 298us/step - loss: 1.3561 - mae: 1.7636 - val_loss: 48.7723 - val_mae: 49.4524\n",
      "Epoch 245/1000\n",
      "1460/1460 [==============================] - 0s 315us/step - loss: 1.2893 - mae: 1.6957 - val_loss: 49.3472 - val_mae: 50.0229\n",
      "Epoch 246/1000\n",
      "1460/1460 [==============================] - 0s 284us/step - loss: 1.2802 - mae: 1.6915 - val_loss: 50.0950 - val_mae: 50.7727\n",
      "Epoch 247/1000\n",
      "1460/1460 [==============================] - 0s 284us/step - loss: 1.2265 - mae: 1.6398 - val_loss: 49.1696 - val_mae: 49.8476\n",
      "Epoch 248/1000\n",
      "1460/1460 [==============================] - 0s 270us/step - loss: 1.4260 - mae: 1.8420 - val_loss: 47.6952 - val_mae: 48.3768\n",
      "Epoch 249/1000\n",
      "1460/1460 [==============================] - 0s 284us/step - loss: 1.4172 - mae: 1.8324 - val_loss: 47.3811 - val_mae: 48.0605\n",
      "Epoch 250/1000\n",
      "1460/1460 [==============================] - 0s 277us/step - loss: 1.4516 - mae: 1.8720 - val_loss: 50.0817 - val_mae: 50.7538\n",
      "Epoch 251/1000\n",
      "1460/1460 [==============================] - 0s 279us/step - loss: 1.4625 - mae: 1.8853 - val_loss: 49.2631 - val_mae: 49.9480\n",
      "Epoch 252/1000\n",
      "1460/1460 [==============================] - 0s 281us/step - loss: 1.6222 - mae: 2.0436 - val_loss: 48.9066 - val_mae: 49.5859\n",
      "Epoch 253/1000\n",
      "1460/1460 [==============================] - 0s 290us/step - loss: 1.2914 - mae: 1.6957 - val_loss: 49.0362 - val_mae: 49.7145\n",
      "Epoch 254/1000\n",
      "1460/1460 [==============================] - 0s 286us/step - loss: 1.3160 - mae: 1.7388 - val_loss: 48.7612 - val_mae: 49.4356\n",
      "Epoch 255/1000\n",
      "1460/1460 [==============================] - 0s 286us/step - loss: 1.2493 - mae: 1.6546 - val_loss: 49.2430 - val_mae: 49.9218\n",
      "Epoch 256/1000\n",
      "1460/1460 [==============================] - 0s 289us/step - loss: 1.3235 - mae: 1.7435 - val_loss: 48.8567 - val_mae: 49.5376\n",
      "Epoch 257/1000\n",
      "1460/1460 [==============================] - 0s 289us/step - loss: 1.3266 - mae: 1.7253 - val_loss: 49.4100 - val_mae: 50.0878\n",
      "Epoch 258/1000\n",
      "1460/1460 [==============================] - 0s 289us/step - loss: 1.3660 - mae: 1.7841 - val_loss: 47.9443 - val_mae: 48.6223\n",
      "Epoch 259/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 1.8008 - mae: 2.2446 - val_loss: 48.5136 - val_mae: 49.1901\n",
      "Epoch 260/1000\n",
      "1460/1460 [==============================] - 0s 291us/step - loss: 1.3013 - mae: 1.7142 - val_loss: 49.7524 - val_mae: 50.4276\n",
      "Epoch 261/1000\n",
      "1460/1460 [==============================] - 0s 287us/step - loss: 1.3546 - mae: 1.7750 - val_loss: 48.7860 - val_mae: 49.4650\n",
      "Epoch 262/1000\n",
      "1460/1460 [==============================] - 0s 286us/step - loss: 1.2926 - mae: 1.7106 - val_loss: 48.5248 - val_mae: 49.2040\n",
      "Epoch 263/1000\n",
      "1460/1460 [==============================] - 0s 283us/step - loss: 1.2124 - mae: 1.6070 - val_loss: 50.1371 - val_mae: 50.8139\n",
      "Epoch 264/1000\n",
      "1460/1460 [==============================] - 0s 289us/step - loss: 1.5709 - mae: 2.0040 - val_loss: 49.7040 - val_mae: 50.3825\n",
      "Epoch 265/1000\n",
      "1460/1460 [==============================] - 0s 291us/step - loss: 1.4689 - mae: 1.8889 - val_loss: 48.4312 - val_mae: 49.1151\n",
      "Epoch 266/1000\n",
      "1460/1460 [==============================] - 0s 283us/step - loss: 1.3239 - mae: 1.7423 - val_loss: 49.2219 - val_mae: 49.9030\n",
      "Epoch 267/1000\n",
      "1460/1460 [==============================] - 0s 282us/step - loss: 1.1973 - mae: 1.5965 - val_loss: 48.9075 - val_mae: 49.5877\n",
      "Epoch 268/1000\n",
      "1460/1460 [==============================] - 0s 278us/step - loss: 1.2297 - mae: 1.6294 - val_loss: 49.6299 - val_mae: 50.3124\n",
      "Epoch 269/1000\n",
      "1460/1460 [==============================] - 0s 283us/step - loss: 1.2911 - mae: 1.7100 - val_loss: 49.7237 - val_mae: 50.4027\n",
      "Epoch 270/1000\n",
      "1460/1460 [==============================] - 0s 284us/step - loss: 1.1708 - mae: 1.5675 - val_loss: 49.7638 - val_mae: 50.4437\n",
      "Epoch 271/1000\n",
      "1460/1460 [==============================] - 0s 285us/step - loss: 1.1718 - mae: 1.5743 - val_loss: 49.0654 - val_mae: 49.7464\n",
      "Epoch 272/1000\n",
      "1460/1460 [==============================] - 0s 285us/step - loss: 1.6028 - mae: 2.0469 - val_loss: 49.4419 - val_mae: 50.1217\n",
      "Epoch 273/1000\n",
      "1460/1460 [==============================] - 0s 283us/step - loss: 1.4110 - mae: 1.8316 - val_loss: 49.1879 - val_mae: 49.8707\n",
      "Epoch 274/1000\n",
      "1460/1460 [==============================] - 0s 280us/step - loss: 1.2645 - mae: 1.6748 - val_loss: 50.0230 - val_mae: 50.6950\n",
      "Epoch 275/1000\n",
      "1460/1460 [==============================] - 0s 287us/step - loss: 1.3572 - mae: 1.7729 - val_loss: 48.6774 - val_mae: 49.3544\n",
      "Epoch 276/1000\n",
      "1460/1460 [==============================] - 0s 290us/step - loss: 1.9055 - mae: 2.3500 - val_loss: 50.2135 - val_mae: 50.8866\n",
      "Epoch 277/1000\n",
      "1460/1460 [==============================] - 0s 291us/step - loss: 1.4147 - mae: 1.8316 - val_loss: 48.8035 - val_mae: 49.4837\n",
      "Epoch 278/1000\n",
      "1460/1460 [==============================] - 0s 293us/step - loss: 1.3115 - mae: 1.7210 - val_loss: 49.4099 - val_mae: 50.0928\n",
      "Epoch 279/1000\n",
      "1460/1460 [==============================] - 0s 283us/step - loss: 1.1391 - mae: 1.5339 - val_loss: 49.4463 - val_mae: 50.1296\n",
      "Epoch 280/1000\n",
      "1460/1460 [==============================] - 0s 303us/step - loss: 1.0933 - mae: 1.4923 - val_loss: 49.3459 - val_mae: 50.0259\n",
      "Epoch 281/1000\n",
      "1460/1460 [==============================] - 0s 286us/step - loss: 1.5071 - mae: 1.9376 - val_loss: 49.1976 - val_mae: 49.8811\n",
      "Epoch 282/1000\n",
      "1460/1460 [==============================] - 0s 289us/step - loss: 1.3158 - mae: 1.7232 - val_loss: 48.9080 - val_mae: 49.5913\n",
      "Epoch 283/1000\n",
      "1460/1460 [==============================] - 0s 303us/step - loss: 1.2914 - mae: 1.6987 - val_loss: 49.5382 - val_mae: 50.2178\n",
      "Epoch 284/1000\n",
      "1460/1460 [==============================] - 0s 315us/step - loss: 1.2207 - mae: 1.6264 - val_loss: 49.7130 - val_mae: 50.3855\n",
      "Epoch 285/1000\n",
      "1460/1460 [==============================] - 0s 278us/step - loss: 1.2822 - mae: 1.6999 - val_loss: 50.8740 - val_mae: 51.5489\n",
      "Epoch 286/1000\n",
      "1460/1460 [==============================] - 0s 282us/step - loss: 1.4623 - mae: 1.8932 - val_loss: 49.4403 - val_mae: 50.1226\n",
      "Epoch 287/1000\n",
      "1460/1460 [==============================] - 0s 285us/step - loss: 1.2415 - mae: 1.6406 - val_loss: 49.6454 - val_mae: 50.3247\n",
      "Epoch 288/1000\n",
      "1460/1460 [==============================] - 0s 282us/step - loss: 1.0141 - mae: 1.3946 - val_loss: 49.5474 - val_mae: 50.2305\n",
      "Epoch 289/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 1.1037 - mae: 1.4999 - val_loss: 50.0615 - val_mae: 50.7417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 290/1000\n",
      "1460/1460 [==============================] - 0s 282us/step - loss: 1.0642 - mae: 1.4697 - val_loss: 49.8241 - val_mae: 50.5053\n",
      "Epoch 291/1000\n",
      "1460/1460 [==============================] - 0s 283us/step - loss: 1.4460 - mae: 1.8771 - val_loss: 49.8596 - val_mae: 50.5374\n",
      "Epoch 292/1000\n",
      "1460/1460 [==============================] - 0s 278us/step - loss: 1.3745 - mae: 1.7899 - val_loss: 48.8174 - val_mae: 49.4986\n",
      "Epoch 293/1000\n",
      "1460/1460 [==============================] - 0s 282us/step - loss: 1.2715 - mae: 1.6737 - val_loss: 50.3738 - val_mae: 51.0540\n",
      "Epoch 294/1000\n",
      "1460/1460 [==============================] - 0s 279us/step - loss: 1.0247 - mae: 1.4071 - val_loss: 49.6948 - val_mae: 50.3768\n",
      "Epoch 295/1000\n",
      "1460/1460 [==============================] - 0s 279us/step - loss: 1.2109 - mae: 1.6187 - val_loss: 49.9197 - val_mae: 50.6030\n",
      "Epoch 296/1000\n",
      "1460/1460 [==============================] - 0s 282us/step - loss: 1.4759 - mae: 1.8916 - val_loss: 49.9146 - val_mae: 50.5902\n",
      "Epoch 297/1000\n",
      "1460/1460 [==============================] - 0s 280us/step - loss: 1.3328 - mae: 1.7533 - val_loss: 49.5660 - val_mae: 50.2469\n",
      "Epoch 298/1000\n",
      "1460/1460 [==============================] - 0s 276us/step - loss: 1.1713 - mae: 1.5575 - val_loss: 50.4487 - val_mae: 51.1263\n",
      "Epoch 299/1000\n",
      "1460/1460 [==============================] - 0s 299us/step - loss: 1.2248 - mae: 1.6356 - val_loss: 49.8737 - val_mae: 50.5515\n",
      "Epoch 300/1000\n",
      "1460/1460 [==============================] - 0s 280us/step - loss: 1.0852 - mae: 1.4892 - val_loss: 49.4091 - val_mae: 50.0854\n",
      "Epoch 301/1000\n",
      "1460/1460 [==============================] - 0s 287us/step - loss: 0.9992 - mae: 1.3804 - val_loss: 49.8669 - val_mae: 50.5482\n",
      "Epoch 302/1000\n",
      "1460/1460 [==============================] - 0s 280us/step - loss: 1.1999 - mae: 1.6040 - val_loss: 49.0495 - val_mae: 49.7297\n",
      "Epoch 303/1000\n",
      "1460/1460 [==============================] - 0s 280us/step - loss: 1.2419 - mae: 1.6433 - val_loss: 50.2526 - val_mae: 50.9302\n",
      "Epoch 304/1000\n",
      "1460/1460 [==============================] - 0s 286us/step - loss: 1.1403 - mae: 1.5342 - val_loss: 49.7372 - val_mae: 50.4162\n",
      "Epoch 305/1000\n",
      "1460/1460 [==============================] - 0s 282us/step - loss: 1.2009 - mae: 1.6044 - val_loss: 49.0319 - val_mae: 49.7157\n",
      "Epoch 306/1000\n",
      "1460/1460 [==============================] - 0s 292us/step - loss: 1.1755 - mae: 1.5738 - val_loss: 50.0560 - val_mae: 50.7361\n",
      "Epoch 307/1000\n",
      "1460/1460 [==============================] - 0s 287us/step - loss: 1.2773 - mae: 1.6859 - val_loss: 49.4755 - val_mae: 50.1526\n",
      "Epoch 308/1000\n",
      "1460/1460 [==============================] - 0s 294us/step - loss: 1.0397 - mae: 1.4290 - val_loss: 49.5857 - val_mae: 50.2652\n",
      "Epoch 309/1000\n",
      "1460/1460 [==============================] - 0s 280us/step - loss: 1.1761 - mae: 1.5786 - val_loss: 48.1451 - val_mae: 48.8282\n",
      "Epoch 310/1000\n",
      "1460/1460 [==============================] - 0s 281us/step - loss: 1.2745 - mae: 1.6821 - val_loss: 49.1139 - val_mae: 49.7958\n",
      "Epoch 311/1000\n",
      "1460/1460 [==============================] - 0s 297us/step - loss: 1.2535 - mae: 1.6484 - val_loss: 49.8611 - val_mae: 50.5397\n",
      "Epoch 312/1000\n",
      "1460/1460 [==============================] - 0s 328us/step - loss: 1.4704 - mae: 1.8949 - val_loss: 48.1511 - val_mae: 48.8335\n",
      "Epoch 313/1000\n",
      "1460/1460 [==============================] - 0s 317us/step - loss: 1.5372 - mae: 1.9610 - val_loss: 49.6447 - val_mae: 50.3246\n",
      "Epoch 314/1000\n",
      "1460/1460 [==============================] - 0s 280us/step - loss: 1.1805 - mae: 1.5799 - val_loss: 49.7971 - val_mae: 50.4736\n",
      "Epoch 315/1000\n",
      "1460/1460 [==============================] - 0s 289us/step - loss: 1.0735 - mae: 1.4632 - val_loss: 49.5365 - val_mae: 50.2171\n",
      "Epoch 316/1000\n",
      "1460/1460 [==============================] - 0s 282us/step - loss: 0.9952 - mae: 1.3818 - val_loss: 48.5336 - val_mae: 49.2157\n",
      "Epoch 317/1000\n",
      "1460/1460 [==============================] - 0s 284us/step - loss: 1.1850 - mae: 1.5927 - val_loss: 48.9445 - val_mae: 49.6276\n",
      "Epoch 318/1000\n",
      "1460/1460 [==============================] - 0s 284us/step - loss: 1.5209 - mae: 1.9414 - val_loss: 49.5031 - val_mae: 50.1794\n",
      "Epoch 319/1000\n",
      "1460/1460 [==============================] - 0s 315us/step - loss: 1.1566 - mae: 1.5550 - val_loss: 48.3765 - val_mae: 49.0562\n",
      "Epoch 320/1000\n",
      "1460/1460 [==============================] - 0s 283us/step - loss: 1.2417 - mae: 1.6484 - val_loss: 49.4625 - val_mae: 50.1412\n",
      "Epoch 321/1000\n",
      "1460/1460 [==============================] - 0s 298us/step - loss: 1.3769 - mae: 1.7937 - val_loss: 49.8325 - val_mae: 50.5138\n",
      "Epoch 322/1000\n",
      "1460/1460 [==============================] - 0s 293us/step - loss: 1.2070 - mae: 1.6061 - val_loss: 49.5119 - val_mae: 50.1908\n",
      "Epoch 323/1000\n",
      "1460/1460 [==============================] - 0s 269us/step - loss: 1.3050 - mae: 1.7139 - val_loss: 49.6070 - val_mae: 50.2883\n",
      "Epoch 324/1000\n",
      "1460/1460 [==============================] - 0s 288us/step - loss: 1.1365 - mae: 1.5372 - val_loss: 49.4580 - val_mae: 50.1374\n",
      "Epoch 325/1000\n",
      "1460/1460 [==============================] - 0s 281us/step - loss: 1.4391 - mae: 1.8470 - val_loss: 49.0986 - val_mae: 49.7789\n",
      "Epoch 326/1000\n",
      "1460/1460 [==============================] - 0s 294us/step - loss: 1.1530 - mae: 1.5443 - val_loss: 49.4283 - val_mae: 50.1058\n",
      "Epoch 327/1000\n",
      "1460/1460 [==============================] - 0s 284us/step - loss: 1.0545 - mae: 1.4443 - val_loss: 49.0409 - val_mae: 49.7217\n",
      "Epoch 328/1000\n",
      "1460/1460 [==============================] - 0s 286us/step - loss: 1.1267 - mae: 1.5172 - val_loss: 48.9976 - val_mae: 49.6776\n",
      "Epoch 329/1000\n",
      "1460/1460 [==============================] - 0s 277us/step - loss: 1.1640 - mae: 1.5606 - val_loss: 48.7954 - val_mae: 49.4743\n",
      "Epoch 330/1000\n",
      "1460/1460 [==============================] - 0s 283us/step - loss: 1.5537 - mae: 1.9781 - val_loss: 48.5040 - val_mae: 49.1857\n",
      "Epoch 331/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 1.3978 - mae: 1.8168 - val_loss: 48.9857 - val_mae: 49.6665\n",
      "Epoch 332/1000\n",
      "1460/1460 [==============================] - 0s 276us/step - loss: 1.0751 - mae: 1.4614 - val_loss: 49.0360 - val_mae: 49.7168\n",
      "Epoch 333/1000\n",
      "1460/1460 [==============================] - 0s 278us/step - loss: 1.1826 - mae: 1.5829 - val_loss: 49.1897 - val_mae: 49.8695\n",
      "Epoch 334/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 0.9931 - mae: 1.3702 - val_loss: 48.2626 - val_mae: 48.9475\n",
      "Epoch 335/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 1.0772 - mae: 1.4549 - val_loss: 48.8352 - val_mae: 49.5167\n",
      "Epoch 336/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 1.0119 - mae: 1.4028 - val_loss: 49.0339 - val_mae: 49.7159\n",
      "Epoch 337/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 1.1946 - mae: 1.6006 - val_loss: 48.7807 - val_mae: 49.4604\n",
      "Epoch 338/1000\n",
      "1460/1460 [==============================] - 0s 270us/step - loss: 1.3733 - mae: 1.7922 - val_loss: 48.7660 - val_mae: 49.4455\n",
      "Epoch 339/1000\n",
      "1460/1460 [==============================] - 0s 270us/step - loss: 1.1701 - mae: 1.5620 - val_loss: 49.2210 - val_mae: 49.9035\n",
      "Epoch 340/1000\n",
      "1460/1460 [==============================] - 0s 270us/step - loss: 1.1328 - mae: 1.5192 - val_loss: 49.0991 - val_mae: 49.7806\n",
      "Epoch 341/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 1.1640 - mae: 1.5617 - val_loss: 49.4522 - val_mae: 50.1302\n",
      "Epoch 342/1000\n",
      "1460/1460 [==============================] - 0s 275us/step - loss: 1.1603 - mae: 1.5513 - val_loss: 49.5689 - val_mae: 50.2505\n",
      "Epoch 343/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 1.3225 - mae: 1.7323 - val_loss: 47.8487 - val_mae: 48.5327\n",
      "Epoch 344/1000\n",
      "1460/1460 [==============================] - 0s 276us/step - loss: 1.1687 - mae: 1.5733 - val_loss: 49.3998 - val_mae: 50.0820\n",
      "Epoch 345/1000\n",
      "1460/1460 [==============================] - 0s 275us/step - loss: 1.2312 - mae: 1.6309 - val_loss: 49.2806 - val_mae: 49.9607\n",
      "Epoch 346/1000\n",
      "1460/1460 [==============================] - 0s 275us/step - loss: 0.9865 - mae: 1.3730 - val_loss: 49.5479 - val_mae: 50.2253\n",
      "Epoch 347/1000\n",
      "1460/1460 [==============================] - 0s 275us/step - loss: 1.0494 - mae: 1.4461 - val_loss: 48.9802 - val_mae: 49.6619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 348/1000\n",
      "1460/1460 [==============================] - 0s 276us/step - loss: 1.1393 - mae: 1.5328 - val_loss: 49.3420 - val_mae: 50.0261\n",
      "Epoch 349/1000\n",
      "1460/1460 [==============================] - 0s 273us/step - loss: 1.1000 - mae: 1.4975 - val_loss: 49.7344 - val_mae: 50.4180\n",
      "Epoch 350/1000\n",
      "1460/1460 [==============================] - 0s 270us/step - loss: 1.1383 - mae: 1.5397 - val_loss: 48.5655 - val_mae: 49.2487\n",
      "Epoch 351/1000\n",
      "1460/1460 [==============================] - 0s 304us/step - loss: 1.0997 - mae: 1.5049 - val_loss: 49.0107 - val_mae: 49.6929\n",
      "Epoch 352/1000\n",
      "1460/1460 [==============================] - 0s 276us/step - loss: 1.2133 - mae: 1.6108 - val_loss: 49.1815 - val_mae: 49.8629\n",
      "Epoch 353/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 1.1635 - mae: 1.5543 - val_loss: 49.1189 - val_mae: 49.8026\n",
      "Epoch 354/1000\n",
      "1460/1460 [==============================] - 0s 270us/step - loss: 0.9206 - mae: 1.2943 - val_loss: 49.6591 - val_mae: 50.3429\n",
      "Epoch 355/1000\n",
      "1460/1460 [==============================] - 0s 273us/step - loss: 0.9041 - mae: 1.2728 - val_loss: 49.7317 - val_mae: 50.4146\n",
      "Epoch 356/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 1.1737 - mae: 1.5747 - val_loss: 50.0974 - val_mae: 50.7759\n",
      "Epoch 357/1000\n",
      "1460/1460 [==============================] - 0s 268us/step - loss: 1.3056 - mae: 1.7191 - val_loss: 49.9761 - val_mae: 50.6557\n",
      "Epoch 358/1000\n",
      "1460/1460 [==============================] - 0s 269us/step - loss: 1.2055 - mae: 1.6193 - val_loss: 49.0308 - val_mae: 49.7080\n",
      "Epoch 359/1000\n",
      "1460/1460 [==============================] - 0s 275us/step - loss: 0.9370 - mae: 1.3126 - val_loss: 49.3217 - val_mae: 50.0007\n",
      "Epoch 360/1000\n",
      "1460/1460 [==============================] - 0s 276us/step - loss: 1.1768 - mae: 1.5752 - val_loss: 49.3230 - val_mae: 50.0033\n",
      "Epoch 361/1000\n",
      "1460/1460 [==============================] - 0s 302us/step - loss: 0.9831 - mae: 1.3537 - val_loss: 49.4950 - val_mae: 50.1742\n",
      "Epoch 362/1000\n",
      "1460/1460 [==============================] - 0s 273us/step - loss: 1.0009 - mae: 1.3871 - val_loss: 49.0426 - val_mae: 49.7251\n",
      "Epoch 363/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 1.1613 - mae: 1.5548 - val_loss: 49.4749 - val_mae: 50.1572\n",
      "Epoch 364/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 1.2400 - mae: 1.6484 - val_loss: 49.9646 - val_mae: 50.6438\n",
      "Epoch 365/1000\n",
      "1460/1460 [==============================] - 0s 273us/step - loss: 0.9959 - mae: 1.3704 - val_loss: 49.6487 - val_mae: 50.3304\n",
      "Epoch 366/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 1.0676 - mae: 1.4462 - val_loss: 49.3891 - val_mae: 50.0702\n",
      "Epoch 367/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 1.1985 - mae: 1.5976 - val_loss: 50.3613 - val_mae: 51.0383\n",
      "Epoch 368/1000\n",
      "1460/1460 [==============================] - 0s 280us/step - loss: 0.9581 - mae: 1.3354 - val_loss: 49.3623 - val_mae: 50.0457\n",
      "Epoch 369/1000\n",
      "1460/1460 [==============================] - 0s 280us/step - loss: 1.0986 - mae: 1.4896 - val_loss: 48.2659 - val_mae: 48.9443\n",
      "Epoch 370/1000\n",
      "1460/1460 [==============================] - 0s 269us/step - loss: 1.2611 - mae: 1.6648 - val_loss: 50.2894 - val_mae: 50.9674\n",
      "Epoch 371/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 1.3199 - mae: 1.7443 - val_loss: 49.4720 - val_mae: 50.1533\n",
      "Epoch 372/1000\n",
      "1460/1460 [==============================] - 0s 268us/step - loss: 1.0375 - mae: 1.4261 - val_loss: 50.1160 - val_mae: 50.7951\n",
      "Epoch 373/1000\n",
      "1460/1460 [==============================] - 0s 269us/step - loss: 1.0939 - mae: 1.4908 - val_loss: 49.8460 - val_mae: 50.5276\n",
      "Epoch 374/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 1.1463 - mae: 1.5416 - val_loss: 49.8061 - val_mae: 50.4884\n",
      "Epoch 375/1000\n",
      "1460/1460 [==============================] - 0s 270us/step - loss: 1.2608 - mae: 1.6776 - val_loss: 49.4701 - val_mae: 50.1501\n",
      "Epoch 376/1000\n",
      "1460/1460 [==============================] - 0s 273us/step - loss: 0.9562 - mae: 1.3382 - val_loss: 49.0851 - val_mae: 49.7680\n",
      "Epoch 377/1000\n",
      "1460/1460 [==============================] - 0s 273us/step - loss: 1.1025 - mae: 1.5011 - val_loss: 49.8752 - val_mae: 50.5524\n",
      "Epoch 378/1000\n",
      "1460/1460 [==============================] - 0s 270us/step - loss: 1.1110 - mae: 1.5079 - val_loss: 49.2065 - val_mae: 49.8895\n",
      "Epoch 379/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 1.0164 - mae: 1.4004 - val_loss: 50.0496 - val_mae: 50.7273\n",
      "Epoch 380/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.9637 - mae: 1.3402 - val_loss: 49.7902 - val_mae: 50.4683\n",
      "Epoch 381/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 1.0166 - mae: 1.3873 - val_loss: 49.2900 - val_mae: 49.9722\n",
      "Epoch 382/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 1.0781 - mae: 1.4650 - val_loss: 48.9345 - val_mae: 49.6192\n",
      "Epoch 383/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 1.1073 - mae: 1.5044 - val_loss: 50.5483 - val_mae: 51.2221\n",
      "Epoch 384/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 1.3692 - mae: 1.7944 - val_loss: 49.5793 - val_mae: 50.2598\n",
      "Epoch 385/1000\n",
      "1460/1460 [==============================] - 0s 273us/step - loss: 1.3737 - mae: 1.7948 - val_loss: 49.2636 - val_mae: 49.9440\n",
      "Epoch 386/1000\n",
      "1460/1460 [==============================] - 0s 273us/step - loss: 1.1406 - mae: 1.5366 - val_loss: 50.7412 - val_mae: 51.4164\n",
      "Epoch 387/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 1.0506 - mae: 1.4474 - val_loss: 50.3812 - val_mae: 51.0595\n",
      "Epoch 388/1000\n",
      "1460/1460 [==============================] - 0s 273us/step - loss: 1.0886 - mae: 1.4793 - val_loss: 49.2896 - val_mae: 49.9707\n",
      "Epoch 389/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 1.2143 - mae: 1.6273 - val_loss: 49.1415 - val_mae: 49.8255\n",
      "Epoch 390/1000\n",
      "1460/1460 [==============================] - 0s 278us/step - loss: 1.1015 - mae: 1.4985 - val_loss: 49.9660 - val_mae: 50.6401\n",
      "Epoch 391/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 1.2851 - mae: 1.7071 - val_loss: 49.5238 - val_mae: 50.2092\n",
      "Epoch 392/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 1.2617 - mae: 1.6674 - val_loss: 49.3406 - val_mae: 50.0223\n",
      "Epoch 393/1000\n",
      "1460/1460 [==============================] - 0s 288us/step - loss: 1.1427 - mae: 1.5302 - val_loss: 49.8552 - val_mae: 50.5341\n",
      "Epoch 394/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 1.0344 - mae: 1.4231 - val_loss: 48.8099 - val_mae: 49.4927\n",
      "Epoch 395/1000\n",
      "1460/1460 [==============================] - 0s 273us/step - loss: 1.0667 - mae: 1.4661 - val_loss: 49.4441 - val_mae: 50.1210\n",
      "Epoch 396/1000\n",
      "1460/1460 [==============================] - 0s 275us/step - loss: 1.1763 - mae: 1.5839 - val_loss: 49.0001 - val_mae: 49.6829\n",
      "Epoch 397/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 1.1050 - mae: 1.4879 - val_loss: 48.6840 - val_mae: 49.3647\n",
      "Epoch 398/1000\n",
      "1460/1460 [==============================] - 0s 275us/step - loss: 1.1027 - mae: 1.5063 - val_loss: 49.0058 - val_mae: 49.6900\n",
      "Epoch 399/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 1.0600 - mae: 1.4491 - val_loss: 48.8127 - val_mae: 49.4938\n",
      "Epoch 400/1000\n",
      "1460/1460 [==============================] - 0s 287us/step - loss: 0.9644 - mae: 1.3492 - val_loss: 49.5339 - val_mae: 50.2132\n",
      "Epoch 401/1000\n",
      "1460/1460 [==============================] - 0s 295us/step - loss: 0.9696 - mae: 1.3423 - val_loss: 49.8130 - val_mae: 50.4918\n",
      "Epoch 402/1000\n",
      "1460/1460 [==============================] - 0s 275us/step - loss: 1.3049 - mae: 1.7190 - val_loss: 49.3998 - val_mae: 50.0800\n",
      "Epoch 403/1000\n",
      "1460/1460 [==============================] - 0s 276us/step - loss: 1.0438 - mae: 1.4313 - val_loss: 50.5728 - val_mae: 51.2485\n",
      "Epoch 404/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 0.9258 - mae: 1.2960 - val_loss: 50.1274 - val_mae: 50.8076\n",
      "Epoch 405/1000\n",
      "1460/1460 [==============================] - 0s 270us/step - loss: 0.9143 - mae: 1.2791 - val_loss: 49.2918 - val_mae: 49.9740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 406/1000\n",
      "1460/1460 [==============================] - 0s 270us/step - loss: 0.8059 - mae: 1.1656 - val_loss: 49.8372 - val_mae: 50.5200\n",
      "Epoch 407/1000\n",
      "1460/1460 [==============================] - 0s 282us/step - loss: 0.9769 - mae: 1.3501 - val_loss: 49.8835 - val_mae: 50.5631\n",
      "Epoch 408/1000\n",
      "1460/1460 [==============================] - 0s 267us/step - loss: 1.0236 - mae: 1.4002 - val_loss: 49.8788 - val_mae: 50.5601\n",
      "Epoch 409/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 1.1333 - mae: 1.5331 - val_loss: 49.0883 - val_mae: 49.7672\n",
      "Epoch 410/1000\n",
      "1460/1460 [==============================] - 0s 269us/step - loss: 1.0949 - mae: 1.4953 - val_loss: 50.4255 - val_mae: 51.1059\n",
      "Epoch 411/1000\n",
      "1460/1460 [==============================] - 0s 264us/step - loss: 0.9818 - mae: 1.3502 - val_loss: 49.2133 - val_mae: 49.8936\n",
      "Epoch 412/1000\n",
      "1460/1460 [==============================] - 0s 264us/step - loss: 0.9669 - mae: 1.3475 - val_loss: 49.7234 - val_mae: 50.4072\n",
      "Epoch 413/1000\n",
      "1460/1460 [==============================] - 0s 283us/step - loss: 1.2519 - mae: 1.6523 - val_loss: 49.7254 - val_mae: 50.4049\n",
      "Epoch 414/1000\n",
      "1460/1460 [==============================] - 0s 264us/step - loss: 1.0257 - mae: 1.4167 - val_loss: 49.2483 - val_mae: 49.9322\n",
      "Epoch 415/1000\n",
      "1460/1460 [==============================] - 0s 265us/step - loss: 1.0625 - mae: 1.4488 - val_loss: 49.5487 - val_mae: 50.2314\n",
      "Epoch 416/1000\n",
      "1460/1460 [==============================] - 0s 265us/step - loss: 0.9918 - mae: 1.3667 - val_loss: 50.3133 - val_mae: 50.9888\n",
      "Epoch 417/1000\n",
      "1460/1460 [==============================] - 0s 264us/step - loss: 1.2535 - mae: 1.6565 - val_loss: 49.1370 - val_mae: 49.8214\n",
      "Epoch 418/1000\n",
      "1460/1460 [==============================] - 0s 266us/step - loss: 0.9533 - mae: 1.3333 - val_loss: 49.5509 - val_mae: 50.2351\n",
      "Epoch 419/1000\n",
      "1460/1460 [==============================] - 0s 266us/step - loss: 0.8888 - mae: 1.2640 - val_loss: 49.9337 - val_mae: 50.6171\n",
      "Epoch 420/1000\n",
      "1460/1460 [==============================] - 0s 266us/step - loss: 0.8959 - mae: 1.2594 - val_loss: 49.1336 - val_mae: 49.8133\n",
      "Epoch 421/1000\n",
      "1460/1460 [==============================] - 0s 270us/step - loss: 1.0101 - mae: 1.4014 - val_loss: 48.7965 - val_mae: 49.4752\n",
      "Epoch 422/1000\n",
      "1460/1460 [==============================] - 0s 269us/step - loss: 1.0597 - mae: 1.4594 - val_loss: 49.4487 - val_mae: 50.1307\n",
      "Epoch 423/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 0.8716 - mae: 1.2520 - val_loss: 49.4312 - val_mae: 50.1146\n",
      "Epoch 424/1000\n",
      "1460/1460 [==============================] - 0s 269us/step - loss: 0.9628 - mae: 1.3392 - val_loss: 49.2851 - val_mae: 49.9665\n",
      "Epoch 425/1000\n",
      "1460/1460 [==============================] - 0s 269us/step - loss: 0.9584 - mae: 1.3365 - val_loss: 49.8006 - val_mae: 50.4814\n",
      "Epoch 426/1000\n",
      "1460/1460 [==============================] - 0s 273us/step - loss: 0.8689 - mae: 1.2437 - val_loss: 50.0704 - val_mae: 50.7481\n",
      "Epoch 427/1000\n",
      "1460/1460 [==============================] - 0s 269us/step - loss: 0.9269 - mae: 1.3081 - val_loss: 49.0754 - val_mae: 49.7587\n",
      "Epoch 428/1000\n",
      "1460/1460 [==============================] - 0s 269us/step - loss: 0.8514 - mae: 1.2130 - val_loss: 49.6985 - val_mae: 50.3784\n",
      "Epoch 429/1000\n",
      "1460/1460 [==============================] - 0s 275us/step - loss: 0.9986 - mae: 1.3782 - val_loss: 49.5825 - val_mae: 50.2627\n",
      "Epoch 430/1000\n",
      "1460/1460 [==============================] - 0s 265us/step - loss: 1.0730 - mae: 1.4679 - val_loss: 49.6522 - val_mae: 50.3317\n",
      "Epoch 431/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 0.9476 - mae: 1.3281 - val_loss: 49.5645 - val_mae: 50.2482\n",
      "Epoch 432/1000\n",
      "1460/1460 [==============================] - 0s 269us/step - loss: 0.8961 - mae: 1.2780 - val_loss: 50.1031 - val_mae: 50.7834\n",
      "Epoch 433/1000\n",
      "1460/1460 [==============================] - 0s 268us/step - loss: 0.8468 - mae: 1.2149 - val_loss: 49.5393 - val_mae: 50.2197\n",
      "Epoch 434/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.9141 - mae: 1.2731 - val_loss: 49.5367 - val_mae: 50.2185\n",
      "Epoch 435/1000\n",
      "1460/1460 [==============================] - 0s 270us/step - loss: 0.9399 - mae: 1.3010 - val_loss: 49.3905 - val_mae: 50.0696\n",
      "Epoch 436/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 0.7985 - mae: 1.1550 - val_loss: 49.9102 - val_mae: 50.5922\n",
      "Epoch 437/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 0.9751 - mae: 1.3380 - val_loss: 49.9776 - val_mae: 50.6584\n",
      "Epoch 438/1000\n",
      "1460/1460 [==============================] - 0s 273us/step - loss: 1.0232 - mae: 1.4023 - val_loss: 50.0029 - val_mae: 50.6855\n",
      "Epoch 439/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 1.0794 - mae: 1.4609 - val_loss: 49.7650 - val_mae: 50.4455\n",
      "Epoch 440/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.8888 - mae: 1.2698 - val_loss: 49.6126 - val_mae: 50.2941\n",
      "Epoch 441/1000\n",
      "1460/1460 [==============================] - 0s 298us/step - loss: 0.8610 - mae: 1.2235 - val_loss: 48.7079 - val_mae: 49.3923\n",
      "Epoch 442/1000\n",
      "1460/1460 [==============================] - 0s 277us/step - loss: 0.8964 - mae: 1.2674 - val_loss: 49.1420 - val_mae: 49.8242\n",
      "Epoch 443/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 0.9030 - mae: 1.2792 - val_loss: 49.3731 - val_mae: 50.0569\n",
      "Epoch 444/1000\n",
      "1460/1460 [==============================] - 0s 270us/step - loss: 1.0969 - mae: 1.4911 - val_loss: 50.2157 - val_mae: 50.8961\n",
      "Epoch 445/1000\n",
      "1460/1460 [==============================] - 0s 268us/step - loss: 1.0290 - mae: 1.4174 - val_loss: 49.5204 - val_mae: 50.2043\n",
      "Epoch 446/1000\n",
      "1460/1460 [==============================] - 0s 276us/step - loss: 0.9203 - mae: 1.2975 - val_loss: 49.3484 - val_mae: 50.0322\n",
      "Epoch 447/1000\n",
      "1460/1460 [==============================] - 0s 281us/step - loss: 0.9342 - mae: 1.2922 - val_loss: 49.9477 - val_mae: 50.6313\n",
      "Epoch 448/1000\n",
      "1460/1460 [==============================] - 0s 308us/step - loss: 1.1004 - mae: 1.4850 - val_loss: 49.4070 - val_mae: 50.0866\n",
      "Epoch 449/1000\n",
      "1460/1460 [==============================] - 0s 285us/step - loss: 1.1615 - mae: 1.5529 - val_loss: 48.7560 - val_mae: 49.4341\n",
      "Epoch 450/1000\n",
      "1460/1460 [==============================] - 0s 294us/step - loss: 1.2290 - mae: 1.6363 - val_loss: 49.8124 - val_mae: 50.4944\n",
      "Epoch 451/1000\n",
      "1460/1460 [==============================] - 0s 303us/step - loss: 0.8873 - mae: 1.2795 - val_loss: 49.1698 - val_mae: 49.8546\n",
      "Epoch 452/1000\n",
      "1460/1460 [==============================] - 0s 317us/step - loss: 0.8601 - mae: 1.2357 - val_loss: 48.8990 - val_mae: 49.5808\n",
      "Epoch 453/1000\n",
      "1460/1460 [==============================] - 0s 299us/step - loss: 1.0213 - mae: 1.4190 - val_loss: 50.4891 - val_mae: 51.1703\n",
      "Epoch 454/1000\n",
      "1460/1460 [==============================] - 0s 290us/step - loss: 0.9201 - mae: 1.2949 - val_loss: 49.9826 - val_mae: 50.6631\n",
      "Epoch 455/1000\n",
      "1460/1460 [==============================] - 0s 292us/step - loss: 0.9574 - mae: 1.3383 - val_loss: 49.3584 - val_mae: 50.0403\n",
      "Epoch 456/1000\n",
      "1460/1460 [==============================] - 0s 305us/step - loss: 0.9860 - mae: 1.3590 - val_loss: 49.7225 - val_mae: 50.4053\n",
      "Epoch 457/1000\n",
      "1460/1460 [==============================] - 0s 291us/step - loss: 0.9505 - mae: 1.3181 - val_loss: 49.8638 - val_mae: 50.5444\n",
      "Epoch 458/1000\n",
      "1460/1460 [==============================] - 0s 278us/step - loss: 0.8411 - mae: 1.2012 - val_loss: 50.4213 - val_mae: 51.1022\n",
      "Epoch 459/1000\n",
      "1460/1460 [==============================] - 0s 302us/step - loss: 0.8595 - mae: 1.2247 - val_loss: 50.0138 - val_mae: 50.6952\n",
      "Epoch 460/1000\n",
      "1460/1460 [==============================] - 0s 312us/step - loss: 0.9284 - mae: 1.3033 - val_loss: 50.2940 - val_mae: 50.9744\n",
      "Epoch 461/1000\n",
      "1460/1460 [==============================] - 0s 287us/step - loss: 1.0854 - mae: 1.4781 - val_loss: 50.6033 - val_mae: 51.2830\n",
      "Epoch 462/1000\n",
      "1460/1460 [==============================] - 1s 345us/step - loss: 1.0113 - mae: 1.4220 - val_loss: 49.5926 - val_mae: 50.2735\n",
      "Epoch 463/1000\n",
      "1460/1460 [==============================] - 0s 311us/step - loss: 0.7916 - mae: 1.1586 - val_loss: 48.7488 - val_mae: 49.4303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 464/1000\n",
      "1460/1460 [==============================] - 0s 293us/step - loss: 0.9271 - mae: 1.3160 - val_loss: 49.8178 - val_mae: 50.4986\n",
      "Epoch 465/1000\n",
      "1460/1460 [==============================] - 0s 298us/step - loss: 0.8829 - mae: 1.2562 - val_loss: 49.8719 - val_mae: 50.5546\n",
      "Epoch 466/1000\n",
      "1460/1460 [==============================] - 0s 288us/step - loss: 0.9272 - mae: 1.3000 - val_loss: 49.7694 - val_mae: 50.4477\n",
      "Epoch 467/1000\n",
      "1460/1460 [==============================] - 0s 292us/step - loss: 0.8130 - mae: 1.1825 - val_loss: 50.0868 - val_mae: 50.7671\n",
      "Epoch 468/1000\n",
      "1460/1460 [==============================] - 0s 305us/step - loss: 0.9420 - mae: 1.3079 - val_loss: 50.1341 - val_mae: 50.8123\n",
      "Epoch 469/1000\n",
      "1460/1460 [==============================] - 0s 318us/step - loss: 0.8750 - mae: 1.2377 - val_loss: 49.6919 - val_mae: 50.3725\n",
      "Epoch 470/1000\n",
      "1460/1460 [==============================] - 0s 301us/step - loss: 0.7690 - mae: 1.1263 - val_loss: 49.5046 - val_mae: 50.1854\n",
      "Epoch 471/1000\n",
      "1460/1460 [==============================] - 0s 286us/step - loss: 0.8888 - mae: 1.2598 - val_loss: 50.0210 - val_mae: 50.7020\n",
      "Epoch 472/1000\n",
      "1460/1460 [==============================] - 0s 278us/step - loss: 0.8878 - mae: 1.2604 - val_loss: 49.3439 - val_mae: 50.0243\n",
      "Epoch 473/1000\n",
      "1460/1460 [==============================] - 0s 309us/step - loss: 0.9077 - mae: 1.2844 - val_loss: 49.4017 - val_mae: 50.0821\n",
      "Epoch 474/1000\n",
      "1460/1460 [==============================] - 0s 278us/step - loss: 0.8534 - mae: 1.2019 - val_loss: 50.4920 - val_mae: 51.1718\n",
      "Epoch 475/1000\n",
      "1460/1460 [==============================] - 0s 286us/step - loss: 0.8784 - mae: 1.2435 - val_loss: 49.4482 - val_mae: 50.1289\n",
      "Epoch 476/1000\n",
      "1460/1460 [==============================] - 0s 283us/step - loss: 1.0324 - mae: 1.4161 - val_loss: 48.6279 - val_mae: 49.3103\n",
      "Epoch 477/1000\n",
      "1460/1460 [==============================] - 0s 289us/step - loss: 0.8986 - mae: 1.2792 - val_loss: 50.0742 - val_mae: 50.7558\n",
      "Epoch 478/1000\n",
      "1460/1460 [==============================] - 0s 302us/step - loss: 0.9595 - mae: 1.3379 - val_loss: 49.8272 - val_mae: 50.5068\n",
      "Epoch 479/1000\n",
      "1460/1460 [==============================] - 0s 293us/step - loss: 1.1566 - mae: 1.5662 - val_loss: 49.8069 - val_mae: 50.4899\n",
      "Epoch 480/1000\n",
      "1460/1460 [==============================] - 0s 291us/step - loss: 0.8826 - mae: 1.2432 - val_loss: 50.6179 - val_mae: 51.2989\n",
      "Epoch 481/1000\n",
      "1460/1460 [==============================] - 0s 283us/step - loss: 0.8512 - mae: 1.2061 - val_loss: 49.8946 - val_mae: 50.5732\n",
      "Epoch 482/1000\n",
      "1460/1460 [==============================] - 0s 283us/step - loss: 1.0076 - mae: 1.3904 - val_loss: 49.8706 - val_mae: 50.5523\n",
      "Epoch 483/1000\n",
      "1460/1460 [==============================] - 0s 294us/step - loss: 0.9664 - mae: 1.3403 - val_loss: 50.2655 - val_mae: 50.9465\n",
      "Epoch 484/1000\n",
      "1460/1460 [==============================] - 0s 292us/step - loss: 0.8384 - mae: 1.1982 - val_loss: 49.8063 - val_mae: 50.4884\n",
      "Epoch 485/1000\n",
      "1460/1460 [==============================] - 0s 279us/step - loss: 0.8183 - mae: 1.1767 - val_loss: 49.3369 - val_mae: 50.0180\n",
      "Epoch 486/1000\n",
      "1460/1460 [==============================] - 0s 280us/step - loss: 0.9498 - mae: 1.3222 - val_loss: 49.5342 - val_mae: 50.2184\n",
      "Epoch 487/1000\n",
      "1460/1460 [==============================] - 0s 290us/step - loss: 1.0068 - mae: 1.4028 - val_loss: 49.8780 - val_mae: 50.5598\n",
      "Epoch 488/1000\n",
      "1460/1460 [==============================] - 0s 279us/step - loss: 0.8565 - mae: 1.2284 - val_loss: 50.5477 - val_mae: 51.2254\n",
      "Epoch 489/1000\n",
      "1460/1460 [==============================] - 0s 288us/step - loss: 0.9379 - mae: 1.3048 - val_loss: 50.4480 - val_mae: 51.1304\n",
      "Epoch 490/1000\n",
      "1460/1460 [==============================] - 0s 279us/step - loss: 0.8312 - mae: 1.1991 - val_loss: 49.6987 - val_mae: 50.3825\n",
      "Epoch 491/1000\n",
      "1460/1460 [==============================] - 0s 286us/step - loss: 0.8543 - mae: 1.2138 - val_loss: 49.8984 - val_mae: 50.5785\n",
      "Epoch 492/1000\n",
      "1460/1460 [==============================] - 0s 277us/step - loss: 1.0062 - mae: 1.3901 - val_loss: 50.9434 - val_mae: 51.6226\n",
      "Epoch 493/1000\n",
      "1460/1460 [==============================] - 0s 282us/step - loss: 0.9078 - mae: 1.2788 - val_loss: 49.9208 - val_mae: 50.6022\n",
      "Epoch 494/1000\n",
      "1460/1460 [==============================] - 0s 269us/step - loss: 0.7863 - mae: 1.1621 - val_loss: 50.2414 - val_mae: 50.9218\n",
      "Epoch 495/1000\n",
      "1460/1460 [==============================] - 0s 286us/step - loss: 0.7895 - mae: 1.1498 - val_loss: 49.7662 - val_mae: 50.4503\n",
      "Epoch 496/1000\n",
      "1460/1460 [==============================] - 0s 282us/step - loss: 0.7709 - mae: 1.1266 - val_loss: 49.8145 - val_mae: 50.4971\n",
      "Epoch 497/1000\n",
      "1460/1460 [==============================] - 0s 284us/step - loss: 0.9460 - mae: 1.3124 - val_loss: 49.5741 - val_mae: 50.2525\n",
      "Epoch 498/1000\n",
      "1460/1460 [==============================] - 0s 280us/step - loss: 0.7800 - mae: 1.1319 - val_loss: 49.4558 - val_mae: 50.1391\n",
      "Epoch 499/1000\n",
      "1460/1460 [==============================] - 0s 278us/step - loss: 1.0939 - mae: 1.4851 - val_loss: 50.1623 - val_mae: 50.8415\n",
      "Epoch 500/1000\n",
      "1460/1460 [==============================] - 0s 282us/step - loss: 0.9845 - mae: 1.3751 - val_loss: 49.1335 - val_mae: 49.8146\n",
      "Epoch 501/1000\n",
      "1460/1460 [==============================] - 0s 283us/step - loss: 1.0551 - mae: 1.4441 - val_loss: 49.6074 - val_mae: 50.2860\n",
      "Epoch 502/1000\n",
      "1460/1460 [==============================] - 0s 283us/step - loss: 1.0133 - mae: 1.4006 - val_loss: 50.0693 - val_mae: 50.7525\n",
      "Epoch 503/1000\n",
      "1460/1460 [==============================] - 0s 283us/step - loss: 0.8423 - mae: 1.2119 - val_loss: 49.3568 - val_mae: 50.0390\n",
      "Epoch 504/1000\n",
      "1460/1460 [==============================] - 0s 286us/step - loss: 0.7574 - mae: 1.1114 - val_loss: 50.3073 - val_mae: 50.9902\n",
      "Epoch 505/1000\n",
      "1460/1460 [==============================] - 0s 279us/step - loss: 0.8680 - mae: 1.2435 - val_loss: 49.1524 - val_mae: 49.8331\n",
      "Epoch 506/1000\n",
      "1460/1460 [==============================] - 0s 285us/step - loss: 0.9318 - mae: 1.3136 - val_loss: 50.2613 - val_mae: 50.9436\n",
      "Epoch 507/1000\n",
      "1460/1460 [==============================] - 0s 281us/step - loss: 0.7934 - mae: 1.1495 - val_loss: 49.3973 - val_mae: 50.0765\n",
      "Epoch 508/1000\n",
      "1460/1460 [==============================] - 0s 282us/step - loss: 1.0862 - mae: 1.4681 - val_loss: 49.6621 - val_mae: 50.3458\n",
      "Epoch 509/1000\n",
      "1460/1460 [==============================] - 0s 282us/step - loss: 0.9098 - mae: 1.2970 - val_loss: 49.1774 - val_mae: 49.8582\n",
      "Epoch 510/1000\n",
      "1460/1460 [==============================] - 0s 280us/step - loss: 0.6970 - mae: 1.0429 - val_loss: 49.6408 - val_mae: 50.3222\n",
      "Epoch 511/1000\n",
      "1460/1460 [==============================] - 0s 283us/step - loss: 0.7382 - mae: 1.0836 - val_loss: 49.6889 - val_mae: 50.3710\n",
      "Epoch 512/1000\n",
      "1460/1460 [==============================] - 0s 282us/step - loss: 0.8096 - mae: 1.1673 - val_loss: 49.4838 - val_mae: 50.1647\n",
      "Epoch 513/1000\n",
      "1460/1460 [==============================] - 0s 286us/step - loss: 0.8558 - mae: 1.2229 - val_loss: 50.0699 - val_mae: 50.7495\n",
      "Epoch 514/1000\n",
      "1460/1460 [==============================] - 0s 323us/step - loss: 0.8928 - mae: 1.2731 - val_loss: 49.6088 - val_mae: 50.2885\n",
      "Epoch 515/1000\n",
      "1460/1460 [==============================] - 0s 280us/step - loss: 0.8208 - mae: 1.1686 - val_loss: 49.7486 - val_mae: 50.4304\n",
      "Epoch 516/1000\n",
      "1460/1460 [==============================] - 0s 296us/step - loss: 0.8793 - mae: 1.2485 - val_loss: 48.6317 - val_mae: 49.3127\n",
      "Epoch 517/1000\n",
      "1460/1460 [==============================] - 0s 309us/step - loss: 1.1294 - mae: 1.5197 - val_loss: 49.3160 - val_mae: 49.9960\n",
      "Epoch 518/1000\n",
      "1460/1460 [==============================] - 0s 284us/step - loss: 1.0059 - mae: 1.3980 - val_loss: 50.5054 - val_mae: 51.1868\n",
      "Epoch 519/1000\n",
      "1460/1460 [==============================] - 0s 295us/step - loss: 0.9056 - mae: 1.2697 - val_loss: 50.5823 - val_mae: 51.2638\n",
      "Epoch 520/1000\n",
      "1460/1460 [==============================] - 0s 278us/step - loss: 0.8482 - mae: 1.2080 - val_loss: 49.9106 - val_mae: 50.5898\n",
      "Epoch 521/1000\n",
      "1460/1460 [==============================] - 0s 277us/step - loss: 0.7749 - mae: 1.1080 - val_loss: 50.1899 - val_mae: 50.8709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 522/1000\n",
      "1460/1460 [==============================] - 0s 277us/step - loss: 0.7990 - mae: 1.1489 - val_loss: 49.7531 - val_mae: 50.4358\n",
      "Epoch 523/1000\n",
      "1460/1460 [==============================] - 0s 275us/step - loss: 0.8372 - mae: 1.2092 - val_loss: 49.8270 - val_mae: 50.5088\n",
      "Epoch 524/1000\n",
      "1460/1460 [==============================] - 0s 273us/step - loss: 0.9195 - mae: 1.2979 - val_loss: 49.3063 - val_mae: 49.9867\n",
      "Epoch 525/1000\n",
      "1460/1460 [==============================] - 0s 270us/step - loss: 0.6870 - mae: 1.0267 - val_loss: 50.3275 - val_mae: 51.0086\n",
      "Epoch 526/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.7576 - mae: 1.1035 - val_loss: 49.5788 - val_mae: 50.2603\n",
      "Epoch 527/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.9003 - mae: 1.2733 - val_loss: 49.6010 - val_mae: 50.2823\n",
      "Epoch 528/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 0.8040 - mae: 1.1622 - val_loss: 49.0752 - val_mae: 49.7530\n",
      "Epoch 529/1000\n",
      "1460/1460 [==============================] - 0s 270us/step - loss: 0.9907 - mae: 1.3601 - val_loss: 50.2786 - val_mae: 50.9584\n",
      "Epoch 530/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 0.7378 - mae: 1.0900 - val_loss: 49.6849 - val_mae: 50.3673\n",
      "Epoch 531/1000\n",
      "1460/1460 [==============================] - 0s 281us/step - loss: 0.8123 - mae: 1.1705 - val_loss: 48.9201 - val_mae: 49.6015\n",
      "Epoch 532/1000\n",
      "1460/1460 [==============================] - 0s 276us/step - loss: 0.8323 - mae: 1.2040 - val_loss: 50.0180 - val_mae: 50.7004\n",
      "Epoch 533/1000\n",
      "1460/1460 [==============================] - 0s 282us/step - loss: 0.7097 - mae: 1.0494 - val_loss: 49.1022 - val_mae: 49.7801\n",
      "Epoch 534/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.8332 - mae: 1.2038 - val_loss: 49.7343 - val_mae: 50.4170\n",
      "Epoch 535/1000\n",
      "1460/1460 [==============================] - 0s 266us/step - loss: 0.8558 - mae: 1.2352 - val_loss: 49.6401 - val_mae: 50.3211\n",
      "Epoch 536/1000\n",
      "1460/1460 [==============================] - 0s 270us/step - loss: 0.8813 - mae: 1.2567 - val_loss: 49.9261 - val_mae: 50.6089\n",
      "Epoch 537/1000\n",
      "1460/1460 [==============================] - 0s 267us/step - loss: 0.9959 - mae: 1.3812 - val_loss: 49.5903 - val_mae: 50.2729\n",
      "Epoch 538/1000\n",
      "1460/1460 [==============================] - 0s 280us/step - loss: 0.8913 - mae: 1.2650 - val_loss: 50.4723 - val_mae: 51.1536\n",
      "Epoch 539/1000\n",
      "1460/1460 [==============================] - 0s 294us/step - loss: 0.9265 - mae: 1.3043 - val_loss: 49.7224 - val_mae: 50.4029\n",
      "Epoch 540/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 0.8457 - mae: 1.2118 - val_loss: 49.7283 - val_mae: 50.4061\n",
      "Epoch 541/1000\n",
      "1460/1460 [==============================] - 0s 269us/step - loss: 0.7674 - mae: 1.1136 - val_loss: 50.2491 - val_mae: 50.9294\n",
      "Epoch 542/1000\n",
      "1460/1460 [==============================] - 0s 281us/step - loss: 0.9100 - mae: 1.2744 - val_loss: 49.7893 - val_mae: 50.4728\n",
      "Epoch 543/1000\n",
      "1460/1460 [==============================] - 0s 282us/step - loss: 0.9632 - mae: 1.3314 - val_loss: 50.3813 - val_mae: 51.0635\n",
      "Epoch 544/1000\n",
      "1460/1460 [==============================] - 0s 290us/step - loss: 0.8120 - mae: 1.1742 - val_loss: 50.2266 - val_mae: 50.9063\n",
      "Epoch 545/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 0.9126 - mae: 1.2906 - val_loss: 50.1345 - val_mae: 50.8175\n",
      "Epoch 546/1000\n",
      "1460/1460 [==============================] - 0s 280us/step - loss: 0.8915 - mae: 1.2611 - val_loss: 50.3037 - val_mae: 50.9830\n",
      "Epoch 547/1000\n",
      "1460/1460 [==============================] - 0s 281us/step - loss: 0.8784 - mae: 1.2406 - val_loss: 49.6220 - val_mae: 50.3034\n",
      "Epoch 548/1000\n",
      "1460/1460 [==============================] - 0s 277us/step - loss: 0.8859 - mae: 1.2517 - val_loss: 49.4653 - val_mae: 50.1459\n",
      "Epoch 549/1000\n",
      "1460/1460 [==============================] - 0s 280us/step - loss: 0.8389 - mae: 1.2013 - val_loss: 49.2760 - val_mae: 49.9589\n",
      "Epoch 550/1000\n",
      "1460/1460 [==============================] - 0s 285us/step - loss: 0.8451 - mae: 1.2055 - val_loss: 49.9701 - val_mae: 50.6503\n",
      "Epoch 551/1000\n",
      "1460/1460 [==============================] - 0s 280us/step - loss: 0.7729 - mae: 1.1189 - val_loss: 50.4732 - val_mae: 51.1555\n",
      "Epoch 552/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 0.7167 - mae: 1.0619 - val_loss: 49.7199 - val_mae: 50.4026\n",
      "Epoch 553/1000\n",
      "1460/1460 [==============================] - 0s 276us/step - loss: 0.8071 - mae: 1.1551 - val_loss: 49.7978 - val_mae: 50.4810\n",
      "Epoch 554/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.9675 - mae: 1.3660 - val_loss: 48.7772 - val_mae: 49.4601\n",
      "Epoch 555/1000\n",
      "1460/1460 [==============================] - 0s 276us/step - loss: 0.7205 - mae: 1.0711 - val_loss: 49.2132 - val_mae: 49.8955\n",
      "Epoch 556/1000\n",
      "1460/1460 [==============================] - 0s 297us/step - loss: 0.8991 - mae: 1.2842 - val_loss: 49.3801 - val_mae: 50.0603\n",
      "Epoch 557/1000\n",
      "1460/1460 [==============================] - 0s 279us/step - loss: 0.7339 - mae: 1.0935 - val_loss: 49.0017 - val_mae: 49.6874\n",
      "Epoch 558/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 0.8166 - mae: 1.1756 - val_loss: 49.1863 - val_mae: 49.8651\n",
      "Epoch 559/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.9165 - mae: 1.2861 - val_loss: 49.1231 - val_mae: 49.8049\n",
      "Epoch 560/1000\n",
      "1460/1460 [==============================] - 0s 273us/step - loss: 0.8303 - mae: 1.1951 - val_loss: 49.2318 - val_mae: 49.9144\n",
      "Epoch 561/1000\n",
      "1460/1460 [==============================] - 0s 273us/step - loss: 0.9592 - mae: 1.3323 - val_loss: 49.6504 - val_mae: 50.3353\n",
      "Epoch 562/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.7277 - mae: 1.0818 - val_loss: 50.2143 - val_mae: 50.8941\n",
      "Epoch 563/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 0.7281 - mae: 1.0736 - val_loss: 50.0050 - val_mae: 50.6879\n",
      "Epoch 564/1000\n",
      "1460/1460 [==============================] - 0s 276us/step - loss: 0.7878 - mae: 1.1305 - val_loss: 49.7246 - val_mae: 50.4043\n",
      "Epoch 565/1000\n",
      "1460/1460 [==============================] - 0s 275us/step - loss: 0.6651 - mae: 0.9983 - val_loss: 49.1815 - val_mae: 49.8637\n",
      "Epoch 566/1000\n",
      "1460/1460 [==============================] - 0s 275us/step - loss: 0.7271 - mae: 1.0789 - val_loss: 50.6674 - val_mae: 51.3473\n",
      "Epoch 567/1000\n",
      "1460/1460 [==============================] - 0s 273us/step - loss: 0.7581 - mae: 1.0996 - val_loss: 49.4170 - val_mae: 50.0964\n",
      "Epoch 568/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 0.7900 - mae: 1.1443 - val_loss: 49.9311 - val_mae: 50.6138\n",
      "Epoch 569/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 0.6352 - mae: 0.9750 - val_loss: 49.4667 - val_mae: 50.1461\n",
      "Epoch 570/1000\n",
      "1460/1460 [==============================] - 0s 276us/step - loss: 0.9178 - mae: 1.2729 - val_loss: 49.2114 - val_mae: 49.8913\n",
      "Epoch 571/1000\n",
      "1460/1460 [==============================] - 0s 277us/step - loss: 1.0079 - mae: 1.3909 - val_loss: 50.2004 - val_mae: 50.8803\n",
      "Epoch 572/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.6837 - mae: 1.0155 - val_loss: 49.4578 - val_mae: 50.1347\n",
      "Epoch 573/1000\n",
      "1460/1460 [==============================] - 0s 277us/step - loss: 0.7438 - mae: 1.0957 - val_loss: 49.8478 - val_mae: 50.5262\n",
      "Epoch 574/1000\n",
      "1460/1460 [==============================] - 0s 275us/step - loss: 0.7388 - mae: 1.0946 - val_loss: 49.7284 - val_mae: 50.4076\n",
      "Epoch 575/1000\n",
      "1460/1460 [==============================] - 0s 270us/step - loss: 0.8271 - mae: 1.1827 - val_loss: 49.6848 - val_mae: 50.3665\n",
      "Epoch 576/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.6781 - mae: 1.0254 - val_loss: 49.6915 - val_mae: 50.3718\n",
      "Epoch 577/1000\n",
      "1460/1460 [==============================] - 0s 276us/step - loss: 0.6767 - mae: 1.0001 - val_loss: 49.6678 - val_mae: 50.3478\n",
      "Epoch 578/1000\n",
      "1460/1460 [==============================] - 0s 270us/step - loss: 0.7857 - mae: 1.1390 - val_loss: 49.9037 - val_mae: 50.5842\n",
      "Epoch 579/1000\n",
      "1460/1460 [==============================] - 0s 270us/step - loss: 1.0503 - mae: 1.4398 - val_loss: 49.6495 - val_mae: 50.3297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 580/1000\n",
      "1460/1460 [==============================] - 0s 278us/step - loss: 0.6981 - mae: 1.0490 - val_loss: 48.8621 - val_mae: 49.5441\n",
      "Epoch 581/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 0.7222 - mae: 1.0656 - val_loss: 49.5992 - val_mae: 50.2798\n",
      "Epoch 582/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 0.8490 - mae: 1.2152 - val_loss: 50.3497 - val_mae: 51.0295\n",
      "Epoch 583/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 0.9231 - mae: 1.2849 - val_loss: 49.5860 - val_mae: 50.2660\n",
      "Epoch 584/1000\n",
      "1460/1460 [==============================] - 0s 268us/step - loss: 0.6941 - mae: 1.0416 - val_loss: 49.9306 - val_mae: 50.6093\n",
      "Epoch 585/1000\n",
      "1460/1460 [==============================] - 0s 275us/step - loss: 0.7626 - mae: 1.0991 - val_loss: 50.3335 - val_mae: 51.0146\n",
      "Epoch 586/1000\n",
      "1460/1460 [==============================] - 0s 270us/step - loss: 0.7366 - mae: 1.0895 - val_loss: 49.2925 - val_mae: 49.9731\n",
      "Epoch 587/1000\n",
      "1460/1460 [==============================] - 0s 270us/step - loss: 0.7689 - mae: 1.1283 - val_loss: 49.9872 - val_mae: 50.6679\n",
      "Epoch 588/1000\n",
      "1460/1460 [==============================] - 0s 270us/step - loss: 0.6791 - mae: 1.0304 - val_loss: 49.4782 - val_mae: 50.1616\n",
      "Epoch 589/1000\n",
      "1460/1460 [==============================] - 0s 267us/step - loss: 0.7733 - mae: 1.1234 - val_loss: 49.7449 - val_mae: 50.4249\n",
      "Epoch 590/1000\n",
      "1460/1460 [==============================] - 0s 284us/step - loss: 0.7238 - mae: 1.0696 - val_loss: 49.9283 - val_mae: 50.6110\n",
      "Epoch 591/1000\n",
      "1460/1460 [==============================] - 0s 294us/step - loss: 0.8391 - mae: 1.2140 - val_loss: 49.7183 - val_mae: 50.3967\n",
      "Epoch 592/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.6948 - mae: 1.0526 - val_loss: 49.3975 - val_mae: 50.0736\n",
      "Epoch 593/1000\n",
      "1460/1460 [==============================] - 0s 295us/step - loss: 0.7966 - mae: 1.1542 - val_loss: 50.0708 - val_mae: 50.7508\n",
      "Epoch 594/1000\n",
      "1460/1460 [==============================] - 0s 276us/step - loss: 0.7610 - mae: 1.1051 - val_loss: 49.5969 - val_mae: 50.2787\n",
      "Epoch 595/1000\n",
      "1460/1460 [==============================] - 0s 301us/step - loss: 0.6966 - mae: 1.0520 - val_loss: 49.4979 - val_mae: 50.1776\n",
      "Epoch 596/1000\n",
      "1460/1460 [==============================] - 0s 292us/step - loss: 0.6837 - mae: 1.0278 - val_loss: 50.0158 - val_mae: 50.6946\n",
      "Epoch 597/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.6824 - mae: 1.0098 - val_loss: 50.3896 - val_mae: 51.0714\n",
      "Epoch 598/1000\n",
      "1460/1460 [==============================] - 0s 275us/step - loss: 1.0285 - mae: 1.4134 - val_loss: 49.8329 - val_mae: 50.5148\n",
      "Epoch 599/1000\n",
      "1460/1460 [==============================] - 0s 268us/step - loss: 0.7080 - mae: 1.0612 - val_loss: 49.7042 - val_mae: 50.3835\n",
      "Epoch 600/1000\n",
      "1460/1460 [==============================] - 0s 277us/step - loss: 0.7493 - mae: 1.1011 - val_loss: 50.3995 - val_mae: 51.0800\n",
      "Epoch 601/1000\n",
      "1460/1460 [==============================] - 0s 270us/step - loss: 0.7762 - mae: 1.1215 - val_loss: 49.2158 - val_mae: 49.8979\n",
      "Epoch 602/1000\n",
      "1460/1460 [==============================] - 0s 269us/step - loss: 0.8556 - mae: 1.2133 - val_loss: 49.7756 - val_mae: 50.4586\n",
      "Epoch 603/1000\n",
      "1460/1460 [==============================] - 0s 268us/step - loss: 1.0225 - mae: 1.4040 - val_loss: 49.3084 - val_mae: 49.9879\n",
      "Epoch 604/1000\n",
      "1460/1460 [==============================] - 0s 280us/step - loss: 0.7188 - mae: 1.0550 - val_loss: 50.4041 - val_mae: 51.0838\n",
      "Epoch 605/1000\n",
      "1460/1460 [==============================] - 0s 279us/step - loss: 0.7558 - mae: 1.1123 - val_loss: 50.6261 - val_mae: 51.3065\n",
      "Epoch 606/1000\n",
      "1460/1460 [==============================] - 0s 309us/step - loss: 0.7777 - mae: 1.1269 - val_loss: 49.9465 - val_mae: 50.6241\n",
      "Epoch 607/1000\n",
      "1460/1460 [==============================] - 0s 275us/step - loss: 0.7443 - mae: 1.0963 - val_loss: 50.2789 - val_mae: 50.9585\n",
      "Epoch 608/1000\n",
      "1460/1460 [==============================] - 0s 273us/step - loss: 0.8093 - mae: 1.1770 - val_loss: 49.8997 - val_mae: 50.5813\n",
      "Epoch 609/1000\n",
      "1460/1460 [==============================] - 0s 278us/step - loss: 0.8754 - mae: 1.2551 - val_loss: 49.7597 - val_mae: 50.4411\n",
      "Epoch 610/1000\n",
      "1460/1460 [==============================] - 0s 275us/step - loss: 0.8944 - mae: 1.2569 - val_loss: 50.0751 - val_mae: 50.7572\n",
      "Epoch 611/1000\n",
      "1460/1460 [==============================] - 0s 275us/step - loss: 0.7615 - mae: 1.0996 - val_loss: 49.7253 - val_mae: 50.4063\n",
      "Epoch 612/1000\n",
      "1460/1460 [==============================] - 0s 275us/step - loss: 0.8420 - mae: 1.1807 - val_loss: 50.5759 - val_mae: 51.2593\n",
      "Epoch 613/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 0.7661 - mae: 1.1126 - val_loss: 49.3878 - val_mae: 50.0675\n",
      "Epoch 614/1000\n",
      "1460/1460 [==============================] - 0s 284us/step - loss: 0.9083 - mae: 1.2771 - val_loss: 50.8847 - val_mae: 51.5612\n",
      "Epoch 615/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 0.7531 - mae: 1.1068 - val_loss: 49.7304 - val_mae: 50.4119\n",
      "Epoch 616/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.7509 - mae: 1.0986 - val_loss: 49.5877 - val_mae: 50.2686\n",
      "Epoch 617/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 0.7436 - mae: 1.0907 - val_loss: 49.7027 - val_mae: 50.3812\n",
      "Epoch 618/1000\n",
      "1460/1460 [==============================] - 0s 323us/step - loss: 0.7572 - mae: 1.0991 - val_loss: 49.7191 - val_mae: 50.4021\n",
      "Epoch 619/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 0.5778 - mae: 0.8876 - val_loss: 50.4856 - val_mae: 51.1659\n",
      "Epoch 620/1000\n",
      "1460/1460 [==============================] - 0s 275us/step - loss: 0.7340 - mae: 1.0923 - val_loss: 50.4177 - val_mae: 51.0975\n",
      "Epoch 621/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 0.6488 - mae: 0.9852 - val_loss: 49.7397 - val_mae: 50.4169\n",
      "Epoch 622/1000\n",
      "1460/1460 [==============================] - 0s 276us/step - loss: 0.8726 - mae: 1.2343 - val_loss: 49.7895 - val_mae: 50.4704\n",
      "Epoch 623/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 0.7286 - mae: 1.0782 - val_loss: 49.9504 - val_mae: 50.6321\n",
      "Epoch 624/1000\n",
      "1460/1460 [==============================] - 0s 268us/step - loss: 0.8450 - mae: 1.2068 - val_loss: 49.5411 - val_mae: 50.2199\n",
      "Epoch 625/1000\n",
      "1460/1460 [==============================] - 0s 273us/step - loss: 0.8309 - mae: 1.1884 - val_loss: 48.9764 - val_mae: 49.6580\n",
      "Epoch 626/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.8273 - mae: 1.1857 - val_loss: 50.2463 - val_mae: 50.9261\n",
      "Epoch 627/1000\n",
      "1460/1460 [==============================] - 0s 273us/step - loss: 0.6916 - mae: 1.0277 - val_loss: 50.1835 - val_mae: 50.8650\n",
      "Epoch 628/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 0.7650 - mae: 1.1010 - val_loss: 50.1755 - val_mae: 50.8588\n",
      "Epoch 629/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 0.8029 - mae: 1.1618 - val_loss: 50.2640 - val_mae: 50.9430\n",
      "Epoch 630/1000\n",
      "1460/1460 [==============================] - 0s 273us/step - loss: 0.8874 - mae: 1.2594 - val_loss: 48.8438 - val_mae: 49.5257\n",
      "Epoch 631/1000\n",
      "1460/1460 [==============================] - 0s 267us/step - loss: 0.9667 - mae: 1.3605 - val_loss: 50.6660 - val_mae: 51.3466\n",
      "Epoch 632/1000\n",
      "1460/1460 [==============================] - 0s 276us/step - loss: 0.8362 - mae: 1.1863 - val_loss: 50.3027 - val_mae: 50.9834\n",
      "Epoch 633/1000\n",
      "1460/1460 [==============================] - 0s 269us/step - loss: 0.7974 - mae: 1.1542 - val_loss: 49.2669 - val_mae: 49.9499\n",
      "Epoch 634/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 0.7736 - mae: 1.1253 - val_loss: 49.6003 - val_mae: 50.2807\n",
      "Epoch 635/1000\n",
      "1460/1460 [==============================] - 0s 285us/step - loss: 0.7606 - mae: 1.0991 - val_loss: 49.8923 - val_mae: 50.5716\n",
      "Epoch 636/1000\n",
      "1460/1460 [==============================] - 0s 282us/step - loss: 0.9354 - mae: 1.3054 - val_loss: 50.1920 - val_mae: 50.8714\n",
      "Epoch 637/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 0.8100 - mae: 1.1544 - val_loss: 50.4066 - val_mae: 51.0871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 638/1000\n",
      "1460/1460 [==============================] - 0s 267us/step - loss: 0.9048 - mae: 1.2561 - val_loss: 50.0407 - val_mae: 50.7198\n",
      "Epoch 639/1000\n",
      "1460/1460 [==============================] - 0s 273us/step - loss: 0.8143 - mae: 1.1566 - val_loss: 49.9536 - val_mae: 50.6332\n",
      "Epoch 640/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 0.8037 - mae: 1.1602 - val_loss: 49.1089 - val_mae: 49.7890\n",
      "Epoch 641/1000\n",
      "1460/1460 [==============================] - 0s 266us/step - loss: 0.7444 - mae: 1.0923 - val_loss: 49.9373 - val_mae: 50.6180\n",
      "Epoch 642/1000\n",
      "1460/1460 [==============================] - 0s 268us/step - loss: 0.6338 - mae: 0.9640 - val_loss: 49.7782 - val_mae: 50.4574\n",
      "Epoch 643/1000\n",
      "1460/1460 [==============================] - 0s 265us/step - loss: 0.6041 - mae: 0.9242 - val_loss: 50.5958 - val_mae: 51.2770\n",
      "Epoch 644/1000\n",
      "1460/1460 [==============================] - 0s 276us/step - loss: 0.6648 - mae: 1.0174 - val_loss: 50.2099 - val_mae: 50.8897\n",
      "Epoch 645/1000\n",
      "1460/1460 [==============================] - 0s 275us/step - loss: 0.6538 - mae: 0.9920 - val_loss: 49.3412 - val_mae: 50.0224\n",
      "Epoch 646/1000\n",
      "1460/1460 [==============================] - 0s 268us/step - loss: 0.6913 - mae: 1.0357 - val_loss: 50.1313 - val_mae: 50.8113\n",
      "Epoch 647/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.6311 - mae: 0.9653 - val_loss: 49.9566 - val_mae: 50.6382\n",
      "Epoch 648/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.6276 - mae: 0.9588 - val_loss: 49.8283 - val_mae: 50.5084\n",
      "Epoch 649/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.8839 - mae: 1.2467 - val_loss: 49.8577 - val_mae: 50.5396\n",
      "Epoch 650/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 0.6681 - mae: 1.0026 - val_loss: 50.8532 - val_mae: 51.5332\n",
      "Epoch 651/1000\n",
      "1460/1460 [==============================] - 0s 269us/step - loss: 0.7055 - mae: 1.0460 - val_loss: 49.4247 - val_mae: 50.1057\n",
      "Epoch 652/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 0.6619 - mae: 0.9740 - val_loss: 49.4797 - val_mae: 50.1608\n",
      "Epoch 653/1000\n",
      "1460/1460 [==============================] - 0s 278us/step - loss: 1.0251 - mae: 1.4138 - val_loss: 50.1968 - val_mae: 50.8773\n",
      "Epoch 654/1000\n",
      "1460/1460 [==============================] - 0s 267us/step - loss: 0.8008 - mae: 1.1480 - val_loss: 49.6981 - val_mae: 50.3793\n",
      "Epoch 655/1000\n",
      "1460/1460 [==============================] - 0s 277us/step - loss: 0.6965 - mae: 1.0361 - val_loss: 50.3814 - val_mae: 51.0590\n",
      "Epoch 656/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 0.5921 - mae: 0.9125 - val_loss: 50.2185 - val_mae: 50.9010\n",
      "Epoch 657/1000\n",
      "1460/1460 [==============================] - 0s 269us/step - loss: 0.6595 - mae: 0.9843 - val_loss: 50.4823 - val_mae: 51.1640\n",
      "Epoch 658/1000\n",
      "1460/1460 [==============================] - 0s 270us/step - loss: 0.7120 - mae: 1.0565 - val_loss: 51.0171 - val_mae: 51.6917\n",
      "Epoch 659/1000\n",
      "1460/1460 [==============================] - 0s 269us/step - loss: 0.7850 - mae: 1.1397 - val_loss: 50.4078 - val_mae: 51.0903\n",
      "Epoch 660/1000\n",
      "1460/1460 [==============================] - 0s 270us/step - loss: 0.7959 - mae: 1.1506 - val_loss: 49.6144 - val_mae: 50.2969\n",
      "Epoch 661/1000\n",
      "1460/1460 [==============================] - 0s 273us/step - loss: 0.7731 - mae: 1.1303 - val_loss: 49.4736 - val_mae: 50.1544\n",
      "Epoch 662/1000\n",
      "1460/1460 [==============================] - 0s 265us/step - loss: 0.8815 - mae: 1.2312 - val_loss: 50.3121 - val_mae: 50.9921\n",
      "Epoch 663/1000\n",
      "1460/1460 [==============================] - 0s 270us/step - loss: 0.7770 - mae: 1.1214 - val_loss: 49.5618 - val_mae: 50.2424\n",
      "Epoch 664/1000\n",
      "1460/1460 [==============================] - 0s 270us/step - loss: 0.7033 - mae: 1.0451 - val_loss: 50.0341 - val_mae: 50.7155\n",
      "Epoch 665/1000\n",
      "1460/1460 [==============================] - 0s 267us/step - loss: 0.6918 - mae: 1.0238 - val_loss: 50.2967 - val_mae: 50.9748\n",
      "Epoch 666/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.6959 - mae: 1.0314 - val_loss: 48.7365 - val_mae: 49.4164\n",
      "Epoch 667/1000\n",
      "1460/1460 [==============================] - 0s 276us/step - loss: 0.8068 - mae: 1.1666 - val_loss: 50.3112 - val_mae: 50.9921\n",
      "Epoch 668/1000\n",
      "1460/1460 [==============================] - 0s 278us/step - loss: 0.6662 - mae: 1.0163 - val_loss: 49.9896 - val_mae: 50.6697\n",
      "Epoch 669/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 0.8892 - mae: 1.2604 - val_loss: 49.7999 - val_mae: 50.4800\n",
      "Epoch 670/1000\n",
      "1460/1460 [==============================] - 0s 281us/step - loss: 0.8893 - mae: 1.2323 - val_loss: 49.9655 - val_mae: 50.6466\n",
      "Epoch 671/1000\n",
      "1460/1460 [==============================] - 0s 293us/step - loss: 0.8684 - mae: 1.2378 - val_loss: 51.0492 - val_mae: 51.7289\n",
      "Epoch 672/1000\n",
      "1460/1460 [==============================] - 0s 270us/step - loss: 0.9082 - mae: 1.2774 - val_loss: 49.7678 - val_mae: 50.4472\n",
      "Epoch 673/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 0.7037 - mae: 1.0478 - val_loss: 50.0549 - val_mae: 50.7360\n",
      "Epoch 674/1000\n",
      "1460/1460 [==============================] - 0s 275us/step - loss: 0.6372 - mae: 0.9849 - val_loss: 49.5682 - val_mae: 50.2495\n",
      "Epoch 675/1000\n",
      "1460/1460 [==============================] - 0s 281us/step - loss: 0.6990 - mae: 1.0424 - val_loss: 49.6514 - val_mae: 50.3322\n",
      "Epoch 676/1000\n",
      "1460/1460 [==============================] - 0s 294us/step - loss: 0.5789 - mae: 0.9070 - val_loss: 50.0716 - val_mae: 50.7520\n",
      "Epoch 677/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 0.6625 - mae: 0.9905 - val_loss: 49.8057 - val_mae: 50.4859\n",
      "Epoch 678/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.7694 - mae: 1.1124 - val_loss: 50.7730 - val_mae: 51.4540\n",
      "Epoch 679/1000\n",
      "1460/1460 [==============================] - 0s 269us/step - loss: 0.9169 - mae: 1.2938 - val_loss: 49.6358 - val_mae: 50.3163\n",
      "Epoch 680/1000\n",
      "1460/1460 [==============================] - 0s 276us/step - loss: 0.7385 - mae: 1.0894 - val_loss: 50.5319 - val_mae: 51.2099\n",
      "Epoch 681/1000\n",
      "1460/1460 [==============================] - 0s 289us/step - loss: 0.8375 - mae: 1.2025 - val_loss: 50.6359 - val_mae: 51.3159\n",
      "Epoch 682/1000\n",
      "1460/1460 [==============================] - 0s 286us/step - loss: 0.7325 - mae: 1.0859 - val_loss: 50.4006 - val_mae: 51.0795\n",
      "Epoch 683/1000\n",
      "1460/1460 [==============================] - 0s 267us/step - loss: 0.7146 - mae: 1.0495 - val_loss: 49.6841 - val_mae: 50.3624\n",
      "Epoch 684/1000\n",
      "1460/1460 [==============================] - 0s 264us/step - loss: 0.7054 - mae: 1.0303 - val_loss: 49.2725 - val_mae: 49.9543\n",
      "Epoch 685/1000\n",
      "1460/1460 [==============================] - 0s 306us/step - loss: 0.7213 - mae: 1.0598 - val_loss: 49.7372 - val_mae: 50.4193\n",
      "Epoch 686/1000\n",
      "1460/1460 [==============================] - 0s 266us/step - loss: 0.7447 - mae: 1.0863 - val_loss: 49.6115 - val_mae: 50.2918\n",
      "Epoch 687/1000\n",
      "1460/1460 [==============================] - 0s 266us/step - loss: 0.6584 - mae: 0.9896 - val_loss: 50.0762 - val_mae: 50.7593\n",
      "Epoch 688/1000\n",
      "1460/1460 [==============================] - 0s 268us/step - loss: 0.6001 - mae: 0.9178 - val_loss: 49.8073 - val_mae: 50.4875\n",
      "Epoch 689/1000\n",
      "1460/1460 [==============================] - 0s 282us/step - loss: 0.7887 - mae: 1.1436 - val_loss: 49.6301 - val_mae: 50.3105\n",
      "Epoch 690/1000\n",
      "1460/1460 [==============================] - 0s 292us/step - loss: 0.7390 - mae: 1.0744 - val_loss: 49.1365 - val_mae: 49.8187\n",
      "Epoch 691/1000\n",
      "1460/1460 [==============================] - 0s 293us/step - loss: 0.7014 - mae: 1.0554 - val_loss: 50.3864 - val_mae: 51.0616\n",
      "Epoch 692/1000\n",
      "1460/1460 [==============================] - 0s 296us/step - loss: 0.5959 - mae: 0.9377 - val_loss: 49.9855 - val_mae: 50.6669\n",
      "Epoch 693/1000\n",
      "1460/1460 [==============================] - 0s 293us/step - loss: 0.6597 - mae: 0.9934 - val_loss: 49.6628 - val_mae: 50.3432\n",
      "Epoch 694/1000\n",
      "1460/1460 [==============================] - 0s 296us/step - loss: 0.9256 - mae: 1.3018 - val_loss: 49.6902 - val_mae: 50.3695\n",
      "Epoch 695/1000\n",
      "1460/1460 [==============================] - 0s 309us/step - loss: 0.7326 - mae: 1.0790 - val_loss: 50.5529 - val_mae: 51.2355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 696/1000\n",
      "1460/1460 [==============================] - 0s 294us/step - loss: 0.6725 - mae: 1.0182 - val_loss: 49.3894 - val_mae: 50.0718\n",
      "Epoch 697/1000\n",
      "1460/1460 [==============================] - 0s 296us/step - loss: 0.9065 - mae: 1.2732 - val_loss: 50.6095 - val_mae: 51.2919\n",
      "Epoch 698/1000\n",
      "1460/1460 [==============================] - 0s 287us/step - loss: 0.9887 - mae: 1.3780 - val_loss: 50.4671 - val_mae: 51.1479\n",
      "Epoch 699/1000\n",
      "1460/1460 [==============================] - 0s 303us/step - loss: 0.7299 - mae: 1.0901 - val_loss: 50.3617 - val_mae: 51.0421\n",
      "Epoch 700/1000\n",
      "1460/1460 [==============================] - 0s 289us/step - loss: 0.7649 - mae: 1.1125 - val_loss: 49.8086 - val_mae: 50.4872\n",
      "Epoch 701/1000\n",
      "1460/1460 [==============================] - 0s 287us/step - loss: 0.7279 - mae: 1.0729 - val_loss: 50.3850 - val_mae: 51.0647\n",
      "Epoch 702/1000\n",
      "1460/1460 [==============================] - 0s 278us/step - loss: 0.5744 - mae: 0.9081 - val_loss: 49.7014 - val_mae: 50.3816\n",
      "Epoch 703/1000\n",
      "1460/1460 [==============================] - 0s 293us/step - loss: 0.6477 - mae: 0.9781 - val_loss: 49.7582 - val_mae: 50.4379\n",
      "Epoch 704/1000\n",
      "1460/1460 [==============================] - 0s 294us/step - loss: 0.6428 - mae: 0.9824 - val_loss: 50.6165 - val_mae: 51.2967\n",
      "Epoch 705/1000\n",
      "1460/1460 [==============================] - 0s 316us/step - loss: 0.7446 - mae: 1.0997 - val_loss: 50.2359 - val_mae: 50.9175\n",
      "Epoch 706/1000\n",
      "1460/1460 [==============================] - 0s 302us/step - loss: 0.7191 - mae: 1.0697 - val_loss: 50.3980 - val_mae: 51.0769\n",
      "Epoch 707/1000\n",
      "1460/1460 [==============================] - 0s 277us/step - loss: 0.7282 - mae: 1.0817 - val_loss: 49.9860 - val_mae: 50.6664\n",
      "Epoch 708/1000\n",
      "1460/1460 [==============================] - 0s 297us/step - loss: 0.6985 - mae: 1.0551 - val_loss: 49.5746 - val_mae: 50.2542\n",
      "Epoch 709/1000\n",
      "1460/1460 [==============================] - 0s 287us/step - loss: 0.6237 - mae: 0.9538 - val_loss: 49.7466 - val_mae: 50.4255\n",
      "Epoch 710/1000\n",
      "1460/1460 [==============================] - 0s 299us/step - loss: 0.6419 - mae: 0.9806 - val_loss: 50.2968 - val_mae: 50.9781\n",
      "Epoch 711/1000\n",
      "1460/1460 [==============================] - 0s 289us/step - loss: 0.6338 - mae: 0.9591 - val_loss: 50.1138 - val_mae: 50.7946\n",
      "Epoch 712/1000\n",
      "1460/1460 [==============================] - 0s 282us/step - loss: 0.7350 - mae: 1.0869 - val_loss: 49.8250 - val_mae: 50.5067\n",
      "Epoch 713/1000\n",
      "1460/1460 [==============================] - 0s 298us/step - loss: 0.7009 - mae: 1.0236 - val_loss: 49.8399 - val_mae: 50.5195\n",
      "Epoch 714/1000\n",
      "1460/1460 [==============================] - 0s 312us/step - loss: 0.6455 - mae: 0.9602 - val_loss: 49.8993 - val_mae: 50.5769\n",
      "Epoch 715/1000\n",
      "1460/1460 [==============================] - 0s 297us/step - loss: 0.6323 - mae: 0.9649 - val_loss: 50.2161 - val_mae: 50.8985\n",
      "Epoch 716/1000\n",
      "1460/1460 [==============================] - 0s 292us/step - loss: 0.7277 - mae: 1.0752 - val_loss: 49.6966 - val_mae: 50.3778\n",
      "Epoch 717/1000\n",
      "1460/1460 [==============================] - 0s 282us/step - loss: 0.5695 - mae: 0.9021 - val_loss: 50.0350 - val_mae: 50.7155\n",
      "Epoch 718/1000\n",
      "1460/1460 [==============================] - 0s 285us/step - loss: 0.5929 - mae: 0.9173 - val_loss: 49.8215 - val_mae: 50.5028\n",
      "Epoch 719/1000\n",
      "1460/1460 [==============================] - 0s 282us/step - loss: 0.5809 - mae: 0.9038 - val_loss: 50.2511 - val_mae: 50.9323\n",
      "Epoch 720/1000\n",
      "1460/1460 [==============================] - 0s 290us/step - loss: 0.6566 - mae: 0.9863 - val_loss: 50.1547 - val_mae: 50.8349\n",
      "Epoch 721/1000\n",
      "1460/1460 [==============================] - 0s 292us/step - loss: 0.8312 - mae: 1.1813 - val_loss: 50.4181 - val_mae: 51.0933\n",
      "Epoch 722/1000\n",
      "1460/1460 [==============================] - 0s 289us/step - loss: 0.6951 - mae: 1.0320 - val_loss: 50.2205 - val_mae: 50.9009\n",
      "Epoch 723/1000\n",
      "1460/1460 [==============================] - 0s 289us/step - loss: 0.8344 - mae: 1.2113 - val_loss: 50.2856 - val_mae: 50.9673\n",
      "Epoch 724/1000\n",
      "1460/1460 [==============================] - 0s 297us/step - loss: 0.6378 - mae: 0.9789 - val_loss: 50.4846 - val_mae: 51.1651\n",
      "Epoch 725/1000\n",
      "1460/1460 [==============================] - 0s 292us/step - loss: 0.6215 - mae: 0.9539 - val_loss: 50.2757 - val_mae: 50.9572\n",
      "Epoch 726/1000\n",
      "1460/1460 [==============================] - 0s 284us/step - loss: 0.6664 - mae: 0.9975 - val_loss: 50.6038 - val_mae: 51.2850\n",
      "Epoch 727/1000\n",
      "1460/1460 [==============================] - 0s 306us/step - loss: 0.6684 - mae: 0.9975 - val_loss: 49.8929 - val_mae: 50.5731\n",
      "Epoch 728/1000\n",
      "1460/1460 [==============================] - 0s 305us/step - loss: 0.6938 - mae: 1.0257 - val_loss: 50.0992 - val_mae: 50.7795\n",
      "Epoch 729/1000\n",
      "1460/1460 [==============================] - 0s 295us/step - loss: 0.6687 - mae: 1.0073 - val_loss: 49.9714 - val_mae: 50.6505\n",
      "Epoch 730/1000\n",
      "1460/1460 [==============================] - 0s 283us/step - loss: 0.6318 - mae: 0.9545 - val_loss: 49.7963 - val_mae: 50.4775\n",
      "Epoch 731/1000\n",
      "1460/1460 [==============================] - 0s 309us/step - loss: 0.7257 - mae: 1.0797 - val_loss: 50.2163 - val_mae: 50.8966\n",
      "Epoch 732/1000\n",
      "1460/1460 [==============================] - 0s 284us/step - loss: 0.5259 - mae: 0.8332 - val_loss: 50.1433 - val_mae: 50.8244\n",
      "Epoch 733/1000\n",
      "1460/1460 [==============================] - 0s 302us/step - loss: 0.5051 - mae: 0.8131 - val_loss: 50.7138 - val_mae: 51.3901\n",
      "Epoch 734/1000\n",
      "1460/1460 [==============================] - 0s 285us/step - loss: 0.7276 - mae: 1.0670 - val_loss: 49.6744 - val_mae: 50.3553\n",
      "Epoch 735/1000\n",
      "1460/1460 [==============================] - 0s 292us/step - loss: 0.6710 - mae: 1.0037 - val_loss: 50.5077 - val_mae: 51.1877\n",
      "Epoch 736/1000\n",
      "1460/1460 [==============================] - 0s 291us/step - loss: 0.5742 - mae: 0.8790 - val_loss: 50.4288 - val_mae: 51.1087\n",
      "Epoch 737/1000\n",
      "1460/1460 [==============================] - 0s 284us/step - loss: 0.6455 - mae: 0.9675 - val_loss: 50.0183 - val_mae: 50.6984\n",
      "Epoch 738/1000\n",
      "1460/1460 [==============================] - 0s 280us/step - loss: 0.6564 - mae: 0.9940 - val_loss: 50.4785 - val_mae: 51.1592\n",
      "Epoch 739/1000\n",
      "1460/1460 [==============================] - 0s 288us/step - loss: 0.9015 - mae: 1.2681 - val_loss: 50.1623 - val_mae: 50.8435\n",
      "Epoch 740/1000\n",
      "1460/1460 [==============================] - 0s 276us/step - loss: 0.6249 - mae: 0.9624 - val_loss: 50.0737 - val_mae: 50.7544\n",
      "Epoch 741/1000\n",
      "1460/1460 [==============================] - 0s 284us/step - loss: 0.6939 - mae: 1.0219 - val_loss: 50.8141 - val_mae: 51.4924\n",
      "Epoch 742/1000\n",
      "1460/1460 [==============================] - 0s 285us/step - loss: 0.6644 - mae: 1.0015 - val_loss: 50.6788 - val_mae: 51.3591\n",
      "Epoch 743/1000\n",
      "1460/1460 [==============================] - 0s 282us/step - loss: 0.8133 - mae: 1.1643 - val_loss: 50.7690 - val_mae: 51.4502\n",
      "Epoch 744/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 0.7988 - mae: 1.1588 - val_loss: 50.4974 - val_mae: 51.1790\n",
      "Epoch 745/1000\n",
      "1460/1460 [==============================] - 0s 276us/step - loss: 0.6440 - mae: 0.9741 - val_loss: 49.8142 - val_mae: 50.4939\n",
      "Epoch 746/1000\n",
      "1460/1460 [==============================] - 0s 297us/step - loss: 0.6477 - mae: 0.9860 - val_loss: 50.2147 - val_mae: 50.8950\n",
      "Epoch 747/1000\n",
      "1460/1460 [==============================] - 0s 284us/step - loss: 0.6586 - mae: 0.9919 - val_loss: 50.6858 - val_mae: 51.3650\n",
      "Epoch 748/1000\n",
      "1460/1460 [==============================] - 0s 288us/step - loss: 0.6452 - mae: 0.9706 - val_loss: 50.4518 - val_mae: 51.1313\n",
      "Epoch 749/1000\n",
      "1460/1460 [==============================] - 0s 290us/step - loss: 0.6354 - mae: 0.9728 - val_loss: 49.7422 - val_mae: 50.4220\n",
      "Epoch 750/1000\n",
      "1460/1460 [==============================] - 0s 290us/step - loss: 0.7163 - mae: 1.0666 - val_loss: 49.7547 - val_mae: 50.4362\n",
      "Epoch 751/1000\n",
      "1460/1460 [==============================] - 0s 319us/step - loss: 0.7993 - mae: 1.1747 - val_loss: 50.4279 - val_mae: 51.1073\n",
      "Epoch 752/1000\n",
      "1460/1460 [==============================] - 0s 308us/step - loss: 0.7538 - mae: 1.0926 - val_loss: 50.0249 - val_mae: 50.7059\n",
      "Epoch 753/1000\n",
      "1460/1460 [==============================] - 0s 290us/step - loss: 0.6813 - mae: 1.0168 - val_loss: 50.3303 - val_mae: 51.0105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 754/1000\n",
      "1460/1460 [==============================] - 0s 286us/step - loss: 0.5194 - mae: 0.8224 - val_loss: 50.5186 - val_mae: 51.1998\n",
      "Epoch 755/1000\n",
      "1460/1460 [==============================] - 0s 288us/step - loss: 0.6184 - mae: 0.9529 - val_loss: 50.0849 - val_mae: 50.7638\n",
      "Epoch 756/1000\n",
      "1460/1460 [==============================] - 0s 284us/step - loss: 0.7042 - mae: 1.0388 - val_loss: 50.6653 - val_mae: 51.3422\n",
      "Epoch 757/1000\n",
      "1460/1460 [==============================] - 0s 286us/step - loss: 0.7738 - mae: 1.1295 - val_loss: 49.8741 - val_mae: 50.5529\n",
      "Epoch 758/1000\n",
      "1460/1460 [==============================] - 0s 288us/step - loss: 0.7438 - mae: 1.0815 - val_loss: 50.1485 - val_mae: 50.8256\n",
      "Epoch 759/1000\n",
      "1460/1460 [==============================] - 0s 292us/step - loss: 0.9777 - mae: 1.3505 - val_loss: 49.4673 - val_mae: 50.1477\n",
      "Epoch 760/1000\n",
      "1460/1460 [==============================] - 0s 291us/step - loss: 0.6573 - mae: 1.0016 - val_loss: 49.3906 - val_mae: 50.0712\n",
      "Epoch 761/1000\n",
      "1460/1460 [==============================] - 0s 288us/step - loss: 0.7878 - mae: 1.1378 - val_loss: 49.3106 - val_mae: 49.9891\n",
      "Epoch 762/1000\n",
      "1460/1460 [==============================] - 0s 292us/step - loss: 0.7227 - mae: 1.0741 - val_loss: 49.9014 - val_mae: 50.5826\n",
      "Epoch 763/1000\n",
      "1460/1460 [==============================] - 0s 292us/step - loss: 0.7488 - mae: 1.0860 - val_loss: 50.2745 - val_mae: 50.9556\n",
      "Epoch 764/1000\n",
      "1460/1460 [==============================] - 0s 290us/step - loss: 0.8410 - mae: 1.2029 - val_loss: 50.6118 - val_mae: 51.2904\n",
      "Epoch 765/1000\n",
      "1460/1460 [==============================] - 0s 293us/step - loss: 0.7396 - mae: 1.0880 - val_loss: 49.8012 - val_mae: 50.4828\n",
      "Epoch 766/1000\n",
      "1460/1460 [==============================] - 0s 299us/step - loss: 0.5874 - mae: 0.8950 - val_loss: 49.6613 - val_mae: 50.3434\n",
      "Epoch 767/1000\n",
      "1460/1460 [==============================] - 0s 291us/step - loss: 0.6342 - mae: 0.9664 - val_loss: 49.6502 - val_mae: 50.3284\n",
      "Epoch 768/1000\n",
      "1460/1460 [==============================] - 0s 289us/step - loss: 0.5849 - mae: 0.9064 - val_loss: 49.5819 - val_mae: 50.2626\n",
      "Epoch 769/1000\n",
      "1460/1460 [==============================] - 0s 291us/step - loss: 0.8241 - mae: 1.1831 - val_loss: 49.8379 - val_mae: 50.5165\n",
      "Epoch 770/1000\n",
      "1460/1460 [==============================] - 0s 290us/step - loss: 0.7048 - mae: 1.0378 - val_loss: 49.7667 - val_mae: 50.4498\n",
      "Epoch 771/1000\n",
      "1460/1460 [==============================] - 0s 291us/step - loss: 0.7126 - mae: 1.0587 - val_loss: 49.8105 - val_mae: 50.4925\n",
      "Epoch 772/1000\n",
      "1460/1460 [==============================] - 0s 287us/step - loss: 0.9166 - mae: 1.2948 - val_loss: 49.8537 - val_mae: 50.5332\n",
      "Epoch 773/1000\n",
      "1460/1460 [==============================] - 0s 282us/step - loss: 0.6554 - mae: 0.9950 - val_loss: 50.0661 - val_mae: 50.7475\n",
      "Epoch 774/1000\n",
      "1460/1460 [==============================] - 0s 270us/step - loss: 0.6697 - mae: 1.0068 - val_loss: 49.7539 - val_mae: 50.4326\n",
      "Epoch 775/1000\n",
      "1460/1460 [==============================] - 0s 270us/step - loss: 0.9622 - mae: 1.3430 - val_loss: 49.6688 - val_mae: 50.3492\n",
      "Epoch 776/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.6887 - mae: 1.0280 - val_loss: 50.1173 - val_mae: 50.7957\n",
      "Epoch 777/1000\n",
      "1460/1460 [==============================] - 0s 270us/step - loss: 0.6049 - mae: 0.9346 - val_loss: 50.6530 - val_mae: 51.3308\n",
      "Epoch 778/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 0.7792 - mae: 1.1335 - val_loss: 49.7918 - val_mae: 50.4723\n",
      "Epoch 779/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 0.6286 - mae: 0.9602 - val_loss: 49.9816 - val_mae: 50.6621\n",
      "Epoch 780/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.7256 - mae: 1.0777 - val_loss: 50.2702 - val_mae: 50.9523\n",
      "Epoch 781/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.6923 - mae: 1.0318 - val_loss: 51.1126 - val_mae: 51.7902\n",
      "Epoch 782/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 0.8924 - mae: 1.2569 - val_loss: 50.6591 - val_mae: 51.3352\n",
      "Epoch 783/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.6065 - mae: 0.9229 - val_loss: 50.1735 - val_mae: 50.8548\n",
      "Epoch 784/1000\n",
      "1460/1460 [==============================] - 0s 287us/step - loss: 0.5366 - mae: 0.8340 - val_loss: 50.0206 - val_mae: 50.7014\n",
      "Epoch 785/1000\n",
      "1460/1460 [==============================] - 0s 293us/step - loss: 0.5463 - mae: 0.8634 - val_loss: 50.4006 - val_mae: 51.0790\n",
      "Epoch 786/1000\n",
      "1460/1460 [==============================] - 0s 287us/step - loss: 0.5844 - mae: 0.9003 - val_loss: 49.7668 - val_mae: 50.4478\n",
      "Epoch 787/1000\n",
      "1460/1460 [==============================] - 0s 286us/step - loss: 0.6214 - mae: 0.9473 - val_loss: 50.2131 - val_mae: 50.8936\n",
      "Epoch 788/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 0.6198 - mae: 0.9582 - val_loss: 49.8491 - val_mae: 50.5281\n",
      "Epoch 789/1000\n",
      "1460/1460 [==============================] - 0s 285us/step - loss: 0.7563 - mae: 1.0928 - val_loss: 50.9041 - val_mae: 51.5821\n",
      "Epoch 790/1000\n",
      "1460/1460 [==============================] - 0s 293us/step - loss: 0.6586 - mae: 0.9711 - val_loss: 50.3303 - val_mae: 51.0075\n",
      "Epoch 791/1000\n",
      "1460/1460 [==============================] - 0s 275us/step - loss: 0.6624 - mae: 1.0078 - val_loss: 50.3030 - val_mae: 50.9830\n",
      "Epoch 792/1000\n",
      "1460/1460 [==============================] - 0s 270us/step - loss: 0.6535 - mae: 0.9936 - val_loss: 49.9819 - val_mae: 50.6625\n",
      "Epoch 793/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 0.7327 - mae: 1.0743 - val_loss: 50.3370 - val_mae: 51.0168\n",
      "Epoch 794/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 0.7288 - mae: 1.0656 - val_loss: 50.6889 - val_mae: 51.3690\n",
      "Epoch 795/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 0.5860 - mae: 0.9166 - val_loss: 50.2878 - val_mae: 50.9698\n",
      "Epoch 796/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 0.5388 - mae: 0.8439 - val_loss: 50.4168 - val_mae: 51.0959\n",
      "Epoch 797/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.6350 - mae: 0.9625 - val_loss: 50.1227 - val_mae: 50.8012\n",
      "Epoch 798/1000\n",
      "1460/1460 [==============================] - 0s 273us/step - loss: 0.6311 - mae: 0.9643 - val_loss: 49.6449 - val_mae: 50.3267\n",
      "Epoch 799/1000\n",
      "1460/1460 [==============================] - 0s 277us/step - loss: 0.5859 - mae: 0.8991 - val_loss: 49.9791 - val_mae: 50.6606\n",
      "Epoch 800/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.5483 - mae: 0.8670 - val_loss: 50.1385 - val_mae: 50.8162\n",
      "Epoch 801/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.6050 - mae: 0.9282 - val_loss: 49.8465 - val_mae: 50.5273\n",
      "Epoch 802/1000\n",
      "1460/1460 [==============================] - 0s 273us/step - loss: 0.5693 - mae: 0.9007 - val_loss: 49.6116 - val_mae: 50.2938\n",
      "Epoch 803/1000\n",
      "1460/1460 [==============================] - 0s 284us/step - loss: 0.6626 - mae: 1.0091 - val_loss: 50.3795 - val_mae: 51.0608\n",
      "Epoch 804/1000\n",
      "1460/1460 [==============================] - 0s 291us/step - loss: 0.5872 - mae: 0.9074 - val_loss: 50.1351 - val_mae: 50.8138\n",
      "Epoch 805/1000\n",
      "1460/1460 [==============================] - 0s 303us/step - loss: 0.6423 - mae: 0.9780 - val_loss: 50.1496 - val_mae: 50.8295\n",
      "Epoch 806/1000\n",
      "1460/1460 [==============================] - 0s 300us/step - loss: 0.6723 - mae: 0.9939 - val_loss: 50.2253 - val_mae: 50.9059\n",
      "Epoch 807/1000\n",
      "1460/1460 [==============================] - 0s 293us/step - loss: 0.6774 - mae: 1.0050 - val_loss: 50.1051 - val_mae: 50.7850\n",
      "Epoch 808/1000\n",
      "1460/1460 [==============================] - 0s 307us/step - loss: 0.5276 - mae: 0.8336 - val_loss: 50.2895 - val_mae: 50.9710\n",
      "Epoch 809/1000\n",
      "1460/1460 [==============================] - 0s 294us/step - loss: 0.5915 - mae: 0.9051 - val_loss: 49.6264 - val_mae: 50.3071\n",
      "Epoch 810/1000\n",
      "1460/1460 [==============================] - 0s 294us/step - loss: 0.7418 - mae: 1.0881 - val_loss: 49.4784 - val_mae: 50.1584\n",
      "Epoch 811/1000\n",
      "1460/1460 [==============================] - 0s 283us/step - loss: 0.5264 - mae: 0.8441 - val_loss: 50.0149 - val_mae: 50.6943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 812/1000\n",
      "1460/1460 [==============================] - 0s 286us/step - loss: 0.5181 - mae: 0.8236 - val_loss: 50.4570 - val_mae: 51.1377\n",
      "Epoch 813/1000\n",
      "1460/1460 [==============================] - 0s 283us/step - loss: 0.5596 - mae: 0.8792 - val_loss: 49.9026 - val_mae: 50.5820\n",
      "Epoch 814/1000\n",
      "1460/1460 [==============================] - 0s 282us/step - loss: 0.5743 - mae: 0.8972 - val_loss: 50.3867 - val_mae: 51.0647\n",
      "Epoch 815/1000\n",
      "1460/1460 [==============================] - 0s 278us/step - loss: 0.7657 - mae: 1.1205 - val_loss: 50.5862 - val_mae: 51.2646\n",
      "Epoch 816/1000\n",
      "1460/1460 [==============================] - 0s 285us/step - loss: 0.6181 - mae: 0.9439 - val_loss: 49.9638 - val_mae: 50.6430\n",
      "Epoch 817/1000\n",
      "1460/1460 [==============================] - 0s 281us/step - loss: 0.5597 - mae: 0.8682 - val_loss: 50.4460 - val_mae: 51.1229\n",
      "Epoch 818/1000\n",
      "1460/1460 [==============================] - 0s 281us/step - loss: 0.4831 - mae: 0.7870 - val_loss: 50.1867 - val_mae: 50.8642\n",
      "Epoch 819/1000\n",
      "1460/1460 [==============================] - 0s 276us/step - loss: 0.5280 - mae: 0.8350 - val_loss: 50.2748 - val_mae: 50.9552\n",
      "Epoch 820/1000\n",
      "1460/1460 [==============================] - 0s 284us/step - loss: 0.5282 - mae: 0.8281 - val_loss: 50.3682 - val_mae: 51.0498\n",
      "Epoch 821/1000\n",
      "1460/1460 [==============================] - 0s 288us/step - loss: 0.6820 - mae: 1.0254 - val_loss: 50.5130 - val_mae: 51.1922\n",
      "Epoch 822/1000\n",
      "1460/1460 [==============================] - 0s 282us/step - loss: 0.6473 - mae: 0.9639 - val_loss: 50.4907 - val_mae: 51.1712\n",
      "Epoch 823/1000\n",
      "1460/1460 [==============================] - 0s 288us/step - loss: 0.5262 - mae: 0.8341 - val_loss: 50.6427 - val_mae: 51.3198\n",
      "Epoch 824/1000\n",
      "1460/1460 [==============================] - 0s 284us/step - loss: 0.6040 - mae: 0.9328 - val_loss: 50.4096 - val_mae: 51.0882\n",
      "Epoch 825/1000\n",
      "1460/1460 [==============================] - 0s 285us/step - loss: 0.6770 - mae: 1.0178 - val_loss: 50.5212 - val_mae: 51.2012\n",
      "Epoch 826/1000\n",
      "1460/1460 [==============================] - 0s 284us/step - loss: 0.4559 - mae: 0.7584 - val_loss: 50.0824 - val_mae: 50.7623\n",
      "Epoch 827/1000\n",
      "1460/1460 [==============================] - 0s 294us/step - loss: 0.7011 - mae: 1.0205 - val_loss: 50.6860 - val_mae: 51.3657\n",
      "Epoch 828/1000\n",
      "1460/1460 [==============================] - 0s 316us/step - loss: 0.5361 - mae: 0.8489 - val_loss: 50.2817 - val_mae: 50.9623\n",
      "Epoch 829/1000\n",
      "1460/1460 [==============================] - 0s 292us/step - loss: 0.8075 - mae: 1.1638 - val_loss: 50.7139 - val_mae: 51.3948\n",
      "Epoch 830/1000\n",
      "1460/1460 [==============================] - 0s 287us/step - loss: 0.6679 - mae: 1.0008 - val_loss: 50.3093 - val_mae: 50.9867\n",
      "Epoch 831/1000\n",
      "1460/1460 [==============================] - 0s 284us/step - loss: 0.5854 - mae: 0.9236 - val_loss: 49.3497 - val_mae: 50.0307\n",
      "Epoch 832/1000\n",
      "1460/1460 [==============================] - 0s 286us/step - loss: 0.6174 - mae: 0.9423 - val_loss: 50.0664 - val_mae: 50.7452\n",
      "Epoch 833/1000\n",
      "1460/1460 [==============================] - 0s 287us/step - loss: 0.6590 - mae: 1.0004 - val_loss: 50.1120 - val_mae: 50.7905\n",
      "Epoch 834/1000\n",
      "1460/1460 [==============================] - 0s 288us/step - loss: 0.6661 - mae: 0.9751 - val_loss: 50.5735 - val_mae: 51.2554\n",
      "Epoch 835/1000\n",
      "1460/1460 [==============================] - 0s 289us/step - loss: 0.6831 - mae: 1.0124 - val_loss: 50.3112 - val_mae: 50.9927\n",
      "Epoch 836/1000\n",
      "1460/1460 [==============================] - 0s 276us/step - loss: 0.7058 - mae: 1.0444 - val_loss: 49.9989 - val_mae: 50.6786\n",
      "Epoch 837/1000\n",
      "1460/1460 [==============================] - 0s 287us/step - loss: 0.5429 - mae: 0.8759 - val_loss: 49.6384 - val_mae: 50.3188\n",
      "Epoch 838/1000\n",
      "1460/1460 [==============================] - 0s 291us/step - loss: 0.6519 - mae: 0.9912 - val_loss: 50.6273 - val_mae: 51.3015\n",
      "Epoch 839/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 0.5865 - mae: 0.9146 - val_loss: 50.7087 - val_mae: 51.3874\n",
      "Epoch 840/1000\n",
      "1460/1460 [==============================] - 0s 268us/step - loss: 0.6215 - mae: 0.9446 - val_loss: 50.6912 - val_mae: 51.3687\n",
      "Epoch 841/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 0.5706 - mae: 0.8852 - val_loss: 49.8576 - val_mae: 50.5354\n",
      "Epoch 842/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.6498 - mae: 0.9830 - val_loss: 50.2247 - val_mae: 50.9018\n",
      "Epoch 843/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 0.6014 - mae: 0.9243 - val_loss: 50.9041 - val_mae: 51.5795\n",
      "Epoch 844/1000\n",
      "1460/1460 [==============================] - 0s 269us/step - loss: 0.6920 - mae: 1.0340 - val_loss: 50.8339 - val_mae: 51.5092\n",
      "Epoch 845/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.7030 - mae: 1.0547 - val_loss: 51.7017 - val_mae: 52.3781\n",
      "Epoch 846/1000\n",
      "1460/1460 [==============================] - 0s 273us/step - loss: 0.7797 - mae: 1.1369 - val_loss: 50.0623 - val_mae: 50.7435\n",
      "Epoch 847/1000\n",
      "1460/1460 [==============================] - 0s 289us/step - loss: 0.5598 - mae: 0.8722 - val_loss: 49.8034 - val_mae: 50.4838\n",
      "Epoch 848/1000\n",
      "1460/1460 [==============================] - 0s 289us/step - loss: 0.6158 - mae: 0.9277 - val_loss: 50.2510 - val_mae: 50.9317\n",
      "Epoch 849/1000\n",
      "1460/1460 [==============================] - 0s 284us/step - loss: 0.6227 - mae: 0.9570 - val_loss: 50.3292 - val_mae: 51.0087\n",
      "Epoch 850/1000\n",
      "1460/1460 [==============================] - 0s 280us/step - loss: 0.7411 - mae: 1.0814 - val_loss: 49.8068 - val_mae: 50.4865\n",
      "Epoch 851/1000\n",
      "1460/1460 [==============================] - 0s 286us/step - loss: 0.7040 - mae: 1.0567 - val_loss: 50.6193 - val_mae: 51.2947\n",
      "Epoch 852/1000\n",
      "1460/1460 [==============================] - 0s 280us/step - loss: 0.4910 - mae: 0.8137 - val_loss: 50.1314 - val_mae: 50.8104\n",
      "Epoch 853/1000\n",
      "1460/1460 [==============================] - 0s 310us/step - loss: 0.4769 - mae: 0.7716 - val_loss: 50.0803 - val_mae: 50.7606\n",
      "Epoch 854/1000\n",
      "1460/1460 [==============================] - 0s 287us/step - loss: 0.5317 - mae: 0.8534 - val_loss: 49.5187 - val_mae: 50.2003\n",
      "Epoch 855/1000\n",
      "1460/1460 [==============================] - 0s 283us/step - loss: 0.6371 - mae: 0.9624 - val_loss: 50.3219 - val_mae: 51.0001\n",
      "Epoch 856/1000\n",
      "1460/1460 [==============================] - 0s 278us/step - loss: 0.5800 - mae: 0.8951 - val_loss: 49.9396 - val_mae: 50.6174\n",
      "Epoch 857/1000\n",
      "1460/1460 [==============================] - 0s 276us/step - loss: 0.5466 - mae: 0.8455 - val_loss: 50.0345 - val_mae: 50.7148\n",
      "Epoch 858/1000\n",
      "1460/1460 [==============================] - 0s 282us/step - loss: 0.5808 - mae: 0.8891 - val_loss: 50.3494 - val_mae: 51.0281\n",
      "Epoch 859/1000\n",
      "1460/1460 [==============================] - 0s 275us/step - loss: 0.5282 - mae: 0.8324 - val_loss: 49.7515 - val_mae: 50.4320\n",
      "Epoch 860/1000\n",
      "1460/1460 [==============================] - 0s 281us/step - loss: 0.6817 - mae: 1.0073 - val_loss: 50.5994 - val_mae: 51.2788\n",
      "Epoch 861/1000\n",
      "1460/1460 [==============================] - 0s 283us/step - loss: 0.4777 - mae: 0.7820 - val_loss: 50.6722 - val_mae: 51.3529\n",
      "Epoch 862/1000\n",
      "1460/1460 [==============================] - 0s 282us/step - loss: 0.5837 - mae: 0.8999 - val_loss: 50.4717 - val_mae: 51.1494\n",
      "Epoch 863/1000\n",
      "1460/1460 [==============================] - 0s 279us/step - loss: 0.6837 - mae: 1.0135 - val_loss: 50.8862 - val_mae: 51.5612\n",
      "Epoch 864/1000\n",
      "1460/1460 [==============================] - 0s 285us/step - loss: 0.8639 - mae: 1.2412 - val_loss: 49.5575 - val_mae: 50.2384\n",
      "Epoch 865/1000\n",
      "1460/1460 [==============================] - 0s 281us/step - loss: 0.8390 - mae: 1.2094 - val_loss: 50.5877 - val_mae: 51.2646\n",
      "Epoch 866/1000\n",
      "1460/1460 [==============================] - 0s 288us/step - loss: 0.6212 - mae: 0.9446 - val_loss: 50.3108 - val_mae: 50.9890\n",
      "Epoch 867/1000\n",
      "1460/1460 [==============================] - 0s 312us/step - loss: 0.5478 - mae: 0.8416 - val_loss: 50.4612 - val_mae: 51.1394\n",
      "Epoch 868/1000\n",
      "1460/1460 [==============================] - 0s 281us/step - loss: 0.5555 - mae: 0.8678 - val_loss: 50.3596 - val_mae: 51.0386\n",
      "Epoch 869/1000\n",
      "1460/1460 [==============================] - 0s 283us/step - loss: 0.5957 - mae: 0.9200 - val_loss: 50.6119 - val_mae: 51.2883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 870/1000\n",
      "1460/1460 [==============================] - 0s 292us/step - loss: 0.7158 - mae: 1.0669 - val_loss: 50.0608 - val_mae: 50.7392\n",
      "Epoch 871/1000\n",
      "1460/1460 [==============================] - 0s 292us/step - loss: 0.6169 - mae: 0.9507 - val_loss: 50.3801 - val_mae: 51.0564\n",
      "Epoch 872/1000\n",
      "1460/1460 [==============================] - 0s 294us/step - loss: 0.5038 - mae: 0.8100 - val_loss: 50.6264 - val_mae: 51.3029\n",
      "Epoch 873/1000\n",
      "1460/1460 [==============================] - 0s 291us/step - loss: 0.7392 - mae: 1.0904 - val_loss: 50.3164 - val_mae: 50.9970\n",
      "Epoch 874/1000\n",
      "1460/1460 [==============================] - 0s 285us/step - loss: 0.4778 - mae: 0.7773 - val_loss: 49.9978 - val_mae: 50.6791\n",
      "Epoch 875/1000\n",
      "1460/1460 [==============================] - 0s 301us/step - loss: 0.5618 - mae: 0.8785 - val_loss: 50.2560 - val_mae: 50.9349\n",
      "Epoch 876/1000\n",
      "1460/1460 [==============================] - 0s 295us/step - loss: 0.5902 - mae: 0.9126 - val_loss: 49.2958 - val_mae: 49.9754\n",
      "Epoch 877/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.6610 - mae: 1.0071 - val_loss: 50.1170 - val_mae: 50.7962\n",
      "Epoch 878/1000\n",
      "1460/1460 [==============================] - 0s 289us/step - loss: 0.4789 - mae: 0.7906 - val_loss: 50.2644 - val_mae: 50.9440\n",
      "Epoch 879/1000\n",
      "1460/1460 [==============================] - 0s 293us/step - loss: 0.5181 - mae: 0.8328 - val_loss: 50.1673 - val_mae: 50.8470\n",
      "Epoch 880/1000\n",
      "1460/1460 [==============================] - 0s 293us/step - loss: 0.7372 - mae: 1.0922 - val_loss: 49.9869 - val_mae: 50.6663\n",
      "Epoch 881/1000\n",
      "1460/1460 [==============================] - 0s 300us/step - loss: 0.7012 - mae: 1.0414 - val_loss: 49.7442 - val_mae: 50.4231\n",
      "Epoch 882/1000\n",
      "1460/1460 [==============================] - 0s 308us/step - loss: 0.7652 - mae: 1.1277 - val_loss: 50.2329 - val_mae: 50.9106\n",
      "Epoch 883/1000\n",
      "1460/1460 [==============================] - 0s 307us/step - loss: 0.6315 - mae: 0.9526 - val_loss: 49.8174 - val_mae: 50.4988\n",
      "Epoch 884/1000\n",
      "1460/1460 [==============================] - 0s 306us/step - loss: 0.4767 - mae: 0.7928 - val_loss: 49.6360 - val_mae: 50.3165\n",
      "Epoch 885/1000\n",
      "1460/1460 [==============================] - 0s 285us/step - loss: 0.5725 - mae: 0.8998 - val_loss: 50.3092 - val_mae: 50.9874\n",
      "Epoch 886/1000\n",
      "1460/1460 [==============================] - 0s 284us/step - loss: 0.5232 - mae: 0.8204 - val_loss: 50.2655 - val_mae: 50.9434\n",
      "Epoch 887/1000\n",
      "1460/1460 [==============================] - 0s 290us/step - loss: 0.5498 - mae: 0.8661 - val_loss: 50.0436 - val_mae: 50.7235\n",
      "Epoch 888/1000\n",
      "1460/1460 [==============================] - 0s 298us/step - loss: 0.4602 - mae: 0.7473 - val_loss: 50.3160 - val_mae: 50.9956\n",
      "Epoch 889/1000\n",
      "1460/1460 [==============================] - 0s 289us/step - loss: 0.5586 - mae: 0.8715 - val_loss: 49.9347 - val_mae: 50.6142\n",
      "Epoch 890/1000\n",
      "1460/1460 [==============================] - 0s 280us/step - loss: 0.5440 - mae: 0.8667 - val_loss: 49.8947 - val_mae: 50.5739\n",
      "Epoch 891/1000\n",
      "1460/1460 [==============================] - 0s 280us/step - loss: 0.6899 - mae: 1.0406 - val_loss: 50.0213 - val_mae: 50.6967\n",
      "Epoch 892/1000\n",
      "1460/1460 [==============================] - 0s 279us/step - loss: 0.6043 - mae: 0.9307 - val_loss: 50.0968 - val_mae: 50.7769\n",
      "Epoch 893/1000\n",
      "1460/1460 [==============================] - 0s 279us/step - loss: 0.5209 - mae: 0.8273 - val_loss: 50.4601 - val_mae: 51.1364\n",
      "Epoch 894/1000\n",
      "1460/1460 [==============================] - 0s 290us/step - loss: 0.5531 - mae: 0.8486 - val_loss: 49.9442 - val_mae: 50.6254\n",
      "Epoch 895/1000\n",
      "1460/1460 [==============================] - 0s 322us/step - loss: 0.6236 - mae: 0.9420 - val_loss: 50.3320 - val_mae: 51.0102\n",
      "Epoch 896/1000\n",
      "1460/1460 [==============================] - 0s 287us/step - loss: 0.6436 - mae: 0.9711 - val_loss: 50.6297 - val_mae: 51.3066\n",
      "Epoch 897/1000\n",
      "1460/1460 [==============================] - 0s 292us/step - loss: 0.6361 - mae: 0.9740 - val_loss: 49.8153 - val_mae: 50.4953\n",
      "Epoch 898/1000\n",
      "1460/1460 [==============================] - 0s 285us/step - loss: 0.5223 - mae: 0.8275 - val_loss: 50.0826 - val_mae: 50.7598\n",
      "Epoch 899/1000\n",
      "1460/1460 [==============================] - 0s 285us/step - loss: 0.5213 - mae: 0.8297 - val_loss: 50.5192 - val_mae: 51.1964\n",
      "Epoch 900/1000\n",
      "1460/1460 [==============================] - 0s 285us/step - loss: 0.5308 - mae: 0.8326 - val_loss: 49.5400 - val_mae: 50.2198\n",
      "Epoch 901/1000\n",
      "1460/1460 [==============================] - 0s 290us/step - loss: 0.6450 - mae: 0.9675 - val_loss: 50.4720 - val_mae: 51.1482\n",
      "Epoch 902/1000\n",
      "1460/1460 [==============================] - 0s 284us/step - loss: 0.6118 - mae: 0.9519 - val_loss: 50.4298 - val_mae: 51.1048\n",
      "Epoch 903/1000\n",
      "1460/1460 [==============================] - 0s 289us/step - loss: 0.4851 - mae: 0.7813 - val_loss: 50.3677 - val_mae: 51.0443\n",
      "Epoch 904/1000\n",
      "1460/1460 [==============================] - 0s 304us/step - loss: 0.5445 - mae: 0.8408 - val_loss: 50.5333 - val_mae: 51.2083\n",
      "Epoch 905/1000\n",
      "1460/1460 [==============================] - 0s 303us/step - loss: 0.5353 - mae: 0.8455 - val_loss: 50.1008 - val_mae: 50.7819\n",
      "Epoch 906/1000\n",
      "1460/1460 [==============================] - 0s 280us/step - loss: 0.4515 - mae: 0.7445 - val_loss: 50.5373 - val_mae: 51.2132\n",
      "Epoch 907/1000\n",
      "1460/1460 [==============================] - 0s 287us/step - loss: 0.5290 - mae: 0.8301 - val_loss: 50.2864 - val_mae: 50.9647\n",
      "Epoch 908/1000\n",
      "1460/1460 [==============================] - 0s 282us/step - loss: 0.4978 - mae: 0.8056 - val_loss: 49.7501 - val_mae: 50.4292\n",
      "Epoch 909/1000\n",
      "1460/1460 [==============================] - 0s 298us/step - loss: 0.6171 - mae: 0.9394 - val_loss: 48.7073 - val_mae: 49.3882\n",
      "Epoch 910/1000\n",
      "1460/1460 [==============================] - 0s 298us/step - loss: 0.6895 - mae: 1.0304 - val_loss: 50.5796 - val_mae: 51.2581\n",
      "Epoch 911/1000\n",
      "1460/1460 [==============================] - 0s 295us/step - loss: 0.4707 - mae: 0.7687 - val_loss: 50.0439 - val_mae: 50.7230\n",
      "Epoch 912/1000\n",
      "1460/1460 [==============================] - 0s 296us/step - loss: 0.5323 - mae: 0.8263 - val_loss: 50.2608 - val_mae: 50.9382\n",
      "Epoch 913/1000\n",
      "1460/1460 [==============================] - 0s 307us/step - loss: 0.4816 - mae: 0.7883 - val_loss: 50.2344 - val_mae: 50.9113\n",
      "Epoch 914/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.5186 - mae: 0.8363 - val_loss: 50.1831 - val_mae: 50.8623\n",
      "Epoch 915/1000\n",
      "1460/1460 [==============================] - 0s 281us/step - loss: 0.5548 - mae: 0.8731 - val_loss: 50.2072 - val_mae: 50.8840\n",
      "Epoch 916/1000\n",
      "1460/1460 [==============================] - 0s 273us/step - loss: 0.6483 - mae: 0.9700 - val_loss: 50.0430 - val_mae: 50.7220\n",
      "Epoch 917/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.4838 - mae: 0.7892 - val_loss: 50.2222 - val_mae: 50.8981\n",
      "Epoch 918/1000\n",
      "1460/1460 [==============================] - 0s 278us/step - loss: 0.5570 - mae: 0.8678 - val_loss: 49.8760 - val_mae: 50.5535\n",
      "Epoch 919/1000\n",
      "1460/1460 [==============================] - 0s 277us/step - loss: 0.4580 - mae: 0.7495 - val_loss: 50.6978 - val_mae: 51.3769\n",
      "Epoch 920/1000\n",
      "1460/1460 [==============================] - 0s 276us/step - loss: 0.5613 - mae: 0.8862 - val_loss: 49.5438 - val_mae: 50.2235\n",
      "Epoch 921/1000\n",
      "1460/1460 [==============================] - 0s 279us/step - loss: 0.5716 - mae: 0.8911 - val_loss: 50.1828 - val_mae: 50.8623\n",
      "Epoch 922/1000\n",
      "1460/1460 [==============================] - 0s 275us/step - loss: 0.6701 - mae: 0.9955 - val_loss: 50.2157 - val_mae: 50.8920\n",
      "Epoch 923/1000\n",
      "1460/1460 [==============================] - 0s 273us/step - loss: 0.5426 - mae: 0.8749 - val_loss: 49.8757 - val_mae: 50.5517\n",
      "Epoch 924/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 0.5637 - mae: 0.8819 - val_loss: 49.7291 - val_mae: 50.4091\n",
      "Epoch 925/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.7104 - mae: 1.0437 - val_loss: 50.0396 - val_mae: 50.7160\n",
      "Epoch 926/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 0.6102 - mae: 0.9446 - val_loss: 49.7220 - val_mae: 50.4012\n",
      "Epoch 927/1000\n",
      "1460/1460 [==============================] - 0s 273us/step - loss: 0.5352 - mae: 0.8504 - val_loss: 50.1648 - val_mae: 50.8424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 928/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 0.4697 - mae: 0.7700 - val_loss: 49.7925 - val_mae: 50.4730\n",
      "Epoch 929/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.7348 - mae: 1.0950 - val_loss: 50.3553 - val_mae: 51.0303\n",
      "Epoch 930/1000\n",
      "1460/1460 [==============================] - 0s 273us/step - loss: 0.5897 - mae: 0.9140 - val_loss: 50.1066 - val_mae: 50.7852\n",
      "Epoch 931/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.5248 - mae: 0.8393 - val_loss: 50.2314 - val_mae: 50.9127\n",
      "Epoch 932/1000\n",
      "1460/1460 [==============================] - 0s 273us/step - loss: 0.5039 - mae: 0.8171 - val_loss: 49.8786 - val_mae: 50.5554\n",
      "Epoch 933/1000\n",
      "1460/1460 [==============================] - 0s 275us/step - loss: 0.6073 - mae: 0.9334 - val_loss: 49.8545 - val_mae: 50.5331\n",
      "Epoch 934/1000\n",
      "1460/1460 [==============================] - 0s 270us/step - loss: 0.4861 - mae: 0.7835 - val_loss: 50.4586 - val_mae: 51.1316\n",
      "Epoch 935/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 0.4923 - mae: 0.7879 - val_loss: 49.5677 - val_mae: 50.2490\n",
      "Epoch 936/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.4470 - mae: 0.7411 - val_loss: 49.7656 - val_mae: 50.4463\n",
      "Epoch 937/1000\n",
      "1460/1460 [==============================] - 0s 270us/step - loss: 0.5709 - mae: 0.8762 - val_loss: 49.8717 - val_mae: 50.5520\n",
      "Epoch 938/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 0.5618 - mae: 0.8892 - val_loss: 49.9789 - val_mae: 50.6579\n",
      "Epoch 939/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 0.4397 - mae: 0.7300 - val_loss: 49.7758 - val_mae: 50.4560\n",
      "Epoch 940/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 0.4960 - mae: 0.7911 - val_loss: 50.4532 - val_mae: 51.1327\n",
      "Epoch 941/1000\n",
      "1460/1460 [==============================] - 0s 299us/step - loss: 0.6593 - mae: 1.0024 - val_loss: 49.7948 - val_mae: 50.4761\n",
      "Epoch 942/1000\n",
      "1460/1460 [==============================] - 0s 273us/step - loss: 0.6460 - mae: 0.9773 - val_loss: 49.5775 - val_mae: 50.2590\n",
      "Epoch 943/1000\n",
      "1460/1460 [==============================] - 0s 287us/step - loss: 0.6143 - mae: 0.9371 - val_loss: 50.0388 - val_mae: 50.7178\n",
      "Epoch 944/1000\n",
      "1460/1460 [==============================] - 0s 316us/step - loss: 0.6672 - mae: 1.0016 - val_loss: 50.0101 - val_mae: 50.6878\n",
      "Epoch 945/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.6527 - mae: 0.9943 - val_loss: 50.1758 - val_mae: 50.8553\n",
      "Epoch 946/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.5485 - mae: 0.8695 - val_loss: 50.2700 - val_mae: 50.9481\n",
      "Epoch 947/1000\n",
      "1460/1460 [==============================] - 0s 306us/step - loss: 0.5940 - mae: 0.9244 - val_loss: 50.3066 - val_mae: 50.9848\n",
      "Epoch 948/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 0.5623 - mae: 0.8842 - val_loss: 50.2441 - val_mae: 50.9248\n",
      "Epoch 949/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.6115 - mae: 0.9359 - val_loss: 50.1347 - val_mae: 50.8152\n",
      "Epoch 950/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 0.7077 - mae: 1.0581 - val_loss: 50.1945 - val_mae: 50.8744\n",
      "Epoch 951/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.5560 - mae: 0.8768 - val_loss: 49.9304 - val_mae: 50.6094\n",
      "Epoch 952/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 0.6171 - mae: 0.9528 - val_loss: 49.9892 - val_mae: 50.6685\n",
      "Epoch 953/1000\n",
      "1460/1460 [==============================] - 0s 267us/step - loss: 0.6099 - mae: 0.9308 - val_loss: 50.2555 - val_mae: 50.9344\n",
      "Epoch 954/1000\n",
      "1460/1460 [==============================] - 0s 268us/step - loss: 0.5271 - mae: 0.8377 - val_loss: 50.5784 - val_mae: 51.2537\n",
      "Epoch 955/1000\n",
      "1460/1460 [==============================] - 0s 267us/step - loss: 0.5504 - mae: 0.8716 - val_loss: 49.8975 - val_mae: 50.5754\n",
      "Epoch 956/1000\n",
      "1460/1460 [==============================] - 0s 269us/step - loss: 0.5416 - mae: 0.8508 - val_loss: 50.2679 - val_mae: 50.9458\n",
      "Epoch 957/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 0.5353 - mae: 0.8606 - val_loss: 50.3191 - val_mae: 50.9950\n",
      "Epoch 958/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 0.5637 - mae: 0.8646 - val_loss: 50.2768 - val_mae: 50.9572\n",
      "Epoch 959/1000\n",
      "1460/1460 [==============================] - 0s 275us/step - loss: 0.4474 - mae: 0.7365 - val_loss: 50.0930 - val_mae: 50.7745\n",
      "Epoch 960/1000\n",
      "1460/1460 [==============================] - 0s 268us/step - loss: 0.5259 - mae: 0.8444 - val_loss: 50.0561 - val_mae: 50.7322\n",
      "Epoch 961/1000\n",
      "1460/1460 [==============================] - 0s 275us/step - loss: 0.5567 - mae: 0.8777 - val_loss: 50.1972 - val_mae: 50.8767\n",
      "Epoch 962/1000\n",
      "1460/1460 [==============================] - 0s 273us/step - loss: 0.5675 - mae: 0.8910 - val_loss: 49.6849 - val_mae: 50.3657\n",
      "Epoch 963/1000\n",
      "1460/1460 [==============================] - 0s 272us/step - loss: 0.5231 - mae: 0.8401 - val_loss: 50.0369 - val_mae: 50.7156\n",
      "Epoch 964/1000\n",
      "1460/1460 [==============================] - 0s 268us/step - loss: 0.5067 - mae: 0.8149 - val_loss: 50.0359 - val_mae: 50.7126\n",
      "Epoch 965/1000\n",
      "1460/1460 [==============================] - 0s 271us/step - loss: 0.4214 - mae: 0.7270 - val_loss: 50.2172 - val_mae: 50.8951\n",
      "Epoch 966/1000\n",
      "1460/1460 [==============================] - 0s 274us/step - loss: 0.4281 - mae: 0.7207 - val_loss: 50.2208 - val_mae: 50.8983\n",
      "Epoch 967/1000\n",
      "1460/1460 [==============================] - 0s 288us/step - loss: 0.4439 - mae: 0.7227 - val_loss: 50.0312 - val_mae: 50.7062\n",
      "Epoch 968/1000\n",
      "1460/1460 [==============================] - 0s 288us/step - loss: 0.5510 - mae: 0.8680 - val_loss: 49.9665 - val_mae: 50.6471\n",
      "Epoch 969/1000\n",
      "1460/1460 [==============================] - 0s 303us/step - loss: 0.5121 - mae: 0.8117 - val_loss: 49.4200 - val_mae: 50.0990\n",
      "Epoch 970/1000\n",
      "1460/1460 [==============================] - 0s 301us/step - loss: 0.7696 - mae: 1.1049 - val_loss: 49.3209 - val_mae: 49.9995\n",
      "Epoch 971/1000\n",
      "1460/1460 [==============================] - 0s 293us/step - loss: 0.9354 - mae: 1.3065 - val_loss: 49.4611 - val_mae: 50.1429\n",
      "Epoch 972/1000\n",
      "1460/1460 [==============================] - 0s 285us/step - loss: 0.6070 - mae: 0.9213 - val_loss: 50.5723 - val_mae: 51.2457\n",
      "Epoch 973/1000\n",
      "1460/1460 [==============================] - 0s 293us/step - loss: 0.4807 - mae: 0.7836 - val_loss: 50.3389 - val_mae: 51.0178\n",
      "Epoch 974/1000\n",
      "1460/1460 [==============================] - 0s 289us/step - loss: 0.5380 - mae: 0.8526 - val_loss: 49.7053 - val_mae: 50.3851\n",
      "Epoch 975/1000\n",
      "1460/1460 [==============================] - 0s 281us/step - loss: 0.5227 - mae: 0.8242 - val_loss: 50.3950 - val_mae: 51.0686\n",
      "Epoch 976/1000\n",
      "1460/1460 [==============================] - 0s 288us/step - loss: 0.5458 - mae: 0.8347 - val_loss: 50.2838 - val_mae: 50.9602\n",
      "Epoch 977/1000\n",
      "1460/1460 [==============================] - 0s 284us/step - loss: 0.5136 - mae: 0.7928 - val_loss: 50.4145 - val_mae: 51.0885\n",
      "Epoch 978/1000\n",
      "1460/1460 [==============================] - 0s 292us/step - loss: 0.5161 - mae: 0.8097 - val_loss: 50.8947 - val_mae: 51.5690\n",
      "Epoch 979/1000\n",
      "1460/1460 [==============================] - 0s 281us/step - loss: 0.5148 - mae: 0.8254 - val_loss: 49.8127 - val_mae: 50.4924\n",
      "Epoch 980/1000\n",
      "1460/1460 [==============================] - 0s 279us/step - loss: 0.5505 - mae: 0.8949 - val_loss: 50.1479 - val_mae: 50.8251\n",
      "Epoch 981/1000\n",
      "1460/1460 [==============================] - 0s 304us/step - loss: 0.6492 - mae: 0.9744 - val_loss: 50.3695 - val_mae: 51.0497\n",
      "Epoch 982/1000\n",
      "1460/1460 [==============================] - 0s 294us/step - loss: 0.4856 - mae: 0.7909 - val_loss: 50.4962 - val_mae: 51.1715\n",
      "Epoch 983/1000\n",
      "1460/1460 [==============================] - 0s 300us/step - loss: 0.7419 - mae: 1.0877 - val_loss: 50.3181 - val_mae: 50.9948\n",
      "Epoch 984/1000\n",
      "1460/1460 [==============================] - 0s 287us/step - loss: 0.5495 - mae: 0.8596 - val_loss: 50.5924 - val_mae: 51.2670\n",
      "Epoch 985/1000\n",
      "1460/1460 [==============================] - 0s 286us/step - loss: 0.5744 - mae: 0.9131 - val_loss: 50.0840 - val_mae: 50.7642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 986/1000\n",
      "1460/1460 [==============================] - 0s 283us/step - loss: 0.4936 - mae: 0.8067 - val_loss: 49.7873 - val_mae: 50.4687\n",
      "Epoch 987/1000\n",
      "1460/1460 [==============================] - 0s 292us/step - loss: 0.4845 - mae: 0.7920 - val_loss: 50.3419 - val_mae: 51.0172\n",
      "Epoch 988/1000\n",
      "1460/1460 [==============================] - 0s 291us/step - loss: 0.4643 - mae: 0.7866 - val_loss: 49.6488 - val_mae: 50.3290\n",
      "Epoch 989/1000\n",
      "1460/1460 [==============================] - 0s 280us/step - loss: 0.5470 - mae: 0.8697 - val_loss: 50.5821 - val_mae: 51.2608\n",
      "Epoch 990/1000\n",
      "1460/1460 [==============================] - 0s 293us/step - loss: 0.6415 - mae: 0.9827 - val_loss: 50.1919 - val_mae: 50.8715\n",
      "Epoch 991/1000\n",
      "1460/1460 [==============================] - 0s 281us/step - loss: 0.4120 - mae: 0.6930 - val_loss: 50.7375 - val_mae: 51.4138\n",
      "Epoch 992/1000\n",
      "1460/1460 [==============================] - 0s 287us/step - loss: 0.4637 - mae: 0.7641 - val_loss: 50.3506 - val_mae: 51.0236\n",
      "Epoch 993/1000\n",
      "1460/1460 [==============================] - 0s 291us/step - loss: 0.4501 - mae: 0.7399 - val_loss: 50.1455 - val_mae: 50.8226\n",
      "Epoch 994/1000\n",
      "1460/1460 [==============================] - 0s 285us/step - loss: 0.3796 - mae: 0.6561 - val_loss: 50.2219 - val_mae: 50.8987\n",
      "Epoch 995/1000\n",
      "1460/1460 [==============================] - 0s 281us/step - loss: 0.5286 - mae: 0.8468 - val_loss: 50.4684 - val_mae: 51.1439\n",
      "Epoch 996/1000\n",
      "1460/1460 [==============================] - 0s 297us/step - loss: 0.6026 - mae: 0.9251 - val_loss: 50.6477 - val_mae: 51.3234\n",
      "Epoch 997/1000\n",
      "1460/1460 [==============================] - 0s 273us/step - loss: 0.4842 - mae: 0.7769 - val_loss: 50.4658 - val_mae: 51.1421\n",
      "Epoch 998/1000\n",
      "1460/1460 [==============================] - 0s 285us/step - loss: 0.4652 - mae: 0.7770 - val_loss: 50.3471 - val_mae: 51.0222\n",
      "Epoch 999/1000\n",
      "1460/1460 [==============================] - 0s 286us/step - loss: 0.4947 - mae: 0.7888 - val_loss: 49.8399 - val_mae: 50.5205\n",
      "Epoch 1000/1000\n",
      "1460/1460 [==============================] - 0s 280us/step - loss: 0.4416 - mae: 0.7278 - val_loss: 50.4940 - val_mae: 51.1709\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fc4286204d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(X_train,Y_train,validation_data=(X_val, Y_val))\n",
    "#Audio(sound_file,autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b6631bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460/1460 [==============================] - 0s 147us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9989266512976384"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for check\n",
    "y_train_pred=regressor.predict(X_train)\n",
    "r2_score(Y_train, y_train_pred) #training score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a30371e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546/546 [==============================] - 0s 94us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-06-01</th>\n",
       "      <td>137.796</td>\n",
       "      <td>121.369370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-02</th>\n",
       "      <td>133.855</td>\n",
       "      <td>107.409111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-03</th>\n",
       "      <td>138.441</td>\n",
       "      <td>100.333031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-04</th>\n",
       "      <td>143.405</td>\n",
       "      <td>99.039787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-05</th>\n",
       "      <td>139.483</td>\n",
       "      <td>93.733871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>112.721184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>108.901176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>114.515366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>113.378914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>104.273972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>546 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             y_test      y_pred\n",
       "Date                           \n",
       "2021-06-01  137.796  121.369370\n",
       "2021-06-02  133.855  107.409111\n",
       "2021-06-03  138.441  100.333031\n",
       "2021-06-04  143.405   99.039787\n",
       "2021-06-05  139.483   93.733871\n",
       "...             ...         ...\n",
       "2022-11-24      NaN  112.721184\n",
       "2022-11-25      NaN  108.901176\n",
       "2022-11-26      NaN  114.515366\n",
       "2022-11-27      NaN  113.378914\n",
       "2022-11-28      NaN  104.273972\n",
       "\n",
       "[546 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=regressor.predict(X_test)\n",
    "pre_df = pd.DataFrame(zip(Y_test,y_pred),columns=['y_test','y_pred'])\n",
    "pre_df.index = Y_test.index\n",
    "pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f424f2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.1489813188312699"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2=r2_score(Y_test[0:-30],y_pred[0:-30]) #testing score/ r^2\n",
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb79f980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.45185770601903"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse=np.sqrt(mean_squared_error(Y_test[],y_pred)) #rmse\n",
    "rmse#太特么大了，感觉数据集划分有问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "746ec495",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df['pred_returns'] = pre_df['y_pred'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dbbbde71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>pred_returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-06-01</th>\n",
       "      <td>182.100</td>\n",
       "      <td>172.572250</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-02</th>\n",
       "      <td>184.982</td>\n",
       "      <td>176.386429</td>\n",
       "      <td>0.022102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-03</th>\n",
       "      <td>190.269</td>\n",
       "      <td>181.590149</td>\n",
       "      <td>0.029502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-04</th>\n",
       "      <td>178.475</td>\n",
       "      <td>168.729507</td>\n",
       "      <td>-0.070822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-05</th>\n",
       "      <td>177.253</td>\n",
       "      <td>171.037537</td>\n",
       "      <td>0.013679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-24</th>\n",
       "      <td>77.689</td>\n",
       "      <td>69.559685</td>\n",
       "      <td>-0.094985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-25</th>\n",
       "      <td>75.830</td>\n",
       "      <td>71.954208</td>\n",
       "      <td>0.034424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-26</th>\n",
       "      <td>76.777</td>\n",
       "      <td>74.224960</td>\n",
       "      <td>0.031558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-27</th>\n",
       "      <td>76.865</td>\n",
       "      <td>72.169876</td>\n",
       "      <td>-0.027687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28</th>\n",
       "      <td>72.672</td>\n",
       "      <td>65.894836</td>\n",
       "      <td>-0.086948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>546 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             y_test      y_pred  pred_returns\n",
       "Date                                         \n",
       "2021-06-01  182.100  172.572250           NaN\n",
       "2021-06-02  184.982  176.386429      0.022102\n",
       "2021-06-03  190.269  181.590149      0.029502\n",
       "2021-06-04  178.475  168.729507     -0.070822\n",
       "2021-06-05  177.253  171.037537      0.013679\n",
       "...             ...         ...           ...\n",
       "2022-11-24   77.689   69.559685     -0.094985\n",
       "2022-11-25   75.830   71.954208      0.034424\n",
       "2022-11-26   76.777   74.224960      0.031558\n",
       "2022-11-27   76.865   72.169876     -0.027687\n",
       "2022-11-28   72.672   65.894836     -0.086948\n",
       "\n",
       "[546 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f79c9982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(zip(Y_test,y_pred),columns=['y_test','y_pred']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b264919",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df.to_csv(\"ltc_NN.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
