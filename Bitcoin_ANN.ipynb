{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5b918ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from datetime import datetime\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n",
    "from keras import losses\n",
    "from keras.constraints import maxnorm\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_validate\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from commons import mean_absolute_percentage_error\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, roc_auc_score\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import Audio\n",
    "#sound_file = 'beep.wav'\n",
    "from keras.callbacks import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21860ca",
   "metadata": {},
   "source": [
    "### Bitcoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "115de443",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuxuyang/opt/anaconda3/envs/mytensorflow/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "regs = pd.read_csv(\"btc_reg.csv\")\n",
    "btc = pd.read_csv(\"btc_Data.csv\")\n",
    "for i in range(len(btc['Date'])):\n",
    "    btc['Date'][i]  =  datetime.strptime(btc['Date'][i], '%Y/%m/%d')\n",
    "\n",
    "btc = btc.set_index(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b19bc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "btcData = btc[regs.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "177010fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuxuyang/opt/anaconda3/envs/mytensorflow/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "btcData['returns'] = btcData['priceUSD'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb2e0737",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = btcData.drop(columns=['priceUSD'])\n",
    "Data = Data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "981b2118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>difficulty14mom</th>\n",
       "      <th>difficulty30mom</th>\n",
       "      <th>difficulty3rsi</th>\n",
       "      <th>difficulty90mom</th>\n",
       "      <th>difficulty90sma</th>\n",
       "      <th>hashrate90std</th>\n",
       "      <th>hashrate90var</th>\n",
       "      <th>median_transaction_fee90rocUSD</th>\n",
       "      <th>median_transaction_feeUSD</th>\n",
       "      <th>mining_profitability</th>\n",
       "      <th>...</th>\n",
       "      <th>price90wmaUSD</th>\n",
       "      <th>sentinusd90emaUSD</th>\n",
       "      <th>sentinusdUSD</th>\n",
       "      <th>top100cap</th>\n",
       "      <th>top100cap14trx</th>\n",
       "      <th>top100cap30rsi</th>\n",
       "      <th>top100cap30trx</th>\n",
       "      <th>top100cap90rsi</th>\n",
       "      <th>top100cap90trx</th>\n",
       "      <th>returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-07-18</th>\n",
       "      <td>1.621380e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.737230e+02</td>\n",
       "      <td>2.055500e+01</td>\n",
       "      <td>8.384389e+08</td>\n",
       "      <td>1.757449e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>401834.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.612000e+03</td>\n",
       "      <td>25.782</td>\n",
       "      <td>0.139</td>\n",
       "      <td>71.191</td>\n",
       "      <td>0.308</td>\n",
       "      <td>79.756</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-19</th>\n",
       "      <td>1.621380e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.737230e+02</td>\n",
       "      <td>2.248600e+01</td>\n",
       "      <td>8.819952e+08</td>\n",
       "      <td>1.944789e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>481473.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.047000e+03</td>\n",
       "      <td>25.685</td>\n",
       "      <td>0.123</td>\n",
       "      <td>68.863</td>\n",
       "      <td>0.300</td>\n",
       "      <td>78.999</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.183196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-20</th>\n",
       "      <td>1.584060e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.735570e+02</td>\n",
       "      <td>2.441400e+01</td>\n",
       "      <td>9.280542e+08</td>\n",
       "      <td>2.153212e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>431831.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.341000e+03</td>\n",
       "      <td>25.602</td>\n",
       "      <td>0.107</td>\n",
       "      <td>66.923</td>\n",
       "      <td>0.292</td>\n",
       "      <td>78.355</td>\n",
       "      <td>0.544</td>\n",
       "      <td>-0.088475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-21</th>\n",
       "      <td>1.580420e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.700790e+02</td>\n",
       "      <td>2.630400e+01</td>\n",
       "      <td>9.761949e+08</td>\n",
       "      <td>2.382391e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>460783.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.122000e+03</td>\n",
       "      <td>25.638</td>\n",
       "      <td>0.092</td>\n",
       "      <td>67.334</td>\n",
       "      <td>0.284</td>\n",
       "      <td>78.431</td>\n",
       "      <td>0.541</td>\n",
       "      <td>-0.020434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-22</th>\n",
       "      <td>1.580420e+02</td>\n",
       "      <td>1.641620e+02</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.700790e+02</td>\n",
       "      <td>2.819400e+01</td>\n",
       "      <td>1.019474e+09</td>\n",
       "      <td>2.598318e+17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>334641.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.570000e+03</td>\n",
       "      <td>25.604</td>\n",
       "      <td>0.078</td>\n",
       "      <td>66.526</td>\n",
       "      <td>0.275</td>\n",
       "      <td>78.166</td>\n",
       "      <td>0.537</td>\n",
       "      <td>-0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-24</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.384949e+13</td>\n",
       "      <td>4.612156e+19</td>\n",
       "      <td>5.317997e+38</td>\n",
       "      <td>165.380</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.0577</td>\n",
       "      <td>...</td>\n",
       "      <td>18820.0</td>\n",
       "      <td>28544682257</td>\n",
       "      <td>6.114577e+09</td>\n",
       "      <td>15.236</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>40.118</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>45.140</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.008075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-25</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.394503e+13</td>\n",
       "      <td>4.542600e+19</td>\n",
       "      <td>5.158804e+38</td>\n",
       "      <td>214.464</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.0541</td>\n",
       "      <td>...</td>\n",
       "      <td>18759.0</td>\n",
       "      <td>28076113671</td>\n",
       "      <td>7.224812e+09</td>\n",
       "      <td>15.231</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>39.880</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>45.047</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.006565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-26</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.404058e+13</td>\n",
       "      <td>4.624135e+19</td>\n",
       "      <td>5.345655e+38</td>\n",
       "      <td>255.004</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.0571</td>\n",
       "      <td>...</td>\n",
       "      <td>18701.0</td>\n",
       "      <td>27566714445</td>\n",
       "      <td>4.898449e+09</td>\n",
       "      <td>15.233</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>40.069</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>45.106</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.004001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-27</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.413612e+13</td>\n",
       "      <td>4.576227e+19</td>\n",
       "      <td>5.235464e+38</td>\n",
       "      <td>113.430</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.0618</td>\n",
       "      <td>...</td>\n",
       "      <td>18642.0</td>\n",
       "      <td>27053515674</td>\n",
       "      <td>4.216170e+09</td>\n",
       "      <td>15.259</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>41.833</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>45.656</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28</th>\n",
       "      <td>1.882952e+11</td>\n",
       "      <td>1.148115e+11</td>\n",
       "      <td>99.86</td>\n",
       "      <td>8.598887e+12</td>\n",
       "      <td>3.423166e+13</td>\n",
       "      <td>4.552126e+19</td>\n",
       "      <td>5.180463e+38</td>\n",
       "      <td>104.294</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.0592</td>\n",
       "      <td>...</td>\n",
       "      <td>18578.0</td>\n",
       "      <td>26631320968</td>\n",
       "      <td>7.843657e+09</td>\n",
       "      <td>15.219</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>39.965</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>44.957</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.019828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4517 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            difficulty14mom  difficulty30mom  difficulty3rsi  difficulty90mom  \\\n",
       "Date                                                                            \n",
       "2010-07-18     1.621380e+02     1.641620e+02          100.00     1.737230e+02   \n",
       "2010-07-19     1.621380e+02     1.641620e+02          100.00     1.737230e+02   \n",
       "2010-07-20     1.584060e+02     1.641620e+02          100.00     1.735570e+02   \n",
       "2010-07-21     1.580420e+02     1.641620e+02          100.00     1.700790e+02   \n",
       "2010-07-22     1.580420e+02     1.641620e+02          100.00     1.700790e+02   \n",
       "...                     ...              ...             ...              ...   \n",
       "2022-11-24     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-25     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-26     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-27     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "2022-11-28     1.882952e+11     1.148115e+11           99.86     8.598887e+12   \n",
       "\n",
       "            difficulty90sma  hashrate90std  hashrate90var  \\\n",
       "Date                                                        \n",
       "2010-07-18     2.055500e+01   8.384389e+08   1.757449e+17   \n",
       "2010-07-19     2.248600e+01   8.819952e+08   1.944789e+17   \n",
       "2010-07-20     2.441400e+01   9.280542e+08   2.153212e+17   \n",
       "2010-07-21     2.630400e+01   9.761949e+08   2.382391e+17   \n",
       "2010-07-22     2.819400e+01   1.019474e+09   2.598318e+17   \n",
       "...                     ...            ...            ...   \n",
       "2022-11-24     3.384949e+13   4.612156e+19   5.317997e+38   \n",
       "2022-11-25     3.394503e+13   4.542600e+19   5.158804e+38   \n",
       "2022-11-26     3.404058e+13   4.624135e+19   5.345655e+38   \n",
       "2022-11-27     3.413612e+13   4.576227e+19   5.235464e+38   \n",
       "2022-11-28     3.423166e+13   4.552126e+19   5.180463e+38   \n",
       "\n",
       "            median_transaction_fee90rocUSD  median_transaction_feeUSD  \\\n",
       "Date                                                                    \n",
       "2010-07-18                           0.000                      0.000   \n",
       "2010-07-19                           0.000                      0.000   \n",
       "2010-07-20                           0.000                      0.000   \n",
       "2010-07-21                           0.000                      0.000   \n",
       "2010-07-22                           0.000                      0.000   \n",
       "...                                    ...                        ...   \n",
       "2022-11-24                         165.380                      0.818   \n",
       "2022-11-25                         214.464                      0.812   \n",
       "2022-11-26                         255.004                      0.809   \n",
       "2022-11-27                         113.430                      0.619   \n",
       "2022-11-28                         104.294                      0.622   \n",
       "\n",
       "            mining_profitability  ...  price90wmaUSD  sentinusd90emaUSD  \\\n",
       "Date                              ...                                     \n",
       "2010-07-18           401834.0000  ...            0.0                  0   \n",
       "2010-07-19           481473.0000  ...            0.0                  0   \n",
       "2010-07-20           431831.0000  ...            0.0                  0   \n",
       "2010-07-21           460783.0000  ...            0.0                  0   \n",
       "2010-07-22           334641.0000  ...            0.0                  0   \n",
       "...                          ...  ...            ...                ...   \n",
       "2022-11-24                0.0577  ...        18820.0        28544682257   \n",
       "2022-11-25                0.0541  ...        18759.0        28076113671   \n",
       "2022-11-26                0.0571  ...        18701.0        27566714445   \n",
       "2022-11-27                0.0618  ...        18642.0        27053515674   \n",
       "2022-11-28                0.0592  ...        18578.0        26631320968   \n",
       "\n",
       "            sentinusdUSD  top100cap  top100cap14trx  top100cap30rsi  \\\n",
       "Date                                                                  \n",
       "2010-07-18  2.612000e+03     25.782           0.139          71.191   \n",
       "2010-07-19  4.047000e+03     25.685           0.123          68.863   \n",
       "2010-07-20  2.341000e+03     25.602           0.107          66.923   \n",
       "2010-07-21  2.122000e+03     25.638           0.092          67.334   \n",
       "2010-07-22  2.570000e+03     25.604           0.078          66.526   \n",
       "...                  ...        ...             ...             ...   \n",
       "2022-11-24  6.114577e+09     15.236          -0.080          40.118   \n",
       "2022-11-25  7.224812e+09     15.231          -0.075          39.880   \n",
       "2022-11-26  4.898449e+09     15.233          -0.070          40.069   \n",
       "2022-11-27  4.216170e+09     15.259          -0.065          41.833   \n",
       "2022-11-28  7.843657e+09     15.219          -0.061          39.965   \n",
       "\n",
       "            top100cap30trx  top100cap90rsi  top100cap90trx   returns  \n",
       "Date                                                                  \n",
       "2010-07-18           0.308          79.756           0.550  0.466667  \n",
       "2010-07-19           0.300          78.999           0.547  0.183196  \n",
       "2010-07-20           0.292          78.355           0.544 -0.088475  \n",
       "2010-07-21           0.284          78.431           0.541 -0.020434  \n",
       "2010-07-22           0.275          78.166           0.537 -0.153846  \n",
       "...                    ...             ...             ...       ...  \n",
       "2022-11-24          -0.042          45.140           0.001  0.008075  \n",
       "2022-11-25          -0.043          45.047           0.001 -0.006565  \n",
       "2022-11-26          -0.043          45.106           0.001  0.004001  \n",
       "2022-11-27          -0.044          45.656           0.000 -0.001147  \n",
       "2022-11-28          -0.044          44.957           0.000 -0.019828  \n",
       "\n",
       "[4517 rows x 32 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1ca4002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide X and Y\n",
    "X = Data.iloc[:,0:]\n",
    "#Y = Data['returns']   # 用returns的话就用这一行，然后把下一行comment掉\n",
    "Y = btcData['priceUSD'][1:] # 反之亦然"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "3490adc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>difficulty14mom</th>\n",
       "      <th>difficulty30mom</th>\n",
       "      <th>difficulty3rsi</th>\n",
       "      <th>difficulty90mom</th>\n",
       "      <th>difficulty90sma</th>\n",
       "      <th>hashrate90std</th>\n",
       "      <th>hashrate90var</th>\n",
       "      <th>median_transaction_fee90rocUSD</th>\n",
       "      <th>median_transaction_feeUSD</th>\n",
       "      <th>mining_profitability</th>\n",
       "      <th>...</th>\n",
       "      <th>price90wmaUSD</th>\n",
       "      <th>sentinusd90emaUSD</th>\n",
       "      <th>sentinusdUSD</th>\n",
       "      <th>top100cap</th>\n",
       "      <th>top100cap14trx</th>\n",
       "      <th>top100cap30rsi</th>\n",
       "      <th>top100cap30trx</th>\n",
       "      <th>top100cap90rsi</th>\n",
       "      <th>top100cap90trx</th>\n",
       "      <th>returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-07-18</th>\n",
       "      <td>162.138</td>\n",
       "      <td>164.162</td>\n",
       "      <td>100.0</td>\n",
       "      <td>173.723</td>\n",
       "      <td>20.555</td>\n",
       "      <td>838438881.0</td>\n",
       "      <td>1.757449e+17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>401834.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2612.0</td>\n",
       "      <td>25.782</td>\n",
       "      <td>0.139</td>\n",
       "      <td>71.191</td>\n",
       "      <td>0.308</td>\n",
       "      <td>79.756</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-19</th>\n",
       "      <td>162.138</td>\n",
       "      <td>164.162</td>\n",
       "      <td>100.0</td>\n",
       "      <td>173.723</td>\n",
       "      <td>22.486</td>\n",
       "      <td>881995244.0</td>\n",
       "      <td>1.944789e+17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>481473.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4047.0</td>\n",
       "      <td>25.685</td>\n",
       "      <td>0.123</td>\n",
       "      <td>68.863</td>\n",
       "      <td>0.300</td>\n",
       "      <td>78.999</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.183196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-20</th>\n",
       "      <td>158.406</td>\n",
       "      <td>164.162</td>\n",
       "      <td>100.0</td>\n",
       "      <td>173.557</td>\n",
       "      <td>24.414</td>\n",
       "      <td>928054231.0</td>\n",
       "      <td>2.153212e+17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>431831.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2341.0</td>\n",
       "      <td>25.602</td>\n",
       "      <td>0.107</td>\n",
       "      <td>66.923</td>\n",
       "      <td>0.292</td>\n",
       "      <td>78.355</td>\n",
       "      <td>0.544</td>\n",
       "      <td>-0.088475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            difficulty14mom  difficulty30mom  difficulty3rsi  difficulty90mom  \\\n",
       "Date                                                                            \n",
       "2010-07-18          162.138          164.162           100.0          173.723   \n",
       "2010-07-19          162.138          164.162           100.0          173.723   \n",
       "2010-07-20          158.406          164.162           100.0          173.557   \n",
       "\n",
       "            difficulty90sma  hashrate90std  hashrate90var  \\\n",
       "Date                                                        \n",
       "2010-07-18           20.555    838438881.0   1.757449e+17   \n",
       "2010-07-19           22.486    881995244.0   1.944789e+17   \n",
       "2010-07-20           24.414    928054231.0   2.153212e+17   \n",
       "\n",
       "            median_transaction_fee90rocUSD  median_transaction_feeUSD  \\\n",
       "Date                                                                    \n",
       "2010-07-18                             0.0                        0.0   \n",
       "2010-07-19                             0.0                        0.0   \n",
       "2010-07-20                             0.0                        0.0   \n",
       "\n",
       "            mining_profitability  ...  price90wmaUSD  sentinusd90emaUSD  \\\n",
       "Date                              ...                                     \n",
       "2010-07-18              401834.0  ...            0.0                  0   \n",
       "2010-07-19              481473.0  ...            0.0                  0   \n",
       "2010-07-20              431831.0  ...            0.0                  0   \n",
       "\n",
       "            sentinusdUSD  top100cap  top100cap14trx  top100cap30rsi  \\\n",
       "Date                                                                  \n",
       "2010-07-18        2612.0     25.782           0.139          71.191   \n",
       "2010-07-19        4047.0     25.685           0.123          68.863   \n",
       "2010-07-20        2341.0     25.602           0.107          66.923   \n",
       "\n",
       "            top100cap30trx  top100cap90rsi  top100cap90trx   returns  \n",
       "Date                                                                  \n",
       "2010-07-18           0.308          79.756           0.550  0.466667  \n",
       "2010-07-19           0.300          78.999           0.547  0.183196  \n",
       "2010-07-20           0.292          78.355           0.544 -0.088475  \n",
       "\n",
       "[3 rows x 32 columns]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "d6e76b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into three data sets\n",
    "X_train = X['2016-01-01':'2019-12-31']\n",
    "X_val = X['2020-01-01':'2021-05-31']\n",
    "X_test = X['2021-06-01':'2023-01-01']\n",
    "\n",
    "Y_train = Y['2016-01-01':'2019-12-31']\n",
    "Y_val = Y['2020-01-01':'2021-05-31']\n",
    "Y_test = Y['2021-06-01':'2023-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "cd266474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............ (step 1 of 2) Processing mixmax, total=   0.0s\n",
      "[Pipeline] ............ (step 2 of 2) Processing robust, total=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('mixmax', MinMaxScaler()), ['robust', RobustScaler()]],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators=[]\n",
    "estimators.append(['mixmax',MinMaxScaler()])\n",
    "estimators.append(['robust',RobustScaler()])\n",
    "scale=Pipeline(estimators,verbose=True)\n",
    "scale.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "6bb24c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=scale.transform(X_train)\n",
    "X_test=scale.transform(X_test)\n",
    "X_val = scale.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c15cd019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "17e54d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "# define neural network model\n",
    "shape=X.shape[1]\n",
    "def sequential_model(initializer='normal', activation='relu', neurons=300, NUM_FEATURES=shape):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_shape=(NUM_FEATURES,), kernel_initializer=initializer, activation=activation))\n",
    "    model.add(Dense(neurons, activation=activation))\n",
    "    model.add(Dense(neurons, activation=activation))\n",
    "    model.add(Dense(1, activation=activation, kernel_initializer=initializer))\n",
    "    # Compile model\n",
    "    adam=keras.optimizers.adam(lr=lr_schedule(0), amsgrad=True)\n",
    "    #sgd=keras.optimizers.SGD(learning_rate=0.08, momentum=0.9, nesterov=False)\n",
    "    model.compile(loss='logcosh', optimizer=adam, metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "9d7c3f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mcp_save = ModelCheckpoint('trained_models/ANN_reg_seven_new.hdf5', save_best_only=True, monitor='val_loss', mode='auto')\n",
    "#earlyStopping = EarlyStopping(monitor='val_loss', patience=100,verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "3145a447",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor=KerasRegressor(build_fn=sequential_model,epochs=1000,verbose=1, shuffle=True,use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b400e4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Train on 1461 samples, validate on 517 samples\n",
      "Epoch 1/1000\n",
      "1461/1461 [==============================] - 1s 713us/step - loss: 4542.1763 - mae: 4542.8691 - val_loss: 17803.1971 - val_mae: 17803.8906\n",
      "Epoch 2/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 2609.4767 - mae: 2610.1697 - val_loss: 3853.4012 - val_mae: 3854.0945\n",
      "Epoch 3/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 1046.6732 - mae: 1047.3660 - val_loss: 3140.4699 - val_mae: 3141.1631\n",
      "Epoch 4/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 793.9743 - mae: 794.6665 - val_loss: 2453.1101 - val_mae: 2453.8032\n",
      "Epoch 5/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 652.6379 - mae: 653.3306 - val_loss: 2329.8031 - val_mae: 2330.4961\n",
      "Epoch 6/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 593.3032 - mae: 593.9957 - val_loss: 1730.6173 - val_mae: 1731.3104\n",
      "Epoch 7/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 488.8780 - mae: 489.5699 - val_loss: 1545.7597 - val_mae: 1546.4529\n",
      "Epoch 8/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 428.8105 - mae: 429.5027 - val_loss: 1713.3056 - val_mae: 1713.9987\n",
      "Epoch 9/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 389.2400 - mae: 389.9309 - val_loss: 1393.1718 - val_mae: 1393.8649\n",
      "Epoch 10/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 357.8876 - mae: 358.5791 - val_loss: 1565.9848 - val_mae: 1566.6777\n",
      "Epoch 11/1000\n",
      "1461/1461 [==============================] - 0s 292us/step - loss: 323.3859 - mae: 324.0774 - val_loss: 1911.2553 - val_mae: 1911.9482\n",
      "Epoch 12/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 315.5238 - mae: 316.2147 - val_loss: 1813.9777 - val_mae: 1814.6709\n",
      "Epoch 13/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 288.4901 - mae: 289.1809 - val_loss: 1582.3167 - val_mae: 1583.0098\n",
      "Epoch 14/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 274.3050 - mae: 274.9962 - val_loss: 1386.0197 - val_mae: 1386.7129\n",
      "Epoch 15/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 291.1349 - mae: 291.8262 - val_loss: 1574.3416 - val_mae: 1575.0347\n",
      "Epoch 16/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 242.5666 - mae: 243.2579 - val_loss: 1548.7492 - val_mae: 1549.4419\n",
      "Epoch 17/1000\n",
      "1461/1461 [==============================] - 0s 297us/step - loss: 234.9434 - mae: 235.6336 - val_loss: 1546.4752 - val_mae: 1547.1658\n",
      "Epoch 18/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 211.4742 - mae: 212.1660 - val_loss: 1452.3675 - val_mae: 1453.0604\n",
      "Epoch 19/1000\n",
      "1461/1461 [==============================] - 0s 281us/step - loss: 189.2286 - mae: 189.9192 - val_loss: 1447.9919 - val_mae: 1448.6837\n",
      "Epoch 20/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 178.8254 - mae: 179.5155 - val_loss: 1427.9090 - val_mae: 1428.6014\n",
      "Epoch 21/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 184.9333 - mae: 185.6221 - val_loss: 1458.2075 - val_mae: 1458.9008\n",
      "Epoch 22/1000\n",
      "1461/1461 [==============================] - 0s 292us/step - loss: 151.2734 - mae: 151.9619 - val_loss: 1715.3515 - val_mae: 1716.0446\n",
      "Epoch 23/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 162.0121 - mae: 162.7015 - val_loss: 1581.1943 - val_mae: 1581.8876\n",
      "Epoch 24/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 159.8294 - mae: 160.5191 - val_loss: 1491.6698 - val_mae: 1492.3625\n",
      "Epoch 25/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 169.6448 - mae: 170.3349 - val_loss: 1458.0460 - val_mae: 1458.7379\n",
      "Epoch 26/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 171.7048 - mae: 172.3938 - val_loss: 1482.8481 - val_mae: 1483.5414\n",
      "Epoch 27/1000\n",
      "1461/1461 [==============================] - 0s 303us/step - loss: 165.3087 - mae: 165.9989 - val_loss: 2161.6239 - val_mae: 2162.3169\n",
      "Epoch 28/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 171.5839 - mae: 172.2738 - val_loss: 1502.9525 - val_mae: 1503.6450\n",
      "Epoch 29/1000\n",
      "1461/1461 [==============================] - 0s 338us/step - loss: 130.7110 - mae: 131.4010 - val_loss: 1693.2440 - val_mae: 1693.9371\n",
      "Epoch 30/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 152.0964 - mae: 152.7857 - val_loss: 1276.9459 - val_mae: 1277.6390\n",
      "Epoch 31/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 137.4480 - mae: 138.1368 - val_loss: 1505.3201 - val_mae: 1506.0120\n",
      "Epoch 32/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 130.3635 - mae: 131.0519 - val_loss: 1746.1847 - val_mae: 1746.8779\n",
      "Epoch 33/1000\n",
      "1461/1461 [==============================] - 1s 492us/step - loss: 128.3001 - mae: 128.9891 - val_loss: 1253.1075 - val_mae: 1253.8003\n",
      "Epoch 34/1000\n",
      "1461/1461 [==============================] - 0s 322us/step - loss: 142.4193 - mae: 143.1089 - val_loss: 1395.4050 - val_mae: 1396.0983\n",
      "Epoch 35/1000\n",
      "1461/1461 [==============================] - 1s 600us/step - loss: 124.4311 - mae: 125.1199 - val_loss: 1525.6136 - val_mae: 1526.3066\n",
      "Epoch 36/1000\n",
      "1461/1461 [==============================] - 1s 381us/step - loss: 117.0938 - mae: 117.7843 - val_loss: 1512.8126 - val_mae: 1513.5059\n",
      "Epoch 37/1000\n",
      "1461/1461 [==============================] - 1s 360us/step - loss: 132.9927 - mae: 133.6813 - val_loss: 1445.2162 - val_mae: 1445.9089\n",
      "Epoch 38/1000\n",
      "1461/1461 [==============================] - 0s 328us/step - loss: 128.7875 - mae: 129.4778 - val_loss: 1839.1008 - val_mae: 1839.7939\n",
      "Epoch 39/1000\n",
      "1461/1461 [==============================] - 1s 361us/step - loss: 114.6492 - mae: 115.3376 - val_loss: 1610.1226 - val_mae: 1610.8149\n",
      "Epoch 40/1000\n",
      "1461/1461 [==============================] - 0s 306us/step - loss: 106.1605 - mae: 106.8491 - val_loss: 1509.7812 - val_mae: 1510.4744\n",
      "Epoch 41/1000\n",
      "1461/1461 [==============================] - 0s 300us/step - loss: 110.7179 - mae: 111.4071 - val_loss: 1415.3950 - val_mae: 1416.0884\n",
      "Epoch 42/1000\n",
      "1461/1461 [==============================] - 0s 314us/step - loss: 146.0319 - mae: 146.7195 - val_loss: 1743.1906 - val_mae: 1743.8832\n",
      "Epoch 43/1000\n",
      "1461/1461 [==============================] - 0s 297us/step - loss: 114.9650 - mae: 115.6522 - val_loss: 1209.7000 - val_mae: 1210.3923\n",
      "Epoch 44/1000\n",
      "1461/1461 [==============================] - 0s 296us/step - loss: 147.7782 - mae: 148.4674 - val_loss: 1422.6150 - val_mae: 1423.3082\n",
      "Epoch 45/1000\n",
      "1461/1461 [==============================] - 0s 299us/step - loss: 111.2678 - mae: 111.9577 - val_loss: 1490.2915 - val_mae: 1490.9846\n",
      "Epoch 46/1000\n",
      "1461/1461 [==============================] - 0s 315us/step - loss: 99.2864 - mae: 99.9735 - val_loss: 1318.5332 - val_mae: 1319.2266\n",
      "Epoch 47/1000\n",
      "1461/1461 [==============================] - 1s 342us/step - loss: 102.1316 - mae: 102.8179 - val_loss: 1735.0935 - val_mae: 1735.7867\n",
      "Epoch 48/1000\n",
      "1461/1461 [==============================] - 0s 294us/step - loss: 118.9005 - mae: 119.5879 - val_loss: 1588.8967 - val_mae: 1589.5898\n",
      "Epoch 49/1000\n",
      "1461/1461 [==============================] - 0s 290us/step - loss: 100.4079 - mae: 101.0965 - val_loss: 1141.3088 - val_mae: 1142.0017\n",
      "Epoch 50/1000\n",
      "1461/1461 [==============================] - 0s 303us/step - loss: 102.2317 - mae: 102.9208 - val_loss: 2150.0983 - val_mae: 2150.7915\n",
      "Epoch 51/1000\n",
      "1461/1461 [==============================] - 0s 301us/step - loss: 121.4825 - mae: 122.1697 - val_loss: 1430.0956 - val_mae: 1430.7887\n",
      "Epoch 52/1000\n",
      "1461/1461 [==============================] - 0s 303us/step - loss: 131.9907 - mae: 132.6788 - val_loss: 995.6197 - val_mae: 996.3115\n",
      "Epoch 53/1000\n",
      "1461/1461 [==============================] - 0s 294us/step - loss: 113.8916 - mae: 114.5798 - val_loss: 1503.2625 - val_mae: 1503.9557\n",
      "Epoch 54/1000\n",
      "1461/1461 [==============================] - 0s 299us/step - loss: 97.9482 - mae: 98.6357 - val_loss: 1189.2068 - val_mae: 1189.9000\n",
      "Epoch 55/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 291us/step - loss: 96.3191 - mae: 97.0082 - val_loss: 1233.9854 - val_mae: 1234.6786\n",
      "Epoch 56/1000\n",
      "1461/1461 [==============================] - 0s 309us/step - loss: 122.0347 - mae: 122.7245 - val_loss: 1419.1776 - val_mae: 1419.8706\n",
      "Epoch 57/1000\n",
      "1461/1461 [==============================] - 0s 311us/step - loss: 139.7451 - mae: 140.4330 - val_loss: 1433.4956 - val_mae: 1434.1888\n",
      "Epoch 58/1000\n",
      "1461/1461 [==============================] - 0s 294us/step - loss: 101.1396 - mae: 101.8274 - val_loss: 1596.5171 - val_mae: 1597.2103\n",
      "Epoch 59/1000\n",
      "1461/1461 [==============================] - 0s 290us/step - loss: 106.4784 - mae: 107.1676 - val_loss: 1241.3191 - val_mae: 1242.0123\n",
      "Epoch 60/1000\n",
      "1461/1461 [==============================] - 0s 306us/step - loss: 86.5931 - mae: 87.2815 - val_loss: 1160.7310 - val_mae: 1161.4224\n",
      "Epoch 61/1000\n",
      "1461/1461 [==============================] - 0s 305us/step - loss: 99.9678 - mae: 100.6578 - val_loss: 1660.6494 - val_mae: 1661.3428\n",
      "Epoch 62/1000\n",
      "1461/1461 [==============================] - 0s 299us/step - loss: 82.2847 - mae: 82.9711 - val_loss: 1523.7481 - val_mae: 1524.4410\n",
      "Epoch 63/1000\n",
      "1461/1461 [==============================] - 0s 308us/step - loss: 84.0612 - mae: 84.7478 - val_loss: 1253.6964 - val_mae: 1254.3896\n",
      "Epoch 64/1000\n",
      "1461/1461 [==============================] - 0s 298us/step - loss: 90.1429 - mae: 90.8308 - val_loss: 935.5727 - val_mae: 936.2645\n",
      "Epoch 65/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 122.1035 - mae: 122.7909 - val_loss: 1418.6927 - val_mae: 1419.3849\n",
      "Epoch 66/1000\n",
      "1461/1461 [==============================] - 0s 300us/step - loss: 78.4414 - mae: 79.1294 - val_loss: 1302.7188 - val_mae: 1303.4120\n",
      "Epoch 67/1000\n",
      "1461/1461 [==============================] - 0s 300us/step - loss: 78.8708 - mae: 79.5578 - val_loss: 1523.2151 - val_mae: 1523.9082\n",
      "Epoch 68/1000\n",
      "1461/1461 [==============================] - 0s 311us/step - loss: 83.9992 - mae: 84.6853 - val_loss: 1391.9628 - val_mae: 1392.6561\n",
      "Epoch 69/1000\n",
      "1461/1461 [==============================] - 0s 302us/step - loss: 110.1036 - mae: 110.7900 - val_loss: 1714.8592 - val_mae: 1715.5525\n",
      "Epoch 70/1000\n",
      "1461/1461 [==============================] - 0s 322us/step - loss: 100.8730 - mae: 101.5588 - val_loss: 1957.3347 - val_mae: 1958.0278\n",
      "Epoch 71/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 105.1796 - mae: 105.8663 - val_loss: 1345.6452 - val_mae: 1346.3384\n",
      "Epoch 72/1000\n",
      "1461/1461 [==============================] - 0s 289us/step - loss: 86.0086 - mae: 86.6966 - val_loss: 1689.2134 - val_mae: 1689.9067\n",
      "Epoch 73/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 86.5038 - mae: 87.1903 - val_loss: 1517.3288 - val_mae: 1518.0221\n",
      "Epoch 74/1000\n",
      "1461/1461 [==============================] - 0s 303us/step - loss: 79.1531 - mae: 79.8402 - val_loss: 1669.2733 - val_mae: 1669.9664\n",
      "Epoch 75/1000\n",
      "1461/1461 [==============================] - 0s 289us/step - loss: 96.5086 - mae: 97.1965 - val_loss: 1517.6782 - val_mae: 1518.3715\n",
      "Epoch 76/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 83.6400 - mae: 84.3285 - val_loss: 1336.7699 - val_mae: 1337.4631\n",
      "Epoch 77/1000\n",
      "1461/1461 [==============================] - 0s 299us/step - loss: 75.1391 - mae: 75.8255 - val_loss: 1407.7821 - val_mae: 1408.4753\n",
      "Epoch 78/1000\n",
      "1461/1461 [==============================] - 0s 281us/step - loss: 76.4189 - mae: 77.1031 - val_loss: 1458.4362 - val_mae: 1459.1295\n",
      "Epoch 79/1000\n",
      "1461/1461 [==============================] - 0s 292us/step - loss: 81.8480 - mae: 82.5339 - val_loss: 1025.1051 - val_mae: 1025.7982\n",
      "Epoch 80/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 83.7044 - mae: 84.3907 - val_loss: 1618.7176 - val_mae: 1619.4106\n",
      "Epoch 81/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 95.1272 - mae: 95.8159 - val_loss: 1618.5778 - val_mae: 1619.2710\n",
      "Epoch 82/1000\n",
      "1461/1461 [==============================] - 0s 298us/step - loss: 79.1162 - mae: 79.8017 - val_loss: 1196.1061 - val_mae: 1196.7993\n",
      "Epoch 83/1000\n",
      "1461/1461 [==============================] - 0s 302us/step - loss: 77.8747 - mae: 78.5600 - val_loss: 1523.5170 - val_mae: 1524.2102\n",
      "Epoch 84/1000\n",
      "1461/1461 [==============================] - 0s 307us/step - loss: 82.0960 - mae: 82.7832 - val_loss: 1347.0645 - val_mae: 1347.7578\n",
      "Epoch 85/1000\n",
      "1461/1461 [==============================] - 0s 295us/step - loss: 115.1614 - mae: 115.8476 - val_loss: 1211.9273 - val_mae: 1212.6205\n",
      "Epoch 86/1000\n",
      "1461/1461 [==============================] - 0s 283us/step - loss: 96.1073 - mae: 96.7924 - val_loss: 1590.9856 - val_mae: 1591.6787\n",
      "Epoch 87/1000\n",
      "1461/1461 [==============================] - 0s 301us/step - loss: 76.6569 - mae: 77.3427 - val_loss: 1283.2118 - val_mae: 1283.9052\n",
      "Epoch 88/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 81.1324 - mae: 81.8190 - val_loss: 1316.0617 - val_mae: 1316.7550\n",
      "Epoch 89/1000\n",
      "1461/1461 [==============================] - 0s 303us/step - loss: 73.0243 - mae: 73.7085 - val_loss: 1105.8645 - val_mae: 1106.5570\n",
      "Epoch 90/1000\n",
      "1461/1461 [==============================] - 0s 294us/step - loss: 81.5401 - mae: 82.2242 - val_loss: 1432.3067 - val_mae: 1433.0000\n",
      "Epoch 91/1000\n",
      "1461/1461 [==============================] - 0s 309us/step - loss: 72.7948 - mae: 73.4811 - val_loss: 1336.2788 - val_mae: 1336.9720\n",
      "Epoch 92/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 83.1948 - mae: 83.8811 - val_loss: 1233.6176 - val_mae: 1234.3108\n",
      "Epoch 93/1000\n",
      "1461/1461 [==============================] - 0s 303us/step - loss: 82.7055 - mae: 83.3914 - val_loss: 1174.8169 - val_mae: 1175.5095\n",
      "Epoch 94/1000\n",
      "1461/1461 [==============================] - 0s 287us/step - loss: 82.6304 - mae: 83.3151 - val_loss: 1674.8380 - val_mae: 1675.5312\n",
      "Epoch 95/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 90.9619 - mae: 91.6493 - val_loss: 1830.6914 - val_mae: 1831.3846\n",
      "Epoch 96/1000\n",
      "1461/1461 [==============================] - 0s 289us/step - loss: 93.9207 - mae: 94.6063 - val_loss: 1217.9903 - val_mae: 1218.6836\n",
      "Epoch 97/1000\n",
      "1461/1461 [==============================] - 0s 292us/step - loss: 81.7496 - mae: 82.4386 - val_loss: 1014.4707 - val_mae: 1015.1635\n",
      "Epoch 98/1000\n",
      "1461/1461 [==============================] - 0s 306us/step - loss: 84.6932 - mae: 85.3823 - val_loss: 1490.8086 - val_mae: 1491.5018\n",
      "Epoch 99/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 89.7526 - mae: 90.4377 - val_loss: 1278.8514 - val_mae: 1279.5446\n",
      "Epoch 100/1000\n",
      "1461/1461 [==============================] - 0s 281us/step - loss: 80.9642 - mae: 81.6494 - val_loss: 1964.1701 - val_mae: 1964.8632\n",
      "Epoch 101/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 79.4935 - mae: 80.1776 - val_loss: 1398.7416 - val_mae: 1399.4341\n",
      "Epoch 102/1000\n",
      "1461/1461 [==============================] - 0s 290us/step - loss: 78.0373 - mae: 78.7221 - val_loss: 1833.7362 - val_mae: 1834.4296\n",
      "Epoch 103/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 94.0939 - mae: 94.7781 - val_loss: 1354.7334 - val_mae: 1355.4266\n",
      "Epoch 104/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 81.5658 - mae: 82.2524 - val_loss: 1188.8661 - val_mae: 1189.5593\n",
      "Epoch 105/1000\n",
      "1461/1461 [==============================] - 0s 281us/step - loss: 69.0035 - mae: 69.6881 - val_loss: 1463.7348 - val_mae: 1464.4281\n",
      "Epoch 106/1000\n",
      "1461/1461 [==============================] - 0s 287us/step - loss: 82.2487 - mae: 82.9356 - val_loss: 1044.7045 - val_mae: 1045.3976\n",
      "Epoch 107/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 69.8245 - mae: 70.5110 - val_loss: 1393.5364 - val_mae: 1394.2296\n",
      "Epoch 108/1000\n",
      "1461/1461 [==============================] - 0s 293us/step - loss: 77.6524 - mae: 78.3399 - val_loss: 1360.5724 - val_mae: 1361.2656\n",
      "Epoch 109/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 75.7820 - mae: 76.4701 - val_loss: 1172.1090 - val_mae: 1172.8022\n",
      "Epoch 110/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 287us/step - loss: 75.1854 - mae: 75.8693 - val_loss: 1604.3223 - val_mae: 1605.0154\n",
      "Epoch 111/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 80.0044 - mae: 80.6902 - val_loss: 1251.9697 - val_mae: 1252.6630\n",
      "Epoch 112/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 68.5362 - mae: 69.2203 - val_loss: 1602.2688 - val_mae: 1602.9620\n",
      "Epoch 113/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 77.5554 - mae: 78.2377 - val_loss: 1421.2114 - val_mae: 1421.9037\n",
      "Epoch 114/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 79.9100 - mae: 80.5963 - val_loss: 1172.7373 - val_mae: 1173.4304\n",
      "Epoch 115/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 86.8860 - mae: 87.5692 - val_loss: 1373.6040 - val_mae: 1374.2971\n",
      "Epoch 116/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 88.6030 - mae: 89.2913 - val_loss: 1205.7977 - val_mae: 1206.4910\n",
      "Epoch 117/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 79.3804 - mae: 80.0655 - val_loss: 1122.9664 - val_mae: 1123.6595\n",
      "Epoch 118/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 67.5208 - mae: 68.2077 - val_loss: 1146.6922 - val_mae: 1147.3854\n",
      "Epoch 119/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 69.8203 - mae: 70.5059 - val_loss: 1141.8341 - val_mae: 1142.5272\n",
      "Epoch 120/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 66.1425 - mae: 66.8279 - val_loss: 1316.0975 - val_mae: 1316.7908\n",
      "Epoch 121/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 71.2079 - mae: 71.8936 - val_loss: 1355.9279 - val_mae: 1356.6211\n",
      "Epoch 122/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 63.3249 - mae: 64.0110 - val_loss: 1260.3416 - val_mae: 1261.0348\n",
      "Epoch 123/1000\n",
      "1461/1461 [==============================] - 0s 296us/step - loss: 64.0504 - mae: 64.7339 - val_loss: 1571.7309 - val_mae: 1572.4242\n",
      "Epoch 124/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 70.9500 - mae: 71.6343 - val_loss: 1583.7320 - val_mae: 1584.4252\n",
      "Epoch 125/1000\n",
      "1461/1461 [==============================] - 0s 315us/step - loss: 74.9924 - mae: 75.6816 - val_loss: 1392.5440 - val_mae: 1393.2373\n",
      "Epoch 126/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 65.3680 - mae: 66.0533 - val_loss: 1242.4249 - val_mae: 1243.1180\n",
      "Epoch 127/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 79.9187 - mae: 80.6052 - val_loss: 1442.8276 - val_mae: 1443.5208\n",
      "Epoch 128/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 71.4015 - mae: 72.0871 - val_loss: 1585.3749 - val_mae: 1586.0681\n",
      "Epoch 129/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 104.0182 - mae: 104.7048 - val_loss: 1106.1837 - val_mae: 1106.8770\n",
      "Epoch 130/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 71.9015 - mae: 72.5894 - val_loss: 1208.0443 - val_mae: 1208.7374\n",
      "Epoch 131/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 74.6308 - mae: 75.3149 - val_loss: 1221.0756 - val_mae: 1221.7688\n",
      "Epoch 132/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 71.1281 - mae: 71.8150 - val_loss: 1286.0037 - val_mae: 1286.6968\n",
      "Epoch 133/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 70.0345 - mae: 70.7222 - val_loss: 1399.2601 - val_mae: 1399.9534\n",
      "Epoch 134/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 69.4566 - mae: 70.1412 - val_loss: 1265.1896 - val_mae: 1265.8828\n",
      "Epoch 135/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 71.9724 - mae: 72.6586 - val_loss: 1313.8663 - val_mae: 1314.5583\n",
      "Epoch 136/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 69.4199 - mae: 70.1018 - val_loss: 1320.3305 - val_mae: 1321.0236\n",
      "Epoch 137/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 72.6497 - mae: 73.3355 - val_loss: 1521.5833 - val_mae: 1522.2765\n",
      "Epoch 138/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 76.5503 - mae: 77.2383 - val_loss: 1545.7846 - val_mae: 1546.4772\n",
      "Epoch 139/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 79.8581 - mae: 80.5458 - val_loss: 1416.4253 - val_mae: 1417.1182\n",
      "Epoch 140/1000\n",
      "1461/1461 [==============================] - 0s 290us/step - loss: 61.5880 - mae: 62.2727 - val_loss: 1498.4670 - val_mae: 1499.1602\n",
      "Epoch 141/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 69.3988 - mae: 70.0856 - val_loss: 1512.3981 - val_mae: 1513.0913\n",
      "Epoch 142/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 65.6803 - mae: 66.3629 - val_loss: 1388.1239 - val_mae: 1388.8173\n",
      "Epoch 143/1000\n",
      "1461/1461 [==============================] - 0s 315us/step - loss: 71.5150 - mae: 72.1993 - val_loss: 1290.7135 - val_mae: 1291.4060\n",
      "Epoch 144/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 61.5294 - mae: 62.2128 - val_loss: 1616.0720 - val_mae: 1616.7651\n",
      "Epoch 145/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 77.3160 - mae: 78.0012 - val_loss: 1219.6985 - val_mae: 1220.3917\n",
      "Epoch 146/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 82.7996 - mae: 83.4843 - val_loss: 1533.8713 - val_mae: 1534.5645\n",
      "Epoch 147/1000\n",
      "1461/1461 [==============================] - 0s 283us/step - loss: 65.3442 - mae: 66.0275 - val_loss: 1436.0691 - val_mae: 1436.7622\n",
      "Epoch 148/1000\n",
      "1461/1461 [==============================] - 0s 319us/step - loss: 65.2564 - mae: 65.9429 - val_loss: 1453.6210 - val_mae: 1454.3141\n",
      "Epoch 149/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 62.2878 - mae: 62.9707 - val_loss: 1425.3212 - val_mae: 1426.0145\n",
      "Epoch 150/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 76.3518 - mae: 77.0372 - val_loss: 1409.3187 - val_mae: 1410.0120\n",
      "Epoch 151/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 72.3767 - mae: 73.0622 - val_loss: 1502.7787 - val_mae: 1503.4718\n",
      "Epoch 152/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 68.8475 - mae: 69.5317 - val_loss: 1313.6646 - val_mae: 1314.3574\n",
      "Epoch 153/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 89.3849 - mae: 90.0714 - val_loss: 1018.6814 - val_mae: 1019.3746\n",
      "Epoch 154/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 88.2359 - mae: 88.9247 - val_loss: 1099.6309 - val_mae: 1100.3231\n",
      "Epoch 155/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 72.7124 - mae: 73.3964 - val_loss: 1169.2548 - val_mae: 1169.9480\n",
      "Epoch 156/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 76.3535 - mae: 77.0359 - val_loss: 877.9957 - val_mae: 878.6863\n",
      "Epoch 157/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 82.8504 - mae: 83.5343 - val_loss: 1666.3597 - val_mae: 1667.0530\n",
      "Epoch 158/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 69.0714 - mae: 69.7572 - val_loss: 1267.6796 - val_mae: 1268.3728\n",
      "Epoch 159/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 71.9770 - mae: 72.6607 - val_loss: 1958.1308 - val_mae: 1958.8240\n",
      "Epoch 160/1000\n",
      "1461/1461 [==============================] - 0s 314us/step - loss: 74.4776 - mae: 75.1614 - val_loss: 1725.7716 - val_mae: 1726.4648\n",
      "Epoch 161/1000\n",
      "1461/1461 [==============================] - 0s 293us/step - loss: 65.8984 - mae: 66.5845 - val_loss: 1219.3280 - val_mae: 1220.0211\n",
      "Epoch 162/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 62.8728 - mae: 63.5546 - val_loss: 1395.6104 - val_mae: 1396.3037\n",
      "Epoch 163/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 59.0812 - mae: 59.7665 - val_loss: 1654.5698 - val_mae: 1655.2632\n",
      "Epoch 164/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 64.8428 - mae: 65.5273 - val_loss: 1330.6933 - val_mae: 1331.3865\n",
      "Epoch 165/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 271us/step - loss: 77.4210 - mae: 78.1065 - val_loss: 1724.7867 - val_mae: 1725.4799\n",
      "Epoch 166/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 73.3359 - mae: 74.0207 - val_loss: 1831.4773 - val_mae: 1832.1704\n",
      "Epoch 167/1000\n",
      "1461/1461 [==============================] - 0s 263us/step - loss: 66.5478 - mae: 67.2291 - val_loss: 1447.9736 - val_mae: 1448.6669\n",
      "Epoch 168/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 77.1604 - mae: 77.8462 - val_loss: 1342.2488 - val_mae: 1342.9420\n",
      "Epoch 169/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 61.4836 - mae: 62.1692 - val_loss: 1577.0740 - val_mae: 1577.7673\n",
      "Epoch 170/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 69.6202 - mae: 70.3018 - val_loss: 703.8638 - val_mae: 704.5567\n",
      "Epoch 171/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 100.3975 - mae: 101.0851 - val_loss: 1525.2374 - val_mae: 1525.9307\n",
      "Epoch 172/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 68.8131 - mae: 69.4994 - val_loss: 1065.7908 - val_mae: 1066.4835\n",
      "Epoch 173/1000\n",
      "1461/1461 [==============================] - 0s 290us/step - loss: 66.1141 - mae: 66.8002 - val_loss: 1422.3409 - val_mae: 1423.0327\n",
      "Epoch 174/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 66.7977 - mae: 67.4830 - val_loss: 1330.3819 - val_mae: 1331.0751\n",
      "Epoch 175/1000\n",
      "1461/1461 [==============================] - 0s 287us/step - loss: 62.1286 - mae: 62.8143 - val_loss: 1677.5485 - val_mae: 1678.2417\n",
      "Epoch 176/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 77.7064 - mae: 78.3899 - val_loss: 1850.1592 - val_mae: 1850.8523\n",
      "Epoch 177/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 64.8743 - mae: 65.5604 - val_loss: 1425.8168 - val_mae: 1426.5100\n",
      "Epoch 178/1000\n",
      "1461/1461 [==============================] - 0s 299us/step - loss: 58.2487 - mae: 58.9321 - val_loss: 1454.1490 - val_mae: 1454.8423\n",
      "Epoch 179/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 69.4235 - mae: 70.1078 - val_loss: 1193.8045 - val_mae: 1194.4977\n",
      "Epoch 180/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 65.6854 - mae: 66.3728 - val_loss: 1789.1927 - val_mae: 1789.8861\n",
      "Epoch 181/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 77.0692 - mae: 77.7541 - val_loss: 1682.5934 - val_mae: 1683.2866\n",
      "Epoch 182/1000\n",
      "1461/1461 [==============================] - 0s 298us/step - loss: 79.6640 - mae: 80.3510 - val_loss: 1838.4667 - val_mae: 1839.1598\n",
      "Epoch 183/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 65.7278 - mae: 66.4119 - val_loss: 1641.5981 - val_mae: 1642.2915\n",
      "Epoch 184/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 59.1527 - mae: 59.8396 - val_loss: 1518.2743 - val_mae: 1518.9675\n",
      "Epoch 185/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 57.7337 - mae: 58.4162 - val_loss: 1618.6620 - val_mae: 1619.3553\n",
      "Epoch 186/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 69.5766 - mae: 70.2572 - val_loss: 1551.9815 - val_mae: 1552.6748\n",
      "Epoch 187/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 60.3141 - mae: 60.9979 - val_loss: 1329.7243 - val_mae: 1330.4172\n",
      "Epoch 188/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 52.8292 - mae: 53.5110 - val_loss: 1240.3921 - val_mae: 1241.0852\n",
      "Epoch 189/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 57.9887 - mae: 58.6748 - val_loss: 1461.6817 - val_mae: 1462.3749\n",
      "Epoch 190/1000\n",
      "1461/1461 [==============================] - 0s 305us/step - loss: 62.2285 - mae: 62.9093 - val_loss: 1196.8044 - val_mae: 1197.4976\n",
      "Epoch 191/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 67.3854 - mae: 68.0696 - val_loss: 1405.0889 - val_mae: 1405.7810\n",
      "Epoch 192/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 54.3406 - mae: 55.0206 - val_loss: 1456.4596 - val_mae: 1457.1528\n",
      "Epoch 193/1000\n",
      "1461/1461 [==============================] - 1s 767us/step - loss: 55.3907 - mae: 56.0763 - val_loss: 1366.7965 - val_mae: 1367.4896\n",
      "Epoch 194/1000\n",
      "1461/1461 [==============================] - 1s 566us/step - loss: 57.6751 - mae: 58.3565 - val_loss: 1574.5261 - val_mae: 1575.2188\n",
      "Epoch 195/1000\n",
      "1461/1461 [==============================] - 0s 292us/step - loss: 54.8045 - mae: 55.4880 - val_loss: 1276.3914 - val_mae: 1277.0846\n",
      "Epoch 196/1000\n",
      "1461/1461 [==============================] - 0s 298us/step - loss: 56.1725 - mae: 56.8559 - val_loss: 1256.1464 - val_mae: 1256.8386\n",
      "Epoch 197/1000\n",
      "1461/1461 [==============================] - 0s 313us/step - loss: 70.2409 - mae: 70.9245 - val_loss: 1212.5343 - val_mae: 1213.2277\n",
      "Epoch 198/1000\n",
      "1461/1461 [==============================] - 0s 308us/step - loss: 57.4935 - mae: 58.1779 - val_loss: 1293.7726 - val_mae: 1294.4658\n",
      "Epoch 199/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 63.0993 - mae: 63.7822 - val_loss: 1028.1087 - val_mae: 1028.8016\n",
      "Epoch 200/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 87.4802 - mae: 88.1653 - val_loss: 1518.2488 - val_mae: 1518.9421\n",
      "Epoch 201/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 102.8621 - mae: 103.5487 - val_loss: 1309.9940 - val_mae: 1310.6873\n",
      "Epoch 202/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 59.4611 - mae: 60.1452 - val_loss: 1480.5224 - val_mae: 1481.2144\n",
      "Epoch 203/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 63.4007 - mae: 64.0852 - val_loss: 1422.6544 - val_mae: 1423.3475\n",
      "Epoch 204/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 64.7892 - mae: 65.4757 - val_loss: 1644.1234 - val_mae: 1644.8167\n",
      "Epoch 205/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 69.8829 - mae: 70.5653 - val_loss: 1206.8813 - val_mae: 1207.5745\n",
      "Epoch 206/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 62.3877 - mae: 63.0720 - val_loss: 1242.8831 - val_mae: 1243.5754\n",
      "Epoch 207/1000\n",
      "1461/1461 [==============================] - 0s 288us/step - loss: 61.6836 - mae: 62.3676 - val_loss: 1846.2912 - val_mae: 1846.9845\n",
      "Epoch 208/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 58.2328 - mae: 58.9171 - val_loss: 1351.0858 - val_mae: 1351.7791\n",
      "Epoch 209/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 68.5565 - mae: 69.2372 - val_loss: 1216.4965 - val_mae: 1217.1897\n",
      "Epoch 210/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 66.3397 - mae: 67.0248 - val_loss: 1053.8349 - val_mae: 1054.5281\n",
      "Epoch 211/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 88.0320 - mae: 88.7188 - val_loss: 1437.7123 - val_mae: 1438.4056\n",
      "Epoch 212/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 60.1035 - mae: 60.7872 - val_loss: 1687.7166 - val_mae: 1688.4097\n",
      "Epoch 213/1000\n",
      "1461/1461 [==============================] - 0s 294us/step - loss: 64.0608 - mae: 64.7459 - val_loss: 1648.1224 - val_mae: 1648.8156\n",
      "Epoch 214/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 75.8473 - mae: 76.5321 - val_loss: 1303.0895 - val_mae: 1303.7823\n",
      "Epoch 215/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 70.7863 - mae: 71.4716 - val_loss: 1507.4301 - val_mae: 1508.1228\n",
      "Epoch 216/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 62.5207 - mae: 63.2037 - val_loss: 1355.3291 - val_mae: 1356.0223\n",
      "Epoch 217/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 68.1043 - mae: 68.7863 - val_loss: 1625.1961 - val_mae: 1625.8892\n",
      "Epoch 218/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 66.7157 - mae: 67.3982 - val_loss: 1551.3609 - val_mae: 1552.0542\n",
      "Epoch 219/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 53.9877 - mae: 54.6700 - val_loss: 1549.5920 - val_mae: 1550.2852\n",
      "Epoch 220/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 341us/step - loss: 76.2374 - mae: 76.9217 - val_loss: 1370.6034 - val_mae: 1371.2965\n",
      "Epoch 221/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 73.1830 - mae: 73.8669 - val_loss: 1132.1940 - val_mae: 1132.8851\n",
      "Epoch 222/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 54.4735 - mae: 55.1539 - val_loss: 1568.6205 - val_mae: 1569.3136\n",
      "Epoch 223/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 49.5219 - mae: 50.2060 - val_loss: 1479.3499 - val_mae: 1480.0432\n",
      "Epoch 224/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 48.6452 - mae: 49.3252 - val_loss: 1199.4439 - val_mae: 1200.1371\n",
      "Epoch 225/1000\n",
      "1461/1461 [==============================] - 0s 306us/step - loss: 66.5411 - mae: 67.2269 - val_loss: 1597.2397 - val_mae: 1597.9329\n",
      "Epoch 226/1000\n",
      "1461/1461 [==============================] - 0s 281us/step - loss: 53.2838 - mae: 53.9661 - val_loss: 1424.8398 - val_mae: 1425.5330\n",
      "Epoch 227/1000\n",
      "1461/1461 [==============================] - 0s 305us/step - loss: 49.5886 - mae: 50.2702 - val_loss: 1416.7616 - val_mae: 1417.4547\n",
      "Epoch 228/1000\n",
      "1461/1461 [==============================] - 0s 304us/step - loss: 62.0765 - mae: 62.7566 - val_loss: 1363.3284 - val_mae: 1364.0215\n",
      "Epoch 229/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 89.4109 - mae: 90.0968 - val_loss: 1365.2154 - val_mae: 1365.9086\n",
      "Epoch 230/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 66.9561 - mae: 67.6389 - val_loss: 1493.3182 - val_mae: 1494.0112\n",
      "Epoch 231/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 66.7512 - mae: 67.4332 - val_loss: 1490.3103 - val_mae: 1491.0035\n",
      "Epoch 232/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 61.9301 - mae: 62.6099 - val_loss: 1528.4540 - val_mae: 1529.1471\n",
      "Epoch 233/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 50.7531 - mae: 51.4361 - val_loss: 1171.3890 - val_mae: 1172.0820\n",
      "Epoch 234/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 57.7265 - mae: 58.4089 - val_loss: 1320.1565 - val_mae: 1320.8494\n",
      "Epoch 235/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 61.8740 - mae: 62.5556 - val_loss: 1453.7503 - val_mae: 1454.4436\n",
      "Epoch 236/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 51.6142 - mae: 52.2945 - val_loss: 1399.4257 - val_mae: 1400.1189\n",
      "Epoch 237/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 70.8362 - mae: 71.5204 - val_loss: 1135.2094 - val_mae: 1135.9026\n",
      "Epoch 238/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 60.9835 - mae: 61.6646 - val_loss: 1468.5946 - val_mae: 1469.2878\n",
      "Epoch 239/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 57.0173 - mae: 57.7004 - val_loss: 1395.3933 - val_mae: 1396.0863\n",
      "Epoch 240/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 58.5251 - mae: 59.2064 - val_loss: 1150.1526 - val_mae: 1150.8453\n",
      "Epoch 241/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 52.4006 - mae: 53.0816 - val_loss: 1644.6011 - val_mae: 1645.2943\n",
      "Epoch 242/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 54.3757 - mae: 55.0581 - val_loss: 1458.7236 - val_mae: 1459.4165\n",
      "Epoch 243/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 74.8444 - mae: 75.5267 - val_loss: 1011.4052 - val_mae: 1012.0979\n",
      "Epoch 244/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 76.4513 - mae: 77.1336 - val_loss: 1716.9960 - val_mae: 1717.6892\n",
      "Epoch 245/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 56.2912 - mae: 56.9757 - val_loss: 1275.9366 - val_mae: 1276.6299\n",
      "Epoch 246/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 63.3233 - mae: 64.0091 - val_loss: 1236.8249 - val_mae: 1237.5175\n",
      "Epoch 247/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 64.1040 - mae: 64.7910 - val_loss: 1269.0694 - val_mae: 1269.7625\n",
      "Epoch 248/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 56.6343 - mae: 57.3169 - val_loss: 1522.5915 - val_mae: 1523.2848\n",
      "Epoch 249/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 52.5992 - mae: 53.2812 - val_loss: 1298.4162 - val_mae: 1299.1093\n",
      "Epoch 250/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 53.9082 - mae: 54.5925 - val_loss: 1563.0578 - val_mae: 1563.7509\n",
      "Epoch 251/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 62.3119 - mae: 62.9957 - val_loss: 1498.4544 - val_mae: 1499.1476\n",
      "Epoch 252/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 54.0663 - mae: 54.7511 - val_loss: 1635.4197 - val_mae: 1636.1129\n",
      "Epoch 253/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 61.6754 - mae: 62.3590 - val_loss: 1079.5358 - val_mae: 1080.2290\n",
      "Epoch 254/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 71.0620 - mae: 71.7477 - val_loss: 1181.3432 - val_mae: 1182.0360\n",
      "Epoch 255/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 60.1689 - mae: 60.8534 - val_loss: 1296.8492 - val_mae: 1297.5424\n",
      "Epoch 256/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 58.3882 - mae: 59.0715 - val_loss: 1266.4163 - val_mae: 1267.1094\n",
      "Epoch 257/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 51.5025 - mae: 52.1855 - val_loss: 1428.0025 - val_mae: 1428.6954\n",
      "Epoch 258/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 69.7254 - mae: 70.4060 - val_loss: 1738.4226 - val_mae: 1739.1157\n",
      "Epoch 259/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 67.3515 - mae: 68.0375 - val_loss: 1449.9708 - val_mae: 1450.6639\n",
      "Epoch 260/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 64.9622 - mae: 65.6447 - val_loss: 1353.9811 - val_mae: 1354.6735\n",
      "Epoch 261/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 53.6212 - mae: 54.3011 - val_loss: 1152.5616 - val_mae: 1153.2544\n",
      "Epoch 262/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 64.8211 - mae: 65.5066 - val_loss: 1446.7708 - val_mae: 1447.4640\n",
      "Epoch 263/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 53.7314 - mae: 54.4103 - val_loss: 1441.1506 - val_mae: 1441.8439\n",
      "Epoch 264/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 50.3098 - mae: 50.9929 - val_loss: 1447.9966 - val_mae: 1448.6886\n",
      "Epoch 265/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 68.6585 - mae: 69.3411 - val_loss: 1211.2656 - val_mae: 1211.9586\n",
      "Epoch 266/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 47.5247 - mae: 48.2042 - val_loss: 1396.7741 - val_mae: 1397.4674\n",
      "Epoch 267/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 54.7863 - mae: 55.4667 - val_loss: 1425.9559 - val_mae: 1426.6489\n",
      "Epoch 268/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 52.4522 - mae: 53.1347 - val_loss: 1368.5032 - val_mae: 1369.1960\n",
      "Epoch 269/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 60.4037 - mae: 61.0853 - val_loss: 1565.0452 - val_mae: 1565.7384\n",
      "Epoch 270/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 68.7269 - mae: 69.4133 - val_loss: 1143.5808 - val_mae: 1144.2740\n",
      "Epoch 271/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 54.1022 - mae: 54.7840 - val_loss: 1515.7454 - val_mae: 1516.4386\n",
      "Epoch 272/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 49.0520 - mae: 49.7344 - val_loss: 1745.5002 - val_mae: 1746.1932\n",
      "Epoch 273/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 61.5490 - mae: 62.2335 - val_loss: 1399.6619 - val_mae: 1400.3551\n",
      "Epoch 274/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 84.4230 - mae: 85.1080 - val_loss: 1454.7460 - val_mae: 1455.4380\n",
      "Epoch 275/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 272us/step - loss: 70.5638 - mae: 71.2503 - val_loss: 1424.0653 - val_mae: 1424.7584\n",
      "Epoch 276/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 60.8240 - mae: 61.5115 - val_loss: 1280.8794 - val_mae: 1281.5725\n",
      "Epoch 277/1000\n",
      "1461/1461 [==============================] - 0s 289us/step - loss: 54.2359 - mae: 54.9181 - val_loss: 1503.3685 - val_mae: 1504.0614\n",
      "Epoch 278/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 51.2752 - mae: 51.9548 - val_loss: 1573.3290 - val_mae: 1574.0203\n",
      "Epoch 279/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 52.3653 - mae: 53.0493 - val_loss: 1342.7018 - val_mae: 1343.3951\n",
      "Epoch 280/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 49.4231 - mae: 50.1041 - val_loss: 1455.3372 - val_mae: 1456.0294\n",
      "Epoch 281/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 49.5286 - mae: 50.2112 - val_loss: 1186.8635 - val_mae: 1187.5565\n",
      "Epoch 282/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 48.3351 - mae: 49.0146 - val_loss: 1189.8962 - val_mae: 1190.5883\n",
      "Epoch 283/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 48.7033 - mae: 49.3827 - val_loss: 1480.7119 - val_mae: 1481.4049\n",
      "Epoch 284/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 58.9270 - mae: 59.6070 - val_loss: 1396.5028 - val_mae: 1397.1948\n",
      "Epoch 285/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 60.4933 - mae: 61.1783 - val_loss: 1373.4574 - val_mae: 1374.1505\n",
      "Epoch 286/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 48.0595 - mae: 48.7403 - val_loss: 1392.8617 - val_mae: 1393.5549\n",
      "Epoch 287/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 49.3941 - mae: 50.0805 - val_loss: 1246.3103 - val_mae: 1247.0035\n",
      "Epoch 288/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 52.5210 - mae: 53.1987 - val_loss: 1595.4230 - val_mae: 1596.1161\n",
      "Epoch 289/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 57.6243 - mae: 58.3039 - val_loss: 1410.3219 - val_mae: 1411.0153\n",
      "Epoch 290/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 71.0505 - mae: 71.7341 - val_loss: 1838.8754 - val_mae: 1839.5687\n",
      "Epoch 291/1000\n",
      "1461/1461 [==============================] - 0s 314us/step - loss: 73.5143 - mae: 74.2015 - val_loss: 1209.7139 - val_mae: 1210.4070\n",
      "Epoch 292/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 63.1299 - mae: 63.8172 - val_loss: 1525.3309 - val_mae: 1526.0242\n",
      "Epoch 293/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 49.3428 - mae: 50.0284 - val_loss: 1442.0641 - val_mae: 1442.7565\n",
      "Epoch 294/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 47.1712 - mae: 47.8548 - val_loss: 1294.0099 - val_mae: 1294.7031\n",
      "Epoch 295/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 55.0248 - mae: 55.7071 - val_loss: 1402.3607 - val_mae: 1403.0540\n",
      "Epoch 296/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 56.1520 - mae: 56.8336 - val_loss: 1056.3837 - val_mae: 1057.0769\n",
      "Epoch 297/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 91.0507 - mae: 91.7388 - val_loss: 1416.9790 - val_mae: 1417.6722\n",
      "Epoch 298/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 59.8758 - mae: 60.5595 - val_loss: 1385.2478 - val_mae: 1385.9408\n",
      "Epoch 299/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 52.1176 - mae: 52.7973 - val_loss: 1726.7583 - val_mae: 1727.4515\n",
      "Epoch 300/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 58.5457 - mae: 59.2291 - val_loss: 1498.3834 - val_mae: 1499.0768\n",
      "Epoch 301/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 57.2458 - mae: 57.9285 - val_loss: 1717.0950 - val_mae: 1717.7882\n",
      "Epoch 302/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 79.4242 - mae: 80.1070 - val_loss: 1337.7560 - val_mae: 1338.4492\n",
      "Epoch 303/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 54.0636 - mae: 54.7486 - val_loss: 1205.9097 - val_mae: 1206.6006\n",
      "Epoch 304/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 45.6529 - mae: 46.3351 - val_loss: 1418.1415 - val_mae: 1418.8347\n",
      "Epoch 305/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 51.1303 - mae: 51.8157 - val_loss: 1244.6052 - val_mae: 1245.2983\n",
      "Epoch 306/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 53.1036 - mae: 53.7847 - val_loss: 1390.0413 - val_mae: 1390.7344\n",
      "Epoch 307/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 50.4013 - mae: 51.0847 - val_loss: 1389.5013 - val_mae: 1390.1945\n",
      "Epoch 308/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 46.6021 - mae: 47.2798 - val_loss: 1476.1467 - val_mae: 1476.8398\n",
      "Epoch 309/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 57.5223 - mae: 58.2045 - val_loss: 1517.2952 - val_mae: 1517.9885\n",
      "Epoch 310/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 62.5242 - mae: 63.2082 - val_loss: 1447.7966 - val_mae: 1448.4897\n",
      "Epoch 311/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 57.1494 - mae: 57.8336 - val_loss: 1029.8941 - val_mae: 1030.5873\n",
      "Epoch 312/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 67.2680 - mae: 67.9507 - val_loss: 1261.7987 - val_mae: 1262.4908\n",
      "Epoch 313/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 62.4201 - mae: 63.1020 - val_loss: 1567.9319 - val_mae: 1568.6251\n",
      "Epoch 314/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 60.6107 - mae: 61.2942 - val_loss: 1344.0020 - val_mae: 1344.6953\n",
      "Epoch 315/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 52.8319 - mae: 53.5130 - val_loss: 1442.5016 - val_mae: 1443.1947\n",
      "Epoch 316/1000\n",
      "1461/1461 [==============================] - 0s 289us/step - loss: 68.1478 - mae: 68.8316 - val_loss: 1587.1030 - val_mae: 1587.7961\n",
      "Epoch 317/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 53.7553 - mae: 54.4373 - val_loss: 1459.1646 - val_mae: 1459.8578\n",
      "Epoch 318/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 51.6661 - mae: 52.3475 - val_loss: 1262.8482 - val_mae: 1263.5404\n",
      "Epoch 319/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 49.8501 - mae: 50.5329 - val_loss: 1424.3614 - val_mae: 1425.0544\n",
      "Epoch 320/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 52.1969 - mae: 52.8770 - val_loss: 1466.7572 - val_mae: 1467.4503\n",
      "Epoch 321/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 53.8566 - mae: 54.5408 - val_loss: 1207.7243 - val_mae: 1208.4167\n",
      "Epoch 322/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 50.7447 - mae: 51.4308 - val_loss: 1384.2177 - val_mae: 1384.9108\n",
      "Epoch 323/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 48.5747 - mae: 49.2583 - val_loss: 1324.8325 - val_mae: 1325.5248\n",
      "Epoch 324/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 54.2636 - mae: 54.9437 - val_loss: 1440.7707 - val_mae: 1441.4637\n",
      "Epoch 325/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 50.8144 - mae: 51.4964 - val_loss: 1380.1832 - val_mae: 1380.8766\n",
      "Epoch 326/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 44.4604 - mae: 45.1432 - val_loss: 1215.7418 - val_mae: 1216.4347\n",
      "Epoch 327/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 60.4305 - mae: 61.1160 - val_loss: 1438.2165 - val_mae: 1438.9097\n",
      "Epoch 328/1000\n",
      "1461/1461 [==============================] - 0s 292us/step - loss: 50.7704 - mae: 51.4496 - val_loss: 1660.3837 - val_mae: 1661.0769\n",
      "Epoch 329/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 59.1085 - mae: 59.7959 - val_loss: 1120.0545 - val_mae: 1120.7469\n",
      "Epoch 330/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 270us/step - loss: 69.6103 - mae: 70.2950 - val_loss: 1455.7949 - val_mae: 1456.4878\n",
      "Epoch 331/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 77.3805 - mae: 78.0650 - val_loss: 1601.3182 - val_mae: 1602.0116\n",
      "Epoch 332/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 64.5024 - mae: 65.1865 - val_loss: 1240.7737 - val_mae: 1241.4669\n",
      "Epoch 333/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 63.4356 - mae: 64.1181 - val_loss: 1465.7385 - val_mae: 1466.4318\n",
      "Epoch 334/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 50.9061 - mae: 51.5855 - val_loss: 1487.1861 - val_mae: 1487.8794\n",
      "Epoch 335/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 45.5386 - mae: 46.2180 - val_loss: 1243.0131 - val_mae: 1243.7063\n",
      "Epoch 336/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 56.6786 - mae: 57.3602 - val_loss: 1441.5334 - val_mae: 1442.2267\n",
      "Epoch 337/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 50.7758 - mae: 51.4582 - val_loss: 1464.7234 - val_mae: 1465.4164\n",
      "Epoch 338/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 48.7867 - mae: 49.4659 - val_loss: 1541.5706 - val_mae: 1542.2628\n",
      "Epoch 339/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 74.3948 - mae: 75.0776 - val_loss: 1358.1560 - val_mae: 1358.8492\n",
      "Epoch 340/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 51.0302 - mae: 51.7136 - val_loss: 1338.7071 - val_mae: 1339.4001\n",
      "Epoch 341/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 53.5313 - mae: 54.2136 - val_loss: 1401.3330 - val_mae: 1402.0260\n",
      "Epoch 342/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 60.8765 - mae: 61.5603 - val_loss: 1475.9112 - val_mae: 1476.6045\n",
      "Epoch 343/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 50.7814 - mae: 51.4621 - val_loss: 1446.2512 - val_mae: 1446.9443\n",
      "Epoch 344/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 54.0019 - mae: 54.6823 - val_loss: 1434.0709 - val_mae: 1434.7640\n",
      "Epoch 345/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 52.7474 - mae: 53.4300 - val_loss: 1673.7006 - val_mae: 1674.3938\n",
      "Epoch 346/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 49.3071 - mae: 49.9887 - val_loss: 1497.4395 - val_mae: 1498.1323\n",
      "Epoch 347/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 45.4991 - mae: 46.1776 - val_loss: 1252.6995 - val_mae: 1253.3926\n",
      "Epoch 348/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 49.2490 - mae: 49.9309 - val_loss: 1351.8370 - val_mae: 1352.5303\n",
      "Epoch 349/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 54.9017 - mae: 55.5825 - val_loss: 1261.7092 - val_mae: 1262.4025\n",
      "Epoch 350/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 45.8194 - mae: 46.5016 - val_loss: 1310.0118 - val_mae: 1310.7039\n",
      "Epoch 351/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 46.7593 - mae: 47.4371 - val_loss: 1466.8679 - val_mae: 1467.5603\n",
      "Epoch 352/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 46.0590 - mae: 46.7402 - val_loss: 1648.6704 - val_mae: 1649.3633\n",
      "Epoch 353/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 64.1004 - mae: 64.7815 - val_loss: 1595.6420 - val_mae: 1596.3350\n",
      "Epoch 354/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 53.4095 - mae: 54.0901 - val_loss: 1359.4373 - val_mae: 1360.1300\n",
      "Epoch 355/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 56.3966 - mae: 57.0795 - val_loss: 1641.5613 - val_mae: 1642.2546\n",
      "Epoch 356/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 57.2459 - mae: 57.9280 - val_loss: 1154.1451 - val_mae: 1154.8383\n",
      "Epoch 357/1000\n",
      "1461/1461 [==============================] - 0s 299us/step - loss: 51.8268 - mae: 52.5100 - val_loss: 1569.2481 - val_mae: 1569.9413\n",
      "Epoch 358/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 47.3602 - mae: 48.0369 - val_loss: 1445.3773 - val_mae: 1446.0697\n",
      "Epoch 359/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 63.9741 - mae: 64.6582 - val_loss: 1362.8375 - val_mae: 1363.5309\n",
      "Epoch 360/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 60.3728 - mae: 61.0584 - val_loss: 1337.9092 - val_mae: 1338.6023\n",
      "Epoch 361/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 49.6020 - mae: 50.2812 - val_loss: 1388.4481 - val_mae: 1389.1414\n",
      "Epoch 362/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 42.6452 - mae: 43.3255 - val_loss: 1479.4875 - val_mae: 1480.1805\n",
      "Epoch 363/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 56.5756 - mae: 57.2551 - val_loss: 1296.3599 - val_mae: 1297.0531\n",
      "Epoch 364/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 69.2092 - mae: 69.8917 - val_loss: 1478.9806 - val_mae: 1479.6737\n",
      "Epoch 365/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 52.1813 - mae: 52.8595 - val_loss: 1212.8145 - val_mae: 1213.5073\n",
      "Epoch 366/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 59.6247 - mae: 60.3068 - val_loss: 1463.9901 - val_mae: 1464.6833\n",
      "Epoch 367/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 62.6360 - mae: 63.3219 - val_loss: 1256.0560 - val_mae: 1256.7491\n",
      "Epoch 368/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 52.6186 - mae: 53.3007 - val_loss: 1188.8754 - val_mae: 1189.5686\n",
      "Epoch 369/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 53.4470 - mae: 54.1311 - val_loss: 1571.7480 - val_mae: 1572.4412\n",
      "Epoch 370/1000\n",
      "1461/1461 [==============================] - 1s 551us/step - loss: 56.8177 - mae: 57.5020 - val_loss: 1233.3926 - val_mae: 1234.0854\n",
      "Epoch 371/1000\n",
      "1461/1461 [==============================] - 1s 663us/step - loss: 49.9603 - mae: 50.6412 - val_loss: 1378.0137 - val_mae: 1378.7068\n",
      "Epoch 372/1000\n",
      "1461/1461 [==============================] - 1s 612us/step - loss: 47.3941 - mae: 48.0741 - val_loss: 1567.0902 - val_mae: 1567.7834\n",
      "Epoch 373/1000\n",
      "1461/1461 [==============================] - 1s 619us/step - loss: 53.7686 - mae: 54.4496 - val_loss: 1286.6739 - val_mae: 1287.3665\n",
      "Epoch 374/1000\n",
      "1461/1461 [==============================] - 1s 694us/step - loss: 58.0625 - mae: 58.7471 - val_loss: 1329.0849 - val_mae: 1329.7780\n",
      "Epoch 375/1000\n",
      "1461/1461 [==============================] - 1s 719us/step - loss: 45.6233 - mae: 46.3004 - val_loss: 1358.9671 - val_mae: 1359.6599\n",
      "Epoch 376/1000\n",
      "1461/1461 [==============================] - 1s 464us/step - loss: 43.7268 - mae: 44.4069 - val_loss: 1458.1437 - val_mae: 1458.8367\n",
      "Epoch 377/1000\n",
      "1461/1461 [==============================] - 0s 304us/step - loss: 44.6665 - mae: 45.3484 - val_loss: 1407.5814 - val_mae: 1408.2743\n",
      "Epoch 378/1000\n",
      "1461/1461 [==============================] - 0s 323us/step - loss: 45.8736 - mae: 46.5474 - val_loss: 1299.0104 - val_mae: 1299.7032\n",
      "Epoch 379/1000\n",
      "1461/1461 [==============================] - 0s 287us/step - loss: 39.1192 - mae: 39.7962 - val_loss: 1246.4648 - val_mae: 1247.1578\n",
      "Epoch 380/1000\n",
      "1461/1461 [==============================] - 0s 283us/step - loss: 48.9929 - mae: 49.6775 - val_loss: 1289.5133 - val_mae: 1290.2065\n",
      "Epoch 381/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 59.8097 - mae: 60.4939 - val_loss: 1270.9599 - val_mae: 1271.6523\n",
      "Epoch 382/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 47.4278 - mae: 48.1108 - val_loss: 1312.8170 - val_mae: 1313.5100\n",
      "Epoch 383/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 49.0311 - mae: 49.7127 - val_loss: 1636.2102 - val_mae: 1636.9033\n",
      "Epoch 384/1000\n",
      "1461/1461 [==============================] - 0s 304us/step - loss: 73.2722 - mae: 73.9596 - val_loss: 1443.1999 - val_mae: 1443.8932\n",
      "Epoch 385/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 292us/step - loss: 61.0067 - mae: 61.6892 - val_loss: 1564.2371 - val_mae: 1564.9304\n",
      "Epoch 386/1000\n",
      "1461/1461 [==============================] - 0s 301us/step - loss: 55.5230 - mae: 56.2055 - val_loss: 1383.7148 - val_mae: 1384.4073\n",
      "Epoch 387/1000\n",
      "1461/1461 [==============================] - 0s 306us/step - loss: 50.2430 - mae: 50.9231 - val_loss: 1276.2603 - val_mae: 1276.9523\n",
      "Epoch 388/1000\n",
      "1461/1461 [==============================] - 0s 289us/step - loss: 53.0621 - mae: 53.7430 - val_loss: 1325.3812 - val_mae: 1326.0741\n",
      "Epoch 389/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 50.1654 - mae: 50.8476 - val_loss: 1450.9812 - val_mae: 1451.6741\n",
      "Epoch 390/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 69.9792 - mae: 70.6590 - val_loss: 1329.8801 - val_mae: 1330.5732\n",
      "Epoch 391/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 58.7088 - mae: 59.3908 - val_loss: 1357.4019 - val_mae: 1358.0950\n",
      "Epoch 392/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 46.8917 - mae: 47.5700 - val_loss: 1215.5549 - val_mae: 1216.2480\n",
      "Epoch 393/1000\n",
      "1461/1461 [==============================] - 0s 287us/step - loss: 61.6293 - mae: 62.3133 - val_loss: 1476.5028 - val_mae: 1477.1959\n",
      "Epoch 394/1000\n",
      "1461/1461 [==============================] - 0s 281us/step - loss: 59.9697 - mae: 60.6516 - val_loss: 1098.2085 - val_mae: 1098.9016\n",
      "Epoch 395/1000\n",
      "1461/1461 [==============================] - 0s 283us/step - loss: 45.9638 - mae: 46.6429 - val_loss: 1355.4254 - val_mae: 1356.1187\n",
      "Epoch 396/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 38.8112 - mae: 39.4887 - val_loss: 1464.1437 - val_mae: 1464.8369\n",
      "Epoch 397/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 52.0051 - mae: 52.6854 - val_loss: 1401.5878 - val_mae: 1402.2802\n",
      "Epoch 398/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 53.1598 - mae: 53.8418 - val_loss: 1246.5252 - val_mae: 1247.2185\n",
      "Epoch 399/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 56.5876 - mae: 57.2703 - val_loss: 1488.6381 - val_mae: 1489.3313\n",
      "Epoch 400/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 52.6692 - mae: 53.3526 - val_loss: 1356.1320 - val_mae: 1356.8252\n",
      "Epoch 401/1000\n",
      "1461/1461 [==============================] - 0s 294us/step - loss: 58.0759 - mae: 58.7610 - val_loss: 1728.4999 - val_mae: 1729.1927\n",
      "Epoch 402/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 44.8906 - mae: 45.5732 - val_loss: 1289.7766 - val_mae: 1290.4696\n",
      "Epoch 403/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 45.4277 - mae: 46.1067 - val_loss: 1396.9294 - val_mae: 1397.6224\n",
      "Epoch 404/1000\n",
      "1461/1461 [==============================] - 0s 297us/step - loss: 46.4824 - mae: 47.1644 - val_loss: 1369.9089 - val_mae: 1370.6019\n",
      "Epoch 405/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 40.7619 - mae: 41.4383 - val_loss: 1589.5971 - val_mae: 1590.2903\n",
      "Epoch 406/1000\n",
      "1461/1461 [==============================] - 0s 297us/step - loss: 46.6833 - mae: 47.3600 - val_loss: 1309.2083 - val_mae: 1309.9014\n",
      "Epoch 407/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 57.6887 - mae: 58.3716 - val_loss: 1305.0307 - val_mae: 1305.7239\n",
      "Epoch 408/1000\n",
      "1461/1461 [==============================] - 0s 290us/step - loss: 49.6577 - mae: 50.3384 - val_loss: 1268.2211 - val_mae: 1268.9143\n",
      "Epoch 409/1000\n",
      "1461/1461 [==============================] - 0s 320us/step - loss: 46.7583 - mae: 47.4388 - val_loss: 1429.2301 - val_mae: 1429.9232\n",
      "Epoch 410/1000\n",
      "1461/1461 [==============================] - 0s 293us/step - loss: 46.4166 - mae: 47.0999 - val_loss: 1576.4346 - val_mae: 1577.1278\n",
      "Epoch 411/1000\n",
      "1461/1461 [==============================] - 0s 281us/step - loss: 44.4576 - mae: 45.1388 - val_loss: 1399.9239 - val_mae: 1400.6172\n",
      "Epoch 412/1000\n",
      "1461/1461 [==============================] - 0s 281us/step - loss: 52.7964 - mae: 53.4766 - val_loss: 1569.6578 - val_mae: 1570.3511\n",
      "Epoch 413/1000\n",
      "1461/1461 [==============================] - 0s 326us/step - loss: 58.0491 - mae: 58.7311 - val_loss: 1297.6931 - val_mae: 1298.3861\n",
      "Epoch 414/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 61.0237 - mae: 61.7087 - val_loss: 1439.6087 - val_mae: 1440.3018\n",
      "Epoch 415/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 64.6054 - mae: 65.2879 - val_loss: 948.8986 - val_mae: 949.5915\n",
      "Epoch 416/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 55.8908 - mae: 56.5721 - val_loss: 1439.1936 - val_mae: 1439.8866\n",
      "Epoch 417/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 46.4657 - mae: 47.1482 - val_loss: 1194.5371 - val_mae: 1195.2302\n",
      "Epoch 418/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 38.1855 - mae: 38.8678 - val_loss: 1393.8457 - val_mae: 1394.5389\n",
      "Epoch 419/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 54.4958 - mae: 55.1800 - val_loss: 1202.9434 - val_mae: 1203.6365\n",
      "Epoch 420/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 50.3825 - mae: 51.0644 - val_loss: 1368.0476 - val_mae: 1368.7408\n",
      "Epoch 421/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 58.3075 - mae: 58.9872 - val_loss: 1010.1763 - val_mae: 1010.8692\n",
      "Epoch 422/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 49.0479 - mae: 49.7276 - val_loss: 1336.2689 - val_mae: 1336.9620\n",
      "Epoch 423/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 46.2668 - mae: 46.9496 - val_loss: 1424.1672 - val_mae: 1424.8604\n",
      "Epoch 424/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 44.5638 - mae: 45.2461 - val_loss: 1353.8408 - val_mae: 1354.5337\n",
      "Epoch 425/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 50.1754 - mae: 50.8545 - val_loss: 1711.8050 - val_mae: 1712.4980\n",
      "Epoch 426/1000\n",
      "1461/1461 [==============================] - 0s 301us/step - loss: 54.1920 - mae: 54.8726 - val_loss: 1658.1778 - val_mae: 1658.8711\n",
      "Epoch 427/1000\n",
      "1461/1461 [==============================] - 0s 281us/step - loss: 49.3641 - mae: 50.0483 - val_loss: 1519.6049 - val_mae: 1520.2977\n",
      "Epoch 428/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 45.7538 - mae: 46.4350 - val_loss: 1548.2077 - val_mae: 1548.9009\n",
      "Epoch 429/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 55.6657 - mae: 56.3483 - val_loss: 1044.0202 - val_mae: 1044.7108\n",
      "Epoch 430/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 52.4954 - mae: 53.1772 - val_loss: 1469.2806 - val_mae: 1469.9738\n",
      "Epoch 431/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 47.8732 - mae: 48.5559 - val_loss: 1412.8831 - val_mae: 1413.5762\n",
      "Epoch 432/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 54.9498 - mae: 55.6336 - val_loss: 1674.6113 - val_mae: 1675.3047\n",
      "Epoch 433/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 48.2765 - mae: 48.9530 - val_loss: 1649.5519 - val_mae: 1650.2452\n",
      "Epoch 434/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 49.1163 - mae: 49.7987 - val_loss: 1484.7599 - val_mae: 1485.4531\n",
      "Epoch 435/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 51.0702 - mae: 51.7484 - val_loss: 1475.3034 - val_mae: 1475.9966\n",
      "Epoch 436/1000\n",
      "1461/1461 [==============================] - 0s 288us/step - loss: 45.2385 - mae: 45.9190 - val_loss: 1539.6956 - val_mae: 1540.3888\n",
      "Epoch 437/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 43.6315 - mae: 44.3103 - val_loss: 1438.6220 - val_mae: 1439.3152\n",
      "Epoch 438/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 44.9608 - mae: 45.6417 - val_loss: 1473.5740 - val_mae: 1474.2672\n",
      "Epoch 439/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 42.6092 - mae: 43.2894 - val_loss: 1236.8165 - val_mae: 1237.5096\n",
      "Epoch 440/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 270us/step - loss: 40.4655 - mae: 41.1456 - val_loss: 1257.3121 - val_mae: 1258.0052\n",
      "Epoch 441/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 57.4079 - mae: 58.0935 - val_loss: 1392.2389 - val_mae: 1392.9320\n",
      "Epoch 442/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 51.7489 - mae: 52.4314 - val_loss: 1392.2216 - val_mae: 1392.9149\n",
      "Epoch 443/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 46.7423 - mae: 47.4241 - val_loss: 1090.3146 - val_mae: 1091.0077\n",
      "Epoch 444/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 65.5575 - mae: 66.2396 - val_loss: 1242.4418 - val_mae: 1243.1350\n",
      "Epoch 445/1000\n",
      "1461/1461 [==============================] - 0s 290us/step - loss: 40.5972 - mae: 41.2784 - val_loss: 1331.7040 - val_mae: 1332.3971\n",
      "Epoch 446/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 45.8486 - mae: 46.5314 - val_loss: 1376.5525 - val_mae: 1377.2457\n",
      "Epoch 447/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 45.3793 - mae: 46.0617 - val_loss: 1175.0226 - val_mae: 1175.7153\n",
      "Epoch 448/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 49.4747 - mae: 50.1550 - val_loss: 1458.4620 - val_mae: 1459.1554\n",
      "Epoch 449/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 39.5891 - mae: 40.2656 - val_loss: 1266.6273 - val_mae: 1267.3204\n",
      "Epoch 450/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 48.6059 - mae: 49.2844 - val_loss: 1486.7874 - val_mae: 1487.4805\n",
      "Epoch 451/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 59.2294 - mae: 59.9101 - val_loss: 1510.1391 - val_mae: 1510.8323\n",
      "Epoch 452/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 46.4318 - mae: 47.1156 - val_loss: 1413.5093 - val_mae: 1414.2026\n",
      "Epoch 453/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 53.8522 - mae: 54.5340 - val_loss: 1379.3957 - val_mae: 1380.0880\n",
      "Epoch 454/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 55.7742 - mae: 56.4546 - val_loss: 1289.6665 - val_mae: 1290.3597\n",
      "Epoch 455/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 45.8733 - mae: 46.5549 - val_loss: 1363.0531 - val_mae: 1363.7463\n",
      "Epoch 456/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 43.8385 - mae: 44.5204 - val_loss: 1326.6151 - val_mae: 1327.3081\n",
      "Epoch 457/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 48.7920 - mae: 49.4732 - val_loss: 1121.3308 - val_mae: 1122.0239\n",
      "Epoch 458/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 45.4217 - mae: 46.1007 - val_loss: 1220.6614 - val_mae: 1221.3533\n",
      "Epoch 459/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 43.8177 - mae: 44.4997 - val_loss: 1485.3374 - val_mae: 1486.0305\n",
      "Epoch 460/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 49.7595 - mae: 50.4430 - val_loss: 1474.3481 - val_mae: 1475.0414\n",
      "Epoch 461/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 51.2203 - mae: 51.9023 - val_loss: 1341.1741 - val_mae: 1341.8673\n",
      "Epoch 462/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 51.0257 - mae: 51.7083 - val_loss: 1469.1289 - val_mae: 1469.8220\n",
      "Epoch 463/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 48.6366 - mae: 49.3200 - val_loss: 1271.9662 - val_mae: 1272.6593\n",
      "Epoch 464/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 49.0932 - mae: 49.7760 - val_loss: 1643.6348 - val_mae: 1644.3280\n",
      "Epoch 465/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 56.7415 - mae: 57.4236 - val_loss: 1433.6866 - val_mae: 1434.3788\n",
      "Epoch 466/1000\n",
      "1461/1461 [==============================] - 0s 297us/step - loss: 47.3639 - mae: 48.0414 - val_loss: 1323.5003 - val_mae: 1324.1936\n",
      "Epoch 467/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 56.7865 - mae: 57.4665 - val_loss: 1597.3010 - val_mae: 1597.9941\n",
      "Epoch 468/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 71.1336 - mae: 71.8172 - val_loss: 1465.0618 - val_mae: 1465.7551\n",
      "Epoch 469/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 58.1425 - mae: 58.8257 - val_loss: 1471.1129 - val_mae: 1471.8060\n",
      "Epoch 470/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 56.3404 - mae: 57.0225 - val_loss: 1329.5265 - val_mae: 1330.2196\n",
      "Epoch 471/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 48.2519 - mae: 48.9355 - val_loss: 1321.6661 - val_mae: 1322.3594\n",
      "Epoch 472/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 44.0647 - mae: 44.7440 - val_loss: 1100.3838 - val_mae: 1101.0769\n",
      "Epoch 473/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 46.5924 - mae: 47.2770 - val_loss: 1348.9350 - val_mae: 1349.6283\n",
      "Epoch 474/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 46.3368 - mae: 47.0182 - val_loss: 1255.6815 - val_mae: 1256.3748\n",
      "Epoch 475/1000\n",
      "1461/1461 [==============================] - 0s 303us/step - loss: 59.3416 - mae: 60.0218 - val_loss: 1128.2959 - val_mae: 1128.9891\n",
      "Epoch 476/1000\n",
      "1461/1461 [==============================] - 0s 301us/step - loss: 69.1308 - mae: 69.8143 - val_loss: 1166.1473 - val_mae: 1166.8405\n",
      "Epoch 477/1000\n",
      "1461/1461 [==============================] - 0s 316us/step - loss: 45.3112 - mae: 45.9932 - val_loss: 1576.1073 - val_mae: 1576.8005\n",
      "Epoch 478/1000\n",
      "1461/1461 [==============================] - 0s 317us/step - loss: 39.7305 - mae: 40.4111 - val_loss: 1469.4932 - val_mae: 1470.1862\n",
      "Epoch 479/1000\n",
      "1461/1461 [==============================] - 1s 651us/step - loss: 44.9273 - mae: 45.6081 - val_loss: 1500.3682 - val_mae: 1501.0614\n",
      "Epoch 480/1000\n",
      "1461/1461 [==============================] - 1s 394us/step - loss: 38.5847 - mae: 39.2615 - val_loss: 1465.8448 - val_mae: 1466.5381\n",
      "Epoch 481/1000\n",
      "1461/1461 [==============================] - 1s 368us/step - loss: 40.8988 - mae: 41.5736 - val_loss: 1366.2682 - val_mae: 1366.9614\n",
      "Epoch 482/1000\n",
      "1461/1461 [==============================] - 1s 885us/step - loss: 43.6874 - mae: 44.3668 - val_loss: 1495.4009 - val_mae: 1496.0941\n",
      "Epoch 483/1000\n",
      "1461/1461 [==============================] - 0s 335us/step - loss: 49.6217 - mae: 50.3046 - val_loss: 1521.5161 - val_mae: 1522.2094\n",
      "Epoch 484/1000\n",
      "1461/1461 [==============================] - 0s 292us/step - loss: 48.0534 - mae: 48.7318 - val_loss: 1370.6321 - val_mae: 1371.3251\n",
      "Epoch 485/1000\n",
      "1461/1461 [==============================] - 0s 313us/step - loss: 47.4188 - mae: 48.0997 - val_loss: 1547.4024 - val_mae: 1548.0950\n",
      "Epoch 486/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 54.2264 - mae: 54.9056 - val_loss: 1413.3101 - val_mae: 1414.0034\n",
      "Epoch 487/1000\n",
      "1461/1461 [==============================] - 0s 336us/step - loss: 47.5391 - mae: 48.2188 - val_loss: 1486.1950 - val_mae: 1486.8882\n",
      "Epoch 488/1000\n",
      "1461/1461 [==============================] - 0s 291us/step - loss: 52.3899 - mae: 53.0673 - val_loss: 1592.4815 - val_mae: 1593.1747\n",
      "Epoch 489/1000\n",
      "1461/1461 [==============================] - 1s 369us/step - loss: 39.6947 - mae: 40.3746 - val_loss: 1356.1642 - val_mae: 1356.8573\n",
      "Epoch 490/1000\n",
      "1461/1461 [==============================] - 1s 463us/step - loss: 38.9636 - mae: 39.6433 - val_loss: 1373.0093 - val_mae: 1373.7026\n",
      "Epoch 491/1000\n",
      "1461/1461 [==============================] - 0s 287us/step - loss: 41.7179 - mae: 42.3978 - val_loss: 1212.3906 - val_mae: 1213.0831\n",
      "Epoch 492/1000\n",
      "1461/1461 [==============================] - 0s 292us/step - loss: 54.9421 - mae: 55.6261 - val_loss: 1304.4519 - val_mae: 1305.1450\n",
      "Epoch 493/1000\n",
      "1461/1461 [==============================] - 0s 306us/step - loss: 44.2345 - mae: 44.9132 - val_loss: 1493.6656 - val_mae: 1494.3588\n",
      "Epoch 494/1000\n",
      "1461/1461 [==============================] - 0s 303us/step - loss: 52.8766 - mae: 53.5597 - val_loss: 1172.2779 - val_mae: 1172.9711\n",
      "Epoch 495/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 1s 346us/step - loss: 62.5381 - mae: 63.2185 - val_loss: 1284.8245 - val_mae: 1285.5177\n",
      "Epoch 496/1000\n",
      "1461/1461 [==============================] - 0s 309us/step - loss: 55.6311 - mae: 56.3150 - val_loss: 1356.9323 - val_mae: 1357.6251\n",
      "Epoch 497/1000\n",
      "1461/1461 [==============================] - 0s 321us/step - loss: 49.7750 - mae: 50.4537 - val_loss: 1247.1724 - val_mae: 1247.8655\n",
      "Epoch 498/1000\n",
      "1461/1461 [==============================] - 0s 342us/step - loss: 54.0484 - mae: 54.7300 - val_loss: 1331.3384 - val_mae: 1332.0312\n",
      "Epoch 499/1000\n",
      "1461/1461 [==============================] - 0s 311us/step - loss: 45.9099 - mae: 46.5908 - val_loss: 1289.1699 - val_mae: 1289.8630\n",
      "Epoch 500/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 45.6419 - mae: 46.3215 - val_loss: 1485.7508 - val_mae: 1486.4432\n",
      "Epoch 501/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 38.8573 - mae: 39.5319 - val_loss: 1351.6887 - val_mae: 1352.3818\n",
      "Epoch 502/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 49.9594 - mae: 50.6425 - val_loss: 1163.6165 - val_mae: 1164.3096\n",
      "Epoch 503/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 52.2123 - mae: 52.8887 - val_loss: 1320.6041 - val_mae: 1321.2961\n",
      "Epoch 504/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 44.6589 - mae: 45.3397 - val_loss: 1419.9407 - val_mae: 1420.6322\n",
      "Epoch 505/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 42.3111 - mae: 42.9897 - val_loss: 1455.9592 - val_mae: 1456.6521\n",
      "Epoch 506/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 47.5147 - mae: 48.1942 - val_loss: 1214.0389 - val_mae: 1214.7319\n",
      "Epoch 507/1000\n",
      "1461/1461 [==============================] - 0s 261us/step - loss: 51.1956 - mae: 51.8778 - val_loss: 1457.1761 - val_mae: 1457.8693\n",
      "Epoch 508/1000\n",
      "1461/1461 [==============================] - 0s 300us/step - loss: 57.4780 - mae: 58.1591 - val_loss: 1388.7669 - val_mae: 1389.4600\n",
      "Epoch 509/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 53.8824 - mae: 54.5628 - val_loss: 1453.2061 - val_mae: 1453.8993\n",
      "Epoch 510/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 47.1737 - mae: 47.8554 - val_loss: 1334.0722 - val_mae: 1334.7654\n",
      "Epoch 511/1000\n",
      "1461/1461 [==============================] - 0s 263us/step - loss: 54.3989 - mae: 55.0792 - val_loss: 1431.2761 - val_mae: 1431.9692\n",
      "Epoch 512/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 45.0538 - mae: 45.7370 - val_loss: 1177.5387 - val_mae: 1178.2318\n",
      "Epoch 513/1000\n",
      "1461/1461 [==============================] - 0s 262us/step - loss: 42.5991 - mae: 43.2802 - val_loss: 1430.4615 - val_mae: 1431.1548\n",
      "Epoch 514/1000\n",
      "1461/1461 [==============================] - 0s 263us/step - loss: 43.7593 - mae: 44.4421 - val_loss: 1322.1855 - val_mae: 1322.8787\n",
      "Epoch 515/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 48.5333 - mae: 49.2113 - val_loss: 1472.9912 - val_mae: 1473.6844\n",
      "Epoch 516/1000\n",
      "1461/1461 [==============================] - 0s 258us/step - loss: 39.8803 - mae: 40.5612 - val_loss: 1418.0020 - val_mae: 1418.6952\n",
      "Epoch 517/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 46.8935 - mae: 47.5756 - val_loss: 1320.7546 - val_mae: 1321.4480\n",
      "Epoch 518/1000\n",
      "1461/1461 [==============================] - 0s 259us/step - loss: 49.3525 - mae: 50.0299 - val_loss: 1393.7359 - val_mae: 1394.4286\n",
      "Epoch 519/1000\n",
      "1461/1461 [==============================] - 0s 259us/step - loss: 45.0749 - mae: 45.7572 - val_loss: 1350.0946 - val_mae: 1350.7880\n",
      "Epoch 520/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 43.8749 - mae: 44.5520 - val_loss: 1342.3288 - val_mae: 1343.0219\n",
      "Epoch 521/1000\n",
      "1461/1461 [==============================] - 0s 259us/step - loss: 48.5108 - mae: 49.1919 - val_loss: 1322.0880 - val_mae: 1322.7798\n",
      "Epoch 522/1000\n",
      "1461/1461 [==============================] - 0s 261us/step - loss: 45.4874 - mae: 46.1697 - val_loss: 1344.5654 - val_mae: 1345.2584\n",
      "Epoch 523/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 46.0947 - mae: 46.7762 - val_loss: 1623.6857 - val_mae: 1624.3789\n",
      "Epoch 524/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 44.1894 - mae: 44.8695 - val_loss: 1639.3951 - val_mae: 1640.0885\n",
      "Epoch 525/1000\n",
      "1461/1461 [==============================] - 0s 261us/step - loss: 40.1290 - mae: 40.8046 - val_loss: 1391.6298 - val_mae: 1392.3230\n",
      "Epoch 526/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 41.2695 - mae: 41.9453 - val_loss: 1248.8402 - val_mae: 1249.5331\n",
      "Epoch 527/1000\n",
      "1461/1461 [==============================] - 0s 258us/step - loss: 46.3346 - mae: 47.0175 - val_loss: 1355.6956 - val_mae: 1356.3888\n",
      "Epoch 528/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 39.7151 - mae: 40.3976 - val_loss: 1464.8417 - val_mae: 1465.5349\n",
      "Epoch 529/1000\n",
      "1461/1461 [==============================] - 0s 258us/step - loss: 46.2173 - mae: 46.8990 - val_loss: 1250.9264 - val_mae: 1251.6195\n",
      "Epoch 530/1000\n",
      "1461/1461 [==============================] - 0s 263us/step - loss: 63.1883 - mae: 63.8712 - val_loss: 1528.9791 - val_mae: 1529.6722\n",
      "Epoch 531/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 51.3723 - mae: 52.0522 - val_loss: 1470.3599 - val_mae: 1471.0530\n",
      "Epoch 532/1000\n",
      "1461/1461 [==============================] - 0s 259us/step - loss: 48.2244 - mae: 48.9024 - val_loss: 1439.3584 - val_mae: 1440.0514\n",
      "Epoch 533/1000\n",
      "1461/1461 [==============================] - 0s 259us/step - loss: 52.7114 - mae: 53.3896 - val_loss: 1443.5463 - val_mae: 1444.2395\n",
      "Epoch 534/1000\n",
      "1461/1461 [==============================] - 0s 256us/step - loss: 43.4751 - mae: 44.1540 - val_loss: 1313.8231 - val_mae: 1314.5159\n",
      "Epoch 535/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 65.2655 - mae: 65.9493 - val_loss: 1237.7338 - val_mae: 1238.4269\n",
      "Epoch 536/1000\n",
      "1461/1461 [==============================] - 0s 257us/step - loss: 51.9078 - mae: 52.5890 - val_loss: 1580.7613 - val_mae: 1581.4546\n",
      "Epoch 537/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 38.8402 - mae: 39.5218 - val_loss: 1412.9584 - val_mae: 1413.6515\n",
      "Epoch 538/1000\n",
      "1461/1461 [==============================] - 0s 261us/step - loss: 40.8985 - mae: 41.5751 - val_loss: 1275.6410 - val_mae: 1276.3331\n",
      "Epoch 539/1000\n",
      "1461/1461 [==============================] - 0s 283us/step - loss: 48.1172 - mae: 48.7962 - val_loss: 1157.5662 - val_mae: 1158.2594\n",
      "Epoch 540/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 44.5444 - mae: 45.2270 - val_loss: 1399.0558 - val_mae: 1399.7483\n",
      "Epoch 541/1000\n",
      "1461/1461 [==============================] - 0s 262us/step - loss: 40.7225 - mae: 41.4030 - val_loss: 1600.8514 - val_mae: 1601.5446\n",
      "Epoch 542/1000\n",
      "1461/1461 [==============================] - 0s 259us/step - loss: 69.1774 - mae: 69.8594 - val_loss: 1756.1808 - val_mae: 1756.8739\n",
      "Epoch 543/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 55.3269 - mae: 56.0091 - val_loss: 1352.8321 - val_mae: 1353.5253\n",
      "Epoch 544/1000\n",
      "1461/1461 [==============================] - 0s 253us/step - loss: 55.5197 - mae: 56.1998 - val_loss: 856.8816 - val_mae: 857.5734\n",
      "Epoch 545/1000\n",
      "1461/1461 [==============================] - 0s 258us/step - loss: 49.8817 - mae: 50.5611 - val_loss: 1391.2003 - val_mae: 1391.8928\n",
      "Epoch 546/1000\n",
      "1461/1461 [==============================] - 0s 262us/step - loss: 44.3552 - mae: 45.0330 - val_loss: 1397.4804 - val_mae: 1398.1735\n",
      "Epoch 547/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 38.9520 - mae: 39.6296 - val_loss: 1245.6960 - val_mae: 1246.3893\n",
      "Epoch 548/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 55.1746 - mae: 55.8540 - val_loss: 1334.9389 - val_mae: 1335.6318\n",
      "Epoch 549/1000\n",
      "1461/1461 [==============================] - 0s 261us/step - loss: 47.5193 - mae: 48.2029 - val_loss: 1386.1496 - val_mae: 1386.8429\n",
      "Epoch 550/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 257us/step - loss: 39.3905 - mae: 40.0675 - val_loss: 1519.1353 - val_mae: 1519.8286\n",
      "Epoch 551/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 40.2762 - mae: 40.9544 - val_loss: 1453.9005 - val_mae: 1454.5936\n",
      "Epoch 552/1000\n",
      "1461/1461 [==============================] - 0s 258us/step - loss: 44.0011 - mae: 44.6809 - val_loss: 1652.4220 - val_mae: 1653.1154\n",
      "Epoch 553/1000\n",
      "1461/1461 [==============================] - 0s 258us/step - loss: 64.5810 - mae: 65.2647 - val_loss: 1263.3238 - val_mae: 1264.0162\n",
      "Epoch 554/1000\n",
      "1461/1461 [==============================] - 0s 258us/step - loss: 51.4693 - mae: 52.1484 - val_loss: 1365.7049 - val_mae: 1366.3970\n",
      "Epoch 555/1000\n",
      "1461/1461 [==============================] - 0s 257us/step - loss: 45.0187 - mae: 45.6988 - val_loss: 1469.9756 - val_mae: 1470.6687\n",
      "Epoch 556/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 49.4976 - mae: 50.1730 - val_loss: 1588.6581 - val_mae: 1589.3512\n",
      "Epoch 557/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 46.8473 - mae: 47.5274 - val_loss: 1223.7148 - val_mae: 1224.4080\n",
      "Epoch 558/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 44.8180 - mae: 45.4997 - val_loss: 1348.7851 - val_mae: 1349.4777\n",
      "Epoch 559/1000\n",
      "1461/1461 [==============================] - 0s 256us/step - loss: 56.4558 - mae: 57.1351 - val_loss: 1448.0891 - val_mae: 1448.7819\n",
      "Epoch 560/1000\n",
      "1461/1461 [==============================] - 0s 258us/step - loss: 51.4396 - mae: 52.1216 - val_loss: 1372.4442 - val_mae: 1373.1375\n",
      "Epoch 561/1000\n",
      "1461/1461 [==============================] - 0s 256us/step - loss: 51.9164 - mae: 52.5970 - val_loss: 1379.5964 - val_mae: 1380.2889\n",
      "Epoch 562/1000\n",
      "1461/1461 [==============================] - 0s 257us/step - loss: 49.0242 - mae: 49.7053 - val_loss: 1468.8124 - val_mae: 1469.5057\n",
      "Epoch 563/1000\n",
      "1461/1461 [==============================] - 0s 258us/step - loss: 47.4132 - mae: 48.0935 - val_loss: 1571.3358 - val_mae: 1572.0291\n",
      "Epoch 564/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 47.1693 - mae: 47.8482 - val_loss: 1475.0336 - val_mae: 1475.7268\n",
      "Epoch 565/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 42.2480 - mae: 42.9261 - val_loss: 1328.4271 - val_mae: 1329.1202\n",
      "Epoch 566/1000\n",
      "1461/1461 [==============================] - 0s 259us/step - loss: 51.0278 - mae: 51.7069 - val_loss: 1206.6927 - val_mae: 1207.3856\n",
      "Epoch 567/1000\n",
      "1461/1461 [==============================] - 0s 259us/step - loss: 45.1817 - mae: 45.8659 - val_loss: 1229.7982 - val_mae: 1230.4913\n",
      "Epoch 568/1000\n",
      "1461/1461 [==============================] - 0s 260us/step - loss: 51.0420 - mae: 51.7235 - val_loss: 1344.3270 - val_mae: 1345.0201\n",
      "Epoch 569/1000\n",
      "1461/1461 [==============================] - 0s 255us/step - loss: 42.3115 - mae: 42.9865 - val_loss: 1472.4391 - val_mae: 1473.1322\n",
      "Epoch 570/1000\n",
      "1461/1461 [==============================] - 0s 256us/step - loss: 51.4229 - mae: 52.1074 - val_loss: 1273.9373 - val_mae: 1274.6302\n",
      "Epoch 571/1000\n",
      "1461/1461 [==============================] - 0s 256us/step - loss: 42.9021 - mae: 43.5834 - val_loss: 1267.2490 - val_mae: 1267.9423\n",
      "Epoch 572/1000\n",
      "1461/1461 [==============================] - 0s 257us/step - loss: 44.7503 - mae: 45.4292 - val_loss: 1627.3666 - val_mae: 1628.0597\n",
      "Epoch 573/1000\n",
      "1461/1461 [==============================] - 0s 289us/step - loss: 44.2312 - mae: 44.9123 - val_loss: 1431.8195 - val_mae: 1432.5127\n",
      "Epoch 574/1000\n",
      "1461/1461 [==============================] - 0s 304us/step - loss: 48.5858 - mae: 49.2656 - val_loss: 1135.8213 - val_mae: 1136.5144\n",
      "Epoch 575/1000\n",
      "1461/1461 [==============================] - 0s 317us/step - loss: 48.2410 - mae: 48.9208 - val_loss: 1262.0066 - val_mae: 1262.6997\n",
      "Epoch 576/1000\n",
      "1461/1461 [==============================] - 1s 349us/step - loss: 46.1888 - mae: 46.8622 - val_loss: 1427.3563 - val_mae: 1428.0496\n",
      "Epoch 577/1000\n",
      "1461/1461 [==============================] - 0s 292us/step - loss: 44.2541 - mae: 44.9328 - val_loss: 1444.9857 - val_mae: 1445.6786\n",
      "Epoch 578/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 45.9098 - mae: 46.5898 - val_loss: 1291.2233 - val_mae: 1291.9159\n",
      "Epoch 579/1000\n",
      "1461/1461 [==============================] - 0s 291us/step - loss: 37.3841 - mae: 38.0643 - val_loss: 1465.8562 - val_mae: 1466.5494\n",
      "Epoch 580/1000\n",
      "1461/1461 [==============================] - 0s 310us/step - loss: 45.3332 - mae: 46.0107 - val_loss: 1387.0207 - val_mae: 1387.7139\n",
      "Epoch 581/1000\n",
      "1461/1461 [==============================] - 1s 410us/step - loss: 44.7478 - mae: 45.4273 - val_loss: 1351.4682 - val_mae: 1352.1608\n",
      "Epoch 582/1000\n",
      "1461/1461 [==============================] - 0s 292us/step - loss: 45.1980 - mae: 45.8731 - val_loss: 1173.9665 - val_mae: 1174.6571\n",
      "Epoch 583/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 44.0684 - mae: 44.7458 - val_loss: 1284.5725 - val_mae: 1285.2646\n",
      "Epoch 584/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 36.3785 - mae: 37.0583 - val_loss: 1315.5551 - val_mae: 1316.2472\n",
      "Epoch 585/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 38.6418 - mae: 39.3172 - val_loss: 1342.0100 - val_mae: 1342.7032\n",
      "Epoch 586/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 40.6810 - mae: 41.3544 - val_loss: 1446.8955 - val_mae: 1447.5889\n",
      "Epoch 587/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 42.6989 - mae: 43.3760 - val_loss: 1587.5884 - val_mae: 1588.2815\n",
      "Epoch 588/1000\n",
      "1461/1461 [==============================] - 0s 340us/step - loss: 48.6315 - mae: 49.3100 - val_loss: 1098.2263 - val_mae: 1098.9194\n",
      "Epoch 589/1000\n",
      "1461/1461 [==============================] - 0s 321us/step - loss: 49.2457 - mae: 49.9267 - val_loss: 1693.2942 - val_mae: 1693.9874\n",
      "Epoch 590/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 43.5152 - mae: 44.1965 - val_loss: 1395.9115 - val_mae: 1396.6047\n",
      "Epoch 591/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 45.1606 - mae: 45.8375 - val_loss: 1568.3519 - val_mae: 1569.0449\n",
      "Epoch 592/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 52.5174 - mae: 53.1965 - val_loss: 1146.2050 - val_mae: 1146.8982\n",
      "Epoch 593/1000\n",
      "1461/1461 [==============================] - 0s 297us/step - loss: 49.5952 - mae: 50.2710 - val_loss: 1668.3619 - val_mae: 1669.0553\n",
      "Epoch 594/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 56.3795 - mae: 57.0560 - val_loss: 1589.4188 - val_mae: 1590.1121\n",
      "Epoch 595/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 45.8432 - mae: 46.5235 - val_loss: 1204.4824 - val_mae: 1205.1757\n",
      "Epoch 596/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 47.6032 - mae: 48.2829 - val_loss: 1191.5695 - val_mae: 1192.2615\n",
      "Epoch 597/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 42.2987 - mae: 42.9729 - val_loss: 1303.8001 - val_mae: 1304.4934\n",
      "Epoch 598/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 47.0916 - mae: 47.7698 - val_loss: 1291.1898 - val_mae: 1291.8829\n",
      "Epoch 599/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 48.0595 - mae: 48.7399 - val_loss: 1244.4291 - val_mae: 1245.1223\n",
      "Epoch 600/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 48.0122 - mae: 48.6907 - val_loss: 1140.0232 - val_mae: 1140.7164\n",
      "Epoch 601/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 42.6138 - mae: 43.2931 - val_loss: 1140.8366 - val_mae: 1141.5292\n",
      "Epoch 602/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 50.6902 - mae: 51.3675 - val_loss: 1312.2828 - val_mae: 1312.9757\n",
      "Epoch 603/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 45.0280 - mae: 45.7052 - val_loss: 1393.2224 - val_mae: 1393.9144\n",
      "Epoch 604/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 60.0846 - mae: 60.7693 - val_loss: 1478.7287 - val_mae: 1479.4215\n",
      "Epoch 605/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 295us/step - loss: 41.8538 - mae: 42.5319 - val_loss: 1288.7422 - val_mae: 1289.4353\n",
      "Epoch 606/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 43.4279 - mae: 44.1058 - val_loss: 1189.8518 - val_mae: 1190.5446\n",
      "Epoch 607/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 46.7396 - mae: 47.4203 - val_loss: 1590.3071 - val_mae: 1591.0002\n",
      "Epoch 608/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 49.9187 - mae: 50.5987 - val_loss: 1286.8298 - val_mae: 1287.5229\n",
      "Epoch 609/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 45.6305 - mae: 46.3106 - val_loss: 1215.0786 - val_mae: 1215.7717\n",
      "Epoch 610/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 40.3469 - mae: 41.0259 - val_loss: 1341.3146 - val_mae: 1342.0077\n",
      "Epoch 611/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 37.3549 - mae: 38.0360 - val_loss: 1415.5745 - val_mae: 1416.2668\n",
      "Epoch 612/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 63.9488 - mae: 64.6326 - val_loss: 1711.4371 - val_mae: 1712.1293\n",
      "Epoch 613/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 48.2156 - mae: 48.8931 - val_loss: 1528.2963 - val_mae: 1528.9895\n",
      "Epoch 614/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 54.7639 - mae: 55.4436 - val_loss: 1416.4131 - val_mae: 1417.1058\n",
      "Epoch 615/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 39.5612 - mae: 40.2401 - val_loss: 1361.2767 - val_mae: 1361.9696\n",
      "Epoch 616/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 45.7591 - mae: 46.4380 - val_loss: 1647.1560 - val_mae: 1647.8491\n",
      "Epoch 617/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 53.6525 - mae: 54.3334 - val_loss: 1241.8832 - val_mae: 1242.5763\n",
      "Epoch 618/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 44.2215 - mae: 44.9033 - val_loss: 1136.9987 - val_mae: 1137.6907\n",
      "Epoch 619/1000\n",
      "1461/1461 [==============================] - 0s 296us/step - loss: 50.5618 - mae: 51.2407 - val_loss: 1301.5213 - val_mae: 1302.2145\n",
      "Epoch 620/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 52.0707 - mae: 52.7540 - val_loss: 1489.7638 - val_mae: 1490.4569\n",
      "Epoch 621/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 37.9295 - mae: 38.6098 - val_loss: 1300.8430 - val_mae: 1301.5352\n",
      "Epoch 622/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 45.4516 - mae: 46.1355 - val_loss: 1354.4050 - val_mae: 1355.0981\n",
      "Epoch 623/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 50.1269 - mae: 50.8071 - val_loss: 1359.2257 - val_mae: 1359.9187\n",
      "Epoch 624/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 39.0479 - mae: 39.7284 - val_loss: 1412.0029 - val_mae: 1412.6959\n",
      "Epoch 625/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 36.3161 - mae: 36.9920 - val_loss: 1321.7956 - val_mae: 1322.4886\n",
      "Epoch 626/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 34.6743 - mae: 35.3489 - val_loss: 1650.5742 - val_mae: 1651.2673\n",
      "Epoch 627/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 42.0224 - mae: 42.6992 - val_loss: 1400.4188 - val_mae: 1401.1111\n",
      "Epoch 628/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 46.7937 - mae: 47.4747 - val_loss: 1483.0800 - val_mae: 1483.7732\n",
      "Epoch 629/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 45.4378 - mae: 46.1204 - val_loss: 1403.1159 - val_mae: 1403.8091\n",
      "Epoch 630/1000\n",
      "1461/1461 [==============================] - 1s 483us/step - loss: 56.3675 - mae: 57.0506 - val_loss: 1288.2051 - val_mae: 1288.8981\n",
      "Epoch 631/1000\n",
      "1461/1461 [==============================] - 0s 293us/step - loss: 50.2860 - mae: 50.9674 - val_loss: 1317.7391 - val_mae: 1318.4323\n",
      "Epoch 632/1000\n",
      "1461/1461 [==============================] - 0s 289us/step - loss: 55.4517 - mae: 56.1352 - val_loss: 1591.0413 - val_mae: 1591.7345\n",
      "Epoch 633/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 43.3428 - mae: 44.0230 - val_loss: 1371.1642 - val_mae: 1371.8572\n",
      "Epoch 634/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 37.2869 - mae: 37.9648 - val_loss: 1411.5690 - val_mae: 1412.2611\n",
      "Epoch 635/1000\n",
      "1461/1461 [==============================] - 0s 281us/step - loss: 38.1405 - mae: 38.8169 - val_loss: 1378.1022 - val_mae: 1378.7952\n",
      "Epoch 636/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 46.4712 - mae: 47.1527 - val_loss: 1451.3134 - val_mae: 1452.0065\n",
      "Epoch 637/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 39.9934 - mae: 40.6699 - val_loss: 1302.2943 - val_mae: 1302.9869\n",
      "Epoch 638/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 42.0745 - mae: 42.7523 - val_loss: 1457.4887 - val_mae: 1458.1818\n",
      "Epoch 639/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 40.9406 - mae: 41.6180 - val_loss: 1415.8733 - val_mae: 1416.5664\n",
      "Epoch 640/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 47.6481 - mae: 48.3305 - val_loss: 1506.6231 - val_mae: 1507.3163\n",
      "Epoch 641/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 49.4111 - mae: 50.0900 - val_loss: 1227.0673 - val_mae: 1227.7604\n",
      "Epoch 642/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 55.4285 - mae: 56.1115 - val_loss: 1232.3009 - val_mae: 1232.9930\n",
      "Epoch 643/1000\n",
      "1461/1461 [==============================] - 0s 305us/step - loss: 42.6232 - mae: 43.3035 - val_loss: 1295.9231 - val_mae: 1296.6165\n",
      "Epoch 644/1000\n",
      "1461/1461 [==============================] - 0s 317us/step - loss: 39.6322 - mae: 40.3101 - val_loss: 1377.3270 - val_mae: 1378.0200\n",
      "Epoch 645/1000\n",
      "1461/1461 [==============================] - 0s 320us/step - loss: 36.7569 - mae: 37.4373 - val_loss: 1576.0282 - val_mae: 1576.7212\n",
      "Epoch 646/1000\n",
      "1461/1461 [==============================] - 0s 310us/step - loss: 48.4379 - mae: 49.1203 - val_loss: 1380.1072 - val_mae: 1380.8003\n",
      "Epoch 647/1000\n",
      "1461/1461 [==============================] - 0s 315us/step - loss: 56.5504 - mae: 57.2315 - val_loss: 1023.7310 - val_mae: 1024.4242\n",
      "Epoch 648/1000\n",
      "1461/1461 [==============================] - 1s 423us/step - loss: 43.9019 - mae: 44.5790 - val_loss: 1217.3024 - val_mae: 1217.9955\n",
      "Epoch 649/1000\n",
      "1461/1461 [==============================] - 0s 287us/step - loss: 45.9390 - mae: 46.6159 - val_loss: 1434.2525 - val_mae: 1434.9457\n",
      "Epoch 650/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 38.0262 - mae: 38.7031 - val_loss: 1407.8746 - val_mae: 1408.5679\n",
      "Epoch 651/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 39.0767 - mae: 39.7529 - val_loss: 1315.8978 - val_mae: 1316.5911\n",
      "Epoch 652/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 54.8810 - mae: 55.5604 - val_loss: 1464.9153 - val_mae: 1465.6084\n",
      "Epoch 653/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 39.5653 - mae: 40.2407 - val_loss: 1411.7829 - val_mae: 1412.4761\n",
      "Epoch 654/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 44.2556 - mae: 44.9357 - val_loss: 1340.2971 - val_mae: 1340.9905\n",
      "Epoch 655/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 40.1963 - mae: 40.8775 - val_loss: 1388.2346 - val_mae: 1388.9275\n",
      "Epoch 656/1000\n",
      "1461/1461 [==============================] - 0s 287us/step - loss: 33.3097 - mae: 33.9838 - val_loss: 1453.7701 - val_mae: 1454.4633\n",
      "Epoch 657/1000\n",
      "1461/1461 [==============================] - 0s 288us/step - loss: 39.7354 - mae: 40.4168 - val_loss: 1302.3994 - val_mae: 1303.0925\n",
      "Epoch 658/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 37.0067 - mae: 37.6822 - val_loss: 1432.8045 - val_mae: 1433.4956\n",
      "Epoch 659/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 36.3645 - mae: 37.0433 - val_loss: 1266.9569 - val_mae: 1267.6500\n",
      "Epoch 660/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 269us/step - loss: 45.1598 - mae: 45.8409 - val_loss: 1107.8058 - val_mae: 1108.4980\n",
      "Epoch 661/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 49.4131 - mae: 50.0941 - val_loss: 1606.1072 - val_mae: 1606.8004\n",
      "Epoch 662/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 52.0545 - mae: 52.7325 - val_loss: 1495.8196 - val_mae: 1496.5126\n",
      "Epoch 663/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 43.8302 - mae: 44.5074 - val_loss: 1342.4688 - val_mae: 1343.1619\n",
      "Epoch 664/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 48.7134 - mae: 49.3932 - val_loss: 1371.0542 - val_mae: 1371.7476\n",
      "Epoch 665/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 49.3150 - mae: 49.9947 - val_loss: 1364.2583 - val_mae: 1364.9507\n",
      "Epoch 666/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 39.8541 - mae: 40.5361 - val_loss: 1293.6706 - val_mae: 1294.3634\n",
      "Epoch 667/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 43.0824 - mae: 43.7639 - val_loss: 1416.7807 - val_mae: 1417.4739\n",
      "Epoch 668/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 40.7391 - mae: 41.4196 - val_loss: 1167.3565 - val_mae: 1168.0492\n",
      "Epoch 669/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 40.3370 - mae: 41.0145 - val_loss: 1502.4694 - val_mae: 1503.1626\n",
      "Epoch 670/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 38.6527 - mae: 39.3276 - val_loss: 1314.1278 - val_mae: 1314.8209\n",
      "Epoch 671/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 44.1770 - mae: 44.8583 - val_loss: 1333.6157 - val_mae: 1334.3081\n",
      "Epoch 672/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 43.4024 - mae: 44.0834 - val_loss: 1190.1125 - val_mae: 1190.8057\n",
      "Epoch 673/1000\n",
      "1461/1461 [==============================] - 0s 297us/step - loss: 48.7714 - mae: 49.4544 - val_loss: 1708.9318 - val_mae: 1709.6251\n",
      "Epoch 674/1000\n",
      "1461/1461 [==============================] - 0s 283us/step - loss: 61.1015 - mae: 61.7846 - val_loss: 1478.1273 - val_mae: 1478.8204\n",
      "Epoch 675/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 37.5885 - mae: 38.2671 - val_loss: 1316.6820 - val_mae: 1317.3749\n",
      "Epoch 676/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 37.4925 - mae: 38.1708 - val_loss: 1585.8658 - val_mae: 1586.5590\n",
      "Epoch 677/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 43.4470 - mae: 44.1217 - val_loss: 1525.0414 - val_mae: 1525.7340\n",
      "Epoch 678/1000\n",
      "1461/1461 [==============================] - 0s 292us/step - loss: 40.9460 - mae: 41.6226 - val_loss: 1278.0192 - val_mae: 1278.7123\n",
      "Epoch 679/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 40.2169 - mae: 40.8981 - val_loss: 1399.5849 - val_mae: 1400.2782\n",
      "Epoch 680/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 40.5429 - mae: 41.2236 - val_loss: 1213.6279 - val_mae: 1214.3209\n",
      "Epoch 681/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 52.3742 - mae: 53.0550 - val_loss: 1044.1435 - val_mae: 1044.8361\n",
      "Epoch 682/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 45.5339 - mae: 46.2137 - val_loss: 1360.3860 - val_mae: 1361.0779\n",
      "Epoch 683/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 42.9569 - mae: 43.6393 - val_loss: 1491.8637 - val_mae: 1492.5568\n",
      "Epoch 684/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 39.7280 - mae: 40.4058 - val_loss: 1375.7550 - val_mae: 1376.4480\n",
      "Epoch 685/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 37.5564 - mae: 38.2321 - val_loss: 1387.8412 - val_mae: 1388.5344\n",
      "Epoch 686/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 40.0205 - mae: 40.7000 - val_loss: 1420.8224 - val_mae: 1421.5156\n",
      "Epoch 687/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 41.6725 - mae: 42.3489 - val_loss: 1501.5398 - val_mae: 1502.2329\n",
      "Epoch 688/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 40.3689 - mae: 41.0408 - val_loss: 1426.0087 - val_mae: 1426.7008\n",
      "Epoch 689/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 45.4107 - mae: 46.0894 - val_loss: 1277.8647 - val_mae: 1278.5576\n",
      "Epoch 690/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 38.5232 - mae: 39.1989 - val_loss: 1340.6871 - val_mae: 1341.3794\n",
      "Epoch 691/1000\n",
      "1461/1461 [==============================] - 0s 301us/step - loss: 37.0114 - mae: 37.6889 - val_loss: 1323.4656 - val_mae: 1324.1588\n",
      "Epoch 692/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 48.4973 - mae: 49.1777 - val_loss: 1192.8606 - val_mae: 1193.5538\n",
      "Epoch 693/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 41.1136 - mae: 41.7901 - val_loss: 1288.3751 - val_mae: 1289.0682\n",
      "Epoch 694/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 47.8594 - mae: 48.5387 - val_loss: 1262.8732 - val_mae: 1263.5652\n",
      "Epoch 695/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 50.4588 - mae: 51.1410 - val_loss: 1513.9653 - val_mae: 1514.6577\n",
      "Epoch 696/1000\n",
      "1461/1461 [==============================] - 0s 297us/step - loss: 42.1763 - mae: 42.8550 - val_loss: 1383.3061 - val_mae: 1383.9994\n",
      "Epoch 697/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 34.3599 - mae: 35.0377 - val_loss: 1102.7573 - val_mae: 1103.4504\n",
      "Epoch 698/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 58.9787 - mae: 59.6564 - val_loss: 1419.1635 - val_mae: 1419.8566\n",
      "Epoch 699/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 35.9370 - mae: 36.6139 - val_loss: 1443.8556 - val_mae: 1444.5487\n",
      "Epoch 700/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 37.5984 - mae: 38.2744 - val_loss: 1173.1365 - val_mae: 1173.8292\n",
      "Epoch 701/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 42.3516 - mae: 43.0316 - val_loss: 1405.7480 - val_mae: 1406.4410\n",
      "Epoch 702/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 37.3412 - mae: 38.0224 - val_loss: 1304.4968 - val_mae: 1305.1901\n",
      "Epoch 703/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 35.5807 - mae: 36.2534 - val_loss: 1249.7402 - val_mae: 1250.4333\n",
      "Epoch 704/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 38.0255 - mae: 38.7016 - val_loss: 1274.0590 - val_mae: 1274.7522\n",
      "Epoch 705/1000\n",
      "1461/1461 [==============================] - 0s 288us/step - loss: 43.8431 - mae: 44.5220 - val_loss: 1553.2930 - val_mae: 1553.9860\n",
      "Epoch 706/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 42.2982 - mae: 42.9810 - val_loss: 1242.0359 - val_mae: 1242.7289\n",
      "Epoch 707/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 48.8042 - mae: 49.4866 - val_loss: 1558.6792 - val_mae: 1559.3722\n",
      "Epoch 708/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 41.2531 - mae: 41.9327 - val_loss: 1486.2691 - val_mae: 1486.9620\n",
      "Epoch 709/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 47.2692 - mae: 47.9473 - val_loss: 1241.9314 - val_mae: 1242.6245\n",
      "Epoch 710/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 37.8466 - mae: 38.5248 - val_loss: 1424.9412 - val_mae: 1425.6343\n",
      "Epoch 711/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 48.8545 - mae: 49.5344 - val_loss: 1182.7657 - val_mae: 1183.4589\n",
      "Epoch 712/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 45.2151 - mae: 45.8920 - val_loss: 1540.8009 - val_mae: 1541.4940\n",
      "Epoch 713/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 47.9413 - mae: 48.6224 - val_loss: 1301.7772 - val_mae: 1302.4702\n",
      "Epoch 714/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 45.0497 - mae: 45.7261 - val_loss: 1358.8142 - val_mae: 1359.5072\n",
      "Epoch 715/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 268us/step - loss: 37.2148 - mae: 37.8975 - val_loss: 1318.4351 - val_mae: 1319.1273\n",
      "Epoch 716/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 46.5374 - mae: 47.2178 - val_loss: 1207.7277 - val_mae: 1208.4207\n",
      "Epoch 717/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 40.0949 - mae: 40.7752 - val_loss: 1414.4950 - val_mae: 1415.1879\n",
      "Epoch 718/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 36.9119 - mae: 37.5882 - val_loss: 1294.2231 - val_mae: 1294.9163\n",
      "Epoch 719/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 33.3333 - mae: 34.0079 - val_loss: 1325.9917 - val_mae: 1326.6848\n",
      "Epoch 720/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 46.2020 - mae: 46.8813 - val_loss: 1327.3670 - val_mae: 1328.0602\n",
      "Epoch 721/1000\n",
      "1461/1461 [==============================] - 0s 305us/step - loss: 41.9222 - mae: 42.5968 - val_loss: 1164.0641 - val_mae: 1164.7572\n",
      "Epoch 722/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 40.3242 - mae: 41.0002 - val_loss: 1250.9699 - val_mae: 1251.6631\n",
      "Epoch 723/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 64.6384 - mae: 65.3246 - val_loss: 1508.4931 - val_mae: 1509.1863\n",
      "Epoch 724/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 44.3659 - mae: 45.0410 - val_loss: 1295.4993 - val_mae: 1296.1925\n",
      "Epoch 725/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 48.2855 - mae: 48.9603 - val_loss: 1608.7870 - val_mae: 1609.4795\n",
      "Epoch 726/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 40.9066 - mae: 41.5848 - val_loss: 1074.9810 - val_mae: 1075.6741\n",
      "Epoch 727/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 39.9238 - mae: 40.5987 - val_loss: 1349.4538 - val_mae: 1350.1467\n",
      "Epoch 728/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 46.1925 - mae: 46.8732 - val_loss: 1502.1029 - val_mae: 1502.7959\n",
      "Epoch 729/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 41.4160 - mae: 42.0948 - val_loss: 1419.5105 - val_mae: 1420.2024\n",
      "Epoch 730/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 34.0584 - mae: 34.7369 - val_loss: 1260.8802 - val_mae: 1261.5734\n",
      "Epoch 731/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 48.7940 - mae: 49.4746 - val_loss: 1270.7119 - val_mae: 1271.4049\n",
      "Epoch 732/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 37.4093 - mae: 38.0870 - val_loss: 1371.0217 - val_mae: 1371.7148\n",
      "Epoch 733/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 44.1928 - mae: 44.8664 - val_loss: 1097.0403 - val_mae: 1097.7323\n",
      "Epoch 734/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 43.6172 - mae: 44.2963 - val_loss: 1301.4240 - val_mae: 1302.1158\n",
      "Epoch 735/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 44.4343 - mae: 45.1190 - val_loss: 1265.9428 - val_mae: 1266.6360\n",
      "Epoch 736/1000\n",
      "1461/1461 [==============================] - 0s 291us/step - loss: 46.0438 - mae: 46.7215 - val_loss: 1383.7768 - val_mae: 1384.4698\n",
      "Epoch 737/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 35.7889 - mae: 36.4680 - val_loss: 1305.6785 - val_mae: 1306.3706\n",
      "Epoch 738/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 40.9064 - mae: 41.5800 - val_loss: 1202.9930 - val_mae: 1203.6859\n",
      "Epoch 739/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 51.1326 - mae: 51.8134 - val_loss: 1412.2164 - val_mae: 1412.9095\n",
      "Epoch 740/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 31.5711 - mae: 32.2500 - val_loss: 1178.4991 - val_mae: 1179.1924\n",
      "Epoch 741/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 36.6580 - mae: 37.3394 - val_loss: 1324.7948 - val_mae: 1325.4875\n",
      "Epoch 742/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 39.6825 - mae: 40.3592 - val_loss: 1158.2324 - val_mae: 1158.9257\n",
      "Epoch 743/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 49.6328 - mae: 50.3122 - val_loss: 1349.2375 - val_mae: 1349.9298\n",
      "Epoch 744/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 37.4540 - mae: 38.1293 - val_loss: 1432.1851 - val_mae: 1432.8784\n",
      "Epoch 745/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 34.5034 - mae: 35.1811 - val_loss: 1409.7603 - val_mae: 1410.4532\n",
      "Epoch 746/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 36.2796 - mae: 36.9535 - val_loss: 1390.0821 - val_mae: 1390.7753\n",
      "Epoch 747/1000\n",
      "1461/1461 [==============================] - 0s 296us/step - loss: 36.5604 - mae: 37.2369 - val_loss: 1276.8401 - val_mae: 1277.5332\n",
      "Epoch 748/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 37.7035 - mae: 38.3801 - val_loss: 1326.1532 - val_mae: 1326.8462\n",
      "Epoch 749/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 37.0231 - mae: 37.7019 - val_loss: 1511.6477 - val_mae: 1512.3407\n",
      "Epoch 750/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 40.2669 - mae: 40.9426 - val_loss: 1144.8141 - val_mae: 1145.5072\n",
      "Epoch 751/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 48.1731 - mae: 48.8515 - val_loss: 1422.4035 - val_mae: 1423.0956\n",
      "Epoch 752/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 37.2155 - mae: 37.8897 - val_loss: 1248.2979 - val_mae: 1248.9906\n",
      "Epoch 753/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 43.8906 - mae: 44.5670 - val_loss: 1602.2497 - val_mae: 1602.9420\n",
      "Epoch 754/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 37.7990 - mae: 38.4775 - val_loss: 1316.3274 - val_mae: 1317.0205\n",
      "Epoch 755/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 43.4018 - mae: 44.0807 - val_loss: 1183.8328 - val_mae: 1184.5260\n",
      "Epoch 756/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 39.1078 - mae: 39.7825 - val_loss: 1352.4537 - val_mae: 1353.1467\n",
      "Epoch 757/1000\n",
      "1461/1461 [==============================] - 0s 264us/step - loss: 40.0384 - mae: 40.7146 - val_loss: 1197.3404 - val_mae: 1198.0330\n",
      "Epoch 758/1000\n",
      "1461/1461 [==============================] - 0s 292us/step - loss: 43.2304 - mae: 43.9103 - val_loss: 1249.5165 - val_mae: 1250.2095\n",
      "Epoch 759/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 35.7367 - mae: 36.4177 - val_loss: 1433.3975 - val_mae: 1434.0884\n",
      "Epoch 760/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 42.0789 - mae: 42.7560 - val_loss: 1388.0688 - val_mae: 1388.7618\n",
      "Epoch 761/1000\n",
      "1461/1461 [==============================] - 0s 321us/step - loss: 43.4607 - mae: 44.1381 - val_loss: 1560.7354 - val_mae: 1561.4287\n",
      "Epoch 762/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 39.3462 - mae: 40.0270 - val_loss: 1431.2152 - val_mae: 1431.9084\n",
      "Epoch 763/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 43.3839 - mae: 44.0629 - val_loss: 1526.1412 - val_mae: 1526.8342\n",
      "Epoch 764/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 48.3505 - mae: 49.0341 - val_loss: 1164.4064 - val_mae: 1165.0995\n",
      "Epoch 765/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 48.9931 - mae: 49.6762 - val_loss: 1280.3953 - val_mae: 1281.0884\n",
      "Epoch 766/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 38.5051 - mae: 39.1839 - val_loss: 1290.5479 - val_mae: 1291.2410\n",
      "Epoch 767/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 38.3079 - mae: 38.9912 - val_loss: 1332.2109 - val_mae: 1332.9039\n",
      "Epoch 768/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 41.8678 - mae: 42.5497 - val_loss: 1451.2875 - val_mae: 1451.9807\n",
      "Epoch 769/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 45.9158 - mae: 46.5959 - val_loss: 1381.6390 - val_mae: 1382.3323\n",
      "Epoch 770/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 280us/step - loss: 38.1682 - mae: 38.8488 - val_loss: 1452.1356 - val_mae: 1452.8289\n",
      "Epoch 771/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 35.5897 - mae: 36.2645 - val_loss: 1399.0100 - val_mae: 1399.7034\n",
      "Epoch 772/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 42.9380 - mae: 43.6168 - val_loss: 1318.1008 - val_mae: 1318.7938\n",
      "Epoch 773/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 43.2023 - mae: 43.8813 - val_loss: 1325.8935 - val_mae: 1326.5858\n",
      "Epoch 774/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 44.1093 - mae: 44.7899 - val_loss: 1284.6074 - val_mae: 1285.2997\n",
      "Epoch 775/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 45.4662 - mae: 46.1508 - val_loss: 1215.9768 - val_mae: 1216.6699\n",
      "Epoch 776/1000\n",
      "1461/1461 [==============================] - 0s 296us/step - loss: 39.0265 - mae: 39.7043 - val_loss: 1506.0745 - val_mae: 1506.7677\n",
      "Epoch 777/1000\n",
      "1461/1461 [==============================] - 1s 351us/step - loss: 42.7764 - mae: 43.4562 - val_loss: 1467.9630 - val_mae: 1468.6561\n",
      "Epoch 778/1000\n",
      "1461/1461 [==============================] - 0s 305us/step - loss: 47.5136 - mae: 48.1954 - val_loss: 1199.8539 - val_mae: 1200.5470\n",
      "Epoch 779/1000\n",
      "1461/1461 [==============================] - 0s 313us/step - loss: 45.1352 - mae: 45.8160 - val_loss: 1274.2442 - val_mae: 1274.9374\n",
      "Epoch 780/1000\n",
      "1461/1461 [==============================] - 0s 293us/step - loss: 51.3097 - mae: 51.9909 - val_loss: 1455.3058 - val_mae: 1455.9989\n",
      "Epoch 781/1000\n",
      "1461/1461 [==============================] - 0s 291us/step - loss: 45.6440 - mae: 46.3246 - val_loss: 1324.5887 - val_mae: 1325.2817\n",
      "Epoch 782/1000\n",
      "1461/1461 [==============================] - 0s 299us/step - loss: 44.5845 - mae: 45.2649 - val_loss: 1318.3973 - val_mae: 1319.0891\n",
      "Epoch 783/1000\n",
      "1461/1461 [==============================] - 0s 293us/step - loss: 38.7468 - mae: 39.4223 - val_loss: 1346.8096 - val_mae: 1347.5017\n",
      "Epoch 784/1000\n",
      "1461/1461 [==============================] - 0s 321us/step - loss: 41.5910 - mae: 42.2734 - val_loss: 1249.6358 - val_mae: 1250.3290\n",
      "Epoch 785/1000\n",
      "1461/1461 [==============================] - 0s 298us/step - loss: 34.6978 - mae: 35.3768 - val_loss: 1265.2344 - val_mae: 1265.9275\n",
      "Epoch 786/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 49.4709 - mae: 50.1511 - val_loss: 1322.0767 - val_mae: 1322.7698\n",
      "Epoch 787/1000\n",
      "1461/1461 [==============================] - 0s 301us/step - loss: 44.9469 - mae: 45.6231 - val_loss: 1580.1638 - val_mae: 1580.8571\n",
      "Epoch 788/1000\n",
      "1461/1461 [==============================] - 0s 291us/step - loss: 47.0192 - mae: 47.6975 - val_loss: 1432.6907 - val_mae: 1433.3838\n",
      "Epoch 789/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 48.9355 - mae: 49.6129 - val_loss: 1107.4468 - val_mae: 1108.1387\n",
      "Epoch 790/1000\n",
      "1461/1461 [==============================] - 0s 299us/step - loss: 43.9543 - mae: 44.6349 - val_loss: 1221.2066 - val_mae: 1221.8998\n",
      "Epoch 791/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 37.6803 - mae: 38.3539 - val_loss: 1461.8481 - val_mae: 1462.5415\n",
      "Epoch 792/1000\n",
      "1461/1461 [==============================] - 0s 300us/step - loss: 35.4110 - mae: 36.0859 - val_loss: 1369.5090 - val_mae: 1370.2023\n",
      "Epoch 793/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 39.7431 - mae: 40.4218 - val_loss: 1276.2436 - val_mae: 1276.9357\n",
      "Epoch 794/1000\n",
      "1461/1461 [==============================] - 0s 308us/step - loss: 35.9293 - mae: 36.6061 - val_loss: 1382.1921 - val_mae: 1382.8851\n",
      "Epoch 795/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 34.5830 - mae: 35.2599 - val_loss: 1526.3845 - val_mae: 1527.0778\n",
      "Epoch 796/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 42.6863 - mae: 43.3625 - val_loss: 1157.6148 - val_mae: 1158.3080\n",
      "Epoch 797/1000\n",
      "1461/1461 [==============================] - 0s 289us/step - loss: 52.7766 - mae: 53.4541 - val_loss: 1240.5968 - val_mae: 1241.2900\n",
      "Epoch 798/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 43.1392 - mae: 43.8209 - val_loss: 1392.3521 - val_mae: 1393.0454\n",
      "Epoch 799/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 45.3459 - mae: 46.0263 - val_loss: 1291.4862 - val_mae: 1292.1794\n",
      "Epoch 800/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 35.4679 - mae: 36.1459 - val_loss: 1492.7726 - val_mae: 1493.4658\n",
      "Epoch 801/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 49.3554 - mae: 50.0356 - val_loss: 1128.1876 - val_mae: 1128.8809\n",
      "Epoch 802/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 44.3894 - mae: 45.0684 - val_loss: 1284.7128 - val_mae: 1285.4060\n",
      "Epoch 803/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 37.0217 - mae: 37.6960 - val_loss: 1557.8235 - val_mae: 1558.5166\n",
      "Epoch 804/1000\n",
      "1461/1461 [==============================] - 0s 304us/step - loss: 36.9864 - mae: 37.6637 - val_loss: 1280.8351 - val_mae: 1281.5281\n",
      "Epoch 805/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 35.6514 - mae: 36.3281 - val_loss: 1350.8053 - val_mae: 1351.4987\n",
      "Epoch 806/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 37.7774 - mae: 38.4578 - val_loss: 1464.2258 - val_mae: 1464.9185\n",
      "Epoch 807/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 35.3249 - mae: 36.0010 - val_loss: 1228.3479 - val_mae: 1229.0403\n",
      "Epoch 808/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 43.3595 - mae: 44.0374 - val_loss: 1408.5586 - val_mae: 1409.2518\n",
      "Epoch 809/1000\n",
      "1461/1461 [==============================] - 0s 296us/step - loss: 38.8381 - mae: 39.5164 - val_loss: 1380.9306 - val_mae: 1381.6224\n",
      "Epoch 810/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 39.3987 - mae: 40.0762 - val_loss: 1334.7812 - val_mae: 1335.4742\n",
      "Epoch 811/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 42.1305 - mae: 42.8102 - val_loss: 1485.5701 - val_mae: 1486.2627\n",
      "Epoch 812/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 46.0689 - mae: 46.7488 - val_loss: 1544.5968 - val_mae: 1545.2902\n",
      "Epoch 813/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 42.9308 - mae: 43.6103 - val_loss: 1274.0815 - val_mae: 1274.7734\n",
      "Epoch 814/1000\n",
      "1461/1461 [==============================] - 0s 316us/step - loss: 34.4578 - mae: 35.1359 - val_loss: 1445.1506 - val_mae: 1445.8438\n",
      "Epoch 815/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 44.3764 - mae: 45.0541 - val_loss: 1506.0838 - val_mae: 1506.7770\n",
      "Epoch 816/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 44.9209 - mae: 45.5996 - val_loss: 1137.3048 - val_mae: 1137.9979\n",
      "Epoch 817/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 40.7499 - mae: 41.4250 - val_loss: 1383.9847 - val_mae: 1384.6774\n",
      "Epoch 818/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 44.7867 - mae: 45.4630 - val_loss: 1383.8981 - val_mae: 1384.5913\n",
      "Epoch 819/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 35.0276 - mae: 35.7047 - val_loss: 1535.2183 - val_mae: 1535.9117\n",
      "Epoch 820/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 42.8851 - mae: 43.5612 - val_loss: 1428.2422 - val_mae: 1428.9353\n",
      "Epoch 821/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 40.0710 - mae: 40.7453 - val_loss: 1412.7546 - val_mae: 1413.4478\n",
      "Epoch 822/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 43.5898 - mae: 44.2703 - val_loss: 1548.2483 - val_mae: 1548.9415\n",
      "Epoch 823/1000\n",
      "1461/1461 [==============================] - 0s 299us/step - loss: 42.4235 - mae: 43.1050 - val_loss: 1418.9956 - val_mae: 1419.6887\n",
      "Epoch 824/1000\n",
      "1461/1461 [==============================] - 0s 283us/step - loss: 47.9808 - mae: 48.6598 - val_loss: 939.6879 - val_mae: 940.3810\n",
      "Epoch 825/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 268us/step - loss: 51.2658 - mae: 51.9484 - val_loss: 1402.5957 - val_mae: 1403.2889\n",
      "Epoch 826/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 42.5473 - mae: 43.2225 - val_loss: 1444.6925 - val_mae: 1445.3857\n",
      "Epoch 827/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 41.4212 - mae: 42.1001 - val_loss: 1059.4295 - val_mae: 1060.1227\n",
      "Epoch 828/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 53.6947 - mae: 54.3761 - val_loss: 1341.1427 - val_mae: 1341.8359\n",
      "Epoch 829/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 40.0758 - mae: 40.7550 - val_loss: 1174.7327 - val_mae: 1175.4257\n",
      "Epoch 830/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 51.9253 - mae: 52.6090 - val_loss: 1554.0536 - val_mae: 1554.7462\n",
      "Epoch 831/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 37.8079 - mae: 38.4837 - val_loss: 1231.4851 - val_mae: 1232.1782\n",
      "Epoch 832/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 52.2179 - mae: 52.8969 - val_loss: 1077.0756 - val_mae: 1077.7681\n",
      "Epoch 833/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 43.3442 - mae: 44.0230 - val_loss: 1352.3591 - val_mae: 1353.0522\n",
      "Epoch 834/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 35.7633 - mae: 36.4373 - val_loss: 1200.0127 - val_mae: 1200.7059\n",
      "Epoch 835/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 36.1817 - mae: 36.8572 - val_loss: 1391.1680 - val_mae: 1391.8607\n",
      "Epoch 836/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 38.8796 - mae: 39.5577 - val_loss: 1230.5845 - val_mae: 1231.2777\n",
      "Epoch 837/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 37.8626 - mae: 38.5447 - val_loss: 1240.7940 - val_mae: 1241.4869\n",
      "Epoch 838/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 45.7322 - mae: 46.4076 - val_loss: 1349.2205 - val_mae: 1349.9136\n",
      "Epoch 839/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 38.3558 - mae: 39.0372 - val_loss: 1295.0712 - val_mae: 1295.7644\n",
      "Epoch 840/1000\n",
      "1461/1461 [==============================] - 0s 265us/step - loss: 36.4961 - mae: 37.1744 - val_loss: 1270.6290 - val_mae: 1271.3223\n",
      "Epoch 841/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 33.3795 - mae: 34.0555 - val_loss: 1161.0167 - val_mae: 1161.7098\n",
      "Epoch 842/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 34.3630 - mae: 35.0370 - val_loss: 1308.5819 - val_mae: 1309.2750\n",
      "Epoch 843/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 31.2052 - mae: 31.8805 - val_loss: 1301.3628 - val_mae: 1302.0559\n",
      "Epoch 844/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 37.2805 - mae: 37.9563 - val_loss: 1289.6835 - val_mae: 1290.3756\n",
      "Epoch 845/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 44.8147 - mae: 45.4946 - val_loss: 1468.0589 - val_mae: 1468.7520\n",
      "Epoch 846/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 46.4437 - mae: 47.1190 - val_loss: 1287.0024 - val_mae: 1287.6957\n",
      "Epoch 847/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 37.2000 - mae: 37.8751 - val_loss: 1308.3301 - val_mae: 1309.0219\n",
      "Epoch 848/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 49.9531 - mae: 50.6323 - val_loss: 1305.5175 - val_mae: 1306.2106\n",
      "Epoch 849/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 40.6233 - mae: 41.2989 - val_loss: 1181.0839 - val_mae: 1181.7771\n",
      "Epoch 850/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 51.4376 - mae: 52.1241 - val_loss: 1344.0642 - val_mae: 1344.7571\n",
      "Epoch 851/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 35.0825 - mae: 35.7600 - val_loss: 1497.3428 - val_mae: 1498.0360\n",
      "Epoch 852/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 43.4374 - mae: 44.1201 - val_loss: 1238.6901 - val_mae: 1239.3820\n",
      "Epoch 853/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 46.5900 - mae: 47.2671 - val_loss: 1254.5102 - val_mae: 1255.2031\n",
      "Epoch 854/1000\n",
      "1461/1461 [==============================] - 0s 300us/step - loss: 50.0362 - mae: 50.7196 - val_loss: 1155.8976 - val_mae: 1156.5905\n",
      "Epoch 855/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 51.2754 - mae: 51.9551 - val_loss: 1535.2362 - val_mae: 1535.9293\n",
      "Epoch 856/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 38.1279 - mae: 38.8071 - val_loss: 1210.3763 - val_mae: 1211.0687\n",
      "Epoch 857/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 49.9414 - mae: 50.6200 - val_loss: 1321.7425 - val_mae: 1322.4352\n",
      "Epoch 858/1000\n",
      "1461/1461 [==============================] - 0s 267us/step - loss: 36.2224 - mae: 36.9007 - val_loss: 1388.0521 - val_mae: 1388.7454\n",
      "Epoch 859/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 40.5069 - mae: 41.1832 - val_loss: 1081.6721 - val_mae: 1082.3651\n",
      "Epoch 860/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 39.3701 - mae: 40.0481 - val_loss: 1369.7833 - val_mae: 1370.4757\n",
      "Epoch 861/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 41.8954 - mae: 42.5672 - val_loss: 1309.6838 - val_mae: 1310.3767\n",
      "Epoch 862/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 36.8870 - mae: 37.5654 - val_loss: 1289.4406 - val_mae: 1290.1337\n",
      "Epoch 863/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 34.5193 - mae: 35.1916 - val_loss: 1327.5544 - val_mae: 1328.2477\n",
      "Epoch 864/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 42.4309 - mae: 43.1089 - val_loss: 1459.1901 - val_mae: 1459.8834\n",
      "Epoch 865/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 42.6793 - mae: 43.3630 - val_loss: 1435.0462 - val_mae: 1435.7393\n",
      "Epoch 866/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 47.0494 - mae: 47.7306 - val_loss: 1338.8050 - val_mae: 1339.4980\n",
      "Epoch 867/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 45.5220 - mae: 46.1972 - val_loss: 1158.3271 - val_mae: 1159.0194\n",
      "Epoch 868/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 41.5307 - mae: 42.2058 - val_loss: 1384.1990 - val_mae: 1384.8922\n",
      "Epoch 869/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 43.7928 - mae: 44.4745 - val_loss: 1291.2490 - val_mae: 1291.9421\n",
      "Epoch 870/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 43.2145 - mae: 43.8956 - val_loss: 1653.6843 - val_mae: 1654.3776\n",
      "Epoch 871/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 42.5761 - mae: 43.2537 - val_loss: 1429.6527 - val_mae: 1430.3458\n",
      "Epoch 872/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 42.2737 - mae: 42.9532 - val_loss: 1292.2620 - val_mae: 1292.9552\n",
      "Epoch 873/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 37.2712 - mae: 37.9462 - val_loss: 1283.5003 - val_mae: 1284.1935\n",
      "Epoch 874/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 34.7875 - mae: 35.4621 - val_loss: 1182.9540 - val_mae: 1183.6472\n",
      "Epoch 875/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 42.0422 - mae: 42.7205 - val_loss: 1289.5456 - val_mae: 1290.2386\n",
      "Epoch 876/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 39.4915 - mae: 40.1670 - val_loss: 1501.6206 - val_mae: 1502.3136\n",
      "Epoch 877/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 42.5249 - mae: 43.2040 - val_loss: 1279.3955 - val_mae: 1280.0879\n",
      "Epoch 878/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 56.7866 - mae: 57.4669 - val_loss: 1478.5027 - val_mae: 1479.1959\n",
      "Epoch 879/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 36.1480 - mae: 36.8233 - val_loss: 1219.6856 - val_mae: 1220.3787\n",
      "Epoch 880/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 302us/step - loss: 40.0376 - mae: 40.7168 - val_loss: 1733.7028 - val_mae: 1734.3961\n",
      "Epoch 881/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 38.7772 - mae: 39.4531 - val_loss: 1578.8400 - val_mae: 1579.5331\n",
      "Epoch 882/1000\n",
      "1461/1461 [==============================] - 0s 301us/step - loss: 33.8257 - mae: 34.5053 - val_loss: 1379.5220 - val_mae: 1380.2152\n",
      "Epoch 883/1000\n",
      "1461/1461 [==============================] - 0s 312us/step - loss: 41.8069 - mae: 42.4850 - val_loss: 1157.3325 - val_mae: 1158.0253\n",
      "Epoch 884/1000\n",
      "1461/1461 [==============================] - 0s 306us/step - loss: 42.6225 - mae: 43.3022 - val_loss: 1418.0006 - val_mae: 1418.6937\n",
      "Epoch 885/1000\n",
      "1461/1461 [==============================] - 0s 294us/step - loss: 37.3401 - mae: 38.0201 - val_loss: 1413.6460 - val_mae: 1414.3392\n",
      "Epoch 886/1000\n",
      "1461/1461 [==============================] - 0s 302us/step - loss: 51.2495 - mae: 51.9293 - val_loss: 1327.8432 - val_mae: 1328.5353\n",
      "Epoch 887/1000\n",
      "1461/1461 [==============================] - 0s 292us/step - loss: 39.8478 - mae: 40.5246 - val_loss: 1407.9759 - val_mae: 1408.6688\n",
      "Epoch 888/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 39.1101 - mae: 39.7888 - val_loss: 1151.8693 - val_mae: 1152.5624\n",
      "Epoch 889/1000\n",
      "1461/1461 [==============================] - 0s 302us/step - loss: 43.4116 - mae: 44.0894 - val_loss: 1678.8741 - val_mae: 1679.5675\n",
      "Epoch 890/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 39.0104 - mae: 39.6895 - val_loss: 1329.4310 - val_mae: 1330.1233\n",
      "Epoch 891/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 44.1659 - mae: 44.8426 - val_loss: 1215.8171 - val_mae: 1216.5103\n",
      "Epoch 892/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 37.7399 - mae: 38.4193 - val_loss: 1168.0867 - val_mae: 1168.7799\n",
      "Epoch 893/1000\n",
      "1461/1461 [==============================] - 0s 300us/step - loss: 37.4434 - mae: 38.1175 - val_loss: 1368.2494 - val_mae: 1368.9427\n",
      "Epoch 894/1000\n",
      "1461/1461 [==============================] - 0s 277us/step - loss: 43.1853 - mae: 43.8664 - val_loss: 1290.2052 - val_mae: 1290.8982\n",
      "Epoch 895/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 31.0348 - mae: 31.7126 - val_loss: 1502.6222 - val_mae: 1503.3154\n",
      "Epoch 896/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 37.7879 - mae: 38.4617 - val_loss: 1449.6698 - val_mae: 1450.3627\n",
      "Epoch 897/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 47.4724 - mae: 48.1508 - val_loss: 1323.7983 - val_mae: 1324.4916\n",
      "Epoch 898/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 49.0029 - mae: 49.6815 - val_loss: 1111.1059 - val_mae: 1111.7981\n",
      "Epoch 899/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 46.7524 - mae: 47.4322 - val_loss: 1308.8085 - val_mae: 1309.5013\n",
      "Epoch 900/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 39.3814 - mae: 40.0580 - val_loss: 1210.7245 - val_mae: 1211.4167\n",
      "Epoch 901/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 49.9969 - mae: 50.6786 - val_loss: 1365.1112 - val_mae: 1365.8042\n",
      "Epoch 902/1000\n",
      "1461/1461 [==============================] - 0s 288us/step - loss: 43.7183 - mae: 44.3985 - val_loss: 1305.2490 - val_mae: 1305.9413\n",
      "Epoch 903/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 49.7357 - mae: 50.4177 - val_loss: 1135.2554 - val_mae: 1135.9486\n",
      "Epoch 904/1000\n",
      "1461/1461 [==============================] - 0s 290us/step - loss: 33.8863 - mae: 34.5649 - val_loss: 1398.5154 - val_mae: 1399.2085\n",
      "Epoch 905/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 43.5053 - mae: 44.1827 - val_loss: 1616.6143 - val_mae: 1617.3074\n",
      "Epoch 906/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 40.8090 - mae: 41.4867 - val_loss: 1382.1999 - val_mae: 1382.8931\n",
      "Epoch 907/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 47.7908 - mae: 48.4634 - val_loss: 1556.5470 - val_mae: 1557.2402\n",
      "Epoch 908/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 51.4314 - mae: 52.1103 - val_loss: 1299.5732 - val_mae: 1300.2664\n",
      "Epoch 909/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 35.8226 - mae: 36.4956 - val_loss: 1392.4259 - val_mae: 1393.1190\n",
      "Epoch 910/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 36.8554 - mae: 37.5275 - val_loss: 1364.1253 - val_mae: 1364.8179\n",
      "Epoch 911/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 34.7592 - mae: 35.4325 - val_loss: 1388.8923 - val_mae: 1389.5842\n",
      "Epoch 912/1000\n",
      "1461/1461 [==============================] - 0s 287us/step - loss: 34.3775 - mae: 35.0511 - val_loss: 1410.2634 - val_mae: 1410.9561\n",
      "Epoch 913/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 38.7685 - mae: 39.4468 - val_loss: 1432.1929 - val_mae: 1432.8860\n",
      "Epoch 914/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 39.6288 - mae: 40.3081 - val_loss: 1206.5762 - val_mae: 1207.2690\n",
      "Epoch 915/1000\n",
      "1461/1461 [==============================] - 0s 291us/step - loss: 35.2930 - mae: 35.9701 - val_loss: 1444.2022 - val_mae: 1444.8953\n",
      "Epoch 916/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 36.8939 - mae: 37.5692 - val_loss: 1230.4692 - val_mae: 1231.1624\n",
      "Epoch 917/1000\n",
      "1461/1461 [==============================] - 0s 288us/step - loss: 42.2659 - mae: 42.9440 - val_loss: 1149.9116 - val_mae: 1150.6045\n",
      "Epoch 918/1000\n",
      "1461/1461 [==============================] - 0s 307us/step - loss: 43.2043 - mae: 43.8808 - val_loss: 1352.6481 - val_mae: 1353.3412\n",
      "Epoch 919/1000\n",
      "1461/1461 [==============================] - 0s 310us/step - loss: 38.7735 - mae: 39.4544 - val_loss: 1234.6753 - val_mae: 1235.3685\n",
      "Epoch 920/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 36.7359 - mae: 37.4124 - val_loss: 1196.1558 - val_mae: 1196.8486\n",
      "Epoch 921/1000\n",
      "1461/1461 [==============================] - 0s 312us/step - loss: 34.4270 - mae: 35.1029 - val_loss: 1390.6164 - val_mae: 1391.3076\n",
      "Epoch 922/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 55.7840 - mae: 56.4625 - val_loss: 1272.5160 - val_mae: 1273.2083\n",
      "Epoch 923/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 41.8170 - mae: 42.4963 - val_loss: 1343.0482 - val_mae: 1343.7415\n",
      "Epoch 924/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 39.8219 - mae: 40.4973 - val_loss: 1498.5393 - val_mae: 1499.2319\n",
      "Epoch 925/1000\n",
      "1461/1461 [==============================] - 0s 287us/step - loss: 31.2268 - mae: 31.9019 - val_loss: 1464.4281 - val_mae: 1465.1213\n",
      "Epoch 926/1000\n",
      "1461/1461 [==============================] - 0s 297us/step - loss: 35.8755 - mae: 36.5497 - val_loss: 1207.9258 - val_mae: 1208.6189\n",
      "Epoch 927/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 47.4496 - mae: 48.1295 - val_loss: 1279.9016 - val_mae: 1280.5938\n",
      "Epoch 928/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 38.2970 - mae: 38.9747 - val_loss: 1354.1434 - val_mae: 1354.8364\n",
      "Epoch 929/1000\n",
      "1461/1461 [==============================] - 0s 278us/step - loss: 35.5143 - mae: 36.1885 - val_loss: 1475.8727 - val_mae: 1476.5659\n",
      "Epoch 930/1000\n",
      "1461/1461 [==============================] - 0s 273us/step - loss: 34.3161 - mae: 34.9929 - val_loss: 1487.7547 - val_mae: 1488.4478\n",
      "Epoch 931/1000\n",
      "1461/1461 [==============================] - 0s 279us/step - loss: 35.2724 - mae: 35.9505 - val_loss: 1238.7594 - val_mae: 1239.4517\n",
      "Epoch 932/1000\n",
      "1461/1461 [==============================] - 0s 298us/step - loss: 33.1974 - mae: 33.8726 - val_loss: 1194.7610 - val_mae: 1195.4542\n",
      "Epoch 933/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 41.0286 - mae: 41.7068 - val_loss: 1231.9759 - val_mae: 1232.6691\n",
      "Epoch 934/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 32.6397 - mae: 33.3149 - val_loss: 1328.4633 - val_mae: 1329.1564\n",
      "Epoch 935/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 269us/step - loss: 41.5086 - mae: 42.1892 - val_loss: 1370.0331 - val_mae: 1370.7260\n",
      "Epoch 936/1000\n",
      "1461/1461 [==============================] - 0s 269us/step - loss: 32.0226 - mae: 32.6992 - val_loss: 1337.6936 - val_mae: 1338.3867\n",
      "Epoch 937/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 40.7207 - mae: 41.3981 - val_loss: 1374.6977 - val_mae: 1375.3904\n",
      "Epoch 938/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 36.2858 - mae: 36.9642 - val_loss: 1071.5209 - val_mae: 1072.2140\n",
      "Epoch 939/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 40.2708 - mae: 40.9471 - val_loss: 1214.2865 - val_mae: 1214.9797\n",
      "Epoch 940/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 49.1143 - mae: 49.7996 - val_loss: 1443.0293 - val_mae: 1443.7224\n",
      "Epoch 941/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 45.6330 - mae: 46.3120 - val_loss: 1185.7036 - val_mae: 1186.3967\n",
      "Epoch 942/1000\n",
      "1461/1461 [==============================] - 0s 283us/step - loss: 36.1668 - mae: 36.8472 - val_loss: 1393.1352 - val_mae: 1393.8284\n",
      "Epoch 943/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 46.8419 - mae: 47.5215 - val_loss: 1187.6502 - val_mae: 1188.3434\n",
      "Epoch 944/1000\n",
      "1461/1461 [==============================] - 0s 276us/step - loss: 36.8803 - mae: 37.5551 - val_loss: 1436.3411 - val_mae: 1437.0343\n",
      "Epoch 945/1000\n",
      "1461/1461 [==============================] - 0s 275us/step - loss: 30.0356 - mae: 30.7116 - val_loss: 1355.0077 - val_mae: 1355.7009\n",
      "Epoch 946/1000\n",
      "1461/1461 [==============================] - 0s 282us/step - loss: 37.5064 - mae: 38.1870 - val_loss: 1138.5147 - val_mae: 1139.2078\n",
      "Epoch 947/1000\n",
      "1461/1461 [==============================] - 0s 281us/step - loss: 41.8363 - mae: 42.5158 - val_loss: 1213.6320 - val_mae: 1214.3252\n",
      "Epoch 948/1000\n",
      "1461/1461 [==============================] - 0s 271us/step - loss: 41.4570 - mae: 42.1355 - val_loss: 1417.1477 - val_mae: 1417.8409\n",
      "Epoch 949/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 38.8779 - mae: 39.5555 - val_loss: 1528.7671 - val_mae: 1529.4596\n",
      "Epoch 950/1000\n",
      "1461/1461 [==============================] - 0s 268us/step - loss: 40.3829 - mae: 41.0584 - val_loss: 1339.1973 - val_mae: 1339.8905\n",
      "Epoch 951/1000\n",
      "1461/1461 [==============================] - 0s 266us/step - loss: 33.6633 - mae: 34.3378 - val_loss: 1335.0733 - val_mae: 1335.7662\n",
      "Epoch 952/1000\n",
      "1461/1461 [==============================] - 0s 272us/step - loss: 34.6799 - mae: 35.3564 - val_loss: 1492.4472 - val_mae: 1493.1403\n",
      "Epoch 953/1000\n",
      "1461/1461 [==============================] - 0s 300us/step - loss: 31.6429 - mae: 32.3168 - val_loss: 1228.6218 - val_mae: 1229.3146\n",
      "Epoch 954/1000\n",
      "1461/1461 [==============================] - 0s 292us/step - loss: 35.0280 - mae: 35.7016 - val_loss: 1412.2674 - val_mae: 1412.9597\n",
      "Epoch 955/1000\n",
      "1461/1461 [==============================] - 0s 280us/step - loss: 44.9928 - mae: 45.6696 - val_loss: 1080.2527 - val_mae: 1080.9457\n",
      "Epoch 956/1000\n",
      "1461/1461 [==============================] - 0s 295us/step - loss: 46.8026 - mae: 47.4818 - val_loss: 1226.0914 - val_mae: 1226.7844\n",
      "Epoch 957/1000\n",
      "1461/1461 [==============================] - 0s 270us/step - loss: 37.5601 - mae: 38.2355 - val_loss: 1450.5107 - val_mae: 1451.2040\n",
      "Epoch 958/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 47.0863 - mae: 47.7713 - val_loss: 935.4993 - val_mae: 936.1903\n",
      "Epoch 959/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 47.6895 - mae: 48.3672 - val_loss: 1418.8211 - val_mae: 1419.5144\n",
      "Epoch 960/1000\n",
      "1461/1461 [==============================] - 0s 312us/step - loss: 33.6967 - mae: 34.3727 - val_loss: 1214.8954 - val_mae: 1215.5886\n",
      "Epoch 961/1000\n",
      "1461/1461 [==============================] - 0s 274us/step - loss: 38.5024 - mae: 39.1797 - val_loss: 1355.0705 - val_mae: 1355.7638\n",
      "Epoch 962/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 41.1866 - mae: 41.8689 - val_loss: 1198.9434 - val_mae: 1199.6362\n",
      "Epoch 963/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 41.4351 - mae: 42.1131 - val_loss: 1441.2920 - val_mae: 1441.9851\n",
      "Epoch 964/1000\n",
      "1461/1461 [==============================] - 0s 283us/step - loss: 39.1124 - mae: 39.7912 - val_loss: 1369.7453 - val_mae: 1370.4384\n",
      "Epoch 965/1000\n",
      "1461/1461 [==============================] - 0s 287us/step - loss: 31.4645 - mae: 32.1434 - val_loss: 1429.0115 - val_mae: 1429.7047\n",
      "Epoch 966/1000\n",
      "1461/1461 [==============================] - 0s 296us/step - loss: 32.2044 - mae: 32.8747 - val_loss: 1374.3979 - val_mae: 1375.0909\n",
      "Epoch 967/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 35.6427 - mae: 36.3140 - val_loss: 1338.5297 - val_mae: 1339.2227\n",
      "Epoch 968/1000\n",
      "1461/1461 [==============================] - 0s 295us/step - loss: 32.6565 - mae: 33.3300 - val_loss: 1300.3859 - val_mae: 1301.0781\n",
      "Epoch 969/1000\n",
      "1461/1461 [==============================] - 0s 290us/step - loss: 31.2587 - mae: 31.9304 - val_loss: 1205.0814 - val_mae: 1205.7740\n",
      "Epoch 970/1000\n",
      "1461/1461 [==============================] - 0s 299us/step - loss: 43.4625 - mae: 44.1399 - val_loss: 1326.5841 - val_mae: 1327.2771\n",
      "Epoch 971/1000\n",
      "1461/1461 [==============================] - 0s 312us/step - loss: 56.9588 - mae: 57.6333 - val_loss: 1017.2854 - val_mae: 1017.9786\n",
      "Epoch 972/1000\n",
      "1461/1461 [==============================] - 0s 283us/step - loss: 56.3558 - mae: 57.0370 - val_loss: 1317.9250 - val_mae: 1318.6173\n",
      "Epoch 973/1000\n",
      "1461/1461 [==============================] - 0s 284us/step - loss: 38.4249 - mae: 39.1012 - val_loss: 1361.5914 - val_mae: 1362.2844\n",
      "Epoch 974/1000\n",
      "1461/1461 [==============================] - 0s 298us/step - loss: 30.5468 - mae: 31.2240 - val_loss: 1250.4608 - val_mae: 1251.1541\n",
      "Epoch 975/1000\n",
      "1461/1461 [==============================] - 0s 287us/step - loss: 45.2122 - mae: 45.8934 - val_loss: 1525.0858 - val_mae: 1525.7791\n",
      "Epoch 976/1000\n",
      "1461/1461 [==============================] - 0s 289us/step - loss: 38.7389 - mae: 39.4140 - val_loss: 1546.0036 - val_mae: 1546.6967\n",
      "Epoch 977/1000\n",
      "1461/1461 [==============================] - 0s 287us/step - loss: 39.5972 - mae: 40.2672 - val_loss: 1356.4090 - val_mae: 1357.1019\n",
      "Epoch 978/1000\n",
      "1461/1461 [==============================] - 0s 295us/step - loss: 37.0690 - mae: 37.7506 - val_loss: 1570.3082 - val_mae: 1571.0015\n",
      "Epoch 979/1000\n",
      "1461/1461 [==============================] - 0s 289us/step - loss: 54.0400 - mae: 54.7194 - val_loss: 1691.2763 - val_mae: 1691.9695\n",
      "Epoch 980/1000\n",
      "1461/1461 [==============================] - 0s 296us/step - loss: 51.5214 - mae: 52.2009 - val_loss: 1253.3158 - val_mae: 1254.0088\n",
      "Epoch 981/1000\n",
      "1461/1461 [==============================] - 0s 289us/step - loss: 29.5918 - mae: 30.2655 - val_loss: 1277.3714 - val_mae: 1278.0642\n",
      "Epoch 982/1000\n",
      "1461/1461 [==============================] - 0s 293us/step - loss: 39.9437 - mae: 40.6219 - val_loss: 1119.7338 - val_mae: 1120.4250\n",
      "Epoch 983/1000\n",
      "1461/1461 [==============================] - 0s 288us/step - loss: 38.9653 - mae: 39.6378 - val_loss: 1290.2932 - val_mae: 1290.9863\n",
      "Epoch 984/1000\n",
      "1461/1461 [==============================] - 0s 288us/step - loss: 34.3380 - mae: 35.0153 - val_loss: 1247.6564 - val_mae: 1248.3489\n",
      "Epoch 985/1000\n",
      "1461/1461 [==============================] - 0s 291us/step - loss: 36.4902 - mae: 37.1667 - val_loss: 1288.9777 - val_mae: 1289.6708\n",
      "Epoch 986/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 46.5409 - mae: 47.2179 - val_loss: 1133.3348 - val_mae: 1134.0281\n",
      "Epoch 987/1000\n",
      "1461/1461 [==============================] - 0s 291us/step - loss: 47.5291 - mae: 48.2113 - val_loss: 1332.6998 - val_mae: 1333.3929\n",
      "Epoch 988/1000\n",
      "1461/1461 [==============================] - 0s 286us/step - loss: 41.0518 - mae: 41.7265 - val_loss: 1163.2277 - val_mae: 1163.9209\n",
      "Epoch 989/1000\n",
      "1461/1461 [==============================] - 0s 285us/step - loss: 43.7604 - mae: 44.4371 - val_loss: 1278.3669 - val_mae: 1279.0598\n",
      "Epoch 990/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 287us/step - loss: 37.8379 - mae: 38.5148 - val_loss: 1351.9616 - val_mae: 1352.6549\n",
      "Epoch 991/1000\n",
      "1461/1461 [==============================] - 0s 309us/step - loss: 34.2751 - mae: 34.9499 - val_loss: 1449.5983 - val_mae: 1450.2916\n",
      "Epoch 992/1000\n",
      "1461/1461 [==============================] - 0s 318us/step - loss: 36.8636 - mae: 37.5439 - val_loss: 1296.3126 - val_mae: 1297.0059\n",
      "Epoch 993/1000\n",
      "1461/1461 [==============================] - 0s 316us/step - loss: 39.1588 - mae: 39.8350 - val_loss: 1542.4102 - val_mae: 1543.1034\n",
      "Epoch 994/1000\n",
      "1461/1461 [==============================] - 0s 309us/step - loss: 32.8260 - mae: 33.5020 - val_loss: 1253.7422 - val_mae: 1254.4354\n",
      "Epoch 995/1000\n",
      "1461/1461 [==============================] - 0s 321us/step - loss: 44.6902 - mae: 45.3684 - val_loss: 1322.0887 - val_mae: 1322.7817\n",
      "Epoch 996/1000\n",
      "1461/1461 [==============================] - 0s 312us/step - loss: 45.5949 - mae: 46.2767 - val_loss: 1523.4362 - val_mae: 1524.1285\n",
      "Epoch 997/1000\n",
      "1461/1461 [==============================] - 0s 323us/step - loss: 46.1782 - mae: 46.8600 - val_loss: 1280.6467 - val_mae: 1281.3384\n",
      "Epoch 998/1000\n",
      "1461/1461 [==============================] - 0s 309us/step - loss: 39.5554 - mae: 40.2342 - val_loss: 1088.2781 - val_mae: 1088.9713\n",
      "Epoch 999/1000\n",
      "1461/1461 [==============================] - 0s 314us/step - loss: 35.1437 - mae: 35.8198 - val_loss: 1463.6896 - val_mae: 1464.3827\n",
      "Epoch 1000/1000\n",
      "1461/1461 [==============================] - 0s 333us/step - loss: 40.9830 - mae: 41.6638 - val_loss: 1414.0884 - val_mae: 1414.7804\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fd21b1aee50>"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(X_train,Y_train,validation_data=(X_val, Y_val))\n",
    "#Audio(sound_file,autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "6293e07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 150us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9997976525406873"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for check\n",
    "y_train_pred=regressor.predict(X_train)\n",
    "r2_score(Y_train, y_train_pred) #training score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "77b0ea8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546/546 [==============================] - 0s 110us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-06-01</th>\n",
       "      <td>36599.0</td>\n",
       "      <td>39957.894531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-02</th>\n",
       "      <td>37295.0</td>\n",
       "      <td>40503.027344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-03</th>\n",
       "      <td>38539.0</td>\n",
       "      <td>41449.289062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-04</th>\n",
       "      <td>37117.0</td>\n",
       "      <td>40811.390625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-05</th>\n",
       "      <td>36607.0</td>\n",
       "      <td>40478.691406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-24</th>\n",
       "      <td>16604.0</td>\n",
       "      <td>20915.765625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-25</th>\n",
       "      <td>16495.0</td>\n",
       "      <td>20869.060547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-26</th>\n",
       "      <td>16561.0</td>\n",
       "      <td>21003.966797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-27</th>\n",
       "      <td>16542.0</td>\n",
       "      <td>20951.753906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28</th>\n",
       "      <td>16214.0</td>\n",
       "      <td>20697.740234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>546 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             y_test        y_pred\n",
       "Date                             \n",
       "2021-06-01  36599.0  39957.894531\n",
       "2021-06-02  37295.0  40503.027344\n",
       "2021-06-03  38539.0  41449.289062\n",
       "2021-06-04  37117.0  40811.390625\n",
       "2021-06-05  36607.0  40478.691406\n",
       "...             ...           ...\n",
       "2022-11-24  16604.0  20915.765625\n",
       "2022-11-25  16495.0  20869.060547\n",
       "2022-11-26  16561.0  21003.966797\n",
       "2022-11-27  16542.0  20951.753906\n",
       "2022-11-28  16214.0  20697.740234\n",
       "\n",
       "[546 rows x 2 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=regressor.predict(X_test)\n",
    "pre_df = pd.DataFrame(zip(Y_test,y_pred),columns=['y_test','y_pred'])\n",
    "pre_df.index = Y_test.index\n",
    "pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "5146122a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8989174061600431"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2=r2_score(Y_test,y_pred) #testing score/ r^2\n",
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "85a71f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4185.654414668638"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse=np.sqrt(mean_squared_error(Y_test,y_pred)) #rmse\n",
    "rmse#太特么大了，感觉数据集划分有问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "658d1f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df['pred_returns'] = pre_df['y_pred'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "61f95861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>pred_returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-06-01</th>\n",
       "      <td>36599.0</td>\n",
       "      <td>39957.894531</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-02</th>\n",
       "      <td>37295.0</td>\n",
       "      <td>40503.027344</td>\n",
       "      <td>0.013643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-03</th>\n",
       "      <td>38539.0</td>\n",
       "      <td>41449.289062</td>\n",
       "      <td>0.023363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-04</th>\n",
       "      <td>37117.0</td>\n",
       "      <td>40811.390625</td>\n",
       "      <td>-0.015390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-05</th>\n",
       "      <td>36607.0</td>\n",
       "      <td>40478.691406</td>\n",
       "      <td>-0.008152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-24</th>\n",
       "      <td>16604.0</td>\n",
       "      <td>20915.765625</td>\n",
       "      <td>0.004465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-25</th>\n",
       "      <td>16495.0</td>\n",
       "      <td>20869.060547</td>\n",
       "      <td>-0.002233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-26</th>\n",
       "      <td>16561.0</td>\n",
       "      <td>21003.966797</td>\n",
       "      <td>0.006464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-27</th>\n",
       "      <td>16542.0</td>\n",
       "      <td>20951.753906</td>\n",
       "      <td>-0.002486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28</th>\n",
       "      <td>16214.0</td>\n",
       "      <td>20697.740234</td>\n",
       "      <td>-0.012124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>546 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             y_test        y_pred  pred_returns\n",
       "Date                                           \n",
       "2021-06-01  36599.0  39957.894531           NaN\n",
       "2021-06-02  37295.0  40503.027344      0.013643\n",
       "2021-06-03  38539.0  41449.289062      0.023363\n",
       "2021-06-04  37117.0  40811.390625     -0.015390\n",
       "2021-06-05  36607.0  40478.691406     -0.008152\n",
       "...             ...           ...           ...\n",
       "2022-11-24  16604.0  20915.765625      0.004465\n",
       "2022-11-25  16495.0  20869.060547     -0.002233\n",
       "2022-11-26  16561.0  21003.966797      0.006464\n",
       "2022-11-27  16542.0  20951.753906     -0.002486\n",
       "2022-11-28  16214.0  20697.740234     -0.012124\n",
       "\n",
       "[546 rows x 3 columns]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c17d5af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
